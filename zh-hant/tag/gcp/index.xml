<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gcp | Che-Chia Chang</title>
    <link>https://chechia.net/zh-hant/tag/gcp/</link>
      <atom:link href="https://chechia.net/zh-hant/tag/gcp/index.xml" rel="self" type="application/rss+xml" />
    <description>gcp</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>zh-Hant</language><lastBuildDate>Sat, 26 Sep 2020 17:24:20 +0800</lastBuildDate>
    <image>
      <url>https://chechia.net/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>gcp</title>
      <link>https://chechia.net/zh-hant/tag/gcp/</link>
    </image>
    
    <item>
      <title>Gcp Preemptible Instance Kubernetes</title>
      <link>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/</link>
      <pubDate>Sat, 26 Sep 2020 17:24:20 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/</guid>
      <description>&lt;h3 id=&#34;先占虛擬機與-kubernetes&#34;&gt;先占虛擬機與 Kubernetes&lt;/h3&gt;
&lt;p&gt;在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;資料中心會 (可預期或不可預期地) 終止先占虛擬機&lt;/li&gt;
&lt;li&gt;先占虛擬機不能自動重啟，而是會被資料中心終止後回收&lt;/li&gt;
&lt;li&gt;GCP 不保證有足夠的先占虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;節點的終止會造成額外的維運成本，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機&lt;/li&gt;
&lt;li&gt;管理多個應用複本，節點終止時，維護整體應用的可用性&lt;/li&gt;
&lt;li&gt;將移除節點上的應用，重新排程到其他可用節點&lt;/li&gt;
&lt;li&gt;動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints)
&lt;ul&gt;
&lt;li&gt;意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效&lt;/li&gt;
&lt;li&gt;配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;補足被移除的 15 個節點&lt;/li&gt;
&lt;li&gt;計算被移除的應用，補足移除的應用數量&lt;/li&gt;
&lt;li&gt;移除失效的應用端點，補上新的應用端點&lt;/li&gt;
&lt;li&gt;持續監控應用狀態&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;沒有自動化管理工具，看了心已累 (貓爪掩面)&lt;/p&gt;
&lt;p&gt;我們使用 Kubernetes 協助維運自動化，在 GCP 上我們使用 GKE，除了上述提到的容器應用管理自動化外，GKE 還額外整合先占虛擬機的使用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;啟用先占虛擬機的節點池 (node-pool)，設定節點池的自動拓展，自動補足先占節點的數量&lt;/li&gt;
&lt;li&gt;GKE 自動維護先占虛擬機的 labels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;關於 GKE 的先占虛擬機的完整細節，請見&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 官方文件&lt;/a&gt;。這份文件底下也提供了 GCP 官方建議的先占虛擬機最佳實踐&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;架構設計需要假設，部分或是全部的先占虛擬機都不可用的情形&lt;/li&gt;
&lt;li&gt;Pod 不一定有時間能優雅終止 (graceful shutdown)&lt;/li&gt;
&lt;li&gt;同時使用隨選虛擬機與先占虛擬機，以維持先占虛擬機不可用時，服務依然可用&lt;/li&gt;
&lt;li&gt;注意節點替換時的 IP 變更&lt;/li&gt;
&lt;li&gt;避免使用有狀態的 Pod 在先占虛擬機上 (這點稍後的文章，我們會試圖超越)&lt;/li&gt;
&lt;li&gt;使用 node taint 來協助排程到先占虛擬機，與非先占虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;總之，由於有容器自動化管理，我們才能輕易的使用先占虛擬機。&lt;/p&gt;
&lt;h3 id=&#34;gke&#34;&gt;GKE&lt;/h3&gt;
&lt;p&gt;然而，決定使用 GKE 後，就有許多關於成本的事情需要討論&lt;/p&gt;
&lt;p&gt;先看 &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/pricing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GKE 的計費方式 pricing&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每個 GKE 集群管理費用 $0.1/hr = $72/hr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這個費用是固定收費，只要開一個集群，不論集群的節點數量。所以在節點多、算力大的集群裡，這個費用會被稀釋，但在節點少的集群裡比例會被放大。&lt;/p&gt;
&lt;p&gt;然後 GKE 還是會有一些自己的毛，俗話說有一好沒兩好，我們使用它的好處同時，也要注意許多眉眉角角。再來爬&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;文件&lt;/a&gt;。如同最前面宣導，用產品就要乖乖把文件看完，不過這裡先針對與先占虛擬機相關的議題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#memory_cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allocatable Resource&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regional Cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cluster autoscaler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;allocatable-resource&#34;&gt;Allocatable Resource&lt;/h3&gt;
&lt;p&gt;在網路上看到這篇好文 &lt;a href=&#34;https://learnk8s.io/allocatable-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GKE 上的可使用的資源 Allocatable Resource&lt;/a&gt;。啥意思呢？難道還有不能使用的資源嗎？&lt;/p&gt;
&lt;p&gt;沒錯，GKE 會保留一定的機器資源 (e.g. cpu, memory, disk)，來維持節點的管理元件，例如 container runtime (e.g. Docker)、kubelet、cAdvisor。&lt;/p&gt;
&lt;p&gt;也就是說，就算我們跟 GCP 購買了算力，有一個比例的資源我們是使用不到的。細節請見 &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#memory_cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;理解 GKE 集群架構&lt;/a&gt;。這會影響我們單一節點的規格，我們也需要一並計算，能實際使用的資源 (allocatable resource)。&lt;/p&gt;
&lt;p&gt;Allocatable = Capacity - Reserved - Eviction Threshold&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Capacity，是機器上實際裝載的資源，例如 n1-standard-4 提供 4 cpu 15 Gb memory&lt;/li&gt;
&lt;li&gt;Reserved，公有雲代管集群，預保留的資源&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-thresholds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eviction Threshold&lt;/a&gt;：Kubernetes 設定的 kubelet 驅逐門檻&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;驅逐門檻-eviction-threshold&#34;&gt;驅逐門檻 (Eviction threshold)&lt;/h3&gt;
&lt;p&gt;Kubelet 會主動監測節點上的資源使用狀況，當節點發生資源不足的狀況時，kubelet 會主動終止某些 Pod 的運行，並回收節點的資源，來避免整個節點資源不足導致的系統不穩定。被終止的 Pod 可以再次排程到其他資源足夠的節點上。細節請見 &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/eviction-policy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文件 Scheduling and Eviction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 Kubernetes 上，我們可以進一步設定驅逐門檻，當節點的可用資源低於驅逐的門檻，kubelet 會觸發 Pod 驅逐機制&lt;/p&gt;
&lt;p&gt;GKE 上每個節點會額外保留 100 MiB 的記憶體，作為驅逐門檻，意思是當節點耗盡資源，導致剩餘記憶體低於 100 MiB 的時候，會直接觸發 GKE 的 Pod Eviction，終止並回收部分的 Pod。換句話說，這 100 MiB 是不能被使用的資源。細節請見&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#eviction_threshold&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;集群保留資源精算&#34;&gt;集群保留資源精算&lt;/h3&gt;
&lt;p&gt;資源的定義，使用雲平台的一般費用大多來自此&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu&lt;/li&gt;
&lt;li&gt;memory&lt;/li&gt;
&lt;li&gt;storage&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然後是這個表，注意保留的資源是累進級距&lt;/p&gt;
&lt;p&gt;255 MiB of memory for machines with less than 1 GB of memory
25% of the first 4GB of memory
20% of the next 4GB of memory (up to 8GB)
10% of the next 8GB of memory (up to 16GB)
6% of the next 112GB of memory (up to 128GB)
2% of any memory above 128GB&lt;/p&gt;
&lt;p&gt;值計上能夠用到的資源，底下 GCP 也整理好了，例如 n1-standard-4 實際使用的是 memory 12.3/15，cpu 3.92/4。&lt;/p&gt;
&lt;p&gt;在維持合理的使用率下，開啟大的機器，可以降低被保留的資源比例，依照筆者公司過去經驗，GKE 起跳就是 n1-standard-4 或是以上規格，如果低於這個規格，可調度的資源比例真的太低，應該重新考慮一下這個解決方案是否合乎成本。&lt;/p&gt;
&lt;p&gt;但究竟什麼規格的機器適合我們的需求，說實在完全要看執行的應用而定。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Introduction</title>
      <link>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/</link>
      <pubDate>Sat, 26 Sep 2020 11:03:40 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/</guid>
      <description>&lt;h1 id=&#34;先占虛擬機技術文件二三事&#34;&gt;先占虛擬機，技術文件二三事&lt;/h1&gt;
&lt;p&gt;第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google 先占虛擬機官方文件&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。&lt;/p&gt;
&lt;p&gt;以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多作功課，仔細查閱官方文件，理解服務的規格&lt;/li&gt;
&lt;li&gt;深入分析自身的需求&lt;/li&gt;
&lt;li&gt;基於上面兩者，量化分析&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;什麼是先占虛擬機器preemptible-instance&#34;&gt;什麼是先占虛擬機器(Preemptible Instance)&lt;/h1&gt;
&lt;p&gt;先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用&lt;/li&gt;
&lt;li&gt;目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。&lt;/p&gt;
&lt;h1 id=&#34;先占機器的優缺點&#34;&gt;先占機器的優缺點&lt;/h1&gt;
&lt;p&gt;除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP 不保證會有足夠的先占虛擬機&lt;/li&gt;
&lt;li&gt;先占虛擬機不能直接轉換成普通虛擬機&lt;/li&gt;
&lt;li&gt;資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉&lt;/li&gt;
&lt;li&gt;先占機器排除在 &lt;a href=&#34;https://cloud.google.com/compute/sla&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 的服務等級協議 (SLA)&lt;/a&gt;之外&lt;/li&gt;
&lt;li&gt;先占虛擬機不適用&lt;a href=&#34;https://cloud.google.com/free&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 免費額度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;費用粗估試算&#34;&gt;費用粗估試算&lt;/h1&gt;
&lt;p&gt;至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。&lt;/p&gt;
&lt;p&gt;以常用的 N1 standard 虛擬機：https://cloud.google.com/compute/vm-instance-pricing#n1_standard_machine_types&lt;/p&gt;
&lt;p&gt;Hourly
Machine type	CPUs	Memory	Price (USD)	Preemptible price (USD)
n1-standard-1	1	3.75GB	$0.0550		$0.0110
n1-standard-2	2	7.5GB	$0.1100		$0.0220
n1-standard-4	4	15GB	$0.2200		$0.0440
n1-standard-8	8	30GB	$0.4400		$0.0880
n1-standard-16	16	60GB	$0.8800		$0.1760
n1-standard-32	32	120GB	$1.7600		$0.3520
n1-standard-64	64	240GB	$3.5200		$0.7040&lt;/p&gt;
&lt;p&gt;如果是用 GPU 運算：https://cloud.google.com/compute/gpus-pricing&lt;/p&gt;
&lt;p&gt;Model			GPUs	GPU memory	GPU price (USD)	Preemptible GPU price (USD)
NVIDIA® Tesla® T4	1 GPU	16 GB GDDR6	$0.35 per GPU	$0.11 per GPU
NVIDIA® Tesla® V100	1 GPU	16 GB HBM2	$2.48 per GPU	$0.74 per GPU&lt;/p&gt;
&lt;p&gt;依據虛擬機規格的不同，先占虛擬機大約是隨選虛擬機價格的 2 到 3 折。在 AWS 與 Azure，由於計費方式不同，有可能拿到 1 折左右的浮動價格。從各種角度來說，都是非常高的折數。&lt;/p&gt;
&lt;p&gt;不妨說，這整系列文章，都是衝這著個折數來的 XD。畢竟成本是實實在在的花費，工作負載 (workload) 合適的話，應該盡量嘗試導入。&lt;/p&gt;
&lt;p&gt;這個折數還有另外一個效果是，可以在相同成本下，添增更多資源算力，作為解決方案。什麼意思呢？就是如果工作負載合適的話，可以使用更高規格的先占節點，整體成本反而會下降。&lt;/p&gt;
&lt;p&gt;至於究竟差多少，需要依據規格與定價詳細試算才知道。底下我們就來算算看。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Resource Calculation</title>
      <link>https://chechia.net/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/</link>
      <pubDate>Fri, 25 Sep 2020 12:22:02 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/</guid>
      <description>&lt;h3 id=&#34;關於資源評估&#34;&gt;關於資源評估&lt;/h3&gt;
&lt;p&gt;架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？&lt;/p&gt;
&lt;p&gt;以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。&lt;/p&gt;
&lt;p&gt;導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載
&lt;ul&gt;
&lt;li&gt;如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載&lt;/li&gt;
&lt;li&gt;此外還有機會因為&lt;a href=&#34;&#34;&gt;上篇&lt;/a&gt;提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源
&lt;ul&gt;
&lt;li&gt;例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;希望整體資源的使用率夠高，當然預留太多的資源會造成浪費&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要控管 Kubernetes 的資源使用量也可設定&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;資源需求與資源限制&lt;/a&gt;，延伸閱讀。&lt;/p&gt;
&lt;p&gt;估計得越準確，當然實際部署的資源掌握度就越高，然而筆者過去的經驗，團隊在交付源碼時未必就能夠做出有效的資源消耗評估，那有沒有什麼辦法可以幫助我們？&lt;/p&gt;
&lt;h3 id=&#34;資源需求估估看&#34;&gt;資源需求估估看&lt;/h3&gt;
&lt;p&gt;如果應用開發團隊，有先作應用的 profiling，然後 release candidate 版本有在 staging 上作壓力測試的話，維運團隊這邊應該就取得的數據，做部署前的資源評估。&lt;/p&gt;
&lt;p&gt;應用在不同狀態或是工作階段，會消耗不同的資源&lt;/p&gt;
&lt;p&gt;例如：運算密集的 batch job 可能會有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;控制節點 (master node) 啟動後會佔有一定的資源，一般來說不會消耗太多，只是需要為控制節點優先保留資源&lt;/li&gt;
&lt;li&gt;工作節點 (worker node) 啟動時會需要預留足夠的資源，接收工作後會逐漸增加資源使用，拉到滿載&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：面向用戶的服務，可能會有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;啟動應用所需的資源&lt;/li&gt;
&lt;li&gt;沒有大量請求，只維持基本應用運行所使用的資源&lt;/li&gt;
&lt;li&gt;負載壓力灌進來時，消耗資源隨用戶請求數量的成長曲線，設定的安全上界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果沒有這些數據，其實維運很難事前估計資源，變成要實際推上線後見招拆招，基於實際的資源消耗去做自動擴容，其實有可能會造成資源的浪費，因此我建議如果開發團隊沒有作 profiling，維運團隊可以在工作流程內簡單加一步 profiling，目前主流語言都有提供相關工具，簡單的執行就可以獲得很多資訊。&lt;/p&gt;
&lt;p&gt;至於壓力測試，也是可以使用基本的工具(例如 &lt;a href=&#34;https://artillery.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artilery&lt;/a&gt;)簡單整到工作流程。特別是面對客戶的應用，務必要進行壓力測試。&lt;/p&gt;
&lt;p&gt;有了上述的資源需求數據，才能事先安排機器的規格。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用是面對客戶的 API server&lt;/li&gt;
&lt;li&gt;基本資源是 200m cpu 1Gi memory，這部分直接寫進 Kubernetes resource request，在排程時就先結點上預留。&lt;/li&gt;
&lt;li&gt;負載拉到 1,000 RPS，latency 95% 20ms  99% 30ms，這時的資源需求的上界大約 2 cpu 4 Gi memory&lt;/li&gt;
&lt;li&gt;超過 1,000 RPS 就應該要透過水平擴展去增加更多 instamce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果單跑這個 API server，就可以安排 memory 8，cpu 4 的 GKE node-pool，讓負載落在可用資源的 60%-70%，這樣還有餘裕可以承受大流量，給自動水平擴展做動的時間。&lt;/p&gt;
&lt;h3 id=&#34;資源調整參考&#34;&gt;資源調整參考&lt;/h3&gt;
&lt;p&gt;當然這些數據都可以依照時實際需求調整，資源要壓縮得更緊或更鬆都是可行的。&lt;/p&gt;
&lt;p&gt;如果應用有整合分析後台，例如 Real-time Uer Monitoring、或是基本的 Google Analytics，都可以觀察這些調整實際對用戶帶來的影響，用戶行為改變對公司營收的影響，全都可以量化。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機器負載拉到 80%，cpu 的壓力，導致 API latency 增加到 95% 50ms 99% 100ms&lt;/li&gt;
&lt;li&gt;此時用戶已經很有感了，會導致 0.1% 用戶跳轉離開&lt;/li&gt;
&lt;li&gt;而這 0.1% 的用戶，以往的平均消費，換算成為公司營收，是 $1,000/month&lt;/li&gt;
&lt;li&gt;把機器負載壓到 60%，只計算 cpu 的數量的話，需要多開 3 台 n1-standard-4 機器，共計 $337.26/month&lt;/li&gt;
&lt;li&gt;提供老闆做參考，老闆可能會趨向加開機器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當然上面的例子都非常簡化，變成國中數學問題，這邊只是提供一個估算的例子。現實中的問題都會複雜百倍，例如機器規格拉上去出現新的瓶頸、例如依賴的服務，message queue、database 壓力上升，或是公司內部問題，就拿不到預算(血淚 SRE)。如果要減少機器，也可以參考，一般來說聽到關機器省錢的話，老闆都會接受的 XD。&lt;/p&gt;
&lt;h3 id=&#34;回到先占機器&#34;&gt;回到先占機器&lt;/h3&gt;
&lt;p&gt;根據上面的國中數學，把應用一個一個都計算清楚，需求逐漸明確了。假設，架構團隊拿到開機器的工單，掐指一算，決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 GKE cluster&lt;/li&gt;
&lt;li&gt;6 n1-standard-4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些都是隨選虛擬機，價格大約是 $747/month (含集群管理費 $73/month)。今天有人腦動大開，那如果全部換成先占節點呢？變成 $265/month，虛擬機費用 $192 / $674 = 0.28 直接打超過三折。&lt;/p&gt;
&lt;p&gt;有人就擔心，這樣真的可以嗎？真的沒問題嗎？會不會影響用戶阿。&lt;/p&gt;
&lt;p&gt;答案是會，就是會影響用戶 XD&lt;/p&gt;
&lt;p&gt;聽到這邊很多人就怕了。但是怎麼個影響法呢，還需要看底下幾個段落，如果換成先占虛擬機，用戶會怎麼受到影響。我們也要試圖量化這個影響，當作要不要導入的判斷依據。&lt;/p&gt;
&lt;p&gt;句個反例，「我覺得可以」「你覺得不行」，或是「某某公司的某某團隊可以啊」「我們公司也來做吧」，這些都是很糟糕的理由。除了對內部毫無說服力之外，也沒有辦法作為導入成效的指標，會讓團隊陷入「導入了也不知道有比導入前好？」或是「具體導入後成效量化」，會影響團隊做出真正有效的判斷，應極力避免。&lt;/p&gt;
&lt;p&gt;此外，問行不行之前，其實需要知道團隊願意為了三折機器，付出多少成本。如果只是每月省個台幣 15,000，工程師薪水都超過了。但如果手下有三十台或三百台以上，也許就非常值得投資。成本不是絕對的，很多時候要與其他成本 (e.g. 開發人力時間成本) 一起考量。&lt;/p&gt;
&lt;p&gt;以上都是說明導入的動機，以下說明先占虛擬機的各種機制，以及對應用的實際影響。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Requirement</title>
      <link>https://chechia.net/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/</link>
      <pubDate>Thu, 24 Sep 2020 13:39:12 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/</guid>
      <description>&lt;h1 id=&#34;需求規劃&#34;&gt;需求規劃&lt;/h1&gt;
&lt;p&gt;使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 &lt;a href=&#34;https://cloud.google.com/products/calculator?hl=zh-tw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Pricing Calculator&lt;/a&gt; 精算成本。&lt;/p&gt;
&lt;p&gt;另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。&lt;/p&gt;
&lt;p&gt;這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。&lt;/p&gt;
&lt;p&gt;所以這邊先定義我們以下幾個需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;執行短期的 batch job&lt;/li&gt;
&lt;li&gt;執行長期的 user-facing API server&lt;/li&gt;
&lt;li&gt;執行長期的 stateful 資料庫、儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;batch-job&#34;&gt;Batch Job&lt;/h3&gt;
&lt;p&gt;常見的範例，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用網路爬蟲 (crawler) 去抓取許多網站的所有內容&lt;/li&gt;
&lt;li&gt;使用 GPU 進行機器學習的 Model Training&lt;/li&gt;
&lt;li&gt;大數據計算&lt;/li&gt;
&lt;li&gt;MapReduce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些任務的核心需求，很簡單直接&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;盡快完成整體工作&lt;/li&gt;
&lt;li&gt;盡可能節省大量算力成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：我手上的機器學習 Model 粗略估計 10000 小時*GPU 的算力需求，才能產出一個有效的Model。由於大量的算力需求，一般來說都會選擇分散式的運算框架 (ex. MapReduce) ，將真正消耗算力的工作，使用分而化之 (divide and conquer) 的架構設計，將分配任務的控制節點 (master)，與實際進行運算的工作節點(worker) 拆分。基於原本的分散式架構，幾乎可以無痛地將工作節點轉移到先占虛擬機上。&lt;/p&gt;
&lt;p&gt;根據上述的需求，這類的工作特性可能有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU / GPU 算力需求高的運算節點 (Worker)&lt;/li&gt;
&lt;li&gt;Worker 本身是無狀態的 Stateless&lt;/li&gt;
&lt;li&gt;可控的即時負載&lt;/li&gt;
&lt;li&gt;將整體工作切分成任務單元 (task)，分配給工作節點&lt;/li&gt;
&lt;li&gt;任務單元的狀態外部保留，工作節點可容錯 (fault-tolerent)，任務單元可復原&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占虛擬機可能是浮動價格，這類工作可以根據優先程度，調整合適的工作時間，例如在資料中心算力需求低，先占虛擬機的費用低廉時，啟用較多的工作節點加快運算，如果費用高時，可以降低先占虛擬機的使用，延後工作，甚至是調用不同區域，費用低的工作節點，來降低整體的成本。&lt;/p&gt;
&lt;p&gt;執得注意的是，這類任務的控制節點 (master)，也許是集中式的，也許是分散式的，需要根據性質考量，是否適合放在先占虛擬機上。有些架構控制節點可以容錯，然而錯誤發生後會需要復原狀態，這時會消耗額外的算力，可能會拖緩整體進度，造成算力的消耗。也許就可以考量使用隨選虛擬機配合使用。&lt;/p&gt;
&lt;h3 id=&#34;user-facing-services&#34;&gt;User-facing services&lt;/h3&gt;
&lt;p&gt;常見的範例，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restful API server&lt;/li&gt;
&lt;li&gt;Websocket Server&lt;/li&gt;
&lt;li&gt;TCP/HTTP reverse proxy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些工作的核心需求如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;整題服務的高可用性 (high availability)&lt;/li&gt;
&lt;li&gt;承受不可預期的負載高峰 (load spikes)&lt;/li&gt;
&lt;li&gt;整體表現需要低延遲 (low latency)&lt;/li&gt;
&lt;li&gt;可以水平擴展 (horizontal scaling)，支撐用戶的成長&lt;/li&gt;
&lt;li&gt;最終平衡效能呈現與成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於會面對使用者，需要能支持使用者的壓力，又同時需要有一定的服務效能，來維持使用者體驗。實務上設計可能採用無狀態應用 (stateless)，多副本 (replica) 部署到集群中。需要儲存的狀態（如用戶狀態），使用外部的共用儲存 (例如：Redis，RDBMS，或是 Non-SQL DB)。請求的流量，透過上游的負載均衡器 (Load Balancer)，送進多個後端，處理完成請求後，再返回給使用者。&lt;/p&gt;
&lt;p&gt;這樣的設計，使用先占虛擬機也不會有太多的問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現代的附載均衡器多半都能像後端做可用性檢測 (health check)，可以把流量導向工作正常的節點，如果後端的虛擬機被資料中心收回了，流量也會移轉到其他節點上，不會遺失用戶請求&lt;/li&gt;
&lt;li&gt;配合自動水平拓展工具 (Auto horizontal scaler)，可以設定期望的服務節點數量，如果資料中心回收先占節點，拓展工具可以同時去取得新的先占節點，或是取得隨選隨用的虛擬機&lt;/li&gt;
&lt;li&gt;配合流量監測，也可以動態調整期望的服務節點數量，例如：偵測到大量用戶連線數時，增加更多服務節點，待流量下降後，再降低服務節點數量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這樣的設計實務上有幾點注意&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;雖然說應用後端本身是無狀態的，但面對用戶也許還是會有部分狀態存在應用外部，例如：User session，或是 websocket 的長連線。特別注意這些服務斷線的時候，對於使用者的影響，配合前端增強使用者體驗&lt;/li&gt;
&lt;li&gt;後端水平拓展後，瓶頸會轉移到其他地方，例如 Database 成為效能瓶頸，應用這邊需要做一定程度的自律( ex. connection limit，rate limit)，避免不斷增長的應用壓垮依賴的服務，如 MessageQueue 或是 Database&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分散式的儲存中心 (distributed DB)，如：cassandra 或是小強 DB。而這樣類型的服務，是否適合放在先占節點上？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Speficication</title>
      <link>https://chechia.net/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/</link>
      <pubDate>Wed, 23 Sep 2020 16:23:14 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/</guid>
      <description>&lt;h1 id=&#34;先占虛擬機終止流程-preemption-process&#34;&gt;先占虛擬機終止流程 (Preemption process)&lt;/h1&gt;
&lt;p&gt;子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。&lt;/p&gt;
&lt;p&gt;這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。&lt;/p&gt;
&lt;p&gt;如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用重啟初始化時 cpu memory 突然拉高&lt;/li&gt;
&lt;li&gt;承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance &amp;hellip;等等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。&lt;/p&gt;
&lt;p&gt;這邊有幾個面向要注意&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GCP 如何終止先占節點&lt;/li&gt;
&lt;li&gt;GCP 移除節點對 GKE 、以及執行中應用的影響&lt;/li&gt;
&lt;li&gt;GKE 集群如何應對的節點失效&lt;/li&gt;
&lt;li&gt;GCP 自動調度補足新的先占節點&lt;/li&gt;
&lt;li&gt;GKE 集群如何應對節點補足&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;三個重點&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機終止對集群的影響&lt;/li&gt;
&lt;li&gt;Pod 隨之終止對應用的影響，是否能夠優雅終止&lt;/li&gt;
&lt;li&gt;有沒有方法可以避免上面兩者的影響&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;劇透一下：有的，有一些招式可以處理。讓我們繼續看下去。&lt;/p&gt;
&lt;h3 id=&#34;gcp-如何終止虛擬機&#34;&gt;GCP 如何終止虛擬機&lt;/h3&gt;
&lt;p&gt;先占虛擬機的硬體終止步驟與一般隨選虛擬機相同，所以我們要先理解&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/stop-start-instance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;虛擬機的停止流程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;這裡指的終止 (Stop) 是&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/instance-life-cycle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;虛擬機生命週期&lt;/a&gt; 的 RUNNING -&amp;gt; instances.stop() -&amp;gt; STOPPING -&amp;gt; TERMINATED 的步驟。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instances.stop()&lt;/li&gt;
&lt;li&gt;ACPI shutdown&lt;/li&gt;
&lt;li&gt;OS 會進行 shutdown 流程，並嘗試執行各個服務的終止流程，以安全的終止服務。如果虛擬機有設定&lt;a href=&#34;https://cloud.google.com/compute/docs/shutdownscript&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shtudown Script&lt;/a&gt; 會在這步驟處理&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/deleting-instance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;等待至少 90 秒&lt;/a&gt;，讓 OS 完成終止的流程
&lt;ul&gt;
&lt;li&gt;逾時的終止流程，GCP 會直接強制終止，就算 shutdown script 還沒跑完&lt;/li&gt;
&lt;li&gt;GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;虛擬機變成 TERMINATED 狀態&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gcp-如何終止先占虛擬機&#34;&gt;GCP 如何終止先占虛擬機&lt;/h3&gt;
&lt;p&gt;與隨選虛擬機不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機的時間 30 秒&lt;/li&gt;
&lt;li&gt;搭配 GKE 使用 Managed Instance Group，終止的虛擬機會被刪除，Autoscaler 會啟動新的虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一樣先看&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#preemption-process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;先占虛擬機的說明文件：終止流程&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;資料中心開始回收先占虛擬機，選中我們專案其中的一台先占虛擬機&lt;/li&gt;
&lt;li&gt;Compute Engine 傳送 ACPI G2 Soft Off，這裡 OS 會試圖安全關變服務，也會執行 &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance?hl=zh-tw#handle_preemption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shutdown script&lt;/a&gt;，可以做簡短的優雅終止&lt;/li&gt;
&lt;li&gt;30秒後，ACPI G3 Mechanical off&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但 30 秒能做什麼？只能快速的交代當前進度。如果應用需要花時間收尾，保存工作進度，可能會產生許多問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內&lt;/li&gt;
&lt;li&gt;在面對大量 IO 的工作，可能會導致者台虛擬機的大量應用一起進入優雅終止，先占虛擬機最後耗盡資源，來不及做完&lt;/li&gt;
&lt;li&gt;如果可能會超時，或是沒完成會有資料遺失風險，就不能在這個階段處理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;依賴 shutdown script 做收尾是危險的，我們之後要想辦法處理這個不保證做完的優雅終止。&lt;/p&gt;
&lt;p&gt;如果應用本身有容錯的框架，或是有容錯機制，我們這邊要額外做的工作就會少很多。例如許多程式框架提供自動重啟的功能，在外部保存 checkpoint，worker 只負責運算，終止信號一進來，也不用保存，直接拋棄未完成的工作進度，留待繼起的 worker 從 checkpoint 接手。&lt;/p&gt;
&lt;h3 id=&#34;preemption-selection&#34;&gt;Preemption selection&lt;/h3&gt;
&lt;p&gt;除了 24 小時的壽命限制會終止虛擬機，資料中心的事件也會觸發主動的虛擬機回收，&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#limitations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;由 GCP 主動觸發的回收機制機率很低&lt;/a&gt;，會根據每日每個區域 (zone) 的狀態而定。這裡描述資料中心啟動的臨時回收。&lt;/p&gt;
&lt;p&gt;GCP 不會把所你手上的 preemptible 機器都收走，而是依照&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#preemption-process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;一定的規則&lt;/a&gt;，選擇一個比例撤換的機器。&lt;/p&gt;
&lt;p&gt;先看文件敘述&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute Engine 會避免從單一客戶移除太多先占虛擬機&lt;/li&gt;
&lt;li&gt;優先移除新的虛擬機，偏向保留舊的虛擬機 (但最多仍活不過 24hr)&lt;/li&gt;
&lt;li&gt;開啟後馬上被移除的先占虛擬機不計費&lt;/li&gt;
&lt;li&gt;機器尺寸較小的機器，可用性較高。例如：16 cpu 的先占虛擬機，比 128 cpu 的先占虛擬機容易取得&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GCP 每天平均會移除一個專案中 5% - 15% 的虛擬機，從用戶的角度我們需要預期至少這個程度的回收。不過這個比例，GCP 也不給予任何保證。以筆者經驗，只能說絕大部分的時候，都不會擔心超過這個比例的回收，但是還是要做好最壞的打算，如果臨時無法取得足夠的先占虛擬機，要有方法暫時補足隨選虛擬機。&lt;/p&gt;
&lt;h3 id=&#34;gke&#34;&gt;GKE&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#kubernetes_constraint_violations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用先占虛擬機會違反 Kubernetes 的設計&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod grace period 會被忽略&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/workloads/pods/disruptions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pod disreuption budget&lt;/a&gt;，不會被遵守 (可能會超過)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;對應用的影響&#34;&gt;對應用的影響&lt;/h3&gt;
&lt;p&gt;GCP 觸發的 Preemptible process，對應用的影響&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;involuntary disruptions，GCP 送 ACPI G2 Soft Off&lt;/li&gt;
&lt;li&gt;OS 終止服務，包含執行 Kubernetes 的 container runtime&lt;/li&gt;
&lt;li&gt;容器內的應用會收到 SIGTERM，啟動 graceful shutdown
&lt;ul&gt;
&lt;li&gt;Kubernetes 提供的 Graceful-shutdown 可能會跑不完&lt;/li&gt;
&lt;li&gt;實務上只是中斷當前工作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;大量節點同時回收&#34;&gt;大量節點同時回收&lt;/h3&gt;
&lt;p&gt;由於 GCP 並不保證收回的機器的數量，同時回收的機器量大，還是會衝擊到服務。例如：一次收回 15% 的算力，當然服務還是會受到衝擊。當然這樣事件的機率並不高，但我們仍是需要為此打算。這邊有幾個做法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;預留更多的算力&lt;/li&gt;
&lt;li&gt;使用 regional cluster，在多個 zone 上分配先占虛擬機&lt;/li&gt;
&lt;li&gt;我們自行控制，提前主動回收虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;預留更多資源&#34;&gt;預留更多資源&lt;/h3&gt;
&lt;p&gt;這點很直觀，由於使用了更加便宜的機器，我們可以用同樣的成本，開更多的機器。&lt;/p&gt;
&lt;p&gt;退一步說，使用打三折的先占虛擬機，然後開原本兩倍的機器數量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;總成本是 0.3 * 2 = 0.6 倍&lt;/li&gt;
&lt;li&gt;同時間可用資源是 2 倍&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占節點回收的單位是一個一個虛擬機&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安排合適的機器尺寸&lt;/li&gt;
&lt;li&gt;尺寸較小的先占虛擬機，可用性較高。意思是零碎的先占虛擬機容易取得&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但當然也不能都開太小的機器，這會嚴重影響應用的分配。至於具體需要開多大，可以根據預計在機器上運行的應用，做綜合考量，例如有以下影用需要執行：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;app A: 1 cpu 5 replica&lt;/li&gt;
&lt;li&gt;app B: 3 cpu 5 replica&lt;/li&gt;
&lt;li&gt;app C: 5 cpu 5 replica&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;總共至少 45 cpu ，預期機器負載8 成的話，需要總共 45 / 0.8 = 56 cpu。也許可以考慮&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;8 cpu * 7 先占虛擬機&lt;/li&gt;
&lt;li&gt;4 cpu * 14&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也舉幾個極端不可行的例子&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;56 cpu * 1
&lt;ul&gt;
&lt;li&gt;這樣的虛擬機回收時的影響範圍 (blast radius) 就是 100% 服務&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1 cpu * 56
&lt;ul&gt;
&lt;li&gt;機器太瑣碎，可能超出 Qouta (節點數量，IP 數量&amp;hellip;)&lt;/li&gt;
&lt;li&gt;應用會更分散，節點間的內部網路流量會增加&lt;/li&gt;
&lt;li&gt;前幾篇提到的 reserved resource 比例高，會影響應用的部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果希望更保險，可以再補上隨選虛擬機混合搭配，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;8 cpu * 7
&lt;ul&gt;
&lt;li&gt;5 先占虛擬機 2 隨選虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4 cpu * 14
&lt;ul&gt;
&lt;li&gt;10 先占虛擬機 4 隨選虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;虛擬機區域&#34;&gt;虛擬機區域&lt;/h3&gt;
&lt;p&gt;虛擬機的回收觸發，也是會依據服務的區域 (zone) 回收。意思是節點回收不會同時觸發 asia-east1 中所有 zone 的節點回收，一般來說時間是錯開的 (不過GCP 也不保證這點 XD)。為了維持 GKE 的可用性，我們都會開多個 node-pool 在多個區域下。&lt;/p&gt;
&lt;p&gt;總之避免把機器都放在同個區域中。&lt;/p&gt;
&lt;h3 id=&#34;自行控制的虛擬機汰換&#34;&gt;自行控制的虛擬機汰換&lt;/h3&gt;
&lt;p&gt;簡單來說我們在 24 hr 期限之前，先分批自盡 XD，打散個各個虛擬機的 24 小時限制。&lt;/p&gt;
&lt;p&gt;使用這個有趣的工具 &lt;a href=&#34;https://github.com/estafette/estafette-gke-preemptible-killer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estafette-gke-preemptible-killer&lt;/a&gt;，自動汰換先占虛擬機，讓整個繼起虛擬機都分散在 24 小時間。&lt;/p&gt;
&lt;p&gt;estafette-gke-preemptible-killer ，使用上簡單，大家自己看著辦 XD。如果大家有興趣，留言的人多的話，我再另外開一篇細講。&lt;/p&gt;
&lt;h3 id=&#34;小結&#34;&gt;小結&lt;/h3&gt;
&lt;p&gt;為了使用先占虛擬機，我們要多做以下幾件事&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;為應用設計可容錯分散式架構，例如應用可以同時執行一樣的 API server 3 個 replica&lt;/li&gt;
&lt;li&gt;分散 Pod 到合適的機器上，例如設置 PodAntiAffinity&lt;/li&gt;
&lt;li&gt;設定合適的虛擬機大小，合適的分散應用&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://github.com/estafette/estafette-gke-preemptible-killer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estafette-gke-preemptible-killer&lt;/a&gt;，自動汰換先占虛擬機&lt;/li&gt;
&lt;li&gt;不依賴應用的 Graceful-shutdown 流程&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Requirement Distributed</title>
      <link>https://chechia.net/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/</link>
      <pubDate>Tue, 22 Sep 2020 14:39:00 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/</guid>
      <description>&lt;p&gt;我們以下幾個需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;執行短期的 batch job&lt;/li&gt;
&lt;li&gt;執行長期的 user-facing API server&lt;/li&gt;
&lt;li&gt;執行長期的 stateful 資料庫、儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;該不該在 Kubernetes 上面跑 database？&lt;/p&gt;
&lt;p&gt;TL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的&lt;/p&gt;
&lt;p&gt;等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。&lt;/p&gt;
&lt;p&gt;應用是否適合部署到 Kubernetes 上，可以看這篇 &lt;a href=&#34;https://cloud.google.com/blog/products/databases/to-run-or-not-to-run-a-database-on-kubernetes-what-to-consider&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Blog: To run or not to run a database on Kubernetes: What to consider&lt;/a&gt;，如果大家有興趣，再留言告訴我，我再進行中文翻譯。&lt;/p&gt;
&lt;p&gt;文中針對三個可能的方案做分析，以 MySQL 為例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sass，GCP 的 Cloud SQL
&lt;ul&gt;
&lt;li&gt;最低的管理維運成本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自架 MySQL 在 GCP 的 VM 上，自行管理
&lt;ul&gt;
&lt;li&gt;自負完全的管理責任，包含可用性，備份 (backup)，以及容錯移轉 (failover)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自架 MySQL 在 Kubernetes 上
&lt;ul&gt;
&lt;li&gt;自負完全的管理責任&lt;/li&gt;
&lt;li&gt;Kubernetes 的複雜抽象層，會加重維運工作的複雜程度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而 RDBMS 的提供商，自家也提供 Operator&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/oracle/mysql-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oracle 自家提供的 MySQL Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/CrunchyData/postgres-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrunchyData 也有提供的 Postgres Operator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你就想，所以這些人是想怎樣，RDBMS 放 Kubernetes 上到底是行不行 XD。Google 的文章說明：如果應用本身並不符合 Kubernetes 的工作流程 (Pod life-cycle)，可以透過上述的 Operator 來自動化許多維運的作業，降低維運的困難。&lt;/p&gt;
&lt;p&gt;然而 DB 有千百種，除了 RDBMS 以外，還有另外一批 Database 天生就具有分散式的架構，這些儲存庫部署到 Kubernetes 上，並不會太痛苦 (還是要付出一定的成本XD)，但是卻可以得益於 Kubernetes 的諸多功能。&lt;/p&gt;
&lt;p&gt;底下我們先根據分散式的儲存庫做概觀描述，本系列文的最後，會根據時間狀況，做實例分享：Cassandra 或是 CockroachDB。提供各位一點發想，並根據需求去選擇需要的儲存庫&lt;/p&gt;
&lt;p&gt;「行不行要問你自己了施主，技術上都可以，維運上要看看你的團隊有沒有那個屁股吃這份藥 XD」&lt;/p&gt;
&lt;h3 id=&#34;distributed-database&#34;&gt;Distributed Database&lt;/h3&gt;
&lt;p&gt;底下非常粗淺的簡介分散式儲存庫的概念，提供一個基準點，幫助接下來討論是否可以使用先占虛擬機。這邊要強調，儲存庫的類型千百種，底層的各種實作差異都非常大，底下的模型是基於 cassandra 但不會走太多細節。 cassandra 的規格有機會再細聊。&lt;/p&gt;
&lt;p&gt;當後端應用已經順利水平拓展之後，整體服務的效能瓶頸往往都壓在後端 DB 上。這些不同的 DB 面向不同的需求，當需求符合時，可以考慮使用這些解決方法。&lt;/p&gt;
&lt;p&gt;這邊要強調，不是放棄現有的 RDBMS ，完全移轉到新的資料庫，這樣的成本太高，也沒有必要性。更好的做法，是搭配既有的關聯性資料庫，將不是核心業務的資料處理抽出，移轉到合適的資料庫上。讓不同需求的資料儲存到更合適的儲存庫，是這段話要強調的重點，關連式資料庫也不是唯一選擇。&lt;/p&gt;
&lt;p&gt;分散式的資料庫有以下特徵&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分散式節點集群 (Cluster)：資料庫是多個節點共存，而非 single master, multiple slaves 的架構
&lt;ul&gt;
&lt;li&gt;配合共識算法 (consensus algorithm) 溝通節點之間的資訊&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;無單點錯誤 (Single-point failure)：e.g. 不會因為 master 錯誤導致整個服務失效&lt;/li&gt;
&lt;li&gt;高可用(High Availitility：可以承受集群中一定數量虛擬機故障，服務仍然可用&lt;/li&gt;
&lt;li&gt;資料 sharding 到不同節點上&lt;/li&gt;
&lt;li&gt;複本 (replica)
&lt;ul&gt;
&lt;li&gt;節點複本 (node replicas)：多個節點提供服務，提供流量的帶寬與可用性&lt;/li&gt;
&lt;li&gt;資料複本 (data replicas)：在多個節點上儲存資料，提供資料的備份，同時也提供讀取帶寬與可用性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從以上特徵來說，使用此架構的服務可以承受先占虛擬機的不定時終止，或許可以使用。&lt;/p&gt;
&lt;p&gt;實務上有非常多需要注意，需要依據各自服務的性質，各自處理。常見的問題舉例如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用可以容錯 (fault-tolerent)，然而錯誤發生後，會需要消耗復原成本，例如重啟後需要花時間初始化，或是在多節點上進行 data rebalance。&lt;/li&gt;
&lt;li&gt;可以承受突然的錯誤，使用先占虛擬機，變成每日固定會承受必然的錯誤。這裡犧牲了部分算力，甚至造成隱性的維護成本，最後是否符合節省成本的需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;都是需要仔細了解解決方案，並且分析需求，來評估是否有合乎成本。&lt;/p&gt;
&lt;h1 id=&#34;gke&#34;&gt;GKE&lt;/h1&gt;
&lt;p&gt;以上分析了三種常見需求例子：從 batch job，user-facing service，與 distributed database。明天會實際搬出 GKE 與 GCP Preemptible Instance 的技術規格，與大家實際討論。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance</title>
      <link>https://chechia.net/zh-hant/post/2020-09-21-gcp-preemptible-instance/</link>
      <pubDate>Mon, 21 Sep 2020 09:22:17 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-21-gcp-preemptible-instance/</guid>
      <description>&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;
&lt;p&gt;鐵人賽的第二部分，要帶來公有雲省錢系列文章。&lt;/p&gt;
&lt;p&gt;架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。&lt;/p&gt;
&lt;p&gt;一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。&lt;/p&gt;
&lt;p&gt;Kubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。&lt;/p&gt;
&lt;p&gt;「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。&lt;/p&gt;
&lt;p&gt;這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。&lt;/p&gt;
&lt;p&gt;這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。&lt;/p&gt;
&lt;h1 id=&#34;先占虛擬機-tldr&#34;&gt;先占虛擬機 TL;DR&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機為隨選虛擬機定價的 2-3 折，使用先占虛擬機可能可以節省 7 成的雲平台支出&lt;/li&gt;
&lt;li&gt;先占虛擬機比起隨選虛擬機，外加有諸多限制，e.g. 最長壽命 24 hr、雲平台會主動終止先占虛擬機&amp;hellip;等&lt;/li&gt;
&lt;li&gt;配合使用自動水平擴展 (auto-scaler)，讓舊的先占虛擬機回收的同時，去購買新的先占虛擬機&lt;/li&gt;
&lt;li&gt;配合可容錯 (fault-tolerent) 的分散式應用，讓應用可以無痛在虛擬機切換轉移，不影響服務&lt;/li&gt;
&lt;li&gt;要讓應用可以容錯，需要做非常多事情&lt;/li&gt;
&lt;li&gt;搭配 kubernetes ，自動化管理來簡化工作&lt;/li&gt;
&lt;li&gt;配合正確的設定，可以穩定的執行有狀態的分散式資料庫或儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或是看 Google 官方 Blog：&lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;預計內容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需求假設、釐清需求，並且精準計價&lt;/li&gt;
&lt;li&gt;精準計價使用先占虛擬機的節省成本&lt;/li&gt;
&lt;li&gt;先占虛擬機的規格、額外限制&lt;/li&gt;
&lt;li&gt;額外限制，造成技術要多做很多額外的事情&lt;/li&gt;
&lt;li&gt;實務經驗分享：API server&lt;/li&gt;
&lt;li&gt;實務經驗：從使用隨選虛擬機，移轉到先占虛擬機，公司實際導入經驗&lt;/li&gt;
&lt;li&gt;實務經驗：Elasticsearch&lt;/li&gt;
&lt;li&gt;實務經驗分享：其他分散式資料庫，也許是 Cassandra 或是 cockroachDB&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的內容不曉得會寫幾篇看感覺 XD&lt;/p&gt;
&lt;p&gt;有寫過鐵人賽的都知道 30 篇真的很漫長，一篇文章幾千字，都要花好幾個小時。我去年後半，真的都會看讀者的留言跟按讚，取暖一波，才有動力繼續寫。留言的人多就會多寫，留言的人少就會少寫，各位覺得文章還看得下去，請務必來我粉專按讚留個言，不管是推推、鞭鞭、或是有想看的文章來許願，都十分歡迎。有你們的支持，我才有動力繼續寫。&lt;/p&gt;
&lt;p&gt;請大家務必以實際行動支持好文章，不要讓劣幣驅逐良幣。不然 iThome 上面之後只剩洗觀看數的熱門文章了 XD&lt;/p&gt;
&lt;p&gt;當然，沒人留言我就會當作自己才是垃圾文 (自知之明XD)，就會收一收回家嚕貓睡覺，掰掰~&lt;/p&gt;
&lt;p&gt;-&amp;gt;我的粉專，等你來留言&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Custom Resource Deployment</title>
      <link>https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resource-deployment/</link>
      <pubDate>Sun, 13 Oct 2019 22:03:08 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resource-deployment/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;p&gt;這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-controller/&#34;&gt;Deploy Nginx Ingress Controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-config/&#34;&gt;Configure Nginx Ingress&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cert-manager (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-10-cert-manager-deployment/&#34;&gt;Deploy cert-manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-11-cert-manager-how-it-work/&#34;&gt;How cert-manager work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-12-cert-manager-complete-workflow/&#34;&gt;Cert-manager complete workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kubernetes CRD &amp;amp; Operator-sdk (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/&#34;&gt;Introduction about custom resource&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/&#34;&gt;Deployment &amp;amp; Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-15-kubernetes-custom-resource-with-operator-sdk/&#34;&gt;Deployment &amp;amp; Usage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;CRD 內容&lt;/li&gt;
&lt;li&gt;Deploy CRD&lt;/li&gt;
&lt;li&gt;Use custom resource&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;recap&#34;&gt;Recap&lt;/h1&gt;
&lt;p&gt;在上次的 cert-manager 內容中我們走過 cert-manager 的安裝步驟，其中有一個步驟是 apply cert-manager 的 manigests 檔案 &lt;code&gt;*.yaml&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jetstack/cert-manager/tree/release-0.11/deploy/manifests&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jetstack/cert-manager/tree/release-0.11/deploy/manifests&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ git clone https://github.com/jetstack/cert-manager
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ git checkout release-0.11
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ls deploy/manifest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;00-crds.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;01-namespace.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;BUILD.bazel	
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;README.md	
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;helm-values.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我們快速看一下這個 00-crds.yaml，這個 yaml 非常長，直接&lt;a href=&#34;https://github.com/jetstack/cert-manager/blob/release-0.11/deploy/manifests/00-crds.yaml#L1786&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;跳到 certificates.certmanager.k8s.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;希望看 golang 源碼文件的話，可以搭配&lt;a href=&#34;https://godoc.org/k8s.io/apiextensions-apiserver/pkg/apis/apiextensions#CustomResourceDefinitionSpec&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;godoc.org/k8s.io/apiextensions&lt;/a&gt; 來閱讀，更能理解 definition。&lt;/p&gt;
&lt;p&gt;在看之前先注意幾件事，CRD 內除了 schema 外，還定義了許多不同情境的使用資料。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CRD 內定義了 custom resource 的資料儲存 .spec.validation.openAPIV3Schema，使用 custom resource 會透過 validator 驗證&lt;/li&gt;
&lt;li&gt;.openAPIV3Schema 內定義了 .spec，以及 rumtime 中紀錄 .status 的資料
&lt;ul&gt;
&lt;li&gt;controller 可以把狀態 sync 到 custom resource 的 .status 中紀錄&lt;/li&gt;
&lt;li&gt;controller 可以比對 .spec 與 .status 來決定是否要 sync 以及如何 sync&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CRD 內定義了與 server 以及 client 互動的方式，
&lt;ul&gt;
&lt;li&gt;names 中定義各種使用情境的 custom resource 名稱&lt;/li&gt;
&lt;li&gt;additionalPrinterColumns 中添加 kubectl 中的顯示內容&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;// 這邊使用的是 v1beta1 的 API (deprecated at v1.16) ，新版開發建議使用 apiextension.k8s.io/v1 的 api
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;apiVersion: apiextensions.k8s.io/v1beta1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kind: CustomResourceDefinition
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;metadata:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  creationTimestamp: null
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  name: certificates.cert-manager.io
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;spec:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  // 使用 kubectl 會額外顯示的資訊內容，透過 jsonpath 去 parse 顯示
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  additionalPrinterColumns:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - JSONPath: .status.conditions[?(@.type==&amp;#34;Ready&amp;#34;)].status
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    name: Ready
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - JSONPath: .spec.secretName
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    name: Secret
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - JSONPath: .spec.issuerRef.name
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    name: Issuer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    priority: 1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - JSONPath: .status.conditions[?(@.type==&amp;#34;Ready&amp;#34;)].message
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    name: Status
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    priority: 1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - JSONPath: .metadata.creationTimestamp
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    description: CreationTimestamp is a timestamp representing the server time when
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      this object was created. It is not guaranteed to be set in happens-before order
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      across separate operations. Clients may not set this value. It is represented
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      in RFC3339 form and is in UTC.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    name: Age
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    type: date
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  group: cert-manager.io
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  // 定義 CRD 在不同情境下使用的名稱
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  names:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    kind: Certificate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    listKind: CertificateList
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    plural: certificates
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    shortNames:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    - cert
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    - certs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    singular: certificate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  scope: Namespaced
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  subresources:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    status: {}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  validation:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    // openAPIV3Schema 中是 custom resource 實際操作會使用的內容
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    // properties 使用 . .description .type ，分別定義名稱，描述，檢查型別
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    openAPIV3Schema:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      description: Certificate is a type to represent a Certificate from ACME
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      properties:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        apiVersion:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          description: &amp;#39;APIVersion defines the versioned schema of this representation
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            of an object. Servers should convert recognized schemas to the latest
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        kind:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          description: &amp;#39;Kind is a string value representing the REST resource this
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            object represents. Servers may infer this from the endpoint the client
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          type: string
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        // custom resource runtime 中的 metadata
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        metadata:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          type: object
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        // custom resource 使用時的 spec，定義 custom resoure 的 desired status
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        spec:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          ...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        // custom controller 監測 custom resource 的 current status，這邊的資料完全視 controller 實作來產生，如果沒有實作 sync status，也可以沒有資料
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        status:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;          ...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      type: object
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  // 這個是 CRD 物件的 version，可以定義多個不同 version 的 CRD，調用時需要註明版本
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  version: v1alpha2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  versions:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  - name: v1alpha2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    served: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    storage: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;// 這個是 CRD 物件的 status，描述 CRD 部署到 API server 的狀態，例如 CRD 儲存適用 configmap 的儲存空間，這邊顯示在 API server 上的儲存狀態。不要跟 custom resource 的 status 弄混了
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;status:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  acceptedNames:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    kind: &amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    plural: &amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  conditions: []
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  storedVersions: []
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;helm-values.yaml 與 01-namespace.yaml 很單純，前者是使用 helm 部署的可設定參數，預設只有 kubernetes resources，後者則是為之後的 cert-manager 元件新增一個 kubernetes namespace。&lt;/p&gt;
&lt;h1 id=&#34;小結-crd-內容-apiextensionsv1beta1&#34;&gt;小結 CRD 內容 (apiextensions/v1beta1)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;CRD 顯示名稱，內容&lt;/li&gt;
&lt;li&gt;CRD spec 驗證
&lt;ul&gt;
&lt;li&gt;custom resource&lt;/li&gt;
&lt;li&gt;custom resource schema&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CRD 自身部署狀態&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;部署&#34;&gt;部署&lt;/h1&gt;
&lt;p&gt;部署相較定義本身就非常簡單，直接 kubectl apply 到 kubernetes 上&lt;/p&gt;
&lt;h1 id=&#34;使用-custom-resource&#34;&gt;使用 custom resource&lt;/h1&gt;
&lt;p&gt;有了 CRD，我們便可以使用 CRUD API，互動模式與其他 build-in kubernetes resources 相同，只是內容會照 CRD 上的定義調整&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get certificates.certmanager.k8s.io
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get certificates
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get certs --all-namespaces
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get cert -n cert-manager
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAMESPACE          NAME                READY   SECRET              AGE
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cert-manager       ingress-nginx-tls   True    ingress-nginx-tls   221d
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;這邊看到的內容可能會有些落差，因為我當初用的版本比較舊，但內容大同小異。&lt;/p&gt;
&lt;p&gt;底下的 describe 內容已經跟上面的 CRD 版本差太多，對不起來了。但我也懶得再佈一組，還要重做 dnsName 與 authotization challenge&lt;/p&gt;
&lt;p&gt;直接讓大家感受一下舊版的內容XD&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl describe cert ingress-nginx-tls
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Name:         ingress-nginx-tls
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Namespace:    cert-manager
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;API Version:  certmanager.k8s.io/v1alpha1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Kind:         Certificate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Metadata:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Creation Timestamp:  2019-03-06T06:48:26Z
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Generation:          4
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Owner References:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    API Version:           extensions/v1beta1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Block Owner Deletion:  true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Controller:            true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Kind:                  Ingress
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Name:                  ingress-nginx
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Self Link:               /apis/certmanager.k8s.io/v1alpha1/namespaces/default/certificates/ingress-nginx-tls
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Spec:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Acme:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Config:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      Domains:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        chechiachang.com
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      Http 01:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        Ingress:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        Ingress Class:  nginx
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Dns Names:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    chechiachang.com
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Issuer Ref:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Kind:       ClusterIssuer
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Name:       letsencrypt-prod
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Secret Name:  ingress-nginx-tls
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;// 當前的 status，controller sync 上來
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;// controller 會比對 .spec 與 .status，判斷是否需要做事，ex. renew
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Status:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Acme:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Order:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      URL:  https://acme-v02.api.letsencrypt.org/acme/order/*
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Conditions:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Last Transition Time:  2019-09-02T03:52:03Z
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Message:               Certificate renewed successfully
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Reason:                CertRenewed
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Status:                True
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Type:                  Ready
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Last Transition Time:  2019-09-02T03:52:01Z
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Message:               Order validated
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Reason:                OrderValidated
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Status:                False
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    Type:                  ValidateFailed
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Events:                    &amp;lt;none&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;想要新增，可以回去看 &lt;a href=&#34;https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html#verifying-the-installation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cert-manager tutorial&lt;/a&gt;，這個是新版的文件&lt;/p&gt;
&lt;p&gt;當然，不爽這個 cert resource 也可以幹掉&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ kubectl delete cert ingress-nginx-tls -n cert-manager
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以上&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;簡介 CRD 與 CRD 內容&lt;/li&gt;
&lt;li&gt;操作 custom resource&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Custom Resources Basic</title>
      <link>https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/</link>
      <pubDate>Sun, 13 Oct 2019 17:28:12 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;p&gt;這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-controller/&#34;&gt;Deploy Nginx Ingress Controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-config/&#34;&gt;Configure Nginx Ingress&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cert-manager (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-10-cert-manager-deployment/&#34;&gt;Deploy cert-manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-11-cert-manager-how-it-work/&#34;&gt;How cert-manager work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-12-cert-manager-complete-workflow/&#34;&gt;Cert-manager complete workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kubernetes CRD &amp;amp; Operator-sdk (3)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/&#34;&gt;Introduction about custom resource&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/&#34;&gt;Deployment &amp;amp; Usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-10-15-kubernetes-custom-resource-with-operator-sdk/&#34;&gt;Deployment &amp;amp; Usage&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;custom resources&lt;/li&gt;
&lt;li&gt;custom controllers&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;簡介-custom-resources&#34;&gt;簡介 custom resources&lt;/h1&gt;
&lt;p&gt;Kubernetes 預先定義許多 resource ，這些 resource 是 &lt;a href=&#34;https://kubernetes.io/docs/reference/using-api/api-overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes API&lt;/a&gt; 預先設置的 &lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;API objects&lt;/a&gt;，例如 kubernetes pods resource 包含許多 pods 物件。&lt;/p&gt;
&lt;p&gt;Custom resoure 則是透過擴充 kubernetes API ，讓自定義的物件也可以在 kubernetes 上使用。上篇 cert-manager 就使用了許多 custom resource，這些 resource 在一般安裝的 kubernetes 上沒有安裝，需要安裝 custom resource difinition，向 kubernetes cluster 定義新的 custom resource。例如 certificates.certmanager.k8s.io 就是 cert-manager 自定義的資源，用來代表產生 x509 certificate 的內容。&lt;/p&gt;
&lt;p&gt;越來越多的 kubernetes core 方法，如今也使用 custom resources 來定義，讓 kubernetes 核心元件更加模組化。&lt;/p&gt;
&lt;p&gt;custom resource 可以在運行中的 kubernetes 集群中註冊 (registration) ，也可以動態註銷，custom resource 並不會影響集群本身的運作。只要向 kubernetes 註冊完 custom resource，就可以透過 API 與 kubectl 控制 custom resource，就像操作 Pod resource 一樣。&lt;/p&gt;
&lt;h1 id=&#34;custom-controllers&#34;&gt;Custom controllers&lt;/h1&gt;
&lt;p&gt;custom resource 一但註冊，就可以依據 resource 的 CRD (custom resource definition) 來操作，因次可以儲存客製化的資料內容。然而在很多情形，我們並不只要 custom resource 來讀寫，而是希望 custom resource 能執行定義的工作，如同 Pod resource 可以在 kubernetes 集群上控制 Pod，在 Pod resource 上描述的 desired state kubernetes 會透過定義在 Pod API 中的 sync 邏輯，來達到 current state 與 desired state 的平衡。&lt;/p&gt;
&lt;p&gt;我們希望 custom resource 也能做到上述的功能，提供 declarative API，讓使用者不需編寫完整的程式邏輯，只要透過控制 custom resource，就可以透過 controller 內定義的邏輯，來實現 desired state。使用者只需要專注在控制 custom resource 上的 desired state，讓 controller 處理細節實作。&lt;/p&gt;
&lt;p&gt;例如：我們在 cert-manager 中設定 certificates.certmanager.k8s.io 資源，來描述我們希望取得 x509 certificate 的 desired state，但我們在 certificates.certmanager 上面沒有寫『透過 Let&amp;rsquo;s Encrypt 取得 x509 certificate』的實現邏輯，仍然能透過 cert-manager 產生 x509 certiticate，因為 cert-manager 內部已經定義 certificates.certmanager.k8s.io 的 custom controller。&lt;/p&gt;
&lt;p&gt;基本的 custom resource 操作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;註冊 custom resource definition，讓 kubernetes API 看得懂 custom resource
&lt;ul&gt;
&lt;li&gt;不然 API 會回覆 error: the server doesn&amp;rsquo;t have a resource type&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有 CRD 便可以 apply custom resource 到集群中&lt;/li&gt;
&lt;li&gt;部署 custom controller，監測 custom resource 的 desired state 內容，並實現達到 desired state 的業務邏輯
&lt;ul&gt;
&lt;li&gt;沒有 custom controller，custom resource 就只是可以 apply 與 update 的資料儲存結構，沒有 cert-manager 中 controller 的邏輯，也還是生不出 x509 certificate。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kubectl get chechiachang
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;error: the server doesn&amp;#39;t have a resource type &amp;#34;chechiachang&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;custom controller 也可以跟其他的 kubernetes resource 互動，例如 cert-manager 在產生 certificate 的時候，會把產生的 certificate 檔案放在 secret 中，cert-manager 會依據 order 中定義的 lifecycle ，持續檢查 certificate 的有效性，如果接近過期，則會觸發新的一輪 order。&lt;/p&gt;
&lt;p&gt;我們也可以寫一個操作 Configmap 與 Deployment Resource 的 custom controller，來進行 deploymnet 的 Image 更新。&lt;/p&gt;
&lt;h1 id=&#34;我需要-custom-resource-嗎&#34;&gt;我需要 custom resource 嗎&lt;/h1&gt;
&lt;p&gt;kubernetes 在&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;should I add custom resource&lt;/a&gt; 有列表分析該不該使用 custom resource ，將你的 API 邏輯整合到 kubernetes API 上。幾個判斷參考:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;API 是 &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;declarative model&lt;/a&gt;，如果不是可能不適合跟 kubernetes API 整合，獨立成為一個自己運行的服務即可&lt;/li&gt;
&lt;li&gt;需要使用 kubernetes&lt;/li&gt;
&lt;li&gt;需要使用 kubectl 控制&lt;/li&gt;
&lt;li&gt;API 需要使用 &lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#common-features&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kubernetes 支援的功能&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;正哉開發全新功能，因為整合舊的服務到 kubernetes API 工程浩大&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;也許-configmapsecret-就可以解決&#34;&gt;也許 configmap/secret 就可以解決&lt;/h1&gt;
&lt;p&gt;如果只是需要將資料儲存在 kubernetes 上，有一個 build-int 的 kubernetes resource 很適合，就是 configmap。可以瘀考以下條件，判斷是否 configmap 搭配能監看 configmap 的 controller 就可以達成需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;已經有完整的 config file，例如 mysql.cnf, nginx.conf&amp;hellip;&lt;/li&gt;
&lt;li&gt;主要用途是把檔案掛載到 Pod 中的 process 使用&lt;/li&gt;
&lt;li&gt;使用時的格式，是整個檔案放在 Pod 中，或是使用環境變數塞到 Pod 裡面，而不是透過 kubernetes API 存取 (ex 使用 kubectl)&lt;/li&gt;
&lt;li&gt;更新 configmpa 時更新 Pod，會比更新 custom resource 時更新 Pod 容易&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果使用 CRD 或 Aggregated kubernetes API，大多符合下列條件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 kubernetes libraries 與客戶端 (ex kubectl) 操作 custom resource&lt;/li&gt;
&lt;li&gt;需要 top level 的 kubernetes 支援，例如可以 kubectl get cheachiachang&lt;/li&gt;
&lt;li&gt;自動化 kubernetes 物件&lt;/li&gt;
&lt;li&gt;需要用到 .spec, .status, .metadata，這些比較 desired state 與 currenty state 的功能&lt;/li&gt;
&lt;li&gt;需要抽象類別來管理一群 controlled resource&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;custom-resource-definition&#34;&gt;Custom Resource Definition&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Custom Resoure Definition&lt;/a&gt; 讓使用者可以定義 custom resource，定義 custom resource 的格式包括名稱與 data schema，然後交給 kubernetes API 去處理 custom resource 的儲存。&lt;/p&gt;
&lt;p&gt;也就是說，透過 CRD 我們不用寫 custom resource 的 API，例如 cert-manager 不用寫 certificates.certmanager.k8s.io 的 API，而是向 kubernetes API server 註冊 CRD，讓 kubernetes API server 看得懂 custom source 的定義，並且直接使用 kubernetes API server，進行 custom resource 的 CRUD。&lt;/p&gt;
&lt;p&gt;我們可以透過 kubectl (API server) 操作 certificates.certmanager.k8s.io，這個請求也是送到 kubernetes API server。&lt;/p&gt;
&lt;h1 id=&#34;api-server-aggregation&#34;&gt;API server aggregation&lt;/h1&gt;
&lt;p&gt;能夠透過註冊 CRD ，就可以使用原來的 kubernetes API 來進行 CRUD ，是因為 kubernetes API 對於普通的 API 操作提供泛型 (generic) 介面，直接使用 CRUD 的邏輯。&lt;/p&gt;
&lt;p&gt;由於是 kubernetes Aggregated API ，所有 kubernetes 的 clients 都一起兼容新註冊的 custom resource，不用在 API 要定義，在冊戶端也要定義。註冊完的 custom resource definition，可以直接透過 kubectl 存取。&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;custom resource 簡介&lt;/li&gt;
&lt;li&gt;custom resource 使用的情境與條件&lt;/li&gt;
&lt;li&gt;custom resource definition 與 Aggregated API&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Self-host ELK stack - Installation</title>
      <link>https://chechia.net/zh-hant/post/2019-09-15-self-host-elk-stack-on-gcp/</link>
      <pubDate>Sun, 15 Sep 2019 11:43:03 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2019-09-15-self-host-elk-stack-on-gcp/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-15-self-host-elk-stack-on-gcp/&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-15-secure-elk-stack/&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-18-monitoring-gce-with-elk/&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-19-monitoring-gke-with-elk/&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-18-elastic-or-not-elastic/&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/zh-hant/post/2019-09-21-logstash-on-gke/&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;h1 id=&#34;簡介-elk-stack&#34;&gt;簡介 ELK stack&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方說明文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;elk-的元件&#34;&gt;ELK 的元件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: 基於 Lucene 的分散式全文搜索引擎&lt;/li&gt;
&lt;li&gt;Logstash: 數據處理 pipeline&lt;/li&gt;
&lt;li&gt;Kibana: ELK stack 的管理後台與數據視覺化工具&lt;/li&gt;
&lt;li&gt;Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elk-的工作流程&#34;&gt;ELK 的工作流程&lt;/h3&gt;
&lt;p&gt;beats -&amp;gt; (logstash) -&amp;gt; elasticsearch -&amp;gt; kibana&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起&lt;/li&gt;
&lt;li&gt;設定 beats 從指定的路徑收集 log 與 metrics&lt;/li&gt;
&lt;li&gt;設定 beats 向後輸出的遠端目標&lt;/li&gt;
&lt;li&gt;(Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch&lt;/li&gt;
&lt;li&gt;beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據&lt;/li&gt;
&lt;li&gt;透過 Kibana，將 elasticsearch 上的 log 顯示&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;官方不是有出文件嗎&#34;&gt;官方不是有出文件嗎&lt;/h1&gt;
&lt;p&gt;Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。&lt;/p&gt;
&lt;p&gt;說穿了本篇的技術含量不高，就只是一個踩雷過程。&lt;/p&gt;
&lt;p&gt;Lets get our hands dirty.&lt;/p&gt;
&lt;h1 id=&#34;warning&#34;&gt;WARNING&lt;/h1&gt;
&lt;p&gt;這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在&lt;a href=&#34;https://www.elastic.co/what-is/open-x-pack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;v6.3.0 以前的版本是不包含安全性模組的&lt;/a&gt;，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。&lt;/p&gt;
&lt;p&gt;我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另起一篇對安全性設定多加說明&lt;/li&gt;
&lt;li&gt;在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。&lt;/p&gt;
&lt;p&gt;如果希望第一次安裝就有完整的 security 設定，請等待下篇 &lt;a href=&#34;#secure-elk-stack&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;討論需求與規格&#34;&gt;討論需求與規格&lt;/h1&gt;
&lt;p&gt;這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;elk-node-standalone 10.140.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;app-node-1          10.140.0.11
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;...                 ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本機的 ELK stack 元件，彼此透過 localhost 連線&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch:  localhost:9200&lt;/li&gt;
&lt;li&gt;Kibana:         localhost:5601&lt;/li&gt;
&lt;li&gt;Apm-server:     localhost:8200&lt;/li&gt;
&lt;li&gt;Self Monitoring Services&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;私有網路中的外部服務透過 10.140.0.10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200&lt;/li&gt;
&lt;li&gt;beats 從其他 node 輸出到 Apm-server:    10.140.0.10:8200&lt;/li&gt;
&lt;li&gt;在內部網路中 透過 browser 存取 Kibana:  10.140.0.10:5601&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;standalone 的好處:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰)&lt;/li&gt;
&lt;li&gt;最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standalone可能造成的問題:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛&lt;/li&gt;
&lt;li&gt;外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果要有 production ready 的 ELK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HA 開起來&lt;/li&gt;
&lt;li&gt;把服務分散到不同 node 上, 方便之後 scale out 多開幾台
&lt;ul&gt;
&lt;li&gt;elasticsearch-1, elasticsearch-2, elasticsearch-3&amp;hellip;&lt;/li&gt;
&lt;li&gt;kibana-1&lt;/li&gt;
&lt;li&gt;apm-server-1, apm-server-2, &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;主機設定&#34;&gt;主機設定&lt;/h1&gt;
&lt;p&gt;Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。&lt;/p&gt;
&lt;p&gt;GCP 上使用 Google Compote Engine 的朋友，可以照 &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/add-persistent-disk?hl=zh-tw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google 官方操作步驟操作&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;完成後接近這樣&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ df -h
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ df --human-readable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Filesystem      Size  Used Avail Use% Mounted on
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/sda1       9.6G  8.9G  682M  93% /
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/sdb        492G   63G  429G  13% /mnt/disks/elk
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ls /mnt/disks/elk
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/mnt/disks/elk/elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/mnt/disks/elk/apm-server
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/mnt/disks/elk/kibana
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。&lt;/p&gt;
&lt;h1 id=&#34;開防火牆&#34;&gt;開防火牆&lt;/h1&gt;
&lt;p&gt;需要開放 10.140.0.10 這台機器的幾個 port&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch           :9200   來源只開放私有網路其他 ip 10.140.0.0/9&lt;/li&gt;
&lt;li&gt;apm-server              :8200   (同上)&lt;/li&gt;
&lt;li&gt;kibana                  :5601   (同上)，如果想從外部透過 browser開，需要 whitelist ip&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;default-allow-internal: :all    :10.140.0.0/9   tcp:0-65535&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;install-elasticsearch&#34;&gt;Install Elasticsearch&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/install-elasticsearch.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Install Elasticsearch 官方文件 7.3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get install apt-transport-https
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;add-apt-repository &amp;#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get update
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get install elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;安裝完後路徑長這樣&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;etc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;etc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;yml&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;etc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;jvm&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;options&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Utility&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;usr&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;share&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bin&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Log&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;var&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs&lt;/p&gt;
&lt;h3 id=&#34;服務控制&#34;&gt;服務控制&lt;/h3&gt;
&lt;p&gt;透過 systemd 管理，我們可以用 systemctl 控制，
用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。&lt;/p&gt;
&lt;p&gt;但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir -p /mnt/disks/elk/elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;設定檔案&#34;&gt;設定檔案&lt;/h3&gt;
&lt;p&gt;ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo vim /etc/elasticsearch/elasticsearch.yml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Change Network
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;network.host: 0.0.0.0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Change data path
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;path.data: /mnt/disks/elk/elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vim /etc/elasticsearch/jvm-options
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Adjust heap to 4G
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-Xms4g
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;-Xmx4g
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Enable xpack.security
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;discovery.seed_hosts: [&amp;#34;10.140.0.10&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;discovery.type: &amp;#34;single-node&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;xpack.security.enabled: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;xpack.security.transport.ssl.enabled: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;xpack.license.self_generated.type: basic
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。&lt;/p&gt;
&lt;p&gt;有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Keep your passwords safe
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然後把啟動 Elasticsearch&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl enable elasticsearch.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl start elasticsearch.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl status elasticsearch.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看一下 log，確定服務有在正常工作&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tail&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;var&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;elasticsearch&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 node 上試打 Elasticsearch API&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ curl localhost:9200
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;name&amp;#34; : &amp;#34;elk&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;cluster_name&amp;#34; : &amp;#34;elasticsearch&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;cluster_uuid&amp;#34; : &amp;#34;uiMZe7VETo-H6JLFLF4SZg&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;version&amp;#34; : {
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;number&amp;#34; : &amp;#34;7.3.1&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;build_flavor&amp;#34; : &amp;#34;default&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;build_type&amp;#34; : &amp;#34;deb&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;build_hash&amp;#34; : &amp;#34;4749ba6&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;build_date&amp;#34; : &amp;#34;2019-08-19T20:19:25.651794Z&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;build_snapshot&amp;#34; : false,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;lucene_version&amp;#34; : &amp;#34;8.1.0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;minimum_wire_compatibility_version&amp;#34; : &amp;#34;6.8.0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &amp;#34;minimum_index_compatibility_version&amp;#34; : &amp;#34;6.0.0-beta1&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  },
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &amp;#34;tagline&amp;#34; : &amp;#34;You Know, for Search&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;kibana&#34;&gt;Kibana&lt;/h1&gt;
&lt;p&gt;有了正常工作的 Elasticsearch，接下來要安裝 kibana，由於 apt repository 已經匯入，這邊直接&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get update
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get install kibana
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一樣快速設定一下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ vim /etc/kibana/kinana.yml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# change server.host from localhost to 0.0.0.0 to allow outside requests
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;server.host: &amp;#34;0.0.0.0&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Add elasticsearch password
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;elasticsearch.username: &amp;#34;kibana&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;elasticsearch.password:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl enable kibana.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl start kibana.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl status kibana.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;檢查 log 並試打一下&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl status kibana
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ curl localhost:5601
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;透過內網 ip 也可以用 browser 存取
使用 elastic 這組帳號密碼登入，可以有管理員權限
可以檢視一下 kibana 的頁面，看一下是否系統功能都上常上線
http://10.140.0.10/app/monitoring#&lt;/p&gt;
&lt;h1 id=&#34;filebeat&#34;&gt;Filebeat&lt;/h1&gt;
&lt;p&gt;以上是 ELK 最基本架構: elasticsearch 引擎與前端視覺化管理工具 Kibana。當然現在進去 kibana 是沒有數據的，所以我們現在來安裝第一個 beat，收集第一筆數據。&lt;/p&gt;
&lt;p&gt;你可能會覺得奇怪: 我現在沒有任何需要監控的應用，去哪收集數據?&lt;/p&gt;
&lt;p&gt;ELK 提供的自我監測 (self-monitoring) 的功能，也就是在 node 上部屬 filebeat 並啟用 modules，便可以把這台 node 上的 elasticsearch 運行的狀況，包含cpu 狀況、記憶體用量、儲存空間用量、安全性告警、&amp;hellip;都做為數據，傳到 elasticsearch 中，並在 Kibana monitoring 頁面製圖顯示。&lt;/p&gt;
&lt;p&gt;這邊也剛好做為我們 ELK stack 的第一筆數據收集。&lt;/p&gt;
&lt;p&gt;WARNING: 這邊一樣要提醒， production 環境多半會使用另外一組的 elasticsearch 來監控主要的這組 elastic stack，以維持 elk stack 的穩定性，才不會自己 monitoring 自己，結果 elastic 掛了，metrics 跟錯誤訊息都看不到。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-installation.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方安裝文件&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get update
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt-get install filebeat
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;預設的 filebeat.yml 設定檔案不是完整的，請到官網下載完整版，但官網沒給檔案連結(慘)，只有網頁版 &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們上 github 把她載下來&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ wget https://raw.githubusercontent.com/elastic/beats/v7.3.1/filebeat/filebeat.reference.yml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ sudo mv filebeat-reference-y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ sudo vim /etc/filebeat/filebeat.yml
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Enable elasticsearch module and kibana module to process metrics of localhost elasticsearch &amp;amp; kibana
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;filebeat.modules:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- module: elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  # Server log
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  server:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    enabled: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- module: kibana
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  # All logs
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  log:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    enabled: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# The name will be added to metadata
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;name: filebeat-elk
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;fields:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  env: elk
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Add additional cloud_metadata since we&amp;#39;re on GCP
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;processors:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- add_cloud_metadata: ~
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Output to elasticsearch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;output.elasticsearch:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  enabled: true
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  hosts: [&amp;#34;localhost:9200&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  protocol: &amp;#34;http&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  username: &amp;#34;elastic&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  password: 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;# Configure kibana with filebeat: add template, dashboards, etc...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;setup.kibana:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  host: &amp;#34;localhost:5601&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  protocol: &amp;#34;http&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  username: &amp;#34;elastic&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  password: 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;啟動 filebeat&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl start filebeat
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看一下 log，filebeat 會開始收集 elasticsearch 的 log 與 metrics，可以在 log 上看到收集的狀況。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-gdscript3&#34; data-lang=&#34;gdscript3&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sudo&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;journalctl&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fu&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filebeat&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Sep&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;06&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;28&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;elk&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;filebeat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9143&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2019&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;09&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T06&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;28&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;50.176&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Z&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;INFO&lt;/span&gt;        &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;monitoring&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;        &lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;go&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;145&lt;/span&gt;        &lt;span class=&#34;n&#34;&gt;Non&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;zero&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;metrics&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;last&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;        &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;monitoring&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;metrics&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;beat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;cpu&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1670860&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ms&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;66&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6964660&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ms&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;336&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6964660&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;user&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ticks&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5293800&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ms&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;270&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;handles&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;limit&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;hard&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4096&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;soft&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;open&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;info&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ephemeral_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;62fd4bfa-1949-4356-9615-338ca6a95075&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;uptime&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ms&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;786150373&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;memstats&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gc_next&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;7681520&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;memory_alloc&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4672576&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;memory_total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;457564560376&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;rss&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32768&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;runtime&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;goroutines&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;98&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;filebeat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;events&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;active&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;29&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;added&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1026&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;done&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1055&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;harvester&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;open_files&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;running&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;libbeat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;config&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;module&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;running&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;events&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;acked&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1055&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;active&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;batches&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;34&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1005&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;read&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bytes&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;248606&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;write&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bytes&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;945393&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;pipeline&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;clients&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;events&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;active&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;32&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;published&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1026&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1026&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;queue&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;acked&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1055&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;registrar&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;states&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;current&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;34&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;update&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1055&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;writes&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;success&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;35&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;total&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;35&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}},&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;system&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;load&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.49&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;15&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.94&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;5&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;1.15&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;norm&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.745&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;15&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.47&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;5&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.575&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}}}}}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果數據都有送出，就可以回到 kibana 的頁面，看一下目前這個 elasticsearch 集群，有開啟 monitoring 功能的元件們，是否都有正常工作。&lt;/p&gt;
&lt;p&gt;http://10.140.0.10/app/monitoring#&lt;/p&gt;
&lt;p&gt;頁面長得像這樣&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;elk/kibana-monitoring.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;100%&#34; height=&#34;100%&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Standalone cluster 中的 filebeat，是還未跟 elasticsearch 配對完成的數據，會顯示在另外一個集群中，配對完後會歸到 elk cluster 中，就是我們的主要 cluster。&lt;/p&gt;
&lt;p&gt;點進去可以看各個元件的服務情形。&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;簡單思考 self-host ELK stack 搭建的架構&lt;/li&gt;
&lt;li&gt;在單一 node 上安裝最簡易的 elastic stack&lt;/li&gt;
&lt;li&gt;設定元件的 output 位置&lt;/li&gt;
&lt;li&gt;設定 self-monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;恭喜各位獲得一個裸奔但是功能完整的 ELK, 我們下篇再向安全性邁進。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
