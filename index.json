[{"content":"📅 活動時間：2025-10-22T (待定) 🔗 活動連結 📘 聯繫我 Facebook 📑 投影片 Workshop Workshop: Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s Components\nOutline K8sGPT 是一個結合 AI 的 Kubernetes 叢集診斷與自動化排錯工具，能自動掃描叢集狀態、辨識異常並用自然語言解釋問題，提供可執行的修復建議，甚至支援自動修復。K8sGPT 讓使用者用更低門檻掌握叢集健康狀況，加速問題定位與排除，是 DevOps 與 SRE 團隊提升營運效率的實用工具。\n在排查問題時，比起人類工程師，K8sGPT 缺乏對 workload 架構的深入了解，也無法存取內部架構設計文件，或是 Runbook 與 SOP。這限制了其診斷能力並可能因為幻覺（hallucination）而提供不正確的建議。K8sGPT 主要依賴 Kubernetes API 與叢集狀態資訊來進行診斷，但這些資訊並不足以涵蓋所有可能的問題情境。\n本次演講展現一個使用案例，嘗試透過 RAG（Retrieval-Augmented Generation 檢索增強生成）技術來增強其診斷能力，展現目前基於大語言模型的解決方案，面對 Kubernetes 叢集問題時的優勢與挑戰。\n參考資料\nhttps://k8sgpt.ai/ KubeCon Europe 2025/04/02 Superpowers for Humans of Kubernetes: How K8sGPT Is Transforming Enter\u0026hellip; Alex Jones \u0026amp; Anais Urlichs Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\n","permalink":"https://chechia.net/posts/2025-10-23-k8s-summit/","summary":"\u003ch3 id=\"-活動時間2025-10-22t-待定\"\u003e📅 活動時間：2025-10-22T (待定)\u003c/h3\u003e\n\u003ch3 id=\"-活動連結\"\u003e🔗 \u003ca href=\"https://k8s.ithome.com.tw/2024/workshop-page/3259\"\u003e活動連結\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-聯繫我-facebook\"\u003e📘 聯繫我 \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-投影片\"\u003e📑 投影片\u003c/h3\u003e\n\u003chr\u003e\n\u003ch2 id=\"workshop\"\u003eWorkshop\u003c/h2\u003e\n\u003cp\u003eWorkshop: Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s Components\u003c/p\u003e\n\u003ch3 id=\"outline\"\u003eOutline\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eK8sGPT\u003c/strong\u003e 是一個結合 AI 的 Kubernetes 叢集診斷與自動化排錯工具，能自動掃描叢集狀態、辨識異常並用自然語言解釋問題，提供可執行的修復建議，甚至支援自動修復。K8sGPT 讓使用者用更低門檻掌握叢集健康狀況，加速問題定位與排除，是 DevOps 與 SRE 團隊提升營運效率的實用工具。\u003c/p\u003e\n\u003cp\u003e在排查問題時，比起人類工程師，K8sGPT 缺乏對 workload 架構的深入了解，也無法存取內部架構設計文件，或是 Runbook 與 SOP。這限制了其診斷能力並可能因為幻覺（hallucination）而提供不正確的建議。K8sGPT 主要依賴 Kubernetes API 與叢集狀態資訊來進行診斷，但這些資訊並不足以涵蓋所有可能的問題情境。\u003c/p\u003e\n\u003cp\u003e本次演講展現一個使用案例，嘗試透過 RAG（Retrieval-Augmented Generation 檢索增強生成）技術來增強其診斷能力，展現目前基於大語言模型的解決方案，面對 Kubernetes 叢集問題時的優勢與挑戰。\u003c/p\u003e\n\u003cp\u003e參考資料\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://k8sgpt.ai/\"\u003ehttps://k8sgpt.ai/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=EXtCejkOJB0\"\u003eKubeCon Europe 2025/04/02 Superpowers for Humans of Kubernetes: How K8sGPT Is Transforming Enter\u0026hellip; Alex Jones \u0026amp; Anais Urlichs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"author\"\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eChe-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\u003c/p\u003e","title":"K8s Summit 2025: RAG + k8sGPT 檢索增強生成與 K8sGPT"},{"content":"📅 活動時間：2025-10-22T (待定) 🔗 活動連結 📘 聯繫我 Facebook 📑 投影片 ❗Etcd workshop 行前準備❗ 本次工作坊有行前準備，請在活動日前完成上方投影片中的行前準備。\n工作坊場次 待定\nWorkshop Workshop: Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s Components\nOutline Etcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。讓參與者透過本次工作坊，可以有操作 k8s control plane 的經驗，更了解 Etcd 的基本操作，以及了解 Kubernetes 的基本架構。\n預計內容：\ndocker 啟動 etcd etcdctl 存取 etcd docker 啟動 etcd cluster docker 啟動 k8s control plane kubectl 存取 k8s control plane 維運 k8s 所需的 etcd operation （規劃中）本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來了解 Etcd 的基本操作。參與者必備個人筆電，透過 SSH 操控遠端機器。\n本次工作坊有行前準備，請在活動日前完成。 https://chechia.net/posts/2025-10-22-k8s-summit/\n必備知識：Linux 操作基本知識，Docker 操作基本知識，會使用 SSH 連線 / Bash / docker。\n參考資料\nEtcd 官方文件 v3.6 Etcd Playground Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\n","permalink":"https://chechia.net/posts/2025-10-22-k8s-summit/","summary":"\u003ch3 id=\"-活動時間2025-10-22t-待定\"\u003e📅 活動時間：2025-10-22T (待定)\u003c/h3\u003e\n\u003ch3 id=\"-活動連結\"\u003e🔗 \u003ca href=\"https://k8s.ithome.com.tw/2024/workshop-page/3259\"\u003e活動連結\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-聯繫我-facebook\"\u003e📘 聯繫我 \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-投影片\"\u003e📑 \u003ca href=\"/slides/2025-10-22-etcd-workshop/\"\u003e投影片\u003c/a\u003e\u003c/h3\u003e\n\u003chr\u003e\n\u003ch3 id=\"etcd-workshop-行前準備\"\u003e❗Etcd workshop 行前準備❗\u003c/h3\u003e\n\u003cp\u003e本次工作坊有行前準備，請在活動日前完成上方投影片中的行前準備。\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"工作坊場次\"\u003e工作坊場次\u003c/h2\u003e\n\u003cp\u003e待定\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"workshop\"\u003eWorkshop\u003c/h2\u003e\n\u003cp\u003eWorkshop: Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s Components\u003c/p\u003e\n\u003ch3 id=\"outline\"\u003eOutline\u003c/h3\u003e\n\u003cp\u003eEtcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。讓參與者透過本次工作坊，可以有操作 k8s control plane 的經驗，更了解 Etcd 的基本操作，以及了解 Kubernetes 的基本架構。\u003c/p\u003e\n\u003cp\u003e預計內容：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edocker 啟動 etcd\u003c/li\u003e\n\u003cli\u003eetcdctl 存取 etcd\u003c/li\u003e\n\u003cli\u003edocker 啟動 etcd cluster\u003c/li\u003e\n\u003cli\u003edocker 啟動 k8s control plane\u003c/li\u003e\n\u003cli\u003ekubectl 存取 k8s control plane\u003c/li\u003e\n\u003cli\u003e維運 k8s 所需的 etcd operation\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（規劃中）本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來了解 Etcd 的基本操作。參與者必備個人筆電，透過 SSH 操控遠端機器。\u003c/p\u003e","title":"K8s Summit 2025: Workshop: Get started with Etcd \u0026 Kubernetes"},{"content":"📅 活動時間：2025-08-09T12:45:00Z 🔗 活動連結 📘 聯繫我 Facebook 📑 投影片WIP Info Model Context Protocol（MCP）是一項由 Anthropic 推出的開放標準，旨在為大型語言模型（LLMs）提供一種標準化的方式，以連接和操作各種資料來源（如本地檔案、資料庫）和工具（如 GitHub、Google Maps）。MCP 的目標是簡化 AI 應用與外部資源的整合過程，類似於 USB-C 為實體設備提供通用連接介面。\n隨著 AI 技術的快速發展，AI 助手需要與各種資料來源和工具進行互動，以提供更豐富和個性化的服務。Model Context Protocol（MCP）作為一種開放標準，為 AI 應用提供了一種統一且安全的方式，連接到不同的資料來源和工具。\n本場演講將介紹 MCP 的架構、設計原則與實作範例，並展示如何使用開源 mcp-server 快速打造一套具備上下文共享、工具調用與多模型協作能力的 Agent Server。最後將透過實機 Demo 展現 MCP 在真實 AI Workflow 中的應用潛力。\n演講大綱\n問題背景與動機 AI 助手在實際應用中面臨的挑戰：需要訪問多種資料來源和工具，現有整合方式的限制：開發成本高、維護困難 認識 Model Context Protocol（MCP）MCP 的定義與目標 MCP 的核心架構：主機、客戶端、伺服器 MCP 如何簡化 AI 應用與外部資源的整合 MCP 的工作原理 MCP 如何建立 AI 應用與資料來源/工具之間的橋樑 MCP 的模組化設計如何支持功能擴展 使用 mcp-server 快速建立多工 Agent Server mcp-server 的功能與架構 如何使用 mcp-server 整合多個 Agent 和工具 實作示範：建立一個能夠協作完成任務的多 Agent 系統 實際應用案例與未來展望 MCP 在企業助手、開發工具等領域的應用 MCP 的安全性與擴展性 未來 AI 系統與 MCP 的整合趨勢 Target group Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\nReferences https://arxiv.org/pdf/2504.16736v2 ","permalink":"https://chechia.net/posts/2025-08-09-coscup/","summary":"\u003ch3 id=\"-活動時間2025-08-09t124500z\"\u003e📅 活動時間：2025-08-09T12:45:00Z\u003c/h3\u003e\n\u003ch3 id=\"-活動連結\"\u003e🔗 \u003ca href=\"https://coscup.org/2025/\"\u003e活動連結\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-聯繫我-facebook\"\u003e📘 聯繫我 \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-投影片wip\"\u003e📑 投影片WIP\u003c/h3\u003e\n\u003chr\u003e\n\u003ch1 id=\"info\"\u003eInfo\u003c/h1\u003e\n\u003cp\u003eModel Context Protocol（MCP）是一項由 Anthropic 推出的開放標準，旨在為大型語言模型（LLMs）提供一種標準化的方式，以連接和操作各種資料來源（如本地檔案、資料庫）和工具（如 GitHub、Google Maps）。MCP 的目標是簡化 AI 應用與外部資源的整合過程，類似於 USB-C 為實體設備提供通用連接介面。\u003c/p\u003e\n\u003cp\u003e隨著 AI 技術的快速發展，AI 助手需要與各種資料來源和工具進行互動，以提供更豐富和個性化的服務。Model Context Protocol（MCP）作為一種開放標準，為 AI 應用提供了一種統一且安全的方式，連接到不同的資料來源和工具。\u003c/p\u003e\n\u003cp\u003e本場演講將介紹 MCP 的架構、設計原則與實作範例，並展示如何使用開源 mcp-server 快速打造一套具備上下文共享、工具調用與多模型協作能力的 Agent Server。最後將透過實機 Demo 展現 MCP 在真實 AI Workflow 中的應用潛力。\u003c/p\u003e\n\u003cp\u003e演講大綱\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e問題背景與動機\n\u003cul\u003e\n\u003cli\u003eAI 助手在實際應用中面臨的挑戰：需要訪問多種資料來源和工具，現有整合方式的限制：開發成本高、維護困難\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e認識 Model Context Protocol（MCP）MCP 的定義與目標\n\u003cul\u003e\n\u003cli\u003eMCP 的核心架構：主機、客戶端、伺服器\u003c/li\u003e\n\u003cli\u003eMCP 如何簡化 AI 應用與外部資源的整合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMCP 的工作原理\n\u003cul\u003e\n\u003cli\u003eMCP 如何建立 AI 應用與資料來源/工具之間的橋樑\u003c/li\u003e\n\u003cli\u003eMCP 的模組化設計如何支持功能擴展\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e使用 mcp-server 快速建立多工 Agent Server\n\u003cul\u003e\n\u003cli\u003emcp-server 的功能與架構\u003c/li\u003e\n\u003cli\u003e如何使用 mcp-server 整合多個 Agent 和工具\u003c/li\u003e\n\u003cli\u003e實作示範：建立一個能夠協作完成任務的多 Agent 系統\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e實際應用案例與未來展望\n\u003cul\u003e\n\u003cli\u003eMCP 在企業助手、開發工具等領域的應用\u003c/li\u003e\n\u003cli\u003eMCP 的安全性與擴展性\u003c/li\u003e\n\u003cli\u003e未來 AI 系統與 MCP 的整合趨勢\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"target-group\"\u003eTarget group\u003c/h1\u003e\n\u003ch1 id=\"author\"\u003eAuthor\u003c/h1\u003e\n\u003cp\u003eChe-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\u003c/p\u003e","title":"COSCUP: 從 Model Context Protocol 初探 AI Agent Protocol：快速打造多工 Agent Server"},{"content":"📅 活動時間: 2025-07-03T12:45:00Z 🔗 活動連結 📘 聯繫我 Facebook 📑 投影片 WIP Info 開發團隊需要將經驗與知識，透過文件化的方式保存下來，以便未來查詢與學習。然而，企業內部文件往往分散於 Confluence、Google Drive、Notion 等平台，傳統關鍵字搜尋難以快速獲取準確資訊，導致溝通成本高、開發流程受阻。又或是，當開發團隊需要查詢特定知識時，往往需要透過 Slack、Email 等方式詢問同事，這樣的溝通成本不僅浪費時間，也容易造成資訊不對稱。\n本演講將介紹如何運用 RAG（Retrieval-Augmented Generation）技術，結合 OpenAI 及向量數據庫，將企業內部文檔轉為智能知識庫。\n我們將探討文件解析、嵌入索引、AI 問答系統的技術架構與實作，幫助開發團隊構建高效 AI 助手，節省溝通成本，加速開發流程，提升決策與問題解決能力。\n大綱：\n為什麼企業知識管理難？ 文件分散（Confluence、Google Drive、Notion、SharePoint） 搜索體驗不佳，開發者找不到關鍵資訊 知識難以沉澱，員工流失即知識流失 RAG 如何解決這些問題？ 透過檢索增強生成（Retrieval-Augmented Generation）提升答案準確性 把內部文件轉為可查詢的知識庫 讓 AI 提供具體、準確、即時的回覆 技術架構與實作指南 文件解析與嵌入（HTML、Markdown、Confluence API） 使用向量數據庫實現高效檢索 OpenAI API + LangChain 打造智能問答系統 案例分享與落地經驗 Target group Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\n","permalink":"https://chechia.net/posts/2025-07-02-cloud-summit/","summary":"\u003ch3 id=\"-活動時間-2025-07-03t124500z\"\u003e📅 活動時間: 2025-07-03T12:45:00Z\u003c/h3\u003e\n\u003ch3 id=\"-活動連結\"\u003e🔗 \u003ca href=\"https://cloudsummit.ithome.com.tw/2025/session-page/3668\"\u003e活動連結\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-聯繫我-facebook\"\u003e📘 聯繫我 \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-投影片-wip\"\u003e📑 \u003ca href=\"/slides/2025-07-02-cloud-summit-rag/\"\u003e投影片 WIP\u003c/a\u003e\u003c/h3\u003e\n\u003chr\u003e\n\u003ch2 id=\"info\"\u003eInfo\u003c/h2\u003e\n\u003cp\u003e開發團隊需要將經驗與知識，透過文件化的方式保存下來，以便未來查詢與學習。然而，企業內部文件往往分散於 Confluence、Google Drive、Notion 等平台，傳統關鍵字搜尋難以快速獲取準確資訊，導致溝通成本高、開發流程受阻。又或是，當開發團隊需要查詢特定知識時，往往需要透過 Slack、Email 等方式詢問同事，這樣的溝通成本不僅浪費時間，也容易造成資訊不對稱。\u003c/p\u003e\n\u003cp\u003e本演講將介紹如何運用 RAG（Retrieval-Augmented Generation）技術，結合 OpenAI 及向量數據庫，將企業內部文檔轉為智能知識庫。\u003c/p\u003e\n\u003cp\u003e我們將探討文件解析、嵌入索引、AI 問答系統的技術架構與實作，幫助開發團隊構建高效 AI 助手，節省溝通成本，加速開發流程，提升決策與問題解決能力。\u003c/p\u003e\n\u003cp\u003e大綱：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e為什麼企業知識管理難？\n\u003cul\u003e\n\u003cli\u003e文件分散（Confluence、Google Drive、Notion、SharePoint）\u003c/li\u003e\n\u003cli\u003e搜索體驗不佳，開發者找不到關鍵資訊\u003c/li\u003e\n\u003cli\u003e知識難以沉澱，員工流失即知識流失\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRAG 如何解決這些問題？\n\u003cul\u003e\n\u003cli\u003e透過檢索增強生成（Retrieval-Augmented Generation）提升答案準確性\u003c/li\u003e\n\u003cli\u003e把內部文件轉為可查詢的知識庫\u003c/li\u003e\n\u003cli\u003e讓 AI 提供具體、準確、即時的回覆\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e技術架構與實作指南\n\u003cul\u003e\n\u003cli\u003e文件解析與嵌入（HTML、Markdown、Confluence API）\u003c/li\u003e\n\u003cli\u003e使用向量數據庫實現高效檢索\u003c/li\u003e\n\u003cli\u003eOpenAI API + LangChain 打造智能問答系統\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e案例分享與落地經驗\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"target-group\"\u003eTarget group\u003c/h2\u003e\n\u003ch2 id=\"author\"\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eChe-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\u003c/p\u003e\n\u003cp\u003e活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\u003c/p\u003e\n\u003cp\u003e個人部落格：https://chechia.net\u003c/p\u003e","title":"Cloud Summit 2025: 用 RAG 打造企業可對話 AI 知識庫《有問題問過 AI 後再來問我》"},{"content":"📅 活動時間: 2025-06-05T12:45:00Z 🔗 活動連結 📘 聯繫我 Facebook 📑 投影片 RAG workshop 行前通知 本次 workshop 以 hands-on 的方式進行，累積操作經驗為主，講解與說明為輔。觀念內容有準備教材，需要參與者自行閱讀。講師會免費提供\nAzure OpenAI models API key (gpt-4.1, gpt-4.1-mini, text-embedding-3, text-embedding-ada-002\u0026hellip;) Azure VM 供同學遠端操作使用 \u0026ndash;\u0026gt; 投影片與教材 \u0026lt;\u0026ndash; Workshop 基本需求 有自己的電腦，可以上網 選項1: 使用自己的電腦，在 docker 啟動開發環境 選項2: 使用自己的電腦，遠端連線講師提供的 VM，在VM 中啟動 docker 開發環境 會使用 docker 會使用 python 與 jupyter notebook 會使用 chatgpt.com 協助除錯 選項1: 使用自己的電腦 在 workshop 開始前，在自己的電腦上\n安裝 docker git clone github repository 啟動 docker 開發環境，下載 docker images 開啟瀏覽器，連線到 Jupyter Notebook，token workshop1234! 在 Jupyter Notebook 中，安裝所需的 Python 套件 git clone https://github.com/chechiachang/rag-workshop.git cd rag-workshop docker compose up -d docker exec -it notebook pip install pandas openai qdrant_client tqdm tenacity wget tenacity unstructured markdown ragas sacrebleu langchain_qdrant langchain-openai langchain_openai langchain_community tiktoken 選項2: 使用遠端 VM 建議使用個人電腦，畢竟免費 VM 名額現場有限 需要有自己的電腦，有穩定的網路，可以連線到遠端 VM 需要註冊 tunnel 工具（沒有業配）ngrok 登入 Login -\u0026gt; 左手邊 Identity \u0026amp; Access -\u0026gt; Authtokens -\u0026gt; Add Tunnel authtoken -\u0026gt; 記在安全的地方 也可以使用 pinggy，但免費有限時 投影片與教材會放在網站上 https://chechia.net/zh-hant/slides/2025-06-05-devops-rag-internal-ai/\n如何存取 VM 也會放在敨影片裡\nRAG workshop 行前通知完，大家當天現場見 Info 開發團隊需要將經驗與知識，透過文件化的方式保存下來，以便未來查詢與學習。然而，企業內部文件往往分散於 Confluence、Google Drive、Notion 等平台，傳統關鍵字搜尋難以快速獲取準確資訊，導致溝通成本高、開發流程受阻。又或是，當開發團隊需要查詢特定知識時，往往需要透過 Slack、Email 等方式詢問同事，這樣的溝通成本不僅浪費時間，也容易造成資訊不對稱。\n學員將學會如何利用 RAG 技術，結合 OpenAI、LangChain、Qdrant 向量數據庫，構建企業內部文檔的智能知識庫，並能設計與實作一個基於自然語言處理（NLP）的查詢系統，來提升開發團隊的效率與知識管理能力。\n什麼是 RAG？ 介紹 RAG（Retrieval-Augmented Generation）技術如何結合檢索與生成提升問答準確性。 為什麼企業需要智能知識庫？ RAG 的應用場景：知識庫建立、開發支援、技術決策等。 Python 與 OpenAI： 使用 OpenAI API 架構與語言模型，快速構建生成模型 LangChain：介紹 LangChain 框架如何簡化 RAG 實現 如何利用 LangChain 整合多種數據源與生成模型 Qdrant 向量數據庫： 向量數據庫的概念與作用 如何利用 Qdrant 儲存與檢索內部文檔的嵌入向量 構建 RAG 系統：實作步驟與演示 數據預處理與文檔轉換 將處理後的文本轉換為向量表示（embedding） 利用 Qdrant 向量數據庫儲存這些嵌入並執行檢索 使用 LangChain 結合 OpenAI 模型與 Qdrant，製作自動化問答系統 Target group 本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來實作基本 RAG AI Agent。參與者必備個人筆電，透過 SSH 操控遠端機器。\n必備知識：Linux 操作基本知識，Docker 操作基本知識，會使用 SSH 連線 / Bash / docker。\n工作坊結束後，學員將能夠：\n理解並實作 RAG 技術，將內部文檔轉化為智能知識庫 使用 Python、LangChain 和 OpenAI 構建基於檢索的問答系統 利用 Qdrant 向量數據庫進行高效檢索，提升開發流程中的知識管理效率。 這樣的工作坊結構能夠平衡理論與實踐，並為學員提供實際動手操作的機會。 Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\n","permalink":"https://chechia.net/posts/2025-06-06-devops-rag-internal-ai/","summary":"\u003ch3 id=\"-活動時間-2025-06-05t124500z\"\u003e📅 活動時間: 2025-06-05T12:45:00Z\u003c/h3\u003e\n\u003ch3 id=\"-活動連結\"\u003e🔗 \u003ca href=\"https://devopsdays.tw/2025/workshop-page/3788\"\u003e活動連結\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-聯繫我-facebook\"\u003e📘 聯繫我 \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"-投影片\"\u003e📑 \u003ca href=\"/slides/2025-06-05-devops-rag-internal-ai/\"\u003e投影片\u003c/a\u003e\u003c/h3\u003e\n\u003chr\u003e\n\u003ch2 id=\"rag-workshop-行前通知\"\u003eRAG workshop 行前通知\u003c/h2\u003e\n\u003cp\u003e本次 workshop 以 hands-on 的方式進行，累積操作經驗為主，講解與說明為輔。觀念內容有準備教材，需要參與者自行閱讀。講師會免費提供\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAzure OpenAI models API key (gpt-4.1, gpt-4.1-mini, text-embedding-3, text-embedding-ada-002\u0026hellip;)\u003c/li\u003e\n\u003cli\u003eAzure VM 供同學遠端操作使用\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"-投影片與教材-\"\u003e\u0026ndash;\u0026gt; \u003ca href=\"/slides/2025-06-05-devops-rag-internal-ai/\"\u003e投影片與教材\u003c/a\u003e \u0026lt;\u0026ndash;\u003c/h1\u003e\n\u003chr\u003e\n\u003ch3 id=\"workshop-基本需求\"\u003eWorkshop 基本需求\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e有自己的電腦，可以上網\u003c/li\u003e\n\u003cli\u003e選項1: 使用自己的電腦，在 docker 啟動開發環境\u003c/li\u003e\n\u003cli\u003e選項2: 使用自己的電腦，遠端連線講師提供的 VM，在VM 中啟動 docker 開發環境\u003c/li\u003e\n\u003cli\u003e會使用 docker\u003c/li\u003e\n\u003cli\u003e會使用 python 與 jupyter notebook\u003c/li\u003e\n\u003cli\u003e會使用 chatgpt.com 協助除錯\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"選項1-使用自己的電腦\"\u003e選項1: 使用自己的電腦\u003c/h3\u003e\n\u003cp\u003e在 workshop 開始前，在自己的電腦上\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e安裝 docker\u003c/li\u003e\n\u003cli\u003egit clone github repository\u003c/li\u003e\n\u003cli\u003e啟動 docker 開發環境，下載 docker images\u003c/li\u003e\n\u003cli\u003e開啟瀏覽器，連線到 Jupyter Notebook，token \u003ccode\u003eworkshop1234!\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e在 Jupyter Notebook 中，安裝所需的 Python 套件\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003egit clone https://github.com/chechiachang/rag-workshop.git\n\ncd rag-workshop\n\ndocker compose up -d\n\ndocker exec -it notebook pip install pandas openai qdrant_client tqdm tenacity wget tenacity unstructured markdown ragas sacrebleu langchain_qdrant langchain-openai langchain_openai langchain_community tiktoken\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch3 id=\"選項2-使用遠端-vm\"\u003e選項2: 使用遠端 VM\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e建議使用個人電腦，畢竟免費 VM 名額現場有限\u003c/li\u003e\n\u003cli\u003e需要有自己的電腦，有穩定的網路，可以連線到遠端 VM\u003c/li\u003e\n\u003cli\u003e需要註冊 tunnel 工具（沒有業配）\u003ca href=\"https://dashboard.ngrok.com/login\"\u003engrok\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e登入 Login -\u0026gt; 左手邊 Identity \u0026amp; Access -\u0026gt; Authtokens -\u0026gt; Add Tunnel authtoken -\u0026gt; 記在安全的地方\u003c/li\u003e\n\u003cli\u003e也可以使用 \u003ca href=\"https://pinggy.io/\"\u003epinggy\u003c/a\u003e，但免費有限時\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch3 id=\"投影片與教材會放在網站上\"\u003e投影片與教材會放在網站上\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://chechia.net/zh-hant/slides/2025-06-05-devops-rag-internal-ai/\"\u003ehttps://chechia.net/zh-hant/slides/2025-06-05-devops-rag-internal-ai/\u003c/a\u003e\u003c/p\u003e","title":"Workshop: DevOpsDay 2025: RAG打造企業AI知識庫：把一甲子功力傳給新人"},{"content":" 活動時間: 2023-10-25T13:20:00Z 活動連結 Facebook Twitter 投影片 Presentation Upgrade A VM Based Clustermin\nTarget group 收穫 本次演講會講解升級操作，但不侷限在具體步驟，而是希望能講解更多 k8s 架構與設計，讓觀眾有以下收穫：如何升級 Kubernetes Cluster 的版本，升級時應考量的事項有哪些，有什麼工具可以協助升級流程，透過升級更了解 Kubernetes 的架構\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2024 Cloud Summit 2024 SRE Conference 2023 DevOpsDay Taipei 2023 Kubernetes Summit 2022 COSCUP 2022 Cloud Summit 2021 Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Cloud Summit 2019 Cloud Summit 2018 Cloud Summit 2018 Kubernetes Summit ","permalink":"https://chechia.net/posts/2024-10-23-k8s-summit/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2023-10-25T13:20:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://k8s.ithome.com.tw/2023/session-page/2331\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2024-10-23-upgrade-k8s-cluster/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"presentation\"\u003ePresentation\u003c/h1\u003e\n\u003cp\u003eUpgrade A VM Based Clustermin\u003c/p\u003e\n\u003ch3 id=\"target-group-收穫\"\u003eTarget group 收穫\u003c/h3\u003e\n\u003cp\u003e本次演講會講解升級操作，但不侷限在具體步驟，而是希望能講解更多 k8s 架構與設計，讓觀眾有以下收穫：如何升級 Kubernetes Cluster 的版本，升級時應考量的事項有哪些，有什麼工具可以協助升級流程，透過升級更了解 Kubernetes 的架構\u003c/p\u003e\n\u003ch1 id=\"author\"\u003eAuthor\u003c/h1\u003e\n\u003cp\u003eChe-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。\nMicrosoft 最有價值從業人員 MVP。\u003c/p\u003e\n\u003cp\u003e目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\u003c/p\u003e\n\u003cp\u003eChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup.\nMicrosoft Most Valuable Professional since 2020.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2024 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2024 SRE Conference\u003c/li\u003e\n\u003cli\u003e2023 DevOpsDay Taipei\u003c/li\u003e\n\u003cli\u003e2023 Kubernetes Summit\u003c/li\u003e\n\u003cli\u003e2022 COSCUP\u003c/li\u003e\n\u003cli\u003e2022 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2021 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform\u003c/li\u003e\n\u003cli\u003e2020 Cloud Native Taiwan 年末聚會\u003c/li\u003e\n\u003cli\u003e2020 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2019 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2018 Cloud Summit\u003c/li\u003e\n\u003cli\u003e2018 Kubernetes Summit\u003c/li\u003e\n\u003c/ul\u003e","title":"Kubernetes Summit: Upgrade A VM Based Cluster"},{"content":" 活動時間: 2024-10-23T13:20:00Z 活動連結 Facebook Twitter 投影片 工作坊內容 兩個場次\n2024-10-23T13:20-14:50 @ 603+604 2024-10-24T13:20-14:50 @ 605 Get Started 請點擊本頁上方投影片連結 Git clone 本次 workshop 資源 https://github.com/chechiachang/etcd-playground 活動當天現場，會提供一台 Linux VM 做使用 Workshop Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s\nOutline Etcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。\n預計內容：環境設定，Etcd 設定與部署，Etcd 基礎操作，部署 kube-apiserver / kube-controller-manager / kube-scheduler，使用 kubectl 操作 Kubernetes Cluster。讓參與者透過本次工作坊，可以有操作 k8s control plane 的經驗，更了解 Etcd 的基本操作，以及了解 Kubernetes 的基本架構。\n（規劃中）本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來了解 Etcd 的基本操作。參與者必備個人筆電，透過 SSH 操控遠端機器。\n必備知識：Linux 操作基本知識，Docker 操作基本知識，會使用 SSH 連線 / Bash / docker。 工作坊時間不多，現場不會細講概念問題，參與者需要事前預習基本概念 https://etcd.io/ 與 http://play.etcd.io/play\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n","permalink":"https://chechia.net/posts/2024-10-24-k8s-summit/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2024-10-23T13:20:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://k8s.ithome.com.tw/2024/workshop-page/3259\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2024-10-24-etcd-workshop/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"工作坊內容\"\u003e工作坊內容\u003c/h2\u003e\n\u003cp\u003e兩個場次\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2024-10-23T13:20-14:50 @ 603+604\u003c/li\u003e\n\u003cli\u003e2024-10-24T13:20-14:50 @ 605\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"get-started\"\u003eGet Started\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e請點擊本頁上方投影片連結\u003c/li\u003e\n\u003cli\u003eGit clone 本次 workshop 資源 \u003ca href=\"https://github.com/chechiachang/etcd-playground\"\u003ehttps://github.com/chechiachang/etcd-playground\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e活動當天現場，會提供一台 Linux VM 做使用\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2 id=\"workshop\"\u003eWorkshop\u003c/h2\u003e\n\u003cp\u003eGet started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s\u003c/p\u003e\n\u003ch3 id=\"outline\"\u003eOutline\u003c/h3\u003e\n\u003cp\u003eEtcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。\u003c/p\u003e\n\u003cp\u003e預計內容：環境設定，Etcd 設定與部署，Etcd 基礎操作，部署 kube-apiserver / kube-controller-manager / kube-scheduler，使用 kubectl 操作 Kubernetes Cluster。讓參與者透過本次工作坊，可以有操作 k8s control plane 的經驗，更了解 Etcd 的基本操作，以及了解 Kubernetes 的基本架構。\u003c/p\u003e\n\u003cp\u003e（規劃中）本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來了解 Etcd 的基本操作。參與者必備個人筆電，透過 SSH 操控遠端機器。\u003c/p\u003e","title":"Workshop: Kubernetes Summit: Get started with Etcd \u0026 Kubernetes"},{"content":" 活動時間: 2024-08-28T11:00:00Z 活動連結 Facebook Twitter 投影片 Info 資料庫管理是一個很大的議題：如何管理資料庫的帳號密碼，如何精確的用戶設定權限，傳遞密碼給用戶，並自動化定期更新密碼。本場演講將分享如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫\n涵蓋以下內容：\ndatabase 帳號密碼管理的難題 簡介 Vault 與 database secret engine 在 vault 中設定 database secret engine vault 在需要時自動產生資料庫帳號密碼 vault 透過安全來源認證 app 身份(使用 k8s service account 與 public cloud 認證(aws iam role)) 完成 app 連線至 database 的工作週期 monitoring / audit：vault audit log + prometheus / grafana dashboard / alert manager 範例：如何使用 terraform 設定 vault 與 database secret engine Target group 有資料庫管理需求的工程師，想要學習如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫。\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","permalink":"https://chechia.net/posts/2024-08-28-hashicorp-vault-database/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2024-08-28T11:00:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.omniwaresoft.com.tw/all-events/aicloud-webinar-20240814-0828/\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2024-08-28-hashicorp-vault-database/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"info\"\u003eInfo\u003c/h1\u003e\n\u003cp\u003e資料庫管理是一個很大的議題：如何管理資料庫的帳號密碼，如何精確的用戶設定權限，傳遞密碼給用戶，並自動化定期更新密碼。本場演講將分享如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫\u003c/p\u003e\n\u003cp\u003e涵蓋以下內容：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edatabase 帳號密碼管理的難題\u003c/li\u003e\n\u003cli\u003e簡介 Vault 與 database secret engine\u003c/li\u003e\n\u003cli\u003e在 vault 中設定 database secret engine\u003c/li\u003e\n\u003cli\u003evault 在需要時自動產生資料庫帳號密碼\u003c/li\u003e\n\u003cli\u003evault 透過安全來源認證 app 身份(使用 k8s service account 與 public cloud 認證(aws iam role))\u003c/li\u003e\n\u003cli\u003e完成 app 連線至 database 的工作週期\u003c/li\u003e\n\u003cli\u003emonitoring / audit：vault audit log + prometheus / grafana dashboard / alert manager\u003c/li\u003e\n\u003cli\u003e範例：如何使用 terraform 設定 vault 與 database secret engine\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"target-group\"\u003eTarget group\u003c/h1\u003e\n\u003cp\u003e有資料庫管理需求的工程師，想要學習如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫。\u003c/p\u003e","title":"Hashicorp: managed database credentials with Hashicorp Vault"},{"content":" 活動時間: 2024-07-03 活動連結 Facebook Twitter 投影片 Info Title: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\n使用 AWS 與 Kubernetes 就是要花錢，如何省錢？ 合理的設定資源，不僅省錢，還提升整體服務穩定度？ 既有服務已經存在，如何逐步改變，降低成本？\n本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan\u0026hellip;，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\nTarget group AWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\n導入工具：有哪些工具可以使用 分析現況：精算各團隊與各專案成本 改變現況：從既有的架構中，找尋可以節省成本的地方 改善流程：有效率的維持省錢 workflow 改變文化：持續性低維持新服務與舊服務的合理成本 Author Che-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\n活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\n個人部落格：https://chechia.net\nChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\nActively engaged in the Taiwanese tech community, he frequently shares insights on DevOps, SRE, Kubernetes, and cloud computing at CNTUG, DevOps Taipei, GDG Taipei, and Golang Taipei Meetup. Passionate about promoting best practices in development and operations, he continuously explores and applies the latest advancements in cloud and AI technologies.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","permalink":"https://chechia.net/posts/2024-07-03-cloud-summit/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2024-07-03\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cloudsummit.ithome.com.tw/2024/session-page/2620\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2024-07-03-saving-money-on-cloud-k8s/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"info\"\u003eInfo\u003c/h2\u003e\n\u003cp\u003eTitle: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\u003c/p\u003e\n\u003cp\u003e使用 AWS 與 Kubernetes 就是要花錢，如何省錢？\n合理的設定資源，不僅省錢，還提升整體服務穩定度？\n既有服務已經存在，如何逐步改變，降低成本？\u003c/p\u003e\n\u003cp\u003e本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan\u0026hellip;，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\u003c/p\u003e\n\u003ch2 id=\"target-group\"\u003eTarget group\u003c/h2\u003e\n\u003cp\u003eAWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e導入工具：有哪些工具可以使用\u003c/li\u003e\n\u003cli\u003e分析現況：精算各團隊與各專案成本\u003c/li\u003e\n\u003cli\u003e改變現況：從既有的架構中，找尋可以節省成本的地方\u003c/li\u003e\n\u003cli\u003e改善流程：有效率的維持省錢 workflow\u003c/li\u003e\n\u003cli\u003e改變文化：持續性低維持新服務與舊服務的合理成本\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"author\"\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eChe-Chia Chang 是一名專注於後端開發、開發維運、容器化應用及 Kubernetes 開發與管理的技術專家，同時也是 Microsoft 最有價值專業人士（MVP）。\u003c/p\u003e\n\u003cp\u003e活躍於台灣技術社群，經常在 CNTUG、DevOps Taipei、GDG Taipei、Golang Taipei Meetup 等社群分享 DevOps、SRE、Kubernetes 及雲端運算相關技術。致力於推動開發與維運的最佳實踐，並熱衷於研究與應用最新的雲端與 AI 技術。\u003c/p\u003e\n\u003cp\u003e個人部落格：https://chechia.net\u003c/p\u003e\n\u003cp\u003eChe-Chia Chang is a technology expert specializing in backend development, DevOps, site reliability engineering (SRE), containerized applications, and Kubernetes development and management. He is also recognized as a Microsoft Most Valuable Professional (MVP).\u003c/p\u003e","title":"Cloud Summit: Cloud Infrastructure Saving Engineering  雲端省錢工程"},{"content":"Get-Started Google \u0026ldquo;how to contribute to kubernetes\u0026rdquo; click the first link you like in every page do what the page says Get-Started https://www.kubernetes.dev https://www.kubernetes.dev/docs/guide/ Contributor Playground Youtube: New Contributor Series 2018-2019 Join the Kubernetes Slack Contribute to kubernetes Documentation Join Group https://www.kubernetes.dev/community/community-groups/ SIG-DOCS Find Issue on Github https://github.com/kubernetes/website/issues/38681 Contributing to Kubernetes Documentation Join sig-docs google groupd Prerequisites In my opinion, the following are the prerequisites to contribute to Kubernetes:\nEnglish proficiency Unorganized Links Community-membership actively contributer Community Forums ","permalink":"https://chechia.net/posts/2024-04-27-contribute-to-k8s/","summary":"\u003ch1 id=\"get-started\"\u003eGet-Started\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle \u0026ldquo;how to contribute to kubernetes\u0026rdquo;\u003c/li\u003e\n\u003cli\u003eclick the first link you like in every page\u003c/li\u003e\n\u003cli\u003edo what the page says\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"get-started-1\"\u003eGet-Started\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.kubernetes.dev\"\u003ehttps://www.kubernetes.dev\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.kubernetes.dev/docs/guide/\"\u003ehttps://www.kubernetes.dev/docs/guide/\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes-sigs/contributor-playground/blob/master/README.md\"\u003eContributor Playground\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/playlist?list=PL69nYSiGNLP3M5X7stuD7N4r3uP2PZQUx\"\u003eYoutube: New Contributor Series 2018-2019\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://communityinviter.com/apps/kubernetes/community\"\u003eJoin the Kubernetes Slack\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://kubernetes.io/docs/contribute/docs/\"\u003eContribute to kubernetes Documentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"join-group\"\u003eJoin Group\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.kubernetes.dev/community/community-groups/\"\u003ehttps://www.kubernetes.dev/community/community-groups/\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/community/blob/master/sig-docs/README.md\"\u003eSIG-DOCS\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"find-issue-on-github\"\u003eFind Issue on Github\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/website/issues/38681\"\u003ehttps://github.com/kubernetes/website/issues/38681\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/website/blob/36fa877ba3be67cb0a8c508757bdc8849cbdb5af/CONTRIBUTING.md\"\u003eContributing to Kubernetes Documentation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://groups.google.com/g/kubernetes-sig-docs\"\u003eJoin sig-docs google groupd\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"prerequisites\"\u003ePrerequisites\u003c/h1\u003e\n\u003cp\u003eIn my opinion, the following are the prerequisites to contribute to Kubernetes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEnglish proficiency\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"unorganized-links\"\u003eUnorganized Links\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/kubernetes/community/blob/master/community-membership.md#member\"\u003eCommunity-membership\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eactively contributer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://discuss.kubernetes.io/\"\u003eCommunity Forums\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"2024 04 27 Contribute to K8s"},{"content":" 活動時間: 2024-04-26 活動連結 Facebook Twitter 投影片 Info Title: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\n使用 AWS 與 Kubernetes 就是要花錢，如何省錢？ 合理的設定資源，不僅省錢，還提升整體服務穩定度？ 既有服務已經存在，如何逐步改變，降低成本？\n本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan\u0026hellip;，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\nTarget group AWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\n導入工具：有哪些工具可以使用 分析現況：精算各團隊與各專案成本 改變現況：從既有的架構中，找尋可以節省成本的地方 改善流程：有效率的維持省錢 workflow 改變文化：持續性低維持新服務與舊服務的合理成本 Author Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","permalink":"https://chechia.net/posts/2024-04-26-sre-conference/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2024-04-26\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sre.ithome.com.tw/2024/session-page/2548\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2024-04-26-saving-money-on-cloud-k8s/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"info\"\u003eInfo\u003c/h2\u003e\n\u003cp\u003eTitle: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\u003c/p\u003e\n\u003cp\u003e使用 AWS 與 Kubernetes 就是要花錢，如何省錢？\n合理的設定資源，不僅省錢，還提升整體服務穩定度？\n既有服務已經存在，如何逐步改變，降低成本？\u003c/p\u003e\n\u003cp\u003e本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan\u0026hellip;，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\u003c/p\u003e\n\u003ch2 id=\"target-group\"\u003eTarget group\u003c/h2\u003e\n\u003cp\u003eAWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e導入工具：有哪些工具可以使用\u003c/li\u003e\n\u003cli\u003e分析現況：精算各團隊與各專案成本\u003c/li\u003e\n\u003cli\u003e改變現況：從既有的架構中，找尋可以節省成本的地方\u003c/li\u003e\n\u003cli\u003e改善流程：有效率的維持省錢 workflow\u003c/li\u003e\n\u003cli\u003e改變文化：持續性低維持新服務與舊服務的合理成本\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"author\"\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eChe-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。\nMicrosoft 最有價值從業人員 MVP。\u003c/p\u003e\n\u003cp\u003e目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\u003c/p\u003e\n\u003cp\u003eChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup.\nMicrosoft Most Valuable Professional since 2020.\u003c/p\u003e","title":"SRE Conference: Cloud Infrastructure Saving Engineering  雲端省錢工程"},{"content":" 活動時間: 2023-10-25T13:20:00Z 活動連結 Facebook Twitter 投影片 Info Title: Resource as Code for Kubernetes: stop kubectl apply\nhttps://k8s.ithome.com.tw/CFP\nInfrastrure as Code (IaC) 與 PaC，在萬物都該 as Code 得時代，你還在不斷的 kubectl apply 嗎？\n手動 apply 的痛點： 人就是會忘：是誰 apply 這個在 k8s 上的？是誰上次漏 apply 所以壞了？ 人就是會寫錯：能否 apply 管理大量的 label, taint, annotation 安全：apply 變更內容是否有經過資訊安全的 review 當服務的 app code base 都已經用 chart 打包，使用 vcs 管理後，為何依賴的 k8s resource (namespace, secret, label, crd, \u0026hellip;) 不需要推上 vcs 管理的？\n本次演講集合幾個管理 k8s 的範例，將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator, \u0026hellip; 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率\ntarget group Kubernetes User who want to increase performance in k8s management\n","permalink":"https://chechia.net/posts/2023-10-25-k8s-summit/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2023-10-25T13:20:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://k8s.ithome.com.tw/2023/session-page/2331\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2023-10-25-k8s-resource-as-code/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"info\"\u003eInfo\u003c/h2\u003e\n\u003cp\u003eTitle: Resource as Code for Kubernetes: stop kubectl apply\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://k8s.ithome.com.tw/CFP\"\u003ehttps://k8s.ithome.com.tw/CFP\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eInfrastrure as Code (IaC) 與 PaC，在萬物都該 as Code 得時代，你還在不斷的 kubectl apply 嗎？\u003c/p\u003e\n\u003ch2 id=\"手動-apply-的痛點\"\u003e手動 apply 的痛點：\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e人就是會忘：是誰 apply 這個在 k8s 上的？是誰上次漏 apply 所以壞了？\u003c/li\u003e\n\u003cli\u003e人就是會寫錯：能否 apply\u003c/li\u003e\n\u003cli\u003e管理大量的 label, taint, annotation\u003c/li\u003e\n\u003cli\u003e安全：apply 變更內容是否有經過資訊安全的 review\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e當服務的 app code base 都已經用 chart 打包，使用 vcs 管理後，為何依賴的 k8s resource (namespace, secret, label, crd, \u0026hellip;) 不需要推上 vcs 管理的？\u003c/p\u003e\n\u003cp\u003e本次演講集合幾個管理 k8s 的範例，將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator, \u0026hellip; 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率\u003c/p\u003e","title":"Kubernetes Summit: Resource as Code for Kubernetes: Stop kubectl apply"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay ??: Integrated Backend 集成存儲\nVault 支援多種存儲選項，用於持久存儲 Vault 資訊。自 Vault 1.4 起，提供了整合式存儲選項。此存儲後端不依賴於任何第三方系統，實現高可用性語義，支援企業複製功能，並提供備份/還原工作流。\n該選項將 Vault 的數據存儲在服務器的文件系統上，並使用共識協議將數據複製到集群中的每個服務器。有關整合式存儲內部的更多信息，請參閱整合式存儲內部文檔。此外，配置文檔可以幫助配置 Vault 以使用整合式存儲。\n以下各節將詳細介紹如何使用整合式存儲操作 Vault。\n服務器間通信 一旦節點加入到彼此，它們開始使用 Vault 的叢集端口進行 mTLS 通信。叢集端口的默認值為 8201。TLS 信息在加入時交換，並按一定的節奏進行輪換。\n整合式存儲的要求之一是必須設置 cluster_addr 配置選項。這允許 Vault 在加入時為節點 ID 分配地址。\n叢集成員資格 本節將概述如何引導和管理運行整合式存儲的 Vault 節點集群。\n整合式存儲在初始化過程中引導，並且結果是大小為 1 的集群。根據所需的部署大小，可以將節點加入到活動 Vault 節點中。\n加入節點 加入是將未初始化的 Vault 節點並使其成為現有集群成員的過程。為了將新節點驗證到集群，它必須使用相同的密封機制。如果使用自動解封，則必須配置新節點以使用與其嘗試加入的集群相同的 KMS 提供程序和密鑰。如果使用 Shamir 密封，則必須在加入過程完成之前為新節點提供解封密鑰。一旦節點成功加入，來自活動節點的數據就可以開始複制到它。一旦節點加入，則不能重新加入到不同的集群。\n您可以通過配置文件自動加入節點，也可以通過 API 手動加入（下面描述了這兩種方法）。在加入節點時，必須使用領導節點的 API 地址。我們建議在所有節點上設置 api_addr 配置選項，以使加入過程更簡單。\nretry_join 配置 此方法允許在配置文件中設置一個或多個目標領導節點。當未初始化的 Vault 服務器啟動時，它將嘗試加入每個已定義的潛在領導者，直到成功。當指定的領導者之一變為活動狀態時，此節點將成功加入。當使用 Shamir 密封時，已加入的節點仍然需要手動解封。當使用自動解封時，節點將能夠自動加入並自動解封。\n下面是一個示例 retry_join 配置：\nstorage \u0026quot;raft\u0026quot; { path = \u0026quot;/var/raft/\u0026quot; node_id = \u0026quot;node3\u0026quot; retry_join { leader_api_addr = \u0026quot;https://node1.vault.local:8200\u0026quot; } retry_join { leader_api_addr = \u0026quot;https://node2.vault.local:8200\u0026quot; } } storage \u0026quot;raft\u0026quot; { path = \u0026quot;/var/raft/\u0026quot; node_id = \u0026quot;node3\u0026quot; retry_join { auto_join = \u0026quot;provider=aws region=eu-west-1 tag_key=vault tag_value=... access_key_id=... secret_access_key=...\u0026quot; } } chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/concepts/integrated-storage 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-10-10-vault-workshop-integrated-storage/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day--integrated-backend\"\u003eDay ??: Integrated Backend\u003c/h1\u003e\n\u003cp\u003e集成存儲\u003c/p\u003e\n\u003cp\u003eVault 支援多種存儲選項，用於持久存儲 Vault 資訊。自 Vault 1.4 起，提供了整合式存儲選項。此存儲後端不依賴於任何第三方系統，實現高可用性語義，支援企業複製功能，並提供備份/還原工作流。\u003c/p\u003e\n\u003cp\u003e該選項將 Vault 的數據存儲在服務器的文件系統上，並使用共識協議將數據複製到集群中的每個服務器。有關整合式存儲內部的更多信息，請參閱整合式存儲內部文檔。此外，配置文檔可以幫助配置 Vault 以使用整合式存儲。\u003c/p\u003e\n\u003cp\u003e以下各節將詳細介紹如何使用整合式存儲操作 Vault。\u003c/p\u003e\n\u003cp\u003e服務器間通信\n一旦節點加入到彼此，它們開始使用 Vault 的叢集端口進行 mTLS 通信。叢集端口的默認值為 8201。TLS 信息在加入時交換，並按一定的節奏進行輪換。\u003c/p\u003e\n\u003cp\u003e整合式存儲的要求之一是必須設置 cluster_addr 配置選項。這允許 Vault 在加入時為節點 ID 分配地址。\u003c/p\u003e\n\u003cp\u003e叢集成員資格\n本節將概述如何引導和管理運行整合式存儲的 Vault 節點集群。\u003c/p\u003e\n\u003cp\u003e整合式存儲在初始化過程中引導，並且結果是大小為 1 的集群。根據所需的部署大小，可以將節點加入到活動 Vault 節點中。\u003c/p\u003e\n\u003cp\u003e加入節點\n加入是將未初始化的 Vault 節點並使其成為現有集群成員的過程。為了將新節點驗證到集群，它必須使用相同的密封機制。如果使用自動解封，則必須配置新節點以使用與其嘗試加入的集群相同的 KMS 提供程序和密鑰。如果使用 Shamir 密封，則必須在加入過程完成之前為新節點提供解封密鑰。一旦節點成功加入，來自活動節點的數據就可以開始複制到它。一旦節點加入，則不能重新加入到不同的集群。\u003c/p\u003e\n\u003cp\u003e您可以通過配置文件自動加入節點，也可以通過 API 手動加入（下面描述了這兩種方法）。在加入節點時，必須使用領導節點的 API 地址。我們建議在所有節點上設置 api_addr 配置選項，以使加入過程更簡單。\u003c/p\u003e\n\u003cp\u003eretry_join 配置\n此方法允許在配置文件中設置一個或多個目標領導節點。當未初始化的 Vault 服務器啟動時，它將嘗試加入每個已定義的潛在領導者，直到成功。當指定的領導者之一變為活動狀態時，此節點將成功加入。當使用 Shamir 密封時，已加入的節點仍然需要手動解封。當使用自動解封時，節點將能夠自動加入並自動解封。\u003c/p\u003e","title":"Vault Workshop ??: Integrated Storage"},{"content":" 活動時間: 2023-09-26T13:20:00Z 活動連結 Facebook Twitter 投影片 HashiCorp Vault 自建金鑰管理最佳入坑姿勢 本次演講從導入 HashiCorp Vault 作為起點，直接提供實務上經驗，分享建議的設定與路上可能有的雷。\nVault 入坑的困難 Vault + Terraform 一入坑就 IaC mount path + role + policy 命名與管理 升級與維護 會依據企業需求提供實際用例 demo，當天提供 github code example 中階\n預期聽眾是有 Vault 使用經驗，希望能更有效率管理 Vault 的人 不會講太多基本功能介紹 vault 介紹 ","permalink":"https://chechia.net/posts/2023-09-26-devopsday/","summary":"\u003cul\u003e\n\u003cli\u003e活動時間: 2023-09-26T13:20:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://devopsdays.tw/2023/session-page/2279\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFacebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/slides/2023-09-26-devopsday-2023-vault/\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"hashicorp-vault-自建金鑰管理最佳入坑姿勢\"\u003eHashiCorp Vault 自建金鑰管理最佳入坑姿勢\u003c/h2\u003e\n\u003cp\u003e本次演講從導入 HashiCorp Vault 作為起點，直接提供實務上經驗，分享建議的設定與路上可能有的雷。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eVault 入坑的困難\u003c/li\u003e\n\u003cli\u003eVault + Terraform 一入坑就 IaC\u003c/li\u003e\n\u003cli\u003emount path + role + policy 命名與管理\u003c/li\u003e\n\u003cli\u003e升級與維護\u003c/li\u003e\n\u003cli\u003e會依據企業需求提供實際用例 demo，當天提供 github code example\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e中階\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e預期聽眾是有 Vault 使用經驗，希望能更有效率管理 Vault 的人\u003c/li\u003e\n\u003cli\u003e不會講太多基本功能介紹 vault 介紹\u003c/li\u003e\n\u003c/ul\u003e","title":"DevOpsDay: HashiCorp Vault 自建金鑰管理最佳入坑姿勢"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Local with Docker minikube r=https://api.github.com/repos/kubernetes/minikube/releases curl -LO $(curl -s $r | grep -o 'http.*download/v.*beta.*/minikube-darwin-arm64' | head -n1) sudo install minikube-darwin-arm64 /usr/local/bin/minikube 如何選擇 VM or Docker or Kubernetes 假議題\nchatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/install 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-21-vault-workshop-deploy-on-kubernetes/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-09-deploy-with-docker\"\u003eDay 09: Deploy with Docker\u003c/h1\u003e\n\u003ch3 id=\"local-with-docker\"\u003eLocal with Docker\u003c/h3\u003e\n\u003ch3 id=\"minikube\"\u003eminikube\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003er=https://api.github.com/repos/kubernetes/minikube/releases\ncurl -LO $(curl -s $r | grep -o 'http.*download/v.*beta.*/minikube-darwin-arm64' | head -n1)\nsudo install minikube-darwin-arm64 /usr/local/bin/minikube\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"如何選擇-vm-or-docker-or-kubernetes\"\u003e如何選擇 VM or Docker or Kubernetes\u003c/h3\u003e\n\u003cp\u003e假議題\u003c/p\u003e\n\u003ch3 id=\"chatgpt\"\u003echatGPT\u003c/h3\u003e\n\u003cp\u003e本段部分內容使用 chatGPT-3.5 翻譯\n\u003ca href=\"https://developer.hashicorp.com/vault/docs/install\"\u003ehttps://developer.hashicorp.com/vault/docs/install\u003c/a\u003e\n內容，並由筆者人工校驗\u003c/p\u003e\n\u003cp\u003ebase context\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。\n\n很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eresult correction\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。\n\n修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。\n\u003c/code\u003e\u003c/pre\u003e","title":"Vault Workshop 10: Deploy with Docker"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Vault configuration chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/install 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-20-vault-workshop-ha-docker/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-09-deploy-with-docker\"\u003eDay 09: Deploy with Docker\u003c/h1\u003e\n\u003ch3 id=\"vault-configuration\"\u003eVault configuration\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003e\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"chatgpt\"\u003echatGPT\u003c/h3\u003e\n\u003cp\u003e本段部分內容使用 chatGPT-3.5 翻譯\n\u003ca href=\"https://developer.hashicorp.com/vault/docs/install\"\u003ehttps://developer.hashicorp.com/vault/docs/install\u003c/a\u003e\n內容，並由筆者人工校驗\u003c/p\u003e\n\u003cp\u003ebase context\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。\n\n很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eresult correction\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。\n\n修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。\n\u003c/code\u003e\u003c/pre\u003e","title":"Vault Workshop 09: Vault HA"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 08: Vault in Docker and Initialization Vault in container 前幾天我們使用 vault dev Server 來啟用測試用的 Vault server。\n在 production 環境我們不會使用 dev Server。Vault 提供許多安裝方法\n可以使用 binary 直接安裝在VM上 也可以透過 hashicorp/vault official docker image，在container 環境中執行 或是使用 hashicorp official helm chart，安裝在kuberntes上 使用範例 repository 你可以使用筆者準備的範例 repository https://github.com/chechiachang/vault-playground\ngit clone git@github.com:chechiachang/vault-playground.git cd deploy/00-docker-dev/ 你可以使用下列指令啟動 vault in docker\n並使用 -v flag 來掛載 ./config/ volume 到 container 中的 /vault/config.d/ docker run --cap-add=IPC_LOCK \\ --volume ./vault/config/:/vault/config.d \\ --volume ./vault/file/:/vault/file \\ --volume ./vault/logs/:/vault/logs \\ -p 8200:8200 \\ --name vault_1 \\ -d \\ hashicorp/vault:1.14.3 \\ vault server -config=/vault/config.d/vault.hcl 然後使用 docker logs 指令檢視 vault server log\ndocker logs -f vault_1 output\n==\u0026gt; Vault server configuration: Administrative Namespace: Api Address: https://0.0.0.0:8200 Cgo: disabled Cluster Address: https://0.0.0.0:8201 Environment Variables: GODEBUG, GOTRACEBACK, HOME, HOSTNAME, NAME, PATH, PWD, SHLVL, VERSION Go Version: go1.20.8 Listener 1: tcp (addr: \u0026quot;0.0.0.0:8200\u0026quot;, cluster address: \u0026quot;0.0.0.0:8201\u0026quot;, max_request_duration: \u0026quot;1m30s\u0026quot;, max_request_size: \u0026quot;33554432\u0026quot;, tls: \u0026quot;disabled\u0026quot;) Log Level: Mlock: supported: true, enabled: false Recovery Mode: false Storage: file Version: Vault v1.14.3, built 2023-09-11T21:23:55Z Version Sha: 56debfa71653e72433345f23cd26276bc90629ce ==\u0026gt; Vault server started! Log data will stream in below: 2023-09-20T13:38:25.914Z [INFO] proxy environment: http_proxy=\u0026quot;\u0026quot; https_proxy=\u0026quot;\u0026quot; no_proxy=\u0026quot;\u0026quot; 2023-09-20T13:38:25.920Z [INFO] core: Initializing version history cache for core 你會發現，vault server 不是以 dev server 的狀態啟動的，需要進行額外的初始化設定\nexport VAULT_ADDR='http://127.0.0.1:8200' 檢查 vault status\nvault status output，顯示一個上未初始化的 vault server 與其 storage backend \u0026ldquo;filesyste\u0026rdquo;\n可以在 ./vault/config/vault.hcl 上看到我們使用新的 Storage Backend \u0026ldquo;file\u0026rdquo; 設定 vault server 的 storage backend 目前是 sealed 狀態 不是 vault server sealed，而是沒有提供 vault server 初始化設定 / unseal keys，所以 vault server 無法解密 storage backend Key Value --- ----- Seal Type shamir Initialized false Sealed true Total Shares 0 Threshold 0 Unseal Progress 0/0 Unseal Nonce n/a Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type file HA Enabled false Storage Backend: filesystem https://developer.hashicorp.com/vault/docs/configuration/storage/filesystem\nfilesystem storage backend將 Vault 的資料存儲在文件系統上，使用標準的目錄結構。\n它可以用於持久的Single Server情況，或者在本地開發時，耐久性不是關鍵問題的情況下使用。\nfilesystem storage backend 不支援高可用性 - 檔案系統後端不支援高可用性。\nfilesystem storage backend 由HashiCorp 官方支援 - 檔案系統後端是由 HashiCorp 官方支援維護的。\n初始化 Vault 初始化是配置 Vault 的過程。僅對第一次使用在 Vault server 上的 Backend 上執行一次。在高可用(HA)模式下運行時，這僅在每個叢集(Vault Cluster)中執行一次，而不是每台Server。\n在初始化期間，將生成加密金鑰、創建 unseal keys，並創建 init root token。\n初始化不需身份驗證，僅適用於全新的 Vault，有資料存在的 Storage Backend 無法用初始化解鎖。\n要初始化 Vault，使用以下命令\nvault operator init output\nUnseal Key 1: 9AYJ...kNCn Unseal Key 2: RqDx...odRU Unseal Key 3: uHUv...lws4 Unseal Key 4: f+KD...aHgL Unseal Key 5: AKyF...e6vd Initial Root Token: hvs.8yU3...7esX Vault initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above. When the Vault is re-sealed, restarted, or stopped, you must supply at least 3 of these keys to unseal it before it can start servicing requests. # Vault 使用了 5 個金鑰份額encryption key share和 3 的 key threshold 進行初始化。請安全地分發上面的金鑰份額。 # 當 Vault 被重新密封、重新啟動或停止時，你必須提供至少 3 個這些金鑰來對其進行解封，然後它才能開始處理請求。 Vault does not store the generated root key. Without at least 3 keys to reconstruct the root key, Vault will remain permanently sealed! # Vault 不存儲生成的根金鑰。如果沒有至少 3 個金鑰來重建根金鑰，Vault 將保持永久密封！ It is possible to generate new unseal keys, provided you have a quorum of existing unseal keys shares. See \u0026quot;vault operator rekey\u0026quot; for more information. # 如果你有足夠的現有解封金鑰份額，則可以生成新的解封金鑰。有關更多信息，請參閱 \u0026quot;vault operator rekey\u0026quot;。 初始化過程輸出了兩個非常重要的信息：解封金鑰和 init root token。這是唯一一次 Vault 顯示這些資料。\n為了這個入門課程，請將這些金鑰保存在某個地方，然後繼續進行操作。\nVault backend storage 認 key 不認人，請收好這五隻 unseal key，放置在五個不同安全的地方 有 3/5 的 unseal key 就能重建 encryption key，也就是能夠解密 storage backend 可以用 3/5 unseal key 產生 root token vault operator generate-root 換句話說，unseal key 是比 root 還大的超級管理員 key 在實際的部署情況下，你永遠不應該將這些金鑰保存在一起。 你可能會使用 Vault 的 PGP 和 Keybase.io 支援，使用使用者的 PGP 金鑰加密每個解封金鑰。 這可以防止單個人擁有所有的解封金鑰。 vault server log，可以看到 vault server 開始進行初始化流程\n2023-09-20T13:47:05.113Z [INFO] core: security barrier not initialized 2023-09-20T13:47:05.117Z [INFO] core: seal configuration missing, not initialized 2023-09-20T14:03:28.506Z [INFO] core: security barrier not initialized 2023-09-20T14:03:28.512Z [INFO] core: seal configuration missing, not initialized 2023-09-20T14:03:28.521Z [INFO] core: security barrier not initialized 2023-09-20T14:03:28.557Z [INFO] core: security barrier initialized: stored=1 shares=5 threshold=3 2023-09-20T14:03:28.589Z [INFO] core: post-unseal setup starting 2023-09-20T14:03:28.609Z [INFO] core: loaded wrapping token key 2023-09-20T14:03:28.609Z [INFO] core: successfully setup plugin catalog: plugin-directory=\u0026quot;\u0026quot; 2023-09-20T14:03:28.613Z [INFO] core: no mounts; adding default mount table 2023-09-20T14:03:28.629Z [INFO] core: successfully mounted: type=cubbyhole version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=cubbyhole/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:03:28.631Z [INFO] core: successfully mounted: type=system version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=sys/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:03:28.631Z [INFO] core: successfully mounted: type=identity version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=identity/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:03:28.686Z [INFO] core: successfully mounted: type=token version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=token/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:03:28.696Z [INFO] rollback: starting rollback manager 2023-09-20T14:03:28.697Z [INFO] core: restoring leases 2023-09-20T14:03:28.698Z [INFO] expiration: lease restore complete 2023-09-20T14:03:28.713Z [INFO] identity: entities restored 2023-09-20T14:03:28.714Z [INFO] identity: groups restored 2023-09-20T14:03:28.720Z [INFO] core: usage gauge collection is disabled 2023-09-20T14:03:28.733Z [INFO] core: Recorded vault version: vault version=1.14.3 upgrade time=\u0026quot;2023-09-20 14:03:28.720230178 +0000 UTC\u0026quot; build date=2023-09-11T21:23:55Z 2023-09-20T14:03:29.120Z [INFO] core: post-unseal setup complete 2023-09-20T14:03:29.146Z [INFO] core: root token generated 2023-09-20T14:03:29.146Z [INFO] core: pre-seal teardown starting 2023-09-20T14:03:29.146Z [INFO] rollback: stopping rollback manager 2023-09-20T14:03:29.146Z [INFO] core: pre-seal teardown complete 你可以在這時檢視 ./vault/file 裡面的內容，./vault/file 在 vault operator init 前是空白的\nls -al ./vault/file output，可以看到 ./vault/file 內已經初始化 filesystem storage backend 的內容\ncore logical sys 你可以檢視 ./vault/file/* 內的檔案內容，會發現內容都是 json 相容格式，而且都經過加密\n在正常的條件下，沒有 unseal key 的人，是無法在有效時間內解鎖這些 filesystem 中的內容，內容還是受到保護 這並不代表你可以隨意放置 filesystem storage backend 的內容 例如將他 commit 到 repository 中(vault-playground 已經添加 .gitignore) 請選擇安全，經過 vault 資安團隊測試過的 backend，來保障資料安全與可用程度 Unseal 每個初始化的 Vault Server 都始於密封狀態。根據配置，Vault 可以訪問 storage backend，但它無法讀取其中的任何資料，因為它不知道如何解密它。教會 Vault 如何解密數據稱為解封(unseal) Vault。\n每次 Vault 開始運行時都必須進行解封。可以通過 API 和命令行來執行解封操作。\n要解封 Vault，你必須擁有解封金鑰的閾值數量(threshold)。在上面的輸出中，請注意 \u0026ldquo;key threshold\u0026rdquo; 是 3。這意味著要解封 Vault，你需要 5 個生成的金鑰中的 3 個。\n注意\nVault 不存儲任何解封金鑰片段(key share)。Vault 使用一種稱為 Shamir\u0026rsquo;s Secret Sharing 的算法來將 root encryption key 分成片段(share)。只有擁有threshold數量的金鑰，才能將其重建，最終訪問你的資料。\n開始解封 Vault：\nvault operator unseal output\nUnseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 5 Threshold 3 Unseal Progress 1/3 Unseal Nonce 9d03a4e0-bb4f-cfd5-326b-5afb55fdce53 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type file HA Enabled false 繼續操作，輸入3/5把 unseal key 就可順利解封 storage backend\nUnseal Key (will be hidden): Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 5 Threshold 3 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type file Cluster Name vault-cluster-f90273e8 Cluster ID 5ecc63c9-11eb-c384-355d-dffea2dd6484 HA Enabled false vault server log，成功解封後\n2023-09-20T14:57:17.077Z [INFO] core.cluster-listener.tcp: starting listener: listener_address=0.0.0.0:8201 2023-09-20T14:57:17.078Z [INFO] core.cluster-listener: serving cluster requests: cluster_listen_address=[::]:8201 2023-09-20T14:57:17.089Z [INFO] core: post-unseal setup starting 2023-09-20T14:57:17.115Z [INFO] core: loaded wrapping token key 2023-09-20T14:57:17.115Z [INFO] core: successfully setup plugin catalog: plugin-directory=\u0026quot;\u0026quot; 2023-09-20T14:57:17.127Z [INFO] core: successfully mounted: type=system version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=sys/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:57:17.128Z [INFO] core: successfully mounted: type=identity version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=identity/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:57:17.128Z [INFO] core: successfully mounted: type=cubbyhole version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=cubbyhole/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:57:17.148Z [INFO] core: successfully mounted: type=token version=\u0026quot;v1.14.3+builtin.vault\u0026quot; path=token/ namespace=\u0026quot;ID: root. Path: \u0026quot; 2023-09-20T14:57:17.154Z [INFO] rollback: starting rollback manager 2023-09-20T14:57:17.154Z [INFO] core: restoring leases 2023-09-20T14:57:17.156Z [INFO] expiration: lease restore complete 2023-09-20T14:57:17.158Z [INFO] identity: entities restored 2023-09-20T14:57:17.159Z [INFO] identity: groups restored 2023-09-20T14:57:17.166Z [INFO] core: usage gauge collection is disabled 2023-09-20T14:57:17.178Z [INFO] core: post-unseal setup complete 2023-09-20T14:57:17.178Z [INFO] core: vault is unsealed Hashicorp Vault docker 使用容器(container) 前需要查閱 image 的官方說明 https://hub.docker.com/r/hashicorp/vault，幾件事情要注意\nBase Image 是選擇使用 Alpine，比起其他 linux distributions 具有相對較小的安全風險，但功能足夠用於開發和測試。\nVault 始終在 dumb-init 下運行，將 process 與 SIG 傳遞給容器中運行的所有process。\n使用的 Binary 由 HashiCorp 構建，並使用 GPG 金鑰簽名，因此你可以驗證Binary。\n在不帶任何參數的情況下運行 Vault，容器將為你提供一個處於dev mode的 Vault Server。\n容器公開了兩個可選的volume：\n/vault/logs，用於寫入 audit log。預設情況下，這裡不會寫入任何內容；必須在Vault config 中啟用文件audit backend。\n/vault/file，用於在使用資料storage plugin時，將資料寫入persistency。預設情況下，這裡不寫入任何內容（dev mode使用 in-memory storage）；在啟動容器之前，必須在 Vault 的配置中啟用文件 data storagebackend。\n容器在 /vault/config 設置了一個 Vault 配置目錄，Server 將通過volume，載入這裡放置的任何 hcl 或 json config file。或者，也可以通過通過環境變數 VAULT_LOCAL_CONFIG 傳遞配置 json 來添加config。\nVault configuration 如同前面說明，透過 --volume ./config/:/vault/config.d flag 將 ./config/vault.hcl 設定檔案掛載進容器中\n你可以檢視目前的 vault server configuration，是以 vault.hcl 格式表示\ncat ./config/vault.hcl output，在這裡可以設置 vault server 的啟動參數\n配置一個 listener \u0026ldquo;tcp\u0026rdquo;，監聽容器內 0.0.0.0:8200 進來的 request 由於 docker run 也設定 -p 8200:8200 將主機網路的 8200 port bind 到容器的 8200 port，讓我們可以在主機網路中存取 vault。 ui = true disable_mlock = true api_addr = \u0026quot;https://0.0.0.0:8200\u0026quot; default_lease_ttl = \u0026quot;168h\u0026quot; max_lease_ttl = \u0026quot;720h\u0026quot; storage \u0026quot;file\u0026quot; { path = \u0026quot;/vault/file\u0026quot; } listener \u0026quot;tcp\u0026quot; { address = \u0026quot;0.0.0.0:8200\u0026quot; tls_disable = \u0026quot;true\u0026quot; } 操作 Vault Vault in Docker 使用起來與透過 binary 執行的 dev server 一樣。\n你可以參照前幾天文章的內容，對 vault server 進行操做，也算是做個練習\nvault login vault secrets list vault secrets enable -version=2 -path=chechia-net kv 在後面的內容，我們會用更有效率的方式管理，設定 vault\n關閉 vault container 你可以使用以下指令關閉 vault容器\ndocker stop vault_1 output，server 接受到後會進行 docker 封鎖\n==\u0026gt; Vault shutdown triggered 2023-09-20T13:06:28.627Z [INFO] core: marked as sealed 2023-09-20T13:06:28.627Z [INFO] core: pre-seal teardown starting 2023-09-20T13:06:28.627Z [INFO] rollback: stopping rollback manager 2023-09-20T13:06:28.628Z [INFO] core: pre-seal teardown complete 2023-09-20T13:06:28.628Z [INFO] core: stopping cluster listeners 2023-09-20T13:06:28.628Z [INFO] core.cluster-listener: forwarding rpc listeners stopped 2023-09-20T13:06:28.641Z [INFO] core.cluster-listener: rpc listeners successfully shut down 2023-09-20T13:06:28.641Z [INFO] core: cluster listeners successfully shut down 2023-09-20T13:06:28.641Z [INFO] core: vault is sealed 如果有啟用 storage backend 的話，vault 資料或留存在 storage \u0026ldquo;file\u0026rdquo; 上，重啟不會如 dev server in-memory storage 一樣，被清空\nvault operator 你可以使用 vault operator 指令來做更多 vault cluster 相關的操作\nvault operator -h 你可以使用下列指令產生新的 root token\nvault operator generate-root -generate-otp output\nA One-Time-Password has been generated for you and is shown in the OTP field. You will need this value to decode the resulting root token, so keep it safe. Nonce 13ed8986-7b01-323c-0d06-2c32536fb8c1 Started true Progress 0/3 Complete false OTP eYr...cMqx OTP Length 28 vault server log 顯示開始產生 root\n2023-09-20T15:12:08.633Z [INFO] core: root generation initialized: nonce=13ed8986-7b01-323c-0d06-2c32536fb8c1 使用 -otp flag，並輸入 unseal key\nvault operator generate-root -otp=\u0026quot;...\u0026quot; output\nOperation nonce: 13ed8986-7b01-323c-0d06-2c32536fb8c1 Unseal Key (will be hidden): Nonce 13ed8986-7b01-323c-0d06-2c32536fb8c1 Started true Progress 1/3 Complete false 持續輸入 3/5 unseal keys，直到取得 encoded token\nOperation nonce: 13ed8986-7b01-323c-0d06-2c32536fb8c1 Unseal Key (will be hidden): Nonce 13ed8986-7b01-323c-0d06-2c32536fb8c1 Started true Progress 3/3 Complete true Encoded Token DS8B...MdGw 使用 otp + encoded token 來進行 decode\nvault operator generate-root \\ -decode=\u0026quot;DS8B...MdGw\u0026quot; \\ -otp=\u0026quot;eYr1...cMqx\u0026quot; output\nhvs.bDKG...Pnlc vault server log 顯示逞生 root 完成\n2023-09-20T15:15:40.541Z [INFO] core: root generation finished: nonce=13ed8986-7b01-323c-0d06-2c32536fb8c1 NOTE: 第一把 init root token 還是有效，加上後來生成的第二把，現在有兩把常效期的 root token\n清空 你可以刪除 ./vault/file/，完全清除 vault server 使用的 filesystem storage backend\nrm -rf ./vault/file/* vault server 本身幾乎是無狀態，清除 storage backend 後就什麼也不剩了\nchatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-deploy https://hub.docker.com/r/hashicorp/vault 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-19-vault-workshop-docker-and-initialization/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-08-vault-in-docker-and-initialization\"\u003eDay 08: Vault in Docker and Initialization\u003c/h1\u003e\n\u003ch3 id=\"vault-in-container\"\u003eVault in container\u003c/h3\u003e\n\u003cp\u003e前幾天我們使用 vault dev Server 來啟用測試用的 Vault server。\u003c/p\u003e\n\u003cp\u003e在 production 環境我們不會使用 dev Server。Vault 提供許多安裝方法\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e可以使用 binary 直接安裝在VM上\u003c/li\u003e\n\u003cli\u003e也可以透過 hashicorp/vault official docker image，在container 環境中執行\u003c/li\u003e\n\u003cli\u003e或是使用 hashicorp official helm chart，安裝在kuberntes上\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"使用範例-repository\"\u003e使用範例 repository\u003c/h3\u003e\n\u003cp\u003e你可以使用筆者準備的範例 repository \u003ca href=\"https://github.com/chechiachang/vault-playground\"\u003ehttps://github.com/chechiachang/vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003egit clone git@github.com:chechiachang/vault-playground.git\n\ncd deploy/00-docker-dev/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e你可以使用下列指令啟動 vault in docker\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e並使用 -v flag 來掛載 ./config/ volume 到 container 中的 /vault/config.d/\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003edocker run --cap-add=IPC_LOCK \\\n  --volume ./vault/config/:/vault/config.d \\\n  --volume ./vault/file/:/vault/file \\\n  --volume ./vault/logs/:/vault/logs \\\n  -p 8200:8200 \\\n  --name vault_1 \\\n  -d \\\n  hashicorp/vault:1.14.3 \\\n  vault server -config=/vault/config.d/vault.hcl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e然後使用 docker logs 指令檢視 vault server log\u003c/p\u003e","title":"Vault Workshop 08: Vault in Docker and Initialization"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 07: Policy \u0026amp; Approle 在 Vault 中，策略（Policies）控制著使用者可以訪問的資源。在上一篇身份驗證中，你已經學到了身份驗證方法(authentication method)。而這一節是關於授權(Authorization)，也就是合法的用戶登入後，應該能夠取得怎樣的權限。\n在身份驗證方面，Vault 提供了多種啟用和使用的選項或方法。Vault 在授權和 policy 方面也使用相同的設計。所有身份驗證方法都將登入者的身份 map 回與 Vault 配置的核心 policy。\n準備環境 在 vault 官方網站文件中，Hashicorp 官方準備了 https://instruqt.com/的 session lab\n筆者個人覺得 interactive lab 的 browser tab 很難用，也不喜歡 Terminal session 連 remote VM 的延遲，以下內容還是會使用 local dev Server 說明。\n觀眾們可以自己斟酌使用。\nPolicy 格式 策略（Policies）是以HCL撰寫的，但也兼容JSON格式。以下是一個範例策略：\n# Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026quot;secret/data/*\u0026quot; { capabilities = [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;] } path \u0026quot;secret/data/foo\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] } 這個範例 policy 授予了對 KV v2 秘密管理（secrets engine）的資源的能力。如果你對於與這個秘密管理引擎相關的路徑不熟悉，可以查閱該秘密管理引擎文件中的 ACL 規則部分。\n根據這個 policy，使用者可以對 secret/data/ 中的任何秘密進行寫入操作，但對於 secret/data/foo 的存取僅允許讀取。policy 的預設行為是 Deny，因此不允許對未指定路徑的資源進行任何存取。\npolicy 格式使用 prefix 匹配系統來確定對 API 路徑的訪問控制。使用最具體的已定義策略，即精確匹配或最長 prefix 匹配。也就是存取 secret/data/foo 路徑符合上面兩條規則，但是由於第二條規則的路徑 match 最長最精確，所以最終拿到的權限是 \u0026ldquo;read\u0026rdquo;\n由於 Vault 中的一切都必須通過 API 進行訪問，這使得對 Vault 的每個方面都有嚴格的控制，包括啟用秘密管理引擎、啟用身份驗證方法、身份驗證，以及存取秘密等。\n有一些內建的策略無法被移除。例如，root policy 和 default policy 是必需的策略，無法被刪除。\ndefault policy 提供了一組常見的權限，並且 default 包含在所有 token 中。 root policy 給予 token 超級管理員權限，類似於Linux機器上的 root 用戶。 啟動本地開發環境 步驟在前幾篇已經出現過數次，這邊就簡單帶過\nvault server -dev -dev-no-store-token export VAULT_ADDR='http://127.0.0.1:8200' unset VAULT_TOKEN vault login Defult policy 你可以用下列指令列出預設的 policy\nvault policy list output\ndefault root 可以用以下指令讀取 default policy 的內容\nvault policy read default output 回傳許多條 policy rule，主要是給予 token 基本的操作權限\n# Allow tokens to look up their own properties path \u0026quot;auth/token/lookup-self\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] } # Allow tokens to renew themselves path \u0026quot;auth/token/renew-self\u0026quot; { capabilities = [\u0026quot;update\u0026quot;] } # Allow tokens to revoke themselves path \u0026quot;auth/token/revoke-self\u0026quot; { capabilities = [\u0026quot;update\u0026quot;] } ... 熟悉 default policy 內容後，可以斟酌使用。或是選擇從頭編寫自己的 policy。\n編寫第一個 policy 要撰寫 policy ，使用 vault policy write 指令。請查閱該指令的幫助文件以進一步了解用法。\nvault policy write -h Usage: vault policy write [options] NAME PATH Uploads a policy with name NAME from the contents of a local file PATH or stdin. If PATH is \u0026quot;-\u0026quot;, the policy is read from stdin. Otherwise, it is loaded from the file at the given path on the local disk. # 使用 vault policy write 指令，可以從本地文件 PATH 或標準輸入（stdin）的內容上傳一個名為 NAME 的策略。如果 PATH 是 \u0026quot;-\u0026quot;，則策略將從 stdin 讀取。否則，它將從本地 disk 上給定路徑的文件中載入。 Upload a policy named \u0026quot;my-policy\u0026quot; from \u0026quot;/tmp/policy.hcl\u0026quot; on the local disk: $ vault policy write my-policy /tmp/policy.hcl Upload a policy from stdin: $ cat my-policy.hcl | vault policy write my-policy - 使用以下指令創建名為 \u0026ldquo;my-policy\u0026rdquo; 的 policy，並將其內容來自 stdin（標準輸入）\nvault policy write my-policy - \u0026lt;\u0026lt; EOF # Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026quot;secret/data/*\u0026quot; { capabilities = [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;] } path \u0026quot;secret/data/foo\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] } EOF output\nSuccess! Uploaded policy: my-policy 使用下列指令列出 policy\nvault policy list output\ndefault my-policy root 讀取 my-policy 的內容\nvault policy read my-policy output\n# Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026quot;secret/data/*\u0026quot; { capabilities = [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;] } path \u0026quot;secret/data/foo\u0026quot; { capabilities = [\u0026quot;read\u0026quot;] } 測試 policy 你所建立的 policy ，提供對 KV-V2 秘密引擎所定義的秘密進行管理。policy 被附加到 Vault 直接生成的 otoken，或透過其各種授權方法生成的 token 上。\n創建一個 token ，添加 my-policy。-field flag 設定只回傳部分 key-vault data，而不是傳整個 metadata table。-policy 設定連結 token 的 policy。\nvault token create -field token -policy=my-policy hvs.CAESIIDh...UxbUU 為了簡化，本內容使用 dev 模式伺服器，直接從 token 授權方法創建 token 。請記住，在大多數 production 環境部署中，token 將由已啟用的授權方法(ex. github auth method)創建。\n可以使用 token login\nvault login output 回傳登入資訊，以及連結 token 的 policy，policy 包含 default 與 my-policy\nSuccess! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.CAESIIDh...UxbUU token_accessor vY3aLwMSezlxeOn7YwcjxuhZ token_duration 767h56m52s token_renewable true token_policies [\u0026quot;default\u0026quot; \u0026quot;my-policy\u0026quot;] identity_policies [] policies [\u0026quot;default\u0026quot; \u0026quot;my-policy\u0026quot;] 你可以執行 vault token lookup，查找目前 token 的完整資訊\nvault token lookup output\nKey Value --- ----- accessor vY3aLwMSezlxeOn7YwcjxuhZ creation_time 1694917062 creation_ttl 768h display_name token entity_id n/a expire_time 2023-10-19T10:17:42.723364+08:00 explicit_max_ttl 0s id hvs.CAESIIDh...UxbUU issue_time 2023-09-17T10:17:42.723365+08:00 meta \u0026lt;nil\u0026gt; num_uses 0 orphan false path auth/token/create policies [default my-policy] renewable true ttl 767h55m45s type service 這個policy 啟用了 secret/ 秘密引擎內每個路徑的創建和更新功能，除了一個例外路徑 secret/data/foo 僅允許 read。\n寫入資料到 secret/data/creds\nvault kv put -mount=secret creds password=\u0026quot;my-long-password\u0026quot; output 回傳成功資訊，秘密已成功創建。\n== Secret Path == secret/data/creds ======= Metadata ======= Key Value --- ----- created_time 2023-09-17T02:26:44.179748Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 該政策僅啟用了對 secret/data/foo 路徑的讀取功能。嘗試向此路徑寫入將導致\u0026quot;權限拒絕\u0026quot;錯誤。請嘗試寫入 secret/data/foo 路徑。\nvault kv put -mount=secret foo robot=beepboop output，顯示權限錯誤。\nError writing data to secret/data/foo: Error making API request. URL: PUT http://127.0.0.1:8200/v1/secret/data/foo Code: 403. Errors: * 1 error occurred: * permission denied 此 policy 定義了一組有限的路徑和功能。如果沒有對 sys 的訪問權限，則系統相關玄線，像 vault policy list 或 vault secrets list 這樣的命令將無法運作。\nkv v2 與 v1 語法 當你使用 vault kv CLI 命令訪問 KV v2 秘密引擎時，我們建議使用 -mount flag 語法，來引用 KV v2 秘密引擎的路徑。\nvault kv get -mount=secret foo\n你也可以使用兼容 KV v1 風格的路徑 prefix 語法，在某個範圍內上是等效的，系統將自動附加 /data 到秘密路徑，可能會引起混淆。\nvault kv get secret/foo\n連結 policy 與 auth method Vault 本身只有唯一的 policy 授權管理機構，不同於身份驗證，你可以啟用多個身份驗證方法。\n你可以配置身份驗證方法，以自動分配一組 policy，給使用某些身份驗證方法創建的 token 。這樣做的方式取決於相關的身份驗證方法，但通常涉及將 role 映射到 policy ，或者將 identity 或 group 映射到 policy。\n例如：當設定 github auth method 的 role 時，你可以使用 token policies 參數來實現這一點。\n我們這邊嘗試建立另外一個 auth method，AppRole，順便在整理一次 vault -\u0026gt; auth method -\u0026gt; policy 的關係\n配置 AppRole 身份驗證方法 在以下步驟中，使用權限更高的 root token 進行操作與登入\nvault login output\nToken (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.uCGr6...749AI token_accessor SlAe0epU6CLngp2iclB5WN2A token_duration ∞ token_renewable false token_policies [\u0026quot;root\u0026quot;] identity_policies [] policies [\u0026quot;root\u0026quot;] 啟用 approle auth method\nvault auth enable approle output\nSuccess! Enabled approle auth method at: approle/ vault server log\n2023-09-17T10:44:26.460+0800 [INFO] core: enabled credential backend: path=approle/ type=approle version=\u0026quot;\u0026quot; approle 配置 role 我們已經啟用 approle 這個 auth method，接下來要設定名為 \u0026ldquo;my-role\u0026rdquo; 的 AppRole role，配置一些基本的token 設定，並將先前定義的 \u0026ldquo;my-policy\u0026rdquo; policy，附加到所有通過該 role 創建的 token。\n你可以使用下列指令，在 auth/approle/role/ 下新增 my-role，path 也代表 approle / role / my-role 的階層關係\nvault write auth/approle/role/my-role \\ secret_id_ttl=10m \\ token_num_uses=10 \\ token_ttl=20m \\ token_max_ttl=30m \\ secret_id_num_uses=40 \\ token_policies=my-policy output\nSuccess! Data written to: auth/approle/role/my-role 你可以通過使用 auth 方法來驗證這個 AppRole role 是否附加了policy。\n要使用 AppRole 進行身份驗證，首先需要獲取角色ID\n首先先讀取目前 approle/role/my-role 的設定\nv read auth/approle/role/my-role Key Value --- ----- bind_secret_id true local_secret_ids false secret_id_bound_cidrs \u0026lt;nil\u0026gt; secret_id_num_uses 40 secret_id_ttl 10m token_bound_cidrs [] token_explicit_max_ttl 0s token_max_ttl 30m token_no_default_policy false token_num_uses 10 token_period 0s token_policies [my-policy] token_ttl 20m token_type default 然後讀取 my-role 的 role-id\nvault read auth/approle/role/my-role/role-id output\nKey Value --- ----- role_id 600dce48-244c-094e-aeae-cb819ec7f5dd 接下來，獲取一個秘密ID(它類似於應用程式用於 AppRole 身份驗證的密碼)，由於我們這個示範並不提供額外參數值，這先需要使用 -f | -force flag，強迫 vault 寫入一個沒有 value 的資料到 auth/approle/role/my-role/secret-id endpoint\nvault write -f auth/approle/role/my-role/secret-id 可以想像是向 API server 打 POST my-role/secret-id，向 approle 註冊一個 app，server 則回傳一組 id，以辨識這個 app。\nKey Value --- ----- secret_id d80db935-9792-faf7-306d-93a2f0c3a18f secret_id_accessor 32451384-f985-a387-175b-67140225856f secret_id_num_uses 40 secret_id_ttl 10m 最後進行 login 取得 token，這裡使用 vault write 進行 AppRole 身份驗證，指定 role path並使用相應的選項傳遞role id和secret id\nexport ROLE_ID=600dce48-244c-094e-aeae-cb819ec7f5dd export SECRET_ID=d80db935-9792-faf7-306d-93a2f0c3a18f vault write auth/approle/login \\ role_id=$ROLE_ID \\ secret_id=$SECRET_ID output，取得合法的 token，並且具有我們為 approle/my-role 配置的 my-policy 權限\nKey Value --- ----- token hvs.CAESIB9gD3aupV5X06IRIWMEvhL4QeBtizO20i2Hh2YderxpGh4KHGh2cy5BWVVWVDVFUVI3dkw5VUFUbmdoZmpnUFA token_accessor QGrKxnHH4e9SVLqhpaqgYsCZ token_duration 20m token_renewable true token_policies [\u0026quot;default\u0026quot; \u0026quot;my-policy\u0026quot;] identity_policies [] policies [\u0026quot;default\u0026quot; \u0026quot;my-policy\u0026quot;] token_meta_role_name my-role 實務中，這個 secret_id 會設置在 application 上，讓 application 具有合法存取 vault 的權限\n實務範例講解 我們可以重複使用設定好的 policy，綁定到 auth method 上，policy 對 auth method 是多對多的，這邊透過我們上篇 github auth method 來對照一下，解釋 auth method 與 role / policy 的關係\nchechia-net 是一個組織\nas a admin，我希望藉由 github auth method，讓有 github 權限的工程師，可以存取 vault\n並且在 github 上的 org / team 管理各個使用者，vault 可以直接複用 github 的權限階層 於是，透過 github auth method，自動配置 vault 權限給 chechia-net github org 下的使用者\nvault auth enable -path=github-chechia github vault write auth/github-chechia/config organization=chechia-net as a github user，我希望藉由 github auth method，直接登入 vault，不用註冊額外的帳號\nvault login -method=github -path=github-chechia 換成 approle 也是相同道理\nas a admin，我希望我的 application 能夠有辦法存取 vault\n可能是一個 badkend golang server，需要存取 vault 中的 mysql database credential vault auth enable -path=approle-golang-server approle 這個 application 可能有複數不同種類工作，所以也希望能夠配置不同的 role，給予不同的 policy 權限\nrole/default 有 default policy role/my-role 有 default, my-policy 兩個 policy 更接近實務的例子，底下可能是常見的 role 用途\napprole/database，當 application 需要向 db 存取時使用，ex. 去取得 mysql 相關的 credential approle/frontend，讓 application 可以向前端存取測試需要 credential approle/qa，也許是 application 自動化測試的 credential 想表達，雖然來源都是相同 application，使用 approle auth method，vault 還是可以設定不同的 policy，讓 application 取用最小的可用權限，而不會每次登入 vault 都是很大的權限 身為 admin，你可以使用以下命令，在 approle-golang-server 中產生一個 default role\nvault write auth/approle-golang-server/role/default \\ secret_id_ttl=10m \\ token_num_uses=10 \\ token_ttl=20m \\ token_max_ttl=30m \\ secret_id_num_uses=40 \\ token_policies=default 增加一個 my-role\nvault write auth/approle-golang-server/role/my-role \\ secret_id_ttl=10m \\ token_num_uses=10 \\ token_ttl=20m \\ token_max_ttl=30m \\ secret_id_num_uses=40 \\ token_policies=default,my-policy 列出在 approle-golang-server 中啟用的 role\nvault list auth/approle-golang-server/role/ output\nKeys ---- default my-role admin 配置完 role 後，記得讀取 role-id，並將 role-id 配置到 application 上\n可以在 application 中維護一個 map，當使用這透過 application，拿到 default role，就使用 default/role-id 搭配接近實務的例子，就是如果是後端工程師使用 application，application 要能夠使用 role/backend/role-id，來向 vault 進行 authentication 你也可以讓 application 有權限讀取 auth/approle-golang-server/role/*/role-id，讓 application 透過 vault API 自動取得 role-id vault read auth/approle-golang-server/role/default/role-id vault read auth/approle-golang-server/role/my-role/role-id output\nKey Value --- ----- role_id f0340d97-a97d-85f9-30d5-65a2058baf11 Key Value --- ----- role_id c1f3cd3e-8f0a-8b1f-91f7-a3a310b9755f 上面是 admin 需要配置的內容\n現在角色換成 application，application 上線了。application 依照情境，選擇要使用那個 role，去取得 secret-id\n例如 application 要去存取 database，就自己使用 role/database 注意，這個 role/database 與 policy/database 上面都沒有配置，請讀者自己練習，配置一個 placeholder 空的權限，或是發揮創意也可以\nvault write -f auth/approle-golang-server/role/my-role/secret-id output，vault 回覆一組短時效的 secret-id，ttl=10 min\nKey Value --- ----- secret_id ad70d84b-56a5-eec2-0a95-6aa7f2cfceef secret_id_accessor 76a5b993-a139-9837-5990-b6adc483542a secret_id_num_uses 40 secret_id_ttl 10m application 可以使用 role-id + secret-id，去取得 token\nvault write auth/approle-golang-server/login \\ role_id=$ROLE_ID \\ secret_id=$SECRET_ID output\nKey Value --- ----- token hvs.CA...ZXRVc token_accessor zjBhj70A6ed72zksuooxCpYt token_duration 20m token_renewable true token_policies [\u0026quot;default\u0026quot; \u0026quot;database\u0026quot;] identity_policies [] policies [\u0026quot;default\u0026quot; \u0026quot;database\u0026quot;] token_meta_role_name database chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-policies https://developer.hashicorp.com/vault/docs/concepts/policies 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-18-vault-workshop-policy-and-approle/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-07-policy--approle\"\u003eDay 07: Policy \u0026amp; Approle\u003c/h1\u003e\n\u003cp\u003e在 Vault 中，策略（Policies）控制著使用者可以訪問的資源。在上一篇身份驗證中，你已經學到了身份驗證方法(authentication method)。而這一節是關於授權(Authorization)，也就是合法的用戶登入後，應該能夠取得怎樣的權限。\u003c/p\u003e\n\u003cp\u003e在身份驗證方面，Vault 提供了多種啟用和使用的選項或方法。Vault 在授權和 policy 方面也使用相同的設計。所有身份驗證方法都將登入者的身份 map 回與 Vault 配置的核心 policy。\u003c/p\u003e\n\u003ch3 id=\"準備環境\"\u003e準備環境\u003c/h3\u003e\n\u003cp\u003e在 \u003ca href=\"https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-policies\"\u003evault 官方網站文件\u003c/a\u003e中，Hashicorp 官方準備了 \u003ca href=\"https://instruqt.com/\"\u003ehttps://instruqt.com/\u003c/a\u003e的 session lab\u003c/p\u003e\n\u003cp\u003e筆者個人覺得 interactive lab 的 browser tab 很難用，也不喜歡 Terminal session 連 remote VM 的延遲，以下內容還是會使用 local dev Server 說明。\u003c/p\u003e\n\u003cp\u003e觀眾們可以自己斟酌使用。\u003c/p\u003e\n\u003ch3 id=\"policy-格式\"\u003ePolicy 格式\u003c/h3\u003e\n\u003cp\u003e策略（Policies）是以HCL撰寫的，但也兼容JSON格式。以下是一個範例策略：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-hcl\"\u003e# Dev servers have version 2 of KV secrets engine mounted by default, so will\n# need these paths to grant permissions:\npath \u0026quot;secret/data/*\u0026quot; {\n  capabilities = [\u0026quot;create\u0026quot;, \u0026quot;update\u0026quot;]\n}\n\npath \u0026quot;secret/data/foo\u0026quot; {\n  capabilities = [\u0026quot;read\u0026quot;]\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e這個範例 policy 授予了對 KV v2 秘密管理（secrets engine）的資源的能力。如果你對於與這個秘密管理引擎相關的路徑不熟悉，可以查閱該秘密管理引擎文件中的 \u003ca href=\"https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v2#acl-rules\"\u003eACL 規則\u003c/a\u003e部分。\u003c/p\u003e","title":"Vault Workshop 07: Policy \u0026 approle"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 06：Github auth method 不需要使用 root token 的 auth method: github Vault 支援用於人員使用者的身份驗證方法。GitHub 身份驗證，使用戶可以通過提供他們的 GitHub 憑證來驗證 Vault，並收到一個 Vault token。\n簡單來說，github organization chechia-net，可以設定適當的權限給成員 chechiachang，讓 chechiachang 可以透過 github 取得有權限的 token。\n注意\n在練習中所描述的這種身份驗證方法，需要你擁有 GitHub、屬於 GitHub org 中的一個 team ，並生成了具有 read:org scope 的 GitHub personal access token。你可以於 github 創建一個 free plan 的 org，並設定一個 team，然後透過個人 Settings / Developer Settings 來產生一把具有 read:org 權限的 personal access token\n啟用 github auth method 你可以使用下列指令，在 path=github/ 下啟用 GitHub 身份驗證方法。\nexport VAULT_TOKEN=hvs.qCqY...9KYv vault auth enable github 這個指令使用 root token 設定 auth method，也是整個 github auth methods 中，唯一需要用到 root token 設定的地方，而且只需要設定一次\noutput\nSuccess! Enabled github auth method at: github/ vault server log 顯示 credential backend 已啟用\n2023-09-16T21:40:10.444+0800 [INFO] core: enabled credential backend: path=github/ type=github version=\u0026quot;\u0026quot; 你可以下列指令，列出所有的 auth methods\nvault auth list output\nPath Type Accessor Description Version ---- ---- -------- ----------- ------- github/ github auth_github_9bc96e5f n/a n/a token/ token auth_token_b7984c52 token based credentials n/a github 身份驗證方法已啟用，並位於路徑 auth/github/。\n設定 github auth method 此身份驗證方法需要你在配置中設置 GitHub organization。GitHub organization中，維護了一個允許與 Vault 驗證的使用者列表。\n設置 GitHub 身份驗證的組織。\nvault write auth/github/config organization=chechia-net output\nSuccess! Data written to: auth/github/config 現在，chechia-net GitHub 組織中的所有使用者都可以進行身份驗證。\n設定 github auth method，給不同 team 不同的 policy GitHub 組織可以定義團隊(team)。每個團隊可能可以在組織維護的所有 repository 中執行不同的操作。這些團隊也可能需要訪問 Vault 內的特定密碼。\n配置 GitHub sre 團隊的身份驗證，以獲得 default 和 application 兩個 policy的授權。\nvault write auth/github/map/teams/sre value=default,application output\nSuccess! Data written to: auth/github/map/teams/sre GitHub sre 團隊中 chechia-net 組織的成員，將使用 default 和 application policy 進行身份驗證和授權。\n備註：application policy 尚未在 Vault 中定義。在該策略定義之前，Vault 仍然允許使用者進行身份驗證，但會產生警告。\n你可以使用指令顯示 Vault 啟用的所有身份驗證方法\nvault auth list 回傳顯示兩個 auth methods，github/ 與 token/\nPath Type Accessor Description Version ---- ---- -------- ----------- ------- github/ github auth_github_9bc96e5f n/a n/a token/ token auth_token_b7984c52 token based credentials n/a 使用 github auth method login 使用 help 指令來了解 github auth method 的說明\nvault auth help github output\nUsage: vault login -method=github [CONFIG K=V...] The GitHub auth method allows users to authenticate using a GitHub personal access token. Users can generate a personal access token from the settings page on their GitHub account. # GitHub 身份驗證方法允許使用者使用 GitHub personal access token進行身份驗證。使用者可以從其 GitHub Settings -\u0026gt; Developer Settings -\u0026gt; Personal access tokens (classic) 生成個人訪問令牌。 Authenticate using a GitHub token: $ vault login -method=github token=abcd1234 Configuration: mount=\u0026lt;string\u0026gt; Path where the GitHub credential method is mounted. This is usually provided via the -path flag in the \u0026quot;vault login\u0026quot; command, but it can be specified here as well. If specified here, it takes precedence over the value for -path. The default value is \u0026quot;github\u0026quot;. # GitHub 憑證方法掛載的路徑。通常透過 \u0026quot;vault login\u0026quot; 指令的 -path flag 提供，但這裡也可以指定。如果在這裡指定，則優先於 -path 的值。預設的路徑值為 \u0026quot;github\u0026quot;。 token=\u0026lt;string\u0026gt; GitHub personal access token to use for authentication. If not provided, Vault will prompt for the value. # 用於身份驗證的 GitHub personal access token。如果未提供，Vault 將提示輸入此值。 輸出顯示了使用 GitHub 方法進行 login 的例子。該方法要求必須定義該方法，並由使用者提供 GitHub personal access token。\n由於你將嘗試使用身份驗證方法進行登錄，請確保在此 shell session 中未設置 VAULT_TOKEN 環境變數，因為其值將優先於你從 Vault 獲取的任何 token。\n取消設定該環境變數。\nunset VAULT_TOKEN 嘗試使用 github auth methods 登入\nvault login -method=github output，使用者使用 github personal access token，取得有效的 vault token\ntoken 格式不同 token 的效期為 768h，短效期 token 的曝險程度較低 這個 token 的 policy 權限只有 default policy metadata 是 github 回傳的，vault 中並沒有建立 username=chechiachang 的資料 org 是 chechia-net username 是 chechiachang GitHub Personal Access Token (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.CAESI...ncU0 token_accessor lKVpMawotXg1fXcgRvpAOAwQ token_duration 768h token_renewable true token_policies [\u0026quot;default\u0026quot;] identity_policies [] policies [\u0026quot;default\u0026quot;] token_meta_org chechia-net token_meta_username chechiachang 當未提供 GitHub personal access token 給命令時，Vault CLI 會提示使用者輸入。\n如果提供了有效的 GitHub personal access token，則使用者將成功登錄 vault，並且輸出會顯示一個 Vault token。使用者可以使用這個 Vault token，直到該 token 被撤銷或其有效期超過了 token_duration。\n你可以使用 vault token lookup 來檢視當前的 token\nvault token lookup output，顯示使用中的 token，使用的 path 與 methods 是 github/，效期 ttl 在倒數中逐漸減少 767h54m27s\nKey Value --- ----- accessor lKVpMawotXg1fXcgRvpAOAwQ creation_time 1694873434 creation_ttl 768h display_name github-chechiachang entity_id 5d8af1ab-53a2-e1c5-61bf-bb0b6c7d190f expire_time 2023-10-18T22:10:34.812089+08:00 explicit_max_ttl 0s id hvs.CAESI...ncU0 issue_time 2023-09-16T22:10:34.812099+08:00 meta map[org:chechia-net username:chechiachang] num_uses 0 orphan true path auth/github/login policies [default] renewable true ttl 767h54m27s type service 記得我們建立 team 的 policy = default + application 嗎？你可以在 github 上把 user 加入到 team/sre\n例如我們把 chechiachang 加入到 org/chechia-net 的 team/sre\n當前登入 session 權限尚未更新，我們直接重新登入一次\nvault login -method=github output，可以看見登入後的 policy 權限增加成為 default + application\nGitHub Personal Access Token (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.CAES...RcmE token_accessor 6DbsDizOenhCuWMmUWspTZwp token_duration 768h token_renewable true token_policies [\u0026quot;application\u0026quot; \u0026quot;default\u0026quot;] identity_policies [] policies [\u0026quot;application\u0026quot; \u0026quot;default\u0026quot;] token_meta_org chechia-net token_meta_username chechiachang 以上是透過 github/ auth methods\n管理者設定 github/ method config 使用者 chechiachang，使用 github/ method login 使用的權限管理是依照 github org / team 來管理，vault 並沒有紀錄 team 與使用者資料 清理：移除 auth methods 使用 root token 再次進行登錄。\nvault login output\nToken (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.J80w...mjoh token_accessor DaI1V1rq9tKZX5EaO4AfMI26 token_duration ∞ token_renewable false token_policies [\u0026quot;root\u0026quot;] identity_policies [] policies [\u0026quot;root\u0026quot;] 使用以下指令撤銷 github/ auth methods 產生的 token\nvault token revoke -mode path auth/github output\nSuccess! Revoked token (if it existed) vault server log，對於 auth/github 路徑的所有登錄生成的令牌都已被撤銷。\n2023-09-16T22:25:06.793+0800 [INFO] expiration: revoked lease: lease_id=auth/github/login/h71720c9bc6fed61d459da4b172dc518a6d55b1dd33c70103cf0b12eb42d7741a 2023-09-16T22:25:06.793+0800 [INFO] expiration: revoked lease: lease_id=auth/github/login/hd2b110a8eda38814ac0e3ba7ccb926e75205c5065480c9c2675ae5f2860f9f97 所有身份驗證方法，除了 token 身份驗證方法，都可以被停用。\n停用 github 身份驗證方法\nvault auth disable github output\nSuccess! Disabled the auth method (if it existed) at: github/ vault server log\n2023-09-16T22:27:42.040+0800 [INFO] core: disabled credential backend: path=auth/github/ 所有使用此身份驗證方法進行登錄生成的token都已被撤銷。\n清理 github 如果有建立 org / team / github personal access token 的話，記得移除他\nchatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-authentication https://developer.hashicorp.com/vault/docs/auth/github 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-17-vault-workshop-authentication-github/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-06github-auth-method\"\u003eDay 06：Github auth method\u003c/h1\u003e\n\u003ch3 id=\"不需要使用-root-token-的-auth-method-github\"\u003e不需要使用 root token 的 auth method: github\u003c/h3\u003e\n\u003cp\u003eVault 支援用於人員使用者的身份驗證方法。GitHub 身份驗證，使用戶可以通過提供他們的 GitHub 憑證來驗證 Vault，並收到一個 Vault token。\u003c/p\u003e\n\u003cp\u003e簡單來說，github organization \u003ca href=\"https://github.com/chechia-net\"\u003echechia-net\u003c/a\u003e，可以設定適當的權限給成員 chechiachang，讓 chechiachang 可以透過 github 取得有權限的 token。\u003c/p\u003e\n\u003cp\u003e注意\u003c/p\u003e\n\u003cp\u003e在練習中所描述的這種身份驗證方法，需要你擁有 GitHub、屬於 GitHub org 中的一個 team ，並生成了具有 \u003ccode\u003eread:org\u003c/code\u003e scope 的 GitHub personal access token。你可以於 github 創建一個 free plan 的 org，並設定一個 team，然後透過個人 Settings / Developer Settings 來產生一把具有 \u003ccode\u003eread:org\u003c/code\u003e 權限的 personal access token\u003c/p\u003e\n\u003ch3 id=\"啟用-github-auth-method\"\u003e啟用 github auth method\u003c/h3\u003e\n\u003cp\u003e你可以使用下列指令，在 path=github/ 下啟用 GitHub 身份驗證方法。\u003c/p\u003e","title":"Vault Workshop 06: Github Auth method"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 05：Authentication 在前面的文章中，你已經創建了第一個秘密，了解了秘密引擎，並在以開發模式啟動的 Vault 服務器中探索了動態秘密。\n接下來的內容中，我們將深入研究使用 Vault token 和 GitHub 憑證進行身份驗證。\nToken 身份驗證 Token 身份驗證已自動啟用。當你啟動開發模式服務器時，輸出顯示了 Root token。Vault CLI 從 $VAULT_TOKEN 環境變數中讀取 Root token。這個根 token 可以在 Vault 內執行任何操作，因為它被分配了 Root policy 權限。允許的權限中包含創建新的 token。\n現在，讓我們創建一個新的 token。\n啟動全新的 dev Vault Server\nvault server -dev output，server log 回傳 dev 模式的警告\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. # 警告！已啟用開發模式！在此模式下，Vault完全運行在內存中，使用單個unseal key解封。 # root token已被CLI驗證，因此你可以立即開始使用Vault。 You may need to set the following environment variables: $ export VAULT_ADDR='http://127.0.0.1:8200' # 以下顯示了unseal key 和 root token，以防你想要封存/解封Vault，或重新進行身份驗證。 The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. # dev 模式預設配置的 unseal key 與 root token Unseal Key: QDUA...im4= Root Token: hvs.J30e...0DJaN # 開發模式絕不應在生產安裝中使用！ Development mode should NOT be used in production installations! 依據提示 export 需要的變數\nexport VAULT_ADDR='http://127.0.0.1:8200' 在 dev mode 下，未使用 token 也可以存取 Vault Server 中的資料，這是因為 vault server 在啟用時，預設使用 token helper，將 root token 寫入到 local filesystem\nVault token helper 說明\n你可以使用以下指令，檢查儲存於本地的 vault token\ncat ~/.vault-token output\nhvs.J30e...0DJaN% vault CLI 自動取用本地儲存的 root token，所以已經自動完成 authentication，不用額外進行 authentication，也可以使用 Vault\n使用 Vault 時，須額外注意本地儲存的 token\n避免 dev 模式下儲存 root token Vault dev server 自動寫入 root token，讓開發 Vault 功能時十分便利。然而在資訊安全的領域，便利往往代表著風險。在非 dev Server 環境中，我們會嚴格控制所有 token 的曝險程度，特別是 root token，根本不該被 print 出，當然也萬萬不能儲存在本地電腦。\n依據你的開發需求，你可能不希望 dev 模式下，long-live 永久有效的 root token 儲存在本地電腦中，你可以使用 -dev-no-store-token flag 來避免 Vault dev server 暴露 root token。\nvault server -dev -dev-no-store-token output\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variables: $ export VAULT_ADDR='http://127.0.0.1:8200' The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: hNi8T3YctTZrLYlKezLAJttfiF97D1Vy7Tq+HMM3y9w= Root Token: hvs.qCqY...p89KYv Development mode should NOT be used in production installations! 你可以檢查本地使否有儲存的的 vault token\ncat ~/.vault-token output\ncat: /Users/che-chia/.vault-token: No such file or directory 然後試圖在沒有 .vault-token 的狀態下，存取 Vault Server\nvault secrets list output，沒有合法的 access token，Vault server 回傳 403 權限被拒\nvault secrets list Error listing secrets engines: Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/mounts Code: 403. Errors: * permission denied Authentication 你可以使用 vault dev Server log 中回傳的 root token 來存取 Vault server\n你可以將 token 在寫回 .vault-token，這是最方便，也最危險\n建議：永遠不要儲存 root token 在本地電腦上。非常容易遺忘自己本地有儲存 root token。\necho hvs.qCqY...p89KYv \u0026gt; ~/.vault-token 另一個方式是使用環境變數 VAULT_TOKEN\nVAULT_TOKEN=hvs.qCqY...p89KYv vault secrets list 上面是在 CLI 前面插入環境變數，下面是 export VAULT_TOKEN 到當前 session 的環境變數\nexport VAULT_TOKEN=hvs.qCqY...p89KYv vault secrets list output\nPath Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9cc57bcf per-token private secret storage identity/ identity identity_41faabef identity store secret/ kv kv_fca914b5 key/value secret storage sys/ system system_5dd83198 system endpoints used for control, policy and debugging 你可以檢查目前環境變數中，與 Vault 有關的 env\nenv | grep VAULT output\nVAULT_ADDR=http://127.0.0.1:8200 VAULT_TOKEN=hvs.qCqY...p89KYv 當然，VAULT_TOKEN 的存在時間越久，token 曝險的機率就越高。\n也很容易忘記 VAULT_TOKEN 有存在的環境變數。\nVAULT TOKEN 雷點 workshop 至今，你目前只有一台 vault dev server，而且裡面並沒有任何的機密資料，這個 vault server 可以隨時拋棄。\n然而，在實務上，Vault 管理員手上可能會有多個 Vault server 需要管理。\n想像你有 dev / stag / prod 的 server，某一天你正在 dev 開發一個新功能，由於本地有 VAULT_TOKEN 與 VAULT_ADDR，你不疑有他的在這台 server 上運行許多測試的指令，包含新增一對測試 secret，刪刪改改現有的 secret。\n然後你的本能忽然覺得怪怪的，我沒有設置任何 vault env，為何可以直接存取 dev server？\n你拉出 env 看，發現是 production server 的 addr + token，上次進入 production server 時的 session，還存有有效的 addr 與 token\nVAULT_ADDR=http://vault.prod.chechia.net VAULT_TOKEN=this-is-prod-token 你麻煩大了\n心得：永遠不要儲存長效期的 token 在本地。本 workshop 會不斷地強調，提醒各種操作中的資安風險，請各位學習過程中就養成良好習慣，以免方便一時，遺憾終身。\nroot token https://developer.hashicorp.com/vault/docs/concepts/tokens#root-tokens\nroot token是附帶root policy的token。root token可以在Vault中執行任何操作。任何操作。\n此外，它們是Vault中唯一可以設置為永不過期，且無需進行任何續訂的 token 類型。由於 root token 權限如此大，Vault 設計上刻意讓創建root token 很困難；實際上只有三種方法可以創建root token：\n在 vault operator init 時生成的初始root token - 此token不會過期\n使用另一個root token 創建 root token。這點有個限制：使用有限期的root token，無法創建永不過期的root token\n在擁有 unseal keys quorum的權限下，使用 vault operator generate-root 來創建root token\nroot token在開發中很有用，但在 production 環境中應該非常謹慎使用。\n事實上，Vault團隊建議 root token 只能進行必要的初始設置，通常是設置身份驗證方法(auth method)，和必要的 policy，以允許管理員獲取更有限的token。或是緊急情況下使用root token，並在不再需要時立即撤銷(revoke)。\n如果需要新的root token，可以使用 operator generate-root 指令，使用相關的 API endpoint即時生成。\n同時，當root token處於活動狀態時，建議團隊中提供多人一起協作，共同檢查 terminal session，養成一種良好的安全實踐。這樣多人可以驗證使用root token執行的任務，以及這些任務完成後，立即撤銷了該token。\n使用 root token 產生 short-live token root token 預設配置 root policy，我們可以產生權限較小的 token，並指配置最小必要權限(least privilege)\nVAULT_TOKEN=hvs.qCqY...p89KYv vault token create Key Value --- ----- token hvs.FbAeX...kjBIp token_accessor rPOOSI06WGnFo9MVOvS8luhn token_duration ∞ token_renewable false token_policies [\u0026quot;root\u0026quot;] identity_policies [] policies [\u0026quot;root\u0026quot;] 已創建token，輸出中以key value的表格描述了此token。創建的token在此處顯示為 hvs.FbAeX\u0026hellip;kjBIp\n此token是root token的子token，並且預設情況下，它會繼承其 parent token的policy 權限。\ntoken是核心身份驗證方法(core auth method)。你可以使用生成的token來 login Vault，只需在提示時複製並粘貼即可。\n你可以使用以下指令登入 vault\nvault login 在回傳的輸入令中輸入新產生的 child token hvs.FbAeX\u0026hellip;kjBIp\nWARNING! The VAULT_TOKEN environment variable is set! The value of this variable will take precedence; if this is unwanted please unset VAULT_TOKEN or update its value accordingly. # 警告！已設置 VAULT_TOKEN 環境變數！此變數的值將優先考慮；如果不需要此設置，請取消設置 VAULT TOKEN 或相應地更新其值。 由於在前面的操作中，環境變數中仍帶有 VAULT_TOKEN，而且是更高權限的永不過期 root token，vault CLI 偵測到 VAULT TOKEN env，跳出警告。\n你可以透過以下指令清除 VAULT_TOKEN 環境變數\nunset VAULT_TOKEN 然後再次執行 vault login，在回傳的輸入令中輸入新產生的 child token hvs.FbAeX\u0026hellip;kjBIp\nvault login output，顯示成功登入，並回傳 token 的相關訊息\nToken (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026quot;vault login\u0026quot; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.FbAeX...kjBIp token_accessor rPOOSI06WGnFo9MVOvS8luhn token_duration ∞ token_renewable false token_policies [\u0026quot;root\u0026quot;] identity_policies [] policies [\u0026quot;root\u0026quot;] 此token是root token的子token，繼承其 parent token的policy 權限，也就是 root。本身是無效期限制的永久 root token。\n在創建一把 token，此時由於 vault login，進入登入狀態，token 已經存在本地中\nvault token create output，第二隻 token 為 hvs.FDjvyRFXo\u0026hellip;QVFG。每一隻 token 都是不重複的。\nKey Value --- ----- token hvs.FDjvyRFXo...QVFG token_accessor gLe7IrUMUq4eb2pZWVdUhbHv token_duration ∞ token_renewable false token_policies [\u0026quot;root\u0026quot;] identity_policies [] policies [\u0026quot;root\u0026quot;] revoke 撤銷 token 當我們不再需要使用 token 時，最好盡快撤銷 revoke token\n目前的 token 樹狀結構\n初始 root token (啟動 dev Server 時預設產生的)\nchild: hvs.FbAeX\u0026hellip;kjBIp grandchild: hvs.FDjvyRFXo\u0026hellip;QVFG 你可以使用指令，撤銷第一把產生的 token (child)\nvault token revoke hvs.FbAeX...kjBIp CLI output，顯示 token 已經撤銷\nSuccess! Revoked token (if it existed) 同時，vault server log 紀錄 revoked lease\n2023-09-16T21:29:21.464+0800 [INFO] expiration: revoked lease: lease_id=auth/token/create/hf153f5e80ed722f6816f592032a43906a5606a889bab4c13ff3368c9a1b95b69 還記得我們是使用地一把產生的 token (child) login 的嗎？目前的登入當然也隨著 token 失效，嘗試存取回傳 403 權限被拒\nvault secrets list Error listing secrets engines: Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/mounts Code: 403. Errors: * permission denied 嘗試重新 login，使用地一把產生的 token (child) 一樣被拒\nvault login output\nToken (will be hidden): Error authenticating: error looking up token: Error making API request. URL: GET http://127.0.0.1:8200/v1/auth/token/lookup-self Code: 403. Errors: * permission denied 撤銷token 時，也會一併撤銷使用這把 token 產生的子 token (grandchild)\n嘗試重新 login，使用第二把產生的 token (grandchild) 一樣被拒\nvault login output\nToken (will be hidden): Error authenticating: error looking up token: Error making API request. URL: GET http://127.0.0.1:8200/v1/auth/token/lookup-self Code: 403. Errors: * permission denied 為何總是要使用 root token 上面講了 vault 核心的 token auth method，相信讀者一定有一個奇怪的感覺\n最開始 login 需要 token 產生 token 在去產生新的 token 這樣產生 token 的 admin 不就需要將 token 先傳給使用者，使用者才能登入嗎？這樣傳遞 token 的行為，不是反而增加更大的曝險。\n這樣真的有比一般使用 username password 登入的方法更安全嗎？\n上面的想法都是有道理的，也就是說，光使用 token auth methods 是無法滿足我們的安全需求。這些安全需求，需要使用其他 auth method 方法來解決，而 vault 強大之處便是支援的許多 auth method，將 vault 的安全建立在其他更穩定的服務上，例如 github / aws / azure / gcp / kubernetes 等。\n下一篇，我們就來實作 github auth method。\nchatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-authentication 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 不要使用敬語，翻譯結果中若出現\u0026quot;您\u0026quot;，請用\u0026quot;你\u0026quot;取代\u0026quot;您\u0026quot;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;數據庫\u0026quot; 改為 \u0026quot;資料庫\u0026quot;，將 \u0026quot;數據\u0026quot; 改為 \u0026quot;資料\u0026quot;，將 \u0026quot;訪問\u0026quot; 改為 \u0026quot;存取\u0026quot;，將 \u0026quot;源代碼\u0026quot; 改為 \u0026quot;原始碼\u0026quot;，將 \u0026quot;信息\u0026quot; 改為 \u0026quot;資訊\u0026quot;，將 \u0026quot;命令\u0026quot; 改為 \u0026quot;指令\u0026quot;，將 \u0026quot;禁用\u0026quot; 改為 \u0026quot;停用\u0026quot;，將 \u0026quot;默認\u0026quot; 改為 \u0026quot;預設\u0026quot;。 ","permalink":"https://chechia.net/posts/2023-09-16-vault-workshop-authentication/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-05authentication\"\u003eDay 05：Authentication\u003c/h1\u003e\n\u003cp\u003e在前面的文章中，你已經創建了第一個秘密，了解了秘密引擎，並在以開發模式啟動的 Vault 服務器中探索了動態秘密。\u003c/p\u003e\n\u003cp\u003e接下來的內容中，我們將深入研究使用 Vault token 和 GitHub 憑證進行身份驗證。\u003c/p\u003e\n\u003ch3 id=\"token-身份驗證\"\u003eToken 身份驗證\u003c/h3\u003e\n\u003cp\u003eToken 身份驗證已自動啟用。當你啟動開發模式服務器時，輸出顯示了 Root token。Vault CLI 從 \u003ccode\u003e$VAULT_TOKEN\u003c/code\u003e 環境變數中讀取 Root token。這個根 token 可以在 Vault 內執行任何操作，因為它被分配了 Root policy 權限。允許的權限中包含創建新的 token。\u003c/p\u003e\n\u003cp\u003e現在，讓我們創建一個新的 token。\u003c/p\u003e\n\u003cp\u003e啟動全新的 dev Vault Server\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003evault server -dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eoutput，server log 回傳 dev 模式的警告\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory\nand starts unsealed with a single unseal key. The root token is already\nauthenticated to the CLI, so you can immediately begin using Vault.\n\n# 警告！已啟用開發模式！在此模式下，Vault完全運行在內存中，使用單個unseal key解封。\n# root token已被CLI驗證，因此你可以立即開始使用Vault。\n\nYou may need to set the following environment variables:\n\n    $ export VAULT_ADDR='http://127.0.0.1:8200'\n\n# 以下顯示了unseal key 和 root token，以防你想要封存/解封Vault，或重新進行身份驗證。\n\n    The unseal key and root token are displayed below in case you want to\n    seal/unseal the Vault or re-authenticate.\n\n# dev 模式預設配置的 unseal key 與 root token\n    Unseal Key: QDUA...im4=\n    Root Token: hvs.J30e...0DJaN\n\n# 開發模式絕不應在生產安裝中使用！\n    Development mode should NOT be used in production installations!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e依據提示 export 需要的變數\u003c/p\u003e","title":"Vault Workshop 05: Authentication"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 04：Secret Engine KV V2 KV秘密引擎，用於在Vault的已配置物理Storage中，存儲任意秘密。\nkey 名稱必須始終是字符串。如果你直接通過CLI編寫非字串 value，它們將被轉換為字串。但是，你可以通過從JSON文件中將key-value pair寫入Vault，或使用HTTP API來保留非字串 value。\n此秘密引擎遵照ACL policy中，創建(create)和更新(update)權限之間的設定。它還支持patch功能，用於表示部分更新(patch)，而更新功能(update)則表示完全覆蓋。\n設置 大多數秘密引擎必須在執行其功能之前事先配置。這些步驟通常由操作人員或配置管理工具完成。\n啟用v2 kv秘密引擎：\n啟動一個乾淨的本地的開發模式 Vault Server，全新的 Vault Server 包含底下預設啟用的引擎\nvault server -dev export VAULT_ADDR='http://127.0.0.1:8200' vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_57a4ca51 per-token private secret storage identity/ identity identity_c2ecbd49 identity store secret/ kv kv_c41afde8 key/value secret storage sys/ system system_c063e514 system endpoints used for control, policy and debugging 你可以在指定的路徑下，啟用一個新的 v2 kv 秘密引擎。若不指定 -path 參數，則預設 path 為 type，也就是 path=kv。\nvault secrets enable -version=2 -path=kv kv Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_57a4ca51 per-token private secret storage identity/ identity identity_c2ecbd49 identity store kv/ kv kv_904eee17 n/a secret/ kv kv_c41afde8 key/value secret storage sys/ system system_c063e514 system endpoints used for control, policy and debugging 或者，你可以將kv-v2作為秘密引擎類型，以參數傳遞：\nvault secrets enable kv-v2 上面的指令與下面的指令等效，重複執行會出現路徑重複錯誤\nvault secrets enable -path kv-v2 -version=2 kv-v2 Error enabling: Error making API request. URL: POST http://127.0.0.1:8200/v1/sys/mounts/kv-v2 Code: 400. Errors: * path is already in use at kv-v2/ 你可以停用一個秘密引擎\nvault secrets disable kv-v2 為了學習效果，往後課程都會使用更完整的指令，作為示範。\n更多啟用秘密引擎的指令，你可以參考 vault secrets enable \u0026ndash;help 的協助指令\nvault secrets enable --help Usage: vault secrets enable [options] TYPE Enables a secrets engine. By default, secrets engines are enabled at the path corresponding to their TYPE, but users can customize the path using the -path option. 啟用秘密引擎。預設情況下，秘密引擎會在與其類型相對應的路徑啟用，但使用者可以使用 -path 選項自訂路徑。 Once enabled, Vault will route all requests which begin with the path to the secrets engine. 一旦啟用，Vault 將將所有以該路徑開頭的請求路由到秘密引擎。 Enable the AWS secrets engine at aws/: $ vault secrets enable aws Enable the SSH secrets engine at ssh-prod/: $ vault secrets enable -path=ssh-prod ssh Enable the database secrets engine with an explicit maximum TTL of 30m: $ vault secrets enable -max-lease-ttl=30m database Enable a custom plugin (after it is registered in the plugin registry): $ vault secrets enable -path=my-secrets -plugin-name=my-plugin plugin OR (preferred way): $ vault secrets enable -path=my-secrets my-plugin 從上面的內容，可以看到秘密引擎支援各種不同的秘密類型，例如以下都是一個秘密引擎類型：aws，ssh，database，plugin。\n注意，請不要弄混 path=aws 的秘密引擎，以及 type=aws 的秘密引擎。底下是一個十分混淆的例子：\nvault secrets enable --version=2 --path=kv-v1 kv vault secrets enable --version=1 --path=kv-v2 kv vault secrets list --detailed Path Plugin Accessor Default TTL Max TTL Force No Cache Replication Seal Wrap External Entropy Access Options Description UUID Version Running Version Running SHA256 Deprecation Status ---- ------ -------- ----------- ------- -------------- ----------- --------- ----------------------- ------- ----------- ---- ------- --------------- -------------- ------------------ cubbyhole/ cubbyhole cubbyhole_57a4ca51 n/a n/a false local false false map[] per-token private secret storage cee0001e-95f8-efaa-6a9a-a3d5c3573abc n/a v1.14.3+builtin.vault n/a n/a identity/ identity identity_c2ecbd49 system system false replicated false false map[] identity store 3b481e0d-1c6d-6698-85c3-c52839b4d6e4 n/a v1.14.3+builtin.vault n/a n/a kv-v1/ kv kv_c8de7a39 system system false replicated false false map[version:2] n/a 98289587-5b5d-06f3-c3cf-469e830eda9e n/a v0.15.0+builtin n/a supported kv-v2/ kv kv_f83d9acd system system false replicated false false map[version:1] n/a 5fc 請不要做這種不良的命名，誤導自己也誤導別人。不建議把 engine type 當作 path，實務上也沒有必要這個做。\nvault secrets disable kv-v1 vault secrets disable kv-v2 使用公司組織，團隊，專案名稱，作為 engine path 的命名，是個不錯的起點。\norg = chechia.net team = sre project = workshop vault secrets enable --version=2 --path=chechia-net/sre/workshop kv 使用 v2 kv 在秘密引擎配置完成，且用戶/機器具備具有適當權限的Vault token後，它可以生成憑證。KV秘密引擎允許將任意vvalue寫入指定的key。\n在KV-v2中仍然可以使用類似路徑的KV-v1語法來引用秘密（secret/foo），但我們建議使用-flag的語法以避免將其錯誤認為是秘密的實際路徑（secret/data/foo是真正的路徑）。\n寫入/讀取任意資料 寫入任意資料：\nvault kv put -mount=chechia-net/sre/workshop my-secret foo=a bar=b ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 讀取任意資料：\nvault kv get -mount=chechia-net/sre/workshop my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 === Data === Key Value --- ----- bar b foo a 多版本控制 v2 kv 支援記錄多版本的秘密。請提供另一個版本，以前的版本仍然可訪問。\n可以選擇傳遞-cas flag以執行寫入前檢查(check-and-set)。如果未設置，將直接允許寫入。\n為了使寫入成功，cas必須設置為秘密的當前版本。如果設置為0，只有在密鑰不存在時才允許寫入，因為未設置的密鑰沒有任何版本信息。\n還要記住，soft delete 不會從存儲中刪除任何底層版本數據。為了寫入 soft delete 的密鑰，cas參數必須匹配密鑰的當前版本。\nvault kv put -mount=chechia-net/sre/workshop -cas=1 my-secret foo=aa bar=bb ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:50:26.621978Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 2 讀取會回傳最新版本的秘密\nvault kv get -mount=chechia-net/sre/workshop my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:50:26.621978Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 2 === Data === Key Value --- ----- bar bb foo aa 可以使用vault kv patch命令進行部分更新。\n該命令將首先嘗試進行HTTP PATCH請求，該請求需要patch ACL功能。如果使用的 token 沒有允許patch功能的policy，則PATCH請求將失敗。\n在現在的範例，完整的PATCH命令將執行讀取、本地更新和後續寫入，這需要讀取和更新ACL policy權限。\n可以選擇傳遞-cas flag 以執行寫入前檢查(check-and-set)。-cas 只會在初始PATCH請求的情況下生效。\nvault kv patch -mount=chechia-net/sre/workshop -cas=2 my-secret bar=bbb ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:54:03.925959Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 3 vault kv patch命令還支持一個 -method 標誌，可用於指定兩種 method\nHTTP PATCH 讀取後寫入(read-and-write) 支持的值分別是patch和rw 使用patch方法\nvault kv patch -mount=chechia-net/sre/workshop -method=patch -cas=3 my-secret bar=bbb ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T17:02:55.522095Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 4 使用read-and-write\n如果使用讀取後寫入(read-and-write)流程，將使用讀取返回的秘密版本值，在後續寫入中，執行寫入前檢查(check-and-set)。\nvault kv patch -mount=chechia-net/sre/workshop -method=rw my-secret bar=ccc ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T17:03:37.331182Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 5 你可以使用 -version flag 來存取更早的版本\nvault kv get -mount=chechia-net/sre/workshop -version=1 my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 === Data === Key Value --- ----- bar b foo a 刪除資料 在刪除資料時，標準的 vault kv delete 命令將執行 soft delet。它將標記版本為已刪除並填充 deletion_time timestamp。soft delete 不會從存儲中刪除底層版本資料，這允許版本可以被還原\n還原版本\nvault kv undelete 只有當秘密的版本數超過 max-versions 設置允許的版本數時，或者使用 vault kv destroy 時，版本的資料才會永久刪除。\n使用 destroy 命令時，底層版本資料將被刪除，並將密鑰 metadata 標記為已銷毀。如果通過超過 max-versions 來清理版本，則版本 metadata 也將從密鑰中刪除。\n可以使用 delete 命令刪除密鑰的最新版本，這也可以使用 -versions 標誌刪除之前的版本：\nvault kv delete -mount=chechia-net/sre/workshop my-secret Success! Data deleted (if it existed) at: chechia-net/sre/workshop/data/my-secret 讀取 version=5 時，只回傳 metadata，無法讀取資料\nvault kv get -mount=chechia-net/sre/workshop -version=5 my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T17:03:37.331182Z custom_metadata \u0026lt;nil\u0026gt; deletion_time 2023-09-15T17:10:01.558586Z destroyed false version 5 讀取 version=4 時，可以順利讀取資料\nvault kv get -mount=chechia-net/sre/workshop -version=4 my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T17:02:55.522095Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 4 === Data === Key Value --- ----- bar bbb foo aa 你可以復原一個刪除的版本 version=5\nvault kv undelete -versions=5 chechia-net/sre/workshop/my-secret Success! Data written to: chechia-net/sre/workshop/undelete/my-secret 這邊 undelete 使用了 legacy 的 secret path，而不使用 -mount flag，是因為 v2 kv 在處理多層 mount path 的 bug Github Issue: #19811，PR #19811 is comming lol。\nvault kv undelete -versions=5 -mount=chechia-net/sre/workshop my-secret Error writing data to workshop/undelete/my-secret: Error making API request. URL: PUT http://127.0.0.1:8200/v1/workshop/undelete/my-secret Code: 404. Errors: * no handler for route \u0026quot;workshop/undelete/my-secret\u0026quot;. route entry not found. metadata 所有版本和密鑰 metadata 可以使用 metadata 命令和API 進行檢查。\n刪除 metadata 密鑰將導致該密鑰的所有 metadata 和版本永久刪除。\n有關更多信息，请參見以下命令：\n可以查看密鑰的所有元數據和版本：\nvault kv metadata get -mount=chechia-net/sre/workshop my-secret ============== Metadata Path ============== chechia-net/sre/workshop/metadata/my-secret ========== Metadata ========== Key Value --- ----- cas_required false created_time 2023-09-15T16:48:45.817265Z current_version 5 custom_metadata \u0026lt;nil\u0026gt; delete_version_after 0s max_versions 0 oldest_version 0 updated_time 2023-09-15T17:03:37.331182Z ====== Version 1 ====== Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z deletion_time n/a destroyed false ====== Version 2 ====== Key Value --- ----- created_time 2023-09-15T16:50:26.621978Z deletion_time n/a destroyed false ====== Version 3 ====== Key Value --- ----- created_time 2023-09-15T16:54:03.925959Z deletion_time n/a destroyed false ====== Version 4 ====== Key Value --- ----- created_time 2023-09-15T17:02:55.522095Z deletion_time n/a destroyed false ====== Version 5 ====== Key Value --- ----- created_time 2023-09-15T17:03:37.331182Z deletion_time 2023-09-15T17:15:40.767682Z destroyed false 你可以透過更改 metadata API 調整 secret 的屬性\nvault kv metadata put -mount=chechia-net/sre/workshop \\ -max-versions 2 \\ -delete-version-after=\u0026quot;3h25m19s\u0026quot; my-secret Success! Data written to: secret/metadata/my-secret Delete-version-after 只會應用在新的版本上，Max versions changes 會應用在下次寫入\nvault kv put -mount=chechia-net/sre/workshop my-secret new=123 ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T17:34:09.61954Z custom_metadata \u0026lt;nil\u0026gt; deletion_time 2023-09-15T20:59:28.61954Z destroyed false version 6 根據 max-version 的設定結果，只保留 version=5 與 version=6，更舊的版本都被刪除\nvault kv metadata get -mount=chechia-net/sre/workshop my-secret ============== Metadata Path ============== chechia-net/sre/workshop/metadata/my-secret ========== Metadata ========== Key Value --- ----- cas_required false created_time 2023-09-15T16:48:45.817265Z current_version 6 custom_metadata \u0026lt;nil\u0026gt; delete_version_after 3h25m19s max_versions 2 oldest_version 5 updated_time 2023-09-15T17:34:09.61954Z ====== Version 5 ====== Key Value --- ----- created_time 2023-09-15T17:03:37.331182Z deletion_time 2023-09-15T17:15:40.767682Z destroyed false ====== Version 6 ====== Key Value --- ----- created_time 2023-09-15T17:34:09.61954Z deletion_time 2023-09-15T20:59:28.61954Z destroyed false Vault 命名空間(namespace) 和掛載(mount)結構 命名空間是功能上創建“Vault中的Vault”的隔離環境。它們具有獨立的登錄路徑，並支持創建和管理與其命名空間隔離的數據。此功能使你能夠將Vault作為服務提供給租戶。\n為什麼這個主題很重要？\nVault中的一切都是基於路徑的。每個路徑對應於Vault中的操作或秘密，Vault API endpoint映射到這些路徑。因此，編寫 policy 會配置對特定秘密 path 的允許操作。\n例如，為了授予管理根命名空間中的令牌的訪問權限，策略路徑是auth/token/。要管理教育命名空間中的令牌，完全合格的路徑在功能上變為education/auth/token/。\n以下圖表示了基於授權方法和秘密引擎啟用位置的API路徑。\n你可以使用命名空間，或為每個Vault客戶端專用的掛載來隔離秘密。\n例如，你可以為每個獨立的租戶創建一個命名空間，他們負責管理其命名空間下的資源。或者，你可以在組織內的每個團隊專用的路徑上安裝專用秘密引擎。\n根據如何隔離秘密，確定了誰負責管理這些秘密，更重要的是與這些秘密相關的策略。\nPrerequisites 如果你對Vault命名空間還不熟悉，請查看帶有命名空間的安全多租戶教程。\n注意\n創建命名空間應由具有高度特權令牌（例如root）的用戶執行，以為每個組織、團隊或應用程序設置隔離環境。\n部署注意事項 要計劃和設計Vault命名空間、授權方法路徑和秘密引擎路徑，你需要考慮如何為你的組織，設計Vault的邏輯對象。\n組織結構 - 你的組織結構是什麼？\n部門、團隊、服務、應用程序之間的層級，在Vault最終設計中需要反映什麼？\nVault policy 如何管理？\n團隊是否需要直接管理其負責範圍的 policy？\nchatGPT 本段內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/secrets/kv/kv-v2 https://developer.hashicorp.com/vault/tutorials/recommended-patterns/namespace-structure 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。不要使用敬語，請用你取代你。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：數據改為資料，數據庫改為資料庫，數據改為資料，訪問改為存取，源代碼改為原始碼，信息改為資訊，命令改為指令，禁用改為停用，默認改為預設。 ","permalink":"https://chechia.net/posts/2023-09-15-vault-workshop-secret-engine-kv-v2/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-04secret-engine-kv-v2\"\u003eDay 04：Secret Engine KV V2\u003c/h1\u003e\n\u003cp\u003eKV秘密引擎，用於在Vault的已配置物理Storage中，存儲任意秘密。\u003c/p\u003e\n\u003cp\u003ekey 名稱必須始終是字符串。如果你直接通過CLI編寫非字串 value，它們將被轉換為字串。但是，你可以通過從JSON文件中將key-value pair寫入Vault，或使用HTTP API來保留非字串 value。\u003c/p\u003e\n\u003cp\u003e此秘密引擎遵照ACL policy中，創建(create)和更新(update)權限之間的設定。它還支持patch功能，用於表示部分更新(patch)，而更新功能(update)則表示完全覆蓋。\u003c/p\u003e\n\u003ch3 id=\"設置\"\u003e設置\u003c/h3\u003e\n\u003cp\u003e大多數秘密引擎必須在執行其功能之前事先配置。這些步驟通常由操作人員或配置管理工具完成。\u003c/p\u003e\n\u003cp\u003e啟用v2 kv秘密引擎：\u003c/p\u003e\n\u003cp\u003e啟動一個乾淨的本地的開發模式 Vault Server，全新的 Vault Server 包含底下預設啟用的引擎\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003evault server -dev\n\nexport VAULT_ADDR='http://127.0.0.1:8200'\n\nvault secrets list\n\nPath          Type         Accessor              Description\n----          ----         --------              -----------\ncubbyhole/    cubbyhole    cubbyhole_57a4ca51    per-token private secret storage\nidentity/     identity     identity_c2ecbd49     identity store\nsecret/       kv           kv_c41afde8           key/value secret storage\nsys/          system       system_c063e514       system endpoints used for control, policy and debugging\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e你可以在指定的路徑下，啟用一個新的 v2 kv 秘密引擎。若不指定 -path 參數，則預設 path 為 type，也就是 path=kv。\u003c/p\u003e","title":"Vault Workshop 04: Secret Engine KV V2"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 03：細探 Secret Engine 秘密引擎 什麼是秘密引擎？ 秘密引擎是Vault的組件，用於存儲、生成或加密秘密。在前面的內容中，你使用了Key/Value v2 秘密引擎來存儲數據。一些秘密引擎，比如鍵/值秘密引擎，僅僅是用來存儲和讀取數據的。其他秘密引擎則連接到其他服務並根據需求生成動態憑證。還有一些秘密引擎提供加密作為服務。\n前面的內容中，默認情況下，key/value v2 秘密引擎已啟用，並準備在 secret/ 下接收請求，因為我們在-dev 模式下啟動了Vault Server。\n在底下我們使用 kv v1 做簡單的範例。\nmount path 建議在使用 KV v2 秘密引擎時，使用可選的 -mount flag 語法，例如\nvault kv get -mount=secret foo 請嘗試以下命令，這將導致錯誤：\nvault kv put foo/bar a=b Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/internal/ui/mounts/foo/bar Code: 403. Errors: * preflight capability check returned 403, please ensure client's policies grant access to path \u0026quot;foo/bar/\u0026quot; Path prefix 路徑前綴告訴 Vault 應該將流量 route 到哪個秘密引擎\n當請求到達 Vault 時，它會使用最長前綴匹配來匹配初始路徑部分，然後將請求傳遞給在該路徑啟用的相應秘密引擎。\n如果 mount 設置為 foo/bar 則會在 foo/bar 這個 path 下的 secret engine，儲存 a=b 如果 mount 設置為 foo 則會在 foo 這個 path 下的 secret engine，儲存在 path /bar，a=b Vault 將這些秘密引擎呈現得類似於文件系統 (ex. )/usr/local/bin/vault)\n在 linux 上存取一個 mount path 不存在的 directory path 在 vault 中存取 foo 處未掛載秘密引擎，所以上面的命令返回了錯誤。 對於 vault kv 命令，也可以使用 -mount flag\n啟用一個秘密引擎 要開始，請在路徑 kv 啟用一個新的 KV 秘密引擎。每個路徑都是完全隔離的，無法與其他路徑通信。例如，\n啟用在 foo 的 KV 秘密引擎無法與啟用在 bar 的 KV秘密引擎通信。\n/foo a=b /bar c=d 啟用新的秘密引擎以前，先查看一下目前 vault server 中已經啟用的引擎。\n除了 default 存在的引擎，還有在 dev 模式下自動建立的 secret/\ndefault\ncubbyhole/ identity/ sys/ dev mode\nsecret/ vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9c6d82c2 per-token private secret storage identity/ identity identity_8feb8f49 identity store secret/ kv kv_6f946f62 key/value secret storage sys/ system system_b45bc416 system endpoints used for control, policy and debugging 然後在 path=kv 下，啟用一個 secret engine\nvault secrets enable -path=kv kv Success! Enabled the kv secrets engine at: kv/ 秘密引擎啟用的路徑默認為秘密引擎的名稱。因此，以下命令等效於執行上面的命令。\nvault secrets enable kv 重複執行這個命令會拋出“路徑已在kv/ 中使用”錯誤。\n為了驗證我們的成功並獲取有關秘密引擎的更多信息，使用以下的 vault secrets list 命令：\nvault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9c6d82c2 per-token private secret storage identity/ identity identity_8feb8f49 identity store kv/ kv kv_2b6528af n/a secret/ kv kv_6f946f62 key/value secret storage sys/ system system_b45bc416 system endpoints used for control, policy and debugging 這顯示了在這個Vault伺服器上已啟用的5個秘密引擎。\n你可以看到秘密引擎的類型、相應的路徑以及可選的描述（如果沒有提供描述，則為“n/a”）。\n使用帶有 -detailed 標誌運行上述命令可以顯示KV秘密引擎的版本和更多信息。\nvault secrets list --detailed Path Plugin Accessor Default TTL Max TTL Force No Cache Replication Seal Wrap External Entropy Access Options Description UUID Version Running Version Running SHA256 Deprecation Status ---- ------ -------- ----------- ------- -------------- ----------- --------- ----------------------- ------- ----------- ---- ------- --------------- -------------- ------------------ cubbyhole/ cubbyhole cubbyhole_9c6d82c2 n/a n/a false local false false map[] per-token private secret storage 6087f484-a02c-f36c-0a1a-aa07840f988c n/a v1.14.3+builtin.vault n/a n/a identity/ identity identity_8feb8f49 system system false replicated false false map[] identity store c3a1e4ae-09c6-29b9-906a-8281b46690f3 n/a v1.14.3+builtin.vault n/a n/a kv/ kv kv_2b6528af system system false replicated false false map[] n/a 5ff9bc4c-6d2c-5fd7-cbe0-e9b7fbf03ee5 n/a v0.15.0+builtin n/a supported secret/ kv kv_6f946f62 system system false replicated false false map[version:2] key/value secret storage ed28307e-0472-c0a7-b7a9-65543a63e68c n/a v0.15.0+builtin n/a supported sys/ system system_b45bc416 n/a n/a false replicated true false map[] system endpoints used for control, policy and debugging 9712d56d-95e3-6c07-796f-d44565de5c07 n/a v1.14.3+builtin.vault n/a n/a sys/ 路徑對應到系統後端。這些路徑與Vault的核心系統交互，對於初學者來說不是必需的。\n建立 Secret 要創建私鑰，使用 kv put 命令。\nvault kv put kv/hello target=world Success! Data written to: kv/hello 要讀取存儲在kv/hello路徑中的私鑰，使用 kv get 命令。\nvault kv get kv/hello ===== Data ===== Key Value --- ----- target world 嘗試建立第二個 kv/my-secret\nvault kv put kv/my-secret value=\u0026quot;s3c(eT\u0026quot; Success! Data written to: kv/my-secret 讀取位於 kv/my-secret 的資料\nvault kv get kv/my-secret ==== Data ==== Key Value --- ----- value s3c(eT 刪除位於 kv/my-secret 的資料\nvault kv delete kv/my-secret Success! Data deleted (if it existed) at: kv/my-secret 列出位於 kv/my-secret 的資料\nvault kv list kv/ Keys ---- hello 停用秘密引擎 當不再需要秘密引擎時，可以將其停用。\n當停用秘密引擎時，所有私鑰都將被撤銷，相應的Vault數據和配置將被刪除。\nvault secrets disable kv/ Success! Disabled the secrets engine (if it existed) at: kv/ 請注意，此命令將路徑作為參數，而不是秘密引擎的類型。\n對原始路徑的任何數據路由請求都將導致錯誤，但現在可以在該路徑啟用另一個秘密引擎。\nvault kv get kv/hello Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/internal/ui/mounts/kv/hello Code: 403. Errors: * preflight capability check returned 403, please ensure client's policies grant access to path \u0026quot;kv/hello/\u0026quot; 秘密引擎是一個抽象 Vault的行為類似於虛擬文件系統。讀取/寫入/刪除/列出操作將轉發到相應的秘密引擎，秘密引擎決定如何對這些操作做出反應。\n這種抽象非常強大。它使Vault能夠直接與物理系統、資料庫、HSM 等進行交互。\n但除了這些物理系統外，Vault還可以與更多獨特的環境進行交互，比如AWS IAM、動態SQL user 創建等，所有這些都使用相同的讀取/寫入 interface。\nchatGPT 本段內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/tutorials/getting-started/getting-started-secrets-engines 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。不要使用敬語，請用你取代您。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary 修正下列翻譯：秘密改為私鑰，數據改為資料，數據庫改為資料庫，數據改為資料，訪問改為存取，源代碼改為原始碼。 ","permalink":"https://chechia.net/posts/2023-09-14-vault-workshop-secret-engine/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"day-03細探-secret-engine-秘密引擎\"\u003eDay 03：細探 Secret Engine 秘密引擎\u003c/h1\u003e\n\u003ch3 id=\"什麼是秘密引擎\"\u003e什麼是秘密引擎？\u003c/h3\u003e\n\u003cp\u003e秘密引擎是Vault的組件，用於存儲、生成或加密秘密。在前面的內容中，你使用了Key/Value v2 秘密引擎來存儲數據。一些秘密引擎，比如鍵/值秘密引擎，僅僅是用來存儲和讀取數據的。其他秘密引擎則連接到其他服務並根據需求生成動態憑證。還有一些秘密引擎提供加密作為服務。\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://developer.hashicorp.com/_next/image?url=https%3A%2F%2Fcontent.hashicorp.com%2Fapi%2Fassets%3Fproduct%3Dtutorials%26version%3Dmain%26asset%3Dpublic%252Fimg%252Fvault%252Fvault-triangle.png%26width%3D1641%26height%3D973\u0026w=3840\u0026q=75\"\u003e\u003c/p\u003e\n\u003cp\u003e前面的內容中，默認情況下，key/value v2 秘密引擎已啟用，並準備在 secret/ 下接收請求，因為我們在-dev 模式下啟動了Vault Server。\u003c/p\u003e\n\u003cp\u003e在底下我們使用 kv v1 做簡單的範例。\u003c/p\u003e\n\u003ch3 id=\"mount-path\"\u003emount path\u003c/h3\u003e\n\u003cp\u003e建議在使用 KV v2 秘密引擎時，使用可選的 -mount flag 語法，例如\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003evault kv get -mount=secret foo\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e請嘗試以下命令，這將導致錯誤：\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003evault kv put foo/bar a=b\n\nError making API request.\n\nURL: GET http://127.0.0.1:8200/v1/sys/internal/ui/mounts/foo/bar\nCode: 403. Errors:\n\n* preflight capability check returned 403, please ensure client's policies grant access to path \u0026quot;foo/bar/\u0026quot;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePath prefix 路徑前綴告訴 Vault 應該將流量 route 到哪個秘密引擎\u003c/p\u003e","title":"Vault Workshop 03: Secret Engine"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 02 準備：執行一個本地開發用途的 Vault 整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\n準備工作 在開始進行 Vault 30 天 workshop 之前，有一些準備工作需要完成：\n步驟1：下載 Vault Binary\n首先，你需要下載HashiCorp Vault的最新版本。你可以在HashiCorp的官方網站上找到Vault的下載鏈接。請確保下載適用於你操作系統的版本。\nhttps://developer.hashicorp.com/vault/downloads\nMacOS amd64 MacOS arm64 Linux 步驟2：安裝Vault\n下載完成後，根據你的操作系統進行安裝。對於大多數Linux和Unix系統，你c可以通過解壓縮壓縮文件來安裝Vault。將Vault Binary文件移至你的PATH中，以便在終端中輕鬆訪問\n筆者使用的是 Mac M1，使用以下指令進行安裝\nwget https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_darwin_arm64.zip unzip vault_1.14.3_darwin_arm64.zip sudo mv vault /usr/local/bin/vault Vault v1.14.3 (56debfa71653e72433345f23cd26276bc90629ce), built 2023-09-11T21:23:55Z 你不需要使用最新版本的 Vault 也可以完成這次 workshop\n啟動 Vault server 使用以下指令在本地啟動 Vault Server。請確保 Vault 伺服器已正確啟動，並且未出現錯誤消息。\nvault server -dev 使用 -dev 選項可以啟動Vault的開發模式，該模式不需要身份驗證即可運行Vault。\n確認Vault運行 打開你的瀏覽器，瀏覽到 http://127.0.0.1:8200\n或者使用 curl 命令發送 GET 請求：\ncurl http://127.0.0.1:8200/v1/sys/health | jq { \u0026quot;initialized\u0026quot;: true, \u0026quot;sealed\u0026quot;: false, \u0026quot;standby\u0026quot;: false, \u0026quot;performance_standby\u0026quot;: false, \u0026quot;replication_performance_mode\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;replication_dr_mode\u0026quot;: \u0026quot;disabled\u0026quot;, \u0026quot;server_time_utc\u0026quot;: 1694693898, \u0026quot;version\u0026quot;: \u0026quot;1.14.3\u0026quot;, \u0026quot;cluster_name\u0026quot;: \u0026quot;vault-cluster-3273d150\u0026quot;, \u0026quot;cluster_id\u0026quot;: \u0026quot;a32d555e-3346-0757-1d57-21e2f646aaf3\u0026quot; } 如果Vault正常運行，你應該會收到一個包含Vault訊息的JSON response。\n也可使用 Vault Binary 來存取 Vault API\nexport Vault Server 的 http endpoint (我們尚未啟動 tls listener，所以沒有 https endpoint)\nexport VAULT_ADDR='http://127.0.0.1:8200' vault status Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 1 Threshold 1 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem Cluster Name vault-cluster-3273d150 Cluster ID a32d555e-3346-0757-1d57-21e2f646aaf3 HA Enabled false 可以看到本地的 Vault server\n已初始化 已解封 版本 未啟用 HA 也可以使用 Vault Binary 中的 operator 指令檢測\nvault operator members Host Name API Address Cluster Address Active Node Version Upgrade Version Redundancy Zone Last Echo --------- ----------- --------------- ----------- ------- --------------- --------------- --------- CheChias-MacBook-Pro.local http://127.0.0.1:8200 https://127.0.0.1:8201 true 1.14.3 n/a n/a n/a 初始化Vault 在開發模式下運行的Vault可能不需要初始化，但如果你使用的是 production 模式，請參考Vault的官方文檔，進行初始化和設置。\nVault server 在 dev 模式下啟動後，會自動產生一隻單一個 unseal key，與一把 Root Token。\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variables: $ export VAULT_ADDR='http://127.0.0.1:8200' The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: JbKrVzhH4VO14nPBRvWE1qxJz9CVhnOFJT0pYLsx9LU= Root Token: hvs.0pmmzGSZ8S7w8vP2E4sm7SqE Development mode should NOT be used in production installations! 你可以使用 vault operator 指令，嘗試 seal Vault server\nvault operator seal Success! Vault is sealed. 再次檢查 status\nvault status Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem HA Enabled false 顯示為密鑰密封（Sealed）狀態\n你可以使用 vault operator 解封 Vault\nvault operator unseal JbKrVzhH4VO14nPBRvWE1qxJz9CVhnOFJT0pYLsx9LU= Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 1 Threshold 1 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem Cluster Name vault-cluster-3273d150 Cluster ID a32d555e-3346-0757-1d57-21e2f646aaf3 HA Enabled false 顯示為解封狀態\n密鑰安全 本 workshop 中使用的 vault server / seal key / root token 都是測試用途的暫時性存在，筆者寫完文章後，這些 server 與密鑰早已經清理完畢，所以沒有資安風險。\n為了示範方便，workshop 會以明碼 plain text 形式表現\n如果你是在公司的 production 環境使用，務必要確保密鑰的安全\n不要在 local 電腦上存放 token 或 seal key 不要在沒有密碼學與資安檢測的平台上儲存 token 或 seal key 不要透過通訊軟體(ex. slack) 明碼傳送 token 或 seal key \u0026hellip; 你可能會問，本地不能存，遠端也不能存，也不能隨意傳送給同事，那應該如何協作與管理？\n在後面的 workshop 內容會與大家一一示範解說\nDev 模式 你可以關閉執行 vault server 的 terminal session 來關閉 vault server\nvault server -dev ctrl + c https://developer.hashicorp.com/vault/docs/concepts/dev-server\n\u0026ldquo;Dev\u0026rdquo; 伺服器模式\ndev 模式的伺服器不需要進一步的設定，且你本地的 vault CLI 已經被認證。這使得使用 Vault 或啟動用於開發的 Vault instance 變得容易。Vault 的每一個功能在 \u0026ldquo;dev\u0026rdquo; 模式中都是可用的。-dev flag 只是將很多設置簡化為不安全的默認值。\n警告：永遠、永遠、永遠不要在生產環境中運行 \u0026ldquo;dev\u0026rdquo; 模式伺服器。這是不安全的，且每次重新啟動都會丟失資料（因為它將資料存儲在記憶體中）。它僅供開發或實驗使用。\ndev 模式特性 dev 伺服器的屬性（有些可以使用命令行 flag 或指定配置文件來覆寫）：\n初始化且未封印: 伺服器將自動初始化且未封印。你不需要使用 vault operator unseal。它立即可用。\n記憶體存儲: 所有資料（加密後）都存儲在記憶體中。Vault 伺服器不需要任何檔案權限。\n綁定到本地地址而不使用 TLS: 伺服器正在監聽 127.0.0.1:8200（默認的伺服器地址）而不使用 TLS。\n自動認證: 伺服器存儲了你的 root access token，所以 vault CLI 存取已經準備好。如果你透過 API 存取 Vault，你需要使用打印出的 token 進行認證。\n單一解封鑰匙: 伺服器使用單一的 unseal key 進行初始化。Vault 已經是解封的，但如果你想嘗試 seal/unseal，那麼只需要輸出的單一鑰匙。\n安裝 key value storage: 在 secret/ 處安裝了一個 v2 KV secret engine。請注意 v1 KV 有所不同。如果你想使用 v1，使用此旗標 -dev-kv-v1。\n第一次存取 vault 資料 建立通用 Key-Value 存儲： 使用以下命令在 Vault 中創建通用的 Key-Value 存儲。在這個例子中，我們將使用 secrets 作為存儲的路徑，你可以根據你的需求自行命名存儲路徑：\nvault kv put secret/mysecrets username=admin password=secretpassword 這個命令將在 Vault 中創建一個 Key-Value 存儲，其中包含一對 Key value pair，分別是 username 和 password。\n檢索存儲的值： 你可以使用以下命令檢索存儲中的值：\nvault kv get secret/mysecrets 這個命令將返回存儲中的所有 Key Value pair。\n更新存儲的值： 如果你需要更新存儲中的值，可以使用以下命令：\nvault kv put secret/mysecrets password=newsecretpassword 這個命令將更新存儲中的 password 鍵的值。\n刪除存儲的值： 若要刪除存儲中的值，可以使用以下命令：\nvault kv delete secret/mysecrets 這個命令將刪除整個存儲路徑及其內容\n關於 kv-v2 dev 模式下，Vault server 自動在 secret/ 處安裝了一個 v2 KV secret engine\nVault中的不同秘密引擎（Secret Engine）提供了不同的功能和特性。本次 workshop 會以最新，功能也最多元的 kv-v2 Secret Engine 為主。\n以下是三種常見的秘密引擎（generic、kv v1 和 kv v2）之間的主要區別：\nGeneric Secrets Engine（通用秘密引擎） 特性： 通用秘密引擎是你的最簡單的引擎之一，它允許你存儲和檢索通用的 Key Value Secret，這些 Secret 不需要特定的結構或格式。\n用法： 通用秘密引擎適用於簡單的秘密存儲需求，其中 Secret 可以是任何形式的數據。你可以使用通用秘密引擎來存儲API金鑰、密碼、配置信息等。\nKey-Value (KV) Version 1 Secrets Engine（KV v1 秘密引擎） 特性： KV v1秘密引擎引入了一個有層次結構的鍵值存儲，允許你組織和版本控制儲存的 Secret。它具有 Secret 版本控制功能，可以保存多個版本的 Secret。\n用法： KV v1引擎適用於需要有組織結構並希望保留秘密版本的情況。它可以用於存儲配置信息、TLS證書、API金鑰等。\nKey-Value (KV) Version 2 Secrets Engine（KV v2 秘密引擎） 特性： KV v2秘密引擎是KV v1的進化版本，它引入了更強大的特性，如可配置的 Secret Data type（如字符串、list 和 dict），並可設置的多版本的 Secret 存儲。\n用法： KV v2引擎適用於更複雜的秘密管理需求，其中秘密可能具有不同的數據類型，並且需要更靈活的版本管理。它也用於配置信息、TLS證書、API金鑰等。\n總結來說，通用秘密引擎適用於簡單的秘密存儲，而KV v1和KV v2秘密引擎適用於更複雜的秘密管理需求，具有更多的組織和版本控制功能。在選擇引擎時，請考慮你的具體需求和Vault的配置。通常，KV v2引擎是較新和更靈活的選擇，但根據你的情況，也可以考慮使用其他引擎。\ndev 使用情境 dev 伺服器應該用於實驗 Vault 的功能，例如不同的認證方法、Secret engine、audit device 等。\n除了實驗，dev 伺服器對於開發環境的自動化非常容易。\n小結 通過執行這些步驟，你可以確保本地安裝的 Vault Binary 正常運行。\n請注意，Vault的實際配置和使用將根據你的需求和環境而有所不同，這些步驟主要用於確保Vault的基本功能正常運行。在 production 環境中，你需要更多的配置和安全性措施。\n","permalink":"https://chechia.net/posts/2023-09-13-vault-workshop-get-started/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003ch1 id=\"day-02-準備執行一個本地開發用途的-vault\"\u003eDay 02 準備：執行一個本地開發用途的 Vault\u003c/h1\u003e\n\u003cp\u003e整篇 Workshop 會使用的範例與原始碼，放在 \u003ca href=\"http://chechia.net/zh-hant/#projects\"\u003eGithub Repository: vault-playground\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"準備工作\"\u003e準備工作\u003c/h3\u003e\n\u003cp\u003e在開始進行 Vault 30 天 workshop 之前，有一些準備工作需要完成：\u003c/p\u003e\n\u003cp\u003e步驟1：下載 Vault Binary\u003c/p\u003e\n\u003cp\u003e首先，你需要下載HashiCorp Vault的最新版本。你可以在HashiCorp的官方網站上找到Vault的下載鏈接。請確保下載適用於你操作系統的版本。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://developer.hashicorp.com/vault/downloads\"\u003ehttps://developer.hashicorp.com/vault/downloads\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_darwin_amd64.zip\"\u003eMacOS amd64\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_darwin_arm64.zip\"\u003eMacOS arm64\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_linux_amd64.zip\"\u003eLinux\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e步驟2：安裝Vault\u003c/p\u003e\n\u003cp\u003e下載完成後，根據你的操作系統進行安裝。對於大多數Linux和Unix系統，你c可以通過解壓縮壓縮文件來安裝Vault。將Vault Binary文件移至你的PATH中，以便在終端中輕鬆訪問\u003c/p\u003e\n\u003cp\u003e筆者使用的是 Mac M1，使用以下指令進行安裝\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003ewget https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_darwin_arm64.zip\nunzip vault_1.14.3_darwin_arm64.zip\n\nsudo mv vault /usr/local/bin/vault\n\nVault v1.14.3 (56debfa71653e72433345f23cd26276bc90629ce), built 2023-09-11T21:23:55Z\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e你不需要使用最新版本的 Vault 也可以完成這次 workshop\u003c/p\u003e\n\u003ch3 id=\"啟動-vault-server\"\u003e啟動 Vault server\u003c/h3\u003e\n\u003cp\u003e使用以下指令在本地啟動 Vault Server。請確保 Vault 伺服器已正確啟動，並且未出現錯誤消息。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-bash\"\u003evault server -dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e使用 -dev 選項可以啟動Vault的開發模式，該模式不需要身份驗證即可運行Vault。\u003c/p\u003e","title":"Vault Workshop 02: Get Started"},{"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 01 前言：從零開始的 Hashicorp Vault Workshop 從零開始 workshop 系列已經做了四年，內容包含 k8s, terraform, aws 等。\n深深覺得要學習一個工具，還是要動手做。\n所以有了 30 天手把手 workshop 系列文，讓有興趣接觸的朋友，能以相對低的門檻入門。\n關於內容\n無基礎的手把手的基礎教學 完整的範例，提供原始程式碼，也提供 production 的經驗與範例 官方最新文件繁中翻譯 (chatGPT based) 建議讀者務必跟著操作，不要只是看過\n其他文章 https://chechia.net/\n過去的 Workshop\n2022 鐵人賽: Terraform IaC Best Practice on AWS Cloud / 在 aws 公有雲上找 IaC 最佳實踐 (因故退賽) 2021 鐵人賽: Terraform Workshop - Infrastructure as Code for Public Cloud 2020 鐵人賽: Kubernetes X DevOps X 從零開始導入工具 X 需求分析 2019 鐵人賽: Kubernetes 預定內容與許願清單 本 workshop 預計有底下內容\nDay 01 前言：從零開始的 Hashicorp Vault Workshop Day 02 準備：執行一個本地開發用途的 Vault Day 03：細探 Secret Engine 秘密引擎 Day 04：Secret Engine KV V2 Day 05：Authentication Day 06: Github auth method Day 07: Policy Day 08: Docker and Initialization 本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDynamic Secrets tls certificate management configuration docker initialization ha storage backend filesystem postgresql Infrastructure as Code for Vault deploy policy auth method Vault Kuberntes integration Azure integration AWS integration token in detail 關於翻譯工具 chatGPT 本系列文章大量使用 chatGPT 翻譯官方文章。目的是使用最新版官方文件內容，提供第一手且精準的資料給讀者，但又能降低非母語讀者得學習門檻。\n有翻譯的段落都會標示出處，著作權皆屬於原出處所有。\n本系列文章以分享資訊，貢獻社群，提高國內整體技術能力為目的，並不用於商業用途。\n使用翻譯工具並不代表作者（譯者）沒有付出相當時間心力，包含規劃文章大綱，測試 prompt engineering，調整參數，並人工校正產生的內容。\n本系列作品在於教導讀者使用工具 Hashicorp Vault，而 chatGPT 等大語言模型工具就是近年最值得學習的工具。如果讀者還不會使用 chatGPT，本系列文章都附上 Prompt 提示詞，可以參考，學習，並自由使用。也許學會使用 chatGPT 所帶來的價值，會比學習 Vault 帶來的還多。\n很重要所以再說一遍，chatGPT 是近年最值得學習的工具，沒有之一。\nhttps://github.com/f/awesome-chatgpt-prompts/blob/main/prompts.csv What is Vault? 本段內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/what-is-vault 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。不要使用敬語，請用你取代您。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary 修正下列翻譯：秘密改為私鑰，數據改為資料，數據庫改為資料庫，數據改為資料，訪問改為存取，源代碼改為原始碼。 What is Vault? HashiCorp Vault 是一套基於身份的私鑰和加密管理系統。所謂的\u0026quot;私鑰\u0026quot;，是你希望嚴格控制存取的資訊，例如 API key、密碼和certificate。Vault 提供加密服務，並透過認證和授權方法進行管控。使用 Vault 的使用者介面、命令列介面或 HTTP API，可以安全地儲存和管理私鑰及其他敏感資料，並能嚴格控制（限制）其存取權，並可進行 audit。\n現代系統需要存取大量的私鑰，包括資料庫憑證、外部服務的 API key、面向服務的架構通訊憑證等。了解誰正在存取哪些私鑰可能相當困難，尤其當這種存取可能因平台而異。而在此基礎上加入key rolling、安全儲存和詳細的audit trail，若無自訂的解決方案幾乎是不可能的。這正是 Vault 發揮作用的地方。\nVault 會驗證並授權客戶端（用戶、機器、應用程序）在提供他們存取秘密或儲存的敏感資料之前。\nVault 如何運作： Vault 主要使用 token，且 token 與客戶端的 policy 相關聯。每一 policy 都是基於 path-based 的，policy rule 限制每個客戶端對於每一 path 的行動和存取能力。使用 Vault，你可以手動建立 token 並指派給你的客戶端，或客戶端可以登入並獲得 token。下面的插圖顯示了 Vault 的核心工作流程。\nVault 工作流程 Vault 的核心工作流程包含四個階段：\n認證：在 Vault 中的認證是指客戶端提供資訊，Vault 使用此資訊來判定他們是否是他們所聲稱的那個人。一旦客戶端根據某種認證方式成功認證，將生成一個與策略相關聯的 token。\n驗證：Vault 根據第三方受信賴的來源，例如 Github、LDAP、AppRole 等，對客戶端進行驗證。\n授權：客戶端根據 Vault 安全策略進行匹配。此策略是一套規則集，定義客戶端使用其 Vault token 可存取哪些 API 端點。策略提供了一種宣告方式來授予或禁止存取 Vault 中的特定路徑和操作。\n存取：根據與客戶端身份相關聯的策略，Vault 通過發出 token 來授予存取秘密、金鑰和加密能力的權限。然後，客戶端可以使用他們的 Vault token 進行未來的操作。\n為什麼選擇 Vault？ 現今的大多數企業都有憑證散布在其組織中。密碼、API 金鑰和憑證存儲在明文中、應用源代碼、配置文件和其他位置。由於這些憑證存在於各處，因此可能很難真正知道誰有存取和授權權限。明文中的憑證也增加了內部和外部攻擊者進行惡意攻擊的可能性。\n考慮到這些挑戰，Vault 被設計出來。Vault 採取所有這些憑證並集中管理，這樣他們就只在一個位置定義，從而減少了憑證的不必要暴露。但 Vault 更進一步，確保用戶、應用程序和系統都被認證並明確授權存取資源，同時也提供一個審計跟踪，捕捉並保留客戶端操作的歷史記錄。\nVault 的主要功能包括 安全的秘密存儲：可以在 Vault 中存儲任意的金鑰/值秘密。Vault 在將這些秘密寫入持久性存儲之前會對其進行加密，因此獲取原始存儲並不足以存取你的秘密。Vault 可以寫入磁盤、Consul 等。\n動態秘密：Vault 可以為某些系統即時生成秘密，例如 AWS 或 SQL 數據庫。例如，當應用程序需要存取 S3 桶時，它會要求 Vault 提供憑證，Vault 將即時生成具有有效權限的 AWS 密鑰對。在創建這些動態秘密後，Vault 也會在租期到期後自動撤銷它們。\n數據加密：Vault 可以在不存儲數據的情況下加密和解密數據。這允許安全團隊定義加密參數，並允許開發人員將加密數據存儲在如 SQL 數據庫之類的地方，而不必設計他們自己的加密方法。\n租賃和續期：Vault 中的所有秘密都有一個與之相關的租期。在租期結束時，Vault 會自動撤銷該秘密。客戶端可以通過內置的續期 API 來續租。\n撤銷：Vault 具有內置的秘密撤銷支援。Vault 不僅可以撤銷單一的秘密，還可以撤銷秘密樹，例如由特定用戶讀取的所有秘密，或特定類型的所有秘密。撤銷在金鑰滾動以及在入侵情況下鎖定系統時都很有幫助。\n什麼是 HCP Vault？ HashiCorp Cloud Platform (HCP) Vault 是 Vault 的託管版本，由 HashiCorp 運營，使組織能夠快速啟動並運行。HCP Vault 使用與自託管的 Vault 相同的二進制文件，這意味著你將擁有一致的用戶體驗。你可以使用相同的 Vault 客戶端與 HCP Vault 通信，就像你使用自託管的 Vault 一樣。請參考 HCP Vault 文檔以獲得更多資訊。\n","permalink":"https://chechia.net/posts/2023-09-12-vault-workshop-introduction/","summary":"\u003cp\u003e如果你希望追蹤最新的草稿，請見\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2023/\"\u003e鐵人賽2023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e本 workshop 也接受網友的許願清單，\u003ca href=\"https://ithelp.ithome.com.tw/articles/10317378\"\u003e如果有興趣的題目可於第一篇底下留言\u003c/a\u003e，筆者會盡力實現，但不做任何保證\u003c/p\u003e\n\u003ch1 id=\"day-01-前言從零開始的-hashicorp-vault-workshop\"\u003eDay 01 前言：從零開始的 Hashicorp Vault Workshop\u003c/h1\u003e\n\u003cp\u003e從零開始 workshop 系列已經做了四年，內容包含 k8s, terraform, aws 等。\u003c/p\u003e\n\u003cp\u003e深深覺得要學習一個工具，還是要動手做。\u003c/p\u003e\n\u003cp\u003e所以有了 30 天手把手 workshop 系列文，讓有興趣接觸的朋友，能以相對低的門檻入門。\u003c/p\u003e\n\u003cp\u003e關於內容\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e無基礎的手把手的基礎教學\u003c/li\u003e\n\u003cli\u003e完整的範例，提供原始程式碼，也提供 production 的經驗與範例\u003c/li\u003e\n\u003cli\u003e官方最新文件繁中翻譯 (chatGPT based)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e建議讀者務必跟著操作，不要只是看過\u003c/p\u003e\n\u003cp\u003e其他文章 \u003ca href=\"https://chechia.net/\"\u003ehttps://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e過去的 Workshop\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2022/\"\u003e2022 鐵人賽: Terraform IaC Best Practice on AWS Cloud / 在 aws 公有雲上找 IaC 最佳實踐 (因故退賽)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://ithelp.ithome.com.tw/users/20120327/ironman/4057\"\u003e2021 鐵人賽: Terraform Workshop - Infrastructure as Code for Public Cloud\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2020/\"\u003e2020 鐵人賽: Kubernetes X DevOps X 從零開始導入工具 X 需求分析\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/zh-hant/tag/%E9%90%B5%E4%BA%BA%E8%B3%BD2019/\"\u003e2019 鐵人賽: Kubernetes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"預定內容與許願清單\"\u003e預定內容與許願清單\u003c/h1\u003e\n\u003cp\u003e本 workshop 預計有底下內容\u003c/p\u003e","title":"Vault Workshop 01: Introduction"},{"content":" 活動日期: 2023-05-10T11:00:00Z 活動連結 聯絡我Facebook Twitter 投影片 Info 現代網路應用需要處理許多私密金鑰的管理，例如：user 的密碼、server 的資料、database 的資料、microservices 彼此 authentication\u0026hellip;。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。 如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。\nHashiCorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，還有許多特點如：\n不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統 server/Kubernetes/VM 支援跨環境的應用，可以串連混合雲中的應用，作為私鑰認證的中心\nTarget group 本次活動將簡介 Hashicorp Vault，並以 AWS Cloud 與本地 Kubernetes 為例，提供幾個基本的操作範例，適合初次接觸 HashiCorp Vault 與尋找私鑰管理平台的團隊。\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","permalink":"https://chechia.net/posts/2023-05-10-hashicorp-vault-intro/","summary":"\u003cul\u003e\n\u003cli\u003e活動日期: 2023-05-10T11:00:00Z\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.accupass.com/event/2304180633007080095620\"\u003e活動連結\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e聯絡我Facebook\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://twitter.com/chechiachang\"\u003eTwitter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/1iex9lm89OCIR8IAoD1RPe4vcW--bcKBmMHoixDybqP8/edit?usp=sharing\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"info\"\u003eInfo\u003c/h3\u003e\n\u003cp\u003e現代網路應用需要處理許多私密金鑰的管理，例如：user 的密碼、server 的資料、database 的資料、microservices 彼此 authentication\u0026hellip;。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。 如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。\u003c/p\u003e\n\u003cp\u003eHashiCorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，還有許多特點如：\u003c/p\u003e\n\u003cp\u003e不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統 server/Kubernetes/VM\n支援跨環境的應用，可以串連混合雲中的應用，作為私鑰認證的中心\u003c/p\u003e\n\u003ch3 id=\"target-group\"\u003eTarget group\u003c/h3\u003e\n\u003cp\u003e本次活動將簡介 Hashicorp Vault，並以 AWS Cloud 與本地 Kubernetes 為例，提供幾個基本的操作範例，適合初次接觸 HashiCorp Vault 與尋找私鑰管理平台的團隊。\u003c/p\u003e\n\u003ch3 id=\"author\"\u003eAuthor\u003c/h3\u003e\n\u003cp\u003eChe-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。\nMicrosoft 最有價值從業人員 MVP。\u003c/p\u003e\n\u003cp\u003e目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\u003c/p\u003e\n\u003cp\u003eChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup.\nMicrosoft Most Valuable Professional since 2020.\u003c/p\u003e","title":"HashiCorp Vault on AWS \u0026 K8s - 雲端地端通吃的私鑰管理平台"},{"content":"target group 金融客戶 vault 4/15 上架\naccupass 5/10 (Wed) 11:00-12:00\nwebbase link (online)\n10:30 上線設備測試\n11:00 Ming 開場\n11:05 主講\n12:00 Q\u0026amp;A (留言)\n當天錄影會上線\n演講大綱 基本介紹 vault 架構 企業需求 self-host 複雜的 policy 用例 demo aws auth k8s auth policy 當天提供 github example Q\u0026amp;A (10mins) 內容 演講主題：Hashicorp vault 雲端地端通吃的私鑰管理平台\n現代網路應用需要處理許多私密金曜的管理，例如：user 的密碼，server 的資料，database 的資料，microservices 彼此 authentication\u0026hellip;。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。 如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。 Hashicorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，有許多特點\n不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統server/Kubernetes/VM 支援跨環境的應用，可以串連混合雲中的應用，作為私要認證的中心 本次演講簡介 Hashicorp Vault，以 aws cloud 與本地 kubernetes 為例，提供幾個基本的操作範例 適合初次接觸的 Hashicorp Vault，與尋找私要管理平台的團隊 講者簡介\nChe-Chia Chang，SRE，喜歡研究公有雲/容器化應用/Kubernetes Microsoft MVP，Ithome 雲端大會/COSCUP講師，常出現 CNTUG / DevOpsTW / Golang Taipei 技術 blog 收錄過往演講與文章 https://chechia.net\nPresentation https://docs.google.com/presentation/d/1iex9lm89OCIR8IAoD1RPe4vcW--bcKBmMHoixDybqP8/edit#slide=id.g2403737215e_0_147\n","permalink":"https://chechia.net/posts/2023-05-10-hashicorp-comminity-presentation-vault-introduction/","summary":"\u003ch1 id=\"target-group\"\u003etarget group\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e金融客戶\u003c/li\u003e\n\u003cli\u003evault\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e4/15 上架\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eaccupass\n5/10 (Wed) 11:00-12:00\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewebbase link (online)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e10:30 上線設備測試\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e11:00 Ming 開場\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e11:05 主講\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e12:00 Q\u0026amp;A (留言)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e當天錄影會上線\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"演講大綱\"\u003e演講大綱\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e基本介紹 vault 架構\u003c/li\u003e\n\u003cli\u003e企業需求\n\u003cul\u003e\n\u003cli\u003eself-host\u003c/li\u003e\n\u003cli\u003e複雜的 policy\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e用例 demo\n\u003cul\u003e\n\u003cli\u003eaws auth\u003c/li\u003e\n\u003cli\u003ek8s auth\u003c/li\u003e\n\u003cli\u003epolicy\u003c/li\u003e\n\u003cli\u003e當天提供 github example\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eQ\u0026amp;A (10mins)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"內容\"\u003e內容\u003c/h1\u003e\n\u003cp\u003e演講主題：Hashicorp vault 雲端地端通吃的私鑰管理平台\u003c/p\u003e\n\u003cp\u003e現代網路應用需要處理許多私密金曜的管理，例如：user 的密碼，server 的資料，database 的資料，microservices 彼此 authentication\u0026hellip;。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。\n如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。\nHashicorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，有許多特點\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統server/Kubernetes/VM\u003c/li\u003e\n\u003cli\u003e支援跨環境的應用，可以串連混合雲中的應用，作為私要認證的中心\n本次演講簡介 Hashicorp Vault，以 aws cloud 與本地 kubernetes 為例，提供幾個基本的操作範例\n適合初次接觸的 Hashicorp Vault，與尋找私要管理平台的團隊\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e講者簡介\u003c/p\u003e","title":"Hashicorp Comminity Presentation Vault Introduction"},{"content":"AWS cross account delegation 依舊 WIP，今天延續昨天內容，使用 github action 做 terraform module 的 CI/CD\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nTerraform Github Action Jobs 我們可以在 terraform repository 中增加 tool checks 到 CI/CD 中，加強 code 的品質控管\nterragrunt-infrastructure-modules PR 在此\n這個 PR 包含幾個 Github Action Workflow\nls .github/workflows checkov.yaml format.yaml plan.yaml security-scan.yaml validate.yaml format.yaml 中執行 terraform fmt -recursive，需求與目的已在昨天說明 validate.yaml 中執行 terraform validate，用來驗證 module 內的 terraform syntax 是否符合語法 checkov.yaml 是多語言的 policy as code 工具，在這邊執行掃描 terraform code 的安全性與 CVEs 檢查 security-scan.yaml 中也是 Policy As Code 使用 tfsec 工具掃描 plan.yaml 中在 pipeline 執行自動化 terraform plan 由於 plan 需要存取 state 與 provider API，設定上有許多權限設定需要開啟，目前是一個 dummy workflow policy as code 內容可以將一篇演講，有興趣請見底下投影片: 2022 DevOpsDay: Policy As Code for Terraform: https://docs.google.com/presentation/d/1yawazO1B_sP5Yiav-XLGJXW3ZS2JTV0wGuJwhrUKQ3A\n對於 Policy as Code 有興趣的朋友請見今年 DevOpsDay 的演講 https://devopsdays.tw/session-page/1146\nTODO 與進度 root 中設定 IAM User aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role cross account iam role iam_cross_account_roles TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy Optional: Cloudtrail Optional: terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-25-14th-ithome-ironman-iac-aws-workshop-11-aws-github-action-ci-cd/","summary":"\u003cp\u003eAWS cross account delegation 依舊 WIP，今天延續昨天內容，使用 github action 做 terraform module 的 CI/CD\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"terraform-github-action-jobs\"\u003eTerraform Github Action Jobs\u003c/h3\u003e\n\u003cp\u003e我們可以在 terraform repository 中增加 tool checks 到 CI/CD 中，加強 code 的品質控管\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/chechiachang/terragrunt-infrastructure-modules/pull/6\"\u003eterragrunt-infrastructure-modules PR 在此\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e這個 PR 包含幾個 Github Action Workflow\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003els .github/workflows\n\ncheckov.yaml\nformat.yaml\nplan.yaml\nsecurity-scan.yaml\nvalidate.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eformat.yaml 中執行 terraform fmt -recursive，需求與目的已在昨天說明\u003c/li\u003e\n\u003cli\u003evalidate.yaml 中執行 terraform validate，用來驗證 module 內的 terraform syntax 是否符合語法\u003c/li\u003e\n\u003cli\u003echeckov.yaml 是多語言的 policy as code 工具，在這邊執行掃描 terraform code 的安全性與 CVEs 檢查\u003c/li\u003e\n\u003cli\u003esecurity-scan.yaml 中也是 Policy As Code 使用 tfsec 工具掃描\u003c/li\u003e\n\u003cli\u003eplan.yaml 中在 pipeline 執行自動化 terraform plan\n\u003cul\u003e\n\u003cli\u003e由於 plan 需要存取 state 與 provider API，設定上有許多權限設定需要開啟，目前是一個 dummy workflow\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003epolicy as code 內容可以將一篇演講，有興趣請見底下投影片: \u003ca href=\"https://docs.google.com/presentation/d/1yawazO1B_sP5Yiav-XLGJXW3ZS2JTV0wGuJwhrUKQ3A\"\u003e2022 DevOpsDay: Policy As Code for Terraform: https://docs.google.com/presentation/d/1yawazO1B_sP5Yiav-XLGJXW3ZS2JTV0wGuJwhrUKQ3A\u003c/a\u003e\u003c/p\u003e","title":"Terraform Github Action CI/CD"},{"content":"TODO 與進度 root 中設定 IAM User aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nAWS cross account with iam roles 要做跨 AWS account 的 IAM Roles access control，我們先看官方文件理解這個功能\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\n回憶我們在 day-04 提到的 multi-accounts 架構\naccount/security 有所有 iam-users 其他 child-account ex. account/test 中會有 iam-roles 需求：security/iam-user 可以使用 test/iam-role 的身份，存取 test 底下的 resource ex.test/ec2 好處 admin 不需要到每一個 child-account 下面去開每一個人的 iam user，統一到 account/security 管理 user 只要登入一個 security 帳號，就可以控制多個 child-account，不用登出又登入不同 account 在 aws official doc 上，也是舉幾個 account 作為 cross account delegation 的範例。由於本 workshop 已經有現存的 child-account，我們會直接使用 workshop 的 accounts 做說明。完成這次 iam role delegation 的設定後，我們會得到以下成果\naccount/security 下面 IAM User 可以 assume 到 account/test 下面的特定的 IAM role account/test 下面有一個 IAM role 可以存取 S3 Bucket 工程師可以使用 web console 登入 security/user，然後 assume role 為 test/role，取得查看 s3 bucket 的權限 工程師也可使用以 security/user 的身份， call aws API 取得 test/role 的暫時的 credential 步驟 我們會先過一次 aws 官方文件上所述的步驟，讓大家了解整個設定步驟，弄清整個 delegation 的流程與概念。之後會轉而使用 Terraform 設定所有的元件。\n首先要在 test (child-account) 設定 iam role 將 account/security 作為 trusted entity 設定 role 的 policy，增加可以存取 s3 bucket 的權限給 test/role 調整 iam role，可以設定需要給予權限，或是 deny 某些權限 最後做測試，是否可以完成 switch role AWS 的範例使用 aws web console 做範例，這個 workshop 後面我們會使用 terraform 來實作。\n外出取材 上面的範例還在 WIP，請當作者外出取材一天，今天先講另外一個工具\nTerraform fmt \u0026amp; lint 要提升 terraform code 品質，有許多工具非常值得在 CI/CD 過程中使用\n例如 terraform 內建的 fmt 與 terragrunt 內建的 hclfmt\n在 module 的 repository 中會需要跑 terraform fmt，確保每個 module release 出去前都有經過 fmt 在使用 terragrunt 的 root module 會需要 hclfmt .hcl 檔案 cd terragrunt-infrastructure-modules terraform fmt -recursive cd terragrunt-infrastructure-live-example terragrunt hclfmt fmt 對於程式碼的品質是基礎但十分重要的\n沒有固定 format 的程式碼會造成 git 使用出現過多 diff，造成團隊協作的困難 format 也會影響自動化，例如 templating / variable expension fmt / lint 是我們第一個帶入 CI/CD 的工具\n手動 fmt fmt 一下\ncd terragrunt-infrastructure-modules terraform fmt -recursive cd terragrunt-infrastructure-live-example terragrunt hclfmt commit 如下\nterragrunt-infrastructure-modules commit terragrunt-infrastructure-live-example Git precommit hook 既然是 code 品質的基礎，應該每次 commit 之前都觸發檢查，這個階段適合使用 git pre-commit hook 前執行\nhttps://github.com/antonbabenko/pre-commit-terraform\n以 terragrunt-infrastructure-live-example 為例\nPR 在此 啟用前需要 install remote script 到本地 git add 後，使用 run 來手動觸發 pre-commit hook 會根據 commit hook 執行腳本，以這邊的範例會執行 id: terraform-fmt id: terraform-validate id: tflint id: terraform_checkov id: terrascan id: terraform_tfsec id: infracost_breakdown 除了 fmt 有說明過以外，其他的 tool 我們後續再說明 pre-commit install pre-commit install pre-commit installed at .git/hooks/pre-commit pre-commit run 如果通過 run 測試，就可以進行 git commit commit 之前會在跑一次 script，所以稱作 pre-commit hook 如果通過測試變化自動 commit 如果沒有通過，則退回這次 commit 如果想要跳過 pre-commit check，可以使用 git commit \u0026ndash;no-verify 通過上述步驟，來確保工程師在本地發出的 commit 有經過基礎的驗證，非常值得團隊導入\nGithub Action ","permalink":"https://chechia.net/posts/2022-09-21-14th-ithome-ironman-iac-aws-workshop-10-aws-cross-account-delegation-and-pre-commit-hook/","summary":"\u003ch3 id=\"todo-與進度\"\u003eTODO 與進度\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e root 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e aws cross account iam role delegation\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e root account MFA policy\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e (Optional) Cloudtrail\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e (Optional) terraform aws config\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 設定 password policy\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 設定 MFA policy\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 中設定 IAM Policy \u0026amp; Group\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e dev 中設定 IAM role\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 允許 security assume dev IAM role\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003ch3 id=\"aws-cross-account-with-iam-roles\"\u003eAWS cross account with iam roles\u003c/h3\u003e\n\u003cp\u003e要做跨 AWS account 的 IAM Roles access control，我們先看官方文件理解這個功能\u003c/p\u003e","title":"Aws Cross Account Delegation \u0026 pre-commit hook"},{"content":"使用 aws module 的好處 為何許多開源的 terraform module 內部使用的都是其他的 module，而不是從 resource 單位開始？\nTerraform 官方文件，如何建立 module\n如何建立一個 module\n根據最常出現的使用情劇與需求 專注於業務的需求與抽象，雃後把實作(terraform resource) 在 module 中組合實作出來 module 也需要考慮 resource 之間的 architecture 也要考慮到觀禮是否方便？使用上是否安全？ ex. 我們需求是產生一個 IAM User\n正常的實作就是寫一個 resource.aws_iam_user 達成需求 更好的實作是：除了 iam_user 外，加上 一定會需要配權限，應搭配 iam_group + iam_policy pgp encrypt 讓資料更安全 password_policy 增加密碼安全 \u0026hellip;等 比起單一一個 iam_user 思考的更全面，更接近最佳實踐 在這次的 Best Practice 追尋之旅中，我們會帶大家去看其他團隊所寫出的 terraform module\n目前已經看了 aws 的 terraform module gruntwork 的 terraform module design (我們沒看到 private module，只是跟隨文件自己刻) 這些有多年企業解決方案經驗的團隊，寫出來的 terraform module 都會考量許多 security 方面的問題 這也是我們前十篇都在專注的方向 一般來說，一個好 module 會帶來很多好處\n精簡 .tf code，透過 terraform function 與判斷式來產生 resource 整合不同 resource 使用時要輸入的 input 可以引導使用者的 architecture 設計 能符合需求，參數方便使用，內容邏輯清楚的 module 就是好 module\n然而要寫好一個 module 需要很多經驗，不僅要對 aws 元件，架構都很熟悉，還要考量管理與安全。我們有機會再來分享。\n為什麼要花這麼多時間講 account / iam / security 的基礎設定？ 因為人家寫出來的就是這麼的安全，開頭直接立於不被駭之地\n昨天使用 reset root IAM user 的密碼，並使用 pgp key 加密保護，今天要進一步強化 IAM 的安全性，包括\n強化 password policy 增加跨帳號 iam role 的 assume permission 增加 MFA 本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\npassword policy 密碼強度的重要性不需要再強調，強迫 user 使用高強度的密碼並且定期更新，才能將地安全風險 想要透過 web console 修改 password policy 的朋友可以看aws doc: setting account password policy。我們這邊會使用 terraform 配置\n修改 terragrunt-infrastructure-modules\n注意：昨天在 module.iam_user 設定的是 login profile 的 password，是管理員配給 user 的第一把 password，並且登入後必須更換密碼 今天要設置的是之後的重設密碼都要遵循的規範 如果想要強化密碼管理\n更長的密碼 更多限制 定期更新 首先先更改 terraform module，開啟 password policy\n增加 resource.aws_iam_account_password_policy 增加 input variable，將 variable 往從上層 input variable 接進來，讓我們可以在最上層調整 可以設定密碼壽命，default 90 天 可以設定最小密碼長度，default 32 字元 其他 policy 參數都寫死固定 需要數字，大小寫英文 允許使用者在密碼過期前重設密碼 密碼過期就鎖住帳號不給登入，要請 admin 來 reset # aws_iam_account_password_policy.tf resource \u0026quot;aws_iam_account_password_policy\u0026quot; \u0026quot;strict\u0026quot; { allow_users_to_change_password = true minimum_password_length = var.minimum_password_length hard_expiry = true max_password_age = var.max_password_age require_lowercase_characters = true require_numbers = true require_uppercase_characters = true require_symbols = true password_reuse_prevention = 0 } # variables.tf variable \u0026quot;minimum_password_length\u0026quot; { type = number description = \u0026quot;The number of days that an user password is valid.\u0026quot; default = 32 } variable \u0026quot;max_password_age\u0026quot; { type = number description = \u0026quot;Minimum length to require for user passwords.\u0026quot; default = 90 } 進行 terragrunt plan，沒問題的話就可以直接 apply\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_iam_account_password_policy.strict will be created + resource \u0026quot;aws_iam_account_password_policy\u0026quot; \u0026quot;strict\u0026quot; { + allow_users_to_change_password = true + expire_passwords = (known after apply) + hard_expiry = true + id = (known after apply) + max_password_age = 90 + minimum_password_length = 32 + password_reuse_prevention = 0 + require_lowercase_characters = true + require_numbers = true + require_symbols = true + require_uppercase_characters = true } Plan: 1 to add, 0 to change, 0 to destroy. aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt apply 注意更改密碼的副作用\n由於更改 password policy 是整個 account 通用的，所以也會影響到 administrator 的 password 雖然影響 password，但不影響 access key，所以 terraform 可以正常工作 Apply passsword policy 後，我們使用原先的密碼走 aws web console 登入看看\n可以使用現有密碼登入 嘗試從右上角 User -\u0026gt; security credentials -\u0026gt; change password，可以發現新的 password policy 已經更新了 我們便手動更改密碼，以符合新的 password policy 更改密碼完成記得存在密碼儲存器中 個人習慣耕買密碼完成後，都重新登入，再登入一次，確定密碼正確，權限也沒問題 MFA for me 接著我們可以為自己的 IAM User 啟用 MFA 裝置\n在右上角 User -\u0026gt; security credentials -\u0026gt; MFA\n點選 Assign MFA Device 選擇 virtual MFA Device，我們這邊示範使用 google authenticator app 畫面上出現 MFA key 的 QRcode，這個 QRcode == key，不能洩漏，離開這個畫面就不會再出現了 使用 google authenticator，新增一組 account，然後掃描 QRCode 底下填入新 account 產生的 6 位數字 token 等待 60 秒 底下填入第二組新 account 產生的 6 位數字 token，確定真的能夠取得正確的 token 之後每次登入都需要輸入 MFA NOTE: 這裡是 IAM User login 時需要輸入 MFA，我們之後會設定 child account 下 iam-role assume 時都需要輸入 MFA\n明天會前十篇的重點之一：cross account 的 iam role 配置\nTODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-20-14th-ithome-ironman-iac-aws-workshop-09-terraform-module-and-password-security/","summary":"\u003ch3 id=\"使用-aws-module-的好處\"\u003e使用 aws module 的好處\u003c/h3\u003e\n\u003cp\u003e為何許多開源的 terraform module 內部使用的都是其他的 module，而不是從 resource 單位開始？\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.terraform.io/language/modules/develop\"\u003eTerraform 官方文件，如何建立 module\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e如何建立一個 module\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e根據最常出現的使用情劇與需求\u003c/li\u003e\n\u003cli\u003e專注於業務的需求與抽象，雃後把實作(terraform resource) 在 module 中組合實作出來\u003c/li\u003e\n\u003cli\u003emodule 也需要考慮 resource 之間的 architecture\u003c/li\u003e\n\u003cli\u003e也要考慮到觀禮是否方便？使用上是否安全？\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eex. 我們需求是產生一個 IAM User\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e正常的實作就是寫一個 \u003ccode\u003eresource.aws_iam_user\u003c/code\u003e 達成需求\u003c/li\u003e\n\u003cli\u003e更好的實作是：除了 \u003ccode\u003eiam_user\u003c/code\u003e 外，加上\n\u003cul\u003e\n\u003cli\u003e一定會需要配權限，應搭配 \u003ccode\u003eiam_group\u003c/code\u003e + \u003ccode\u003eiam_policy\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003epgp encrypt 讓資料更安全\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epassword_policy\u003c/code\u003e 增加密碼安全\u003c/li\u003e\n\u003cli\u003e\u0026hellip;等\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e比起單一一個 \u003ccode\u003eiam_user\u003c/code\u003e 思考的更全面，更接近最佳實踐\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在這次的 Best Practice 追尋之旅中，我們會帶大家去看其他團隊所寫出的 terraform module\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e目前已經看了\n\u003cul\u003e\n\u003cli\u003eaws 的 terraform module\u003c/li\u003e\n\u003cli\u003egruntwork 的 terraform module design (我們沒看到 private module，只是跟隨文件自己刻)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e這些有多年企業解決方案經驗的團隊，寫出來的 terraform module 都會考量許多 security 方面的問題\u003c/li\u003e\n\u003cli\u003e這也是我們前十篇都在專注的方向\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一般來說，一個好 module 會帶來很多好處\u003c/p\u003e","title":"Modules and password security"},{"content":"昨天處理完 Accounting 的 reset password，今天要來 reset root account Administrator 的權限\n本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nReset IAM root user Administrator Password 依照上面的步驟，我們確定可以正常登入，於是可以來可以新建 Administrator 的 login profile\n由於 Administrator 會需要 access key，所以我們也把他設成 true # terragrunt.hcl users = { Administrator = { groups = [\u0026quot;full-access\u0026quot;] pgp_key = \u0026quot;keybase:chechiachang\u0026quot; create_login_profile = true create_access_keys = true }, Accounting = { groups = [\u0026quot;billing\u0026quot;] pgp_key = \u0026quot;keybase:chechiachang\u0026quot; create_login_profile = true create_access_keys = false # accounting always use web console, won't use access key } } 記得更改 module/output.tf，增加 secret key 的 output，而不要只留在 terraform state 裡面\n# output.tf output \u0026quot;keybase_secret_key_decrypt_command\u0026quot; { description = \u0026quot;Decrypt access secret key command\u0026quot; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_secret_key_decrypt_command } } output \u0026quot;keybase_secret_key_pgp_message\u0026quot; { description = \u0026quot;Encrypted access secret key\u0026quot; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_secret_key_pgp_message } } 試著 plan 一下，預期\nAdministrator 會產生一組新的 login profile Administrator 會產生一組新的 access key aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0] will be created + resource \u0026quot;aws_iam_access_key\u0026quot; \u0026quot;this\u0026quot; { + create_date = (known after apply) + encrypted_secret = (known after apply) + encrypted_ses_smtp_password_v4 = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + pgp_key = \u0026quot;keybase:chechiachang\u0026quot; + secret = (sensitive value) + ses_smtp_password_v4 = (sensitive value) + status = \u0026quot;Active\u0026quot; + user = \u0026quot;Administrator\u0026quot; } # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user_login_profile.this[0] will be created + resource \u0026quot;aws_iam_user_login_profile\u0026quot; \u0026quot;this\u0026quot; { + encrypted_password = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 + password_reset_required = false + pgp_key = \u0026quot;keybase:chechiachang\u0026quot; + user = \u0026quot;Administrator\u0026quot; } Plan: 2 to add, 0 to change, 0 to destroy. Changes to Outputs: + iam_access_key_id = { + Accounting = \u0026quot;\u0026quot; + Administrator = null -\u0026gt; (known after apply) } ~ keybase_password_decrypt_command = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } ~ keybase_password_pgp_message = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } + keybase_secret_key_decrypt_command = { + Accounting = null + Administrator = (known after apply) } + keybase_secret_key_pgp_message = { + Accounting = null + Administrator = (known after apply) } 符合預期!! 可喜可賀\n但是我們已經有 access key 在使用了，我又不想要走 day 3 aws-vault add key 的步驟換一組新的，這時該怎辦\n沒錯，我們可以像 Day3 一樣，使用 import 匯入已經存在的 resource 到 address 裡面\ngoogle \u0026ldquo;terraform aws iam user access key\u0026rdquo;，找到 aws iam user access key 的說明頁面 address 是 module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0] access key ID 可以在密碼管理器查看 或是在 aws web console -\u0026gt; IAM -\u0026gt; User -\u0026gt; Administrator -\u0026gt; Security Credential -\u0026gt; Access Keys 查到 aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt import 'module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0]' AKIA1234567890 module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0]: Importing from ID \u0026quot;AKIA2I2H7ERWKI4RIF4Z\u0026quot;... module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0]: Import prepared! Prepared aws_iam_access_key for import module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0]: Refreshing state... [id=AKIA2I2H7ERWKI4RIF4Z] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Releasing state lock. This may take a few moments... 順便 login profile 也 import 近來\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt import 'module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user_login_profile.this[0]' Administrator 再次 plan 就不需要新建一把 access key 了\u0026hellip;吧？！\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create -/+ destroy and then create replacement Terraform will perform the following actions: # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_access_key.this[0] must be replaced -/+ resource \u0026quot;aws_iam_access_key\u0026quot; \u0026quot;this\u0026quot; { ~ create_date = \u0026quot;2022-09-16T13:05:35Z\u0026quot; -\u0026gt; (known after apply) + encrypted_secret = (known after apply) + encrypted_ses_smtp_password_v4 = (known after apply) ~ id = \u0026quot;AKIA2I2H7ERWKI4RIF4Z\u0026quot; -\u0026gt; (known after apply) + key_fingerprint = (known after apply) + pgp_key = \u0026quot;keybase:chechiachang\u0026quot; # forces replacement + secret = (sensitive value) + ses_smtp_password_v4 = (sensitive value) # (2 unchanged attributes hidden) } # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user_login_profile.this[0] must be replaced -/+ resource \u0026quot;aws_iam_user_login_profile\u0026quot; \u0026quot;this\u0026quot; { + encrypted_password = (known after apply) ~ id = \u0026quot;Administrator\u0026quot; -\u0026gt; (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 # forces replacement + pgp_key = \u0026quot;keybase:chechiachang\u0026quot; # forces replacement # (2 unchanged attributes hidden) } Plan: 2 to add, 0 to change, 2 to destroy. Changes to Outputs: + iam_access_key_id = { + Accounting = \u0026quot;\u0026quot; + Administrator = \u0026quot;AKIA2I2H7ERWARJJWIVK\u0026quot; } ~ keybase_password_decrypt_command = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } ~ keybase_password_pgp_message = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } ~ keybase_secret_key_decrypt_command = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } ~ keybase_secret_key_pgp_message = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } 誒結果不符合預期，\naws_iam_access_key.this 需要重建 原因是我們更改了 pgp_key = \u0026quot;keybase:chechiachang\u0026quot; 造成 forces replacement 因為 想一想也很合理 第一次創建 access key 的時候，keybase 是亂寫的，這時用 module 的 code 去跑，產生無加密的 access key 放在 state 中 這次 import 後，module 中的 .tf code 已經改過，不再存無加密的 access key，而是加密後儲存 GPG Encrypted Message 這個改變讓 provider 覺得沒有辦法符合，只好刪掉重建 .aws_iam_user_login_profile.this 需要重建，原因也是一樣，因為我們改了 .tf code 邏輯，造成 provider 勢必要重建 login profile 執行 terraform apply\nNOTE: 注意有 destroy 的 terraform plan 都要特別小心 review，因為很多 aws API delete 發出去就是無法復原了\n例如 access key 與 password 刪掉後，是無法 recover 的 有些 API request 會有副作用 reset password 後，aws web console 會被登出 access key 換掉 terraform 就不能用，有可能導致 terraform 進行到一半被 permission denied (這個 apply 不會，只是提醒大家要注意) 有 destroy 的部分都要小心 review，看不懂就找 peering review 有 destroy 的部分都要小心 review，看不懂就找 peering review 有 destroy 的部分都要小心 review，看不懂就找 peering review\n很重要所以說三次\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt apply Apply complete! Resources: 2 added, 0 changed, 2 destroyed. Outputs: keybase_password_decrypt_command = { \u0026quot;Accounting\u0026quot; = \u0026lt;\u0026lt;-EOT echo \u0026quot;wcFMA4U4ylYlNYTfAQ/+KLtNeJqOqK/V3pNdcRh9nvyDw/huJfaYF7Ab/YdKqLKNVySBqc6HY8xedmER5Wn78RlG962/f772u8UlQN4XCOjOLjyOaNL8K0tkeoSVg+XXPx2qJO9Myc1d+74rG57biUD26XpwfS7+ryIjaHf+NzjisExZy2mgiMIzzAlfqTzRAc+jYP11wZcyFGwOe9pMq6BJy7FH5935ndVgNLCgYtSjgIV5b5kWQ3SDr0E9egAHnoAcHs8mni71x7OL7OCc/bS3nJSLz1jRkMoukWyQQ3+lM7Nwi36NSJneIKxW5f7lSxr7Bx+W3mx6gZgb18yRVIwbVW5iDFsAQOFb0zOHUf8tm3tAQ0F79Yeqy9sfLCfUUjVpCENEJe5FtwJkI8EbHKe8mGnYA4dbHaCkIDgVa8TZ4mdXl7yVZx0adlkKFIS48niwPHxMErZYqnRBitUCGdaH0bHcpPcnSVolLljZmXZUrntMjIEb9FGYbgl6hOiRF9MR3RQL5DNmpS7uSqU7V6TyhVbAOJO2oR01Z7hXdAMxZ54O+h06jN7fzvLrUSMWvUz/wp+JJnIDYNwu75Dvicn6NmXrP3fH3M8xmYIEPhpSvOldNzB4DMqQdD6tN+DphFFyvFwuLnWKWKOY+pbNXkd5q0H3A69owxgsONoOL1JqjZgOdcLtWTq+g9XqU5XSRQGo1ei5Ctv2f5yAHZuI2smwJkTA88SuyZqJNwRixw5OPpUDOuWxhK6xftsN7M1/TnioX8Ch3RTow7H0dvMFFD3ocvjG4A==\u0026quot; | base64 --decode | keybase pgp decrypt EOT \u0026quot;Administrator\u0026quot; = \u0026lt;\u0026lt;-EOT echo \u0026quot;wcFMA4U4ylYlNYTfAQ/+JcRNkO9SxMm9U196p2WuuXdSWo9ooktKJMp3q4Xf5xMG5z/ccphl5+fVDmkp2Pr76UJ9NmiBw8rzScFUEU4U+xZepNojSdN/rmZPWB6PKCdJMyItw6HOnn7OSa0i1arr0PHaC0bpVs4Bc+9Oqtw1SuU8uvwBpCeM+h33qOROnvKnGgvOHV2Zk1XcgT+sg7xMTv8q77KePoNLTK3joQL7fXdPWUGWCp8srWpxzJ8LD4aNMmsrjA/s/7sNbAAdhffRwwA6diuZfeYscZ6MlURUtrSx3iAuNV33kmhuyYimlmXstbfs/nIJVI5OfcVexPKzHK5O9DikXApAzbf/YACz0OebOcxnzh6+rQw9+XWwRtARcK3tViMhxqnEu4PHROmj1sNmLZLJqCniwN91J2iry7BAx2Zoyvvw7vd7Sw9LddaM+j2XHhSf21250sNMn/9R6beUlkWQgPci8mU51rnqBffbOJnvVh7QCTLssoMTgKRrgxb+oZbSsKLLQO/621ZcTY9rzC74w2kZWJTS7nx5G/0KQkwXP4NGRSTXYg3/6TcwT7LefQiJpWB2z92zT1sp5lPhtpNneqrtFFnirplfP5coeelRSlTEbHDwbHSX1LfHU7A5WHjf7oEHvn2hMhdgTlvGt4vL7rXiy9p8GWjcudGxZs+/WNs8CCkZd8j2gVvSRQEmNNtCd/U3jmq+o2VncJjBhetJ7eCyvKOAMwcCLHE0zleX77mXVtq7UVLXrcypxIbVjJAIko8y71fATyrRomcijS7Sgw==\u0026quot; | base64 --decode | keybase pgp decrypt EOT } keybase_password_pgp_message = { \u0026quot;Accounting\u0026quot; = \u0026lt;\u0026lt;-EOT -----BEGIN PGP MESSAGE----- Version: Keybase OpenPGP v2.0.76 Comment: https://keybase.io/crypto wcFMA4U4ylYlNYTfAQ/+KLtNeJqOqK/V3pNdcRh9nvyDw/huJfaYF7Ab/YdKqLKNVySBqc6HY8xedmER5Wn78RlG962/f772u8UlQN4XCOjOLjyOaNL8K0tkeoSVg+XXPx2qJO9Myc1d+74rG57biUD26XpwfS7+ryIjaHf+NzjisExZy2mgiMIzzAlfqTzRAc+jYP11wZcyFGwOe9pMq6BJy7FH5935ndVgNLCgYtSjgIV5b5kWQ3SDr0E9egAHnoAcHs8mni71x7OL7OCc/bS3nJSLz1jRkMoukWyQQ3+lM7Nwi36NSJneIKxW5f7lSxr7Bx+W3mx6gZgb18yRVIwbVW5iDFsAQOFb0zOHUf8tm3tAQ0F79Yeqy9sfLCfUUjVpCENEJe5FtwJkI8EbHKe8mGnYA4dbHaCkIDgVa8TZ4mdXl7yVZx0adlkKFIS48niwPHxMErZYqnRBitUCGdaH0bHcpPcnSVolLljZmXZUrntMjIEb9FGYbgl6hOiRF9MR3RQL5DNmpS7uSqU7V6TyhVbAOJO2oR01Z7hXdAMxZ54O+h06jN7fzvLrUSMWvUz/wp+JJnIDYNwu75Dvicn6NmXrP3fH3M8xmYIEPhpSvOldNzB4DMqQdD6tN+DphFFyvFwuLnWKWKOY+pbNXkd5q0H3A69owxgsONoOL1JqjZgOdcLtWTq+g9XqU5XSRQGo1ei5Ctv2f5yAHZuI2smwJkTA88SuyZqJNwRixw5OPpUDOuWxhK6xftsN7M1/TnioX8Ch3RTow7H0dvMFFD3ocvjG4A== -----END PGP MESSAGE----- EOT \u0026quot;Administrator\u0026quot; = \u0026lt;\u0026lt;-EOT -----BEGIN PGP MESSAGE----- Version: Keybase OpenPGP v2.0.76 Comment: https://keybase.io/crypto wcFMA4U4ylYlNYTfAQ/+JcRNkO9SxMm9U196p2WuuXdSWo9ooktKJMp3q4Xf5xMG5z/ccphl5+fVDmkp2Pr76UJ9NmiBw8rzScFUEU4U+xZepNojSdN/rmZPWB6PKCdJMyItw6HOnn7OSa0i1arr0PHaC0bpVs4Bc+9Oqtw1SuU8uvwBpCeM+h33qOROnvKnGgvOHV2Zk1XcgT+sg7xMTv8q77KePoNLTK3joQL7fXdPWUGWCp8srWpxzJ8LD4aNMmsrjA/s/7sNbAAdhffRwwA6diuZfeYscZ6MlURUtrSx3iAuNV33kmhuyYimlmXstbfs/nIJVI5OfcVexPKzHK5O9DikXApAzbf/YACz0OebOcxnzh6+rQw9+XWwRtARcK3tViMhxqnEu4PHROmj1sNmLZLJqCniwN91J2iry7BAx2Zoyvvw7vd7Sw9LddaM+j2XHhSf21250sNMn/9R6beUlkWQgPci8mU51rnqBffbOJnvVh7QCTLssoMTgKRrgxb+oZbSsKLLQO/621ZcTY9rzC74w2kZWJTS7nx5G/0KQkwXP4NGRSTXYg3/6TcwT7LefQiJpWB2z92zT1sp5lPhtpNneqrtFFnirplfP5coeelRSlTEbHDwbHSX1LfHU7A5WHjf7oEHvn2hMhdgTlvGt4vL7rXiy9p8GWjcudGxZs+/WNs8CCkZd8j2gVvSRQEmNNtCd/U3jmq+o2VncJjBhetJ7eCyvKOAMwcCLHE0zleX77mXVtq7UVLXrcypxIbVjJAIko8y71fATyrRomcijS7Sgw== -----END PGP MESSAGE----- EOT } keybase_secret_key_decrypt_command = { \u0026quot;Accounting\u0026quot; = tostring(null) \u0026quot;Administrator\u0026quot; = \u0026lt;\u0026lt;-EOT echo \u0026quot;wcFMA4U4ylYlNYTfARAAivQzF+bh6N6OH/3a1S7XQCrIceoLn9EXsc8Gr0p09TVieSqwwv1RojaCULjYXYu5UnxRFwaavN+ULxrl2c4hRBIMgNEAo1Nxy16vfDEj4W7S9l+yA/TU3ewbs8WqKHIFrO8+AcjxUV4Ol5cmziDu70UR0H1/f0w7lqC/s36Zqa1Rz5Mb1n7ATLfs4jiPDQiFlr+E6gSe+I23sAQqdDS/WEbwAVz6o2B9mvs+OznNzeDoTwd7nE2ca+jYDZKjEjD6+qfDAnVzftwSrJIb2yJYlKIeSKJLC9loEQk2wdR5fkKrPg//9S4gYQJjzkwWyJAQsc6AuOPZXjqVe5yV9EpG2r0938G/acR3SEjt53ckXjh6Fi0/EgOjfeTVMO0EH6Se/vurdXBRyMggTieyN76Ts8nnn32GBldTZBUgWTL1Udj6B9ueqFCFuB8LFGNNNoHYjp3hAeGEQdtgI9TQ9r+jRDKpO8zcDoRMFK7wv4DJ31Nl/aS++a8nFEwJp7D5XkiSPbJ9JE1MwvuufldAHqr8iRbt3LmKeB4qC3TFbIhNWY4gs2VZ5LTepIn18sQfQ4f5s9o1lpem4yQw2XeEA7Fd1sPVypVeOj4z8Y080duGES896LiGpsAZdQ3bm/RiuIMLSabArgWCP/0LgrOQU8SwrIdN3k4682nw41KVnhjY2IDSWQHfEUCTNkgNfdj8v7FiD/OHX5G6VX/NAOwiAQ7Qh9u6UOmyAVY3QTjgVYEF4+oUyX+zZWprhhJkvHRDTTTNqozxVBfbYqrKTxSa00MRwAFIwQJh3xXbvmlF\u0026quot; | base64 --decode | keybase pgp decrypt EOT } keybase_secret_key_pgp_message = { \u0026quot;Accounting\u0026quot; = tostring(null) \u0026quot;Administrator\u0026quot; = \u0026lt;\u0026lt;-EOT -----BEGIN PGP MESSAGE----- Version: Keybase OpenPGP v2.0.76 Comment: https://keybase.io/crypto wcFMA4U4ylYlNYTfARAAivQzF+bh6N6OH/3a1S7XQCrIceoLn9EXsc8Gr0p09TVieSqwwv1RojaCULjYXYu5UnxRFwaavN+ULxrl2c4hRBIMgNEAo1Nxy16vfDEj4W7S9l+yA/TU3ewbs8WqKHIFrO8+AcjxUV4Ol5cmziDu70UR0H1/f0w7lqC/s36Zqa1Rz5Mb1n7ATLfs4jiPDQiFlr+E6gSe+I23sAQqdDS/WEbwAVz6o2B9mvs+OznNzeDoTwd7nE2ca+jYDZKjEjD6+qfDAnVzftwSrJIb2yJYlKIeSKJLC9loEQk2wdR5fkKrPg//9S4gYQJjzkwWyJAQsc6AuOPZXjqVe5yV9EpG2r0938G/acR3SEjt53ckXjh6Fi0/EgOjfeTVMO0EH6Se/vurdXBRyMggTieyN76Ts8nnn32GBldTZBUgWTL1Udj6B9ueqFCFuB8LFGNNNoHYjp3hAeGEQdtgI9TQ9r+jRDKpO8zcDoRMFK7wv4DJ31Nl/aS++a8nFEwJp7D5XkiSPbJ9JE1MwvuufldAHqr8iRbt3LmKeB4qC3TFbIhNWY4gs2VZ5LTepIn18sQfQ4f5s9o1lpem4yQw2XeEA7Fd1sPVypVeOj4z8Y080duGES896LiGpsAZdQ3bm/RiuIMLSabArgWCP/0LgrOQU8SwrIdN3k4682nw41KVnhjY2IDSWQHfEUCTNkgNfdj8v7FiD/OHX5G6VX/NAOwiAQ7Qh9u6UOmyAVY3QTjgVYEF4+oUyX+zZWprhhJkvHRDTTTNqozxVBfbYqrKTxSa00MRwAFIwQJh3xXbvmlF -----END PGP MESSAGE----- EOT } 這時嘗試在使用 terraform plan，會無法取得 state s3 bucket\n因為 access key 已經重建，現在 terraform 使用舊的 access key 連 s3 會 404 not found aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Remote state S3 bucket chechia-root-ap-northeast-1-terraform-state does not exist or you don't have permissions to access it. Would you like Terragrunt to create it? (y/n) n ERRO[0004] remote state S3 bucket chechia-root-ap-northeast-1-terraform-state does not exist or you don't have permissions to access it ERRO[0004] Unable to determine underlying exit code, so Terragrunt will exit with error code 1 取得 output 後，一樣做 gpg decrypt 取得\npassword access key 完成後使用 aws-vault 移除舊的 profile + credential，再從新匯入 access key\naws-vault remove terraform-30day-root-iam-user Delete credentials for profile \u0026quot;terraform-30day-root-iam-user\u0026quot;? (y|N) y Deleted credentials. aws-vault add terraform-30day-root-iam-user Enter Access Key ID: Enter Secret Access Key: Added credentials to profile \u0026quot;terraform-30day-root-iam-user\u0026quot; in vault 完成後就可以正常使用 aws-vault plan 了\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan 上面的變更我們的 PR 如下\nterragrunt-infrastructure-modules PR terragrunt-infrastructure-live-example PR TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-19-14th-ithome-ironman-iac-aws-workshop-08-reset-iam-user-administrator/","summary":"\u003cp\u003e昨天處理完 Accounting 的 reset password，今天要來 reset root account Administrator 的權限\u003c/p\u003e\n\u003cp\u003e本日進度\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e root 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 將手動產生的 Administrator 的 IAM User import terraform 中\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Policy\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Group\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e reset root account IAM user login profile \u0026amp; pgp key\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"reset-iam-root-user-administrator-password\"\u003eReset IAM root user Administrator Password\u003c/h3\u003e\n\u003cp\u003e依照上面的步驟，我們確定可以正常登入，於是可以來可以新建 Administrator 的 login profile\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e由於 Administrator 會需要 access key，所以我們也把他設成 true\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e# terragrunt.hcl\n  users = {\n    Administrator = {\n      groups               = [\u0026quot;full-access\u0026quot;]\n      pgp_key              = \u0026quot;keybase:chechiachang\u0026quot;\n      create_login_profile = true\n      create_access_keys   = true\n    },\n    Accounting = {\n      groups               = [\u0026quot;billing\u0026quot;]\n      pgp_key              = \u0026quot;keybase:chechiachang\u0026quot;\n      create_login_profile = true\n      create_access_keys   = false # accounting always use web console, won't use access key\n    }\n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e記得更改 module/output.tf，增加 secret key 的 output，而不要只留在 terraform state 裡面\u003c/p\u003e","title":"Reset Iam User Administrator"},{"content":"昨天我們建立 IAM Group 與 policy，並說明 policy 管理原則。\n然而昨天最後創建 user 時，我們關閉了 create_login_profile = false 的選項 這是為了避免當前的登入機制被覆蓋掉，影響 root account Administrator 的使用 更改 login 方式，在某些極端的情形下，有可能讓 Administrator 自己覆蓋自己的 login 設定後，讓自己無法登入 如果你是管理員，在更改 admin 帳號的權限與登入設定時，一定要多加注意\n如果不幸改壞，無法登入，就只能再去找出 root account root user 帳號來解救了 如果是 root account root user 把自己的 login 改壞，就會很痛苦，要請 aws support 來救你 本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nEnhance User Login Security Gruntwork Guide 在完成 group 與 policy 設定後，下一個需要做的是加強 user 登入的安全性\n需要做的事情有\n啟用 MFA policy 設定嚴格的 password policy 建立 login profile: password 後，使用各個成員的 gpg key 加密 (encrypt) password，只有成員自己可以解開自己的 password MFA Policy Multi-factor authentication 是常用的認證方式\n在使用 AWS API 的時候，除了帳號密碼 / access key 以外，需要第二層登入裝置做驗證 AWS 支援許多不同形式的 MFA 裝置 虛擬的 MFA 裝置 ex. 跟 google authenticator 一起使用 硬體的 MFA 裝置 ex. 符合 FIDO2 標準的 YubiKey 等等 本 workshop 會實作虛擬的 MFA 裝置，讓使用者使用 Google Authenticator 登入才能使用 API 然而 如 Gruntwork Guide 在 MFA Policy所述，要 enforce MFA 到所有的 AWS API 會有諸多困難\n如果我們要求所有 AWS API 都過 MFA，有些來源可能很難做 MFA ex. EC2 VM 上的一個 application 也需要打 AWS API，但在 VM 上就很難取得 Google Authenticator 的認證碼 ex. 新的 IAM User 剛新建，沒有 MFA，也就無法登入設定自己的 MFA，變成雞生蛋蛋生雞 Gruntwork Guide 中建議在以下情境中啟用 MFA 就好\nsecurity 以外的 child-account (dev/stag/prod) 的所有 iam-role 都開啟 MFA 當我們要使用 security/iam-user assuem 到另外一個 account/iam-role 時，需要 MFA 認證才可 assume role 然後把 security 以外的 child-account 開啟 global 的 trust policy ，這樣設定就很單純 不會 block iam-user 登入，因為所有 user 都在 security 底下，其他 child-account 沒有 user 在 root 與 security account 內，才有 IAM User 與 IAM Group，在這邊啟用 MFA 將 MFA 依據 policy 去設定，透過 policy - group - user 去 enforce user 使用 MFA 新 user 建立時，需要額外建立 \u0026ldquo;self-management\u0026rdquo; permissions policy，讓新用戶可以不用 MFA，進行第一次的 MFA 設定 Password Policy Password Policy 指的是對於 user 自己設定的密碼要符合一定的規範\nAWS official doc: 如何設定 password policy ex. 密碼至少要 32 字元長度 ex. 密碼要包含英文，數字，大小寫，或特殊字元 ex. 密碼多久需要更新一次 這些 password policy 可以大幅強化所有 User 的密碼安全性 由於我們使用的 external module terraform-aws-iam 已經整合了 password policy，所以我們這邊直接在 input 中開啟即可以使用\nPGP key pgp (Pretty Good Privacy) 是非常常用的非對稱加密工具\n比較常用的工具如 OpenPGP 或是 gpg (GnuPG) 為何 iam user 創建會牽扯到 pgp？\n不管是 gruntwork module 或是 aws terraform-aws-iam module 都支援 pgp 在 admin 新建 User 與 login profile 後，aws api 產生 password，回傳給 admin terraform，這時 admin 需要把第一把 password 傳給 user，讓 user 做第一次登入 這整個傳遞過程，不管從 aws api -\u0026gt; terraform state -\u0026gt; terraform output -\u0026gt; admin -\u0026gt; user，如果是明碼未加密的狀態下，實在不宜傳遞，很有可能被有心中中途攔截 因此我們可以使用 pgp 相關工具來加密 admin 以 gpg(GnuPG) 為例非對稱加密 (local exposure)\nadmin 取得新 user (ex. accounting 人員) 的 gpg public key admin 產生 user password 後，使用 gpg 工具，以 account 人員的 public key 加密 password 獲得一串 encrypted text，傳給 accounting 人員 accounting 人員使用自己的 private key 用 gpg 工具解密，即可取得明碼 password 只有有 private key 的人可以解開 encrypted text accounting 人員使用 password 登入後，會因為 login profile 要求，而被迫更改新密碼，捨棄 admin local 有曝險過的 password 上面的做法 terraform state 與 admin local 會看到明碼 password，不夠安全。\nPGP \u0026amp; keybase.io aws 提供一個改進的做法，terraform state 只存 encrypted text\n這樣可以避免在 terraform state 中與 admin local 的 password 曝險 pre-requisite\ngpg tool keybase.io 註冊 安裝 gpg\nsudo port install gpg gpg --version 如果第一次使用，可以創建一對 key pair\ngpg --full-generate-key 列出自己有的 secret key (private key)\ngpg --list-secret-keys --keyid-format=long export public key\ngpg --armor --export 3AA5C34371567BD2 -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF9oN7ABEADeJ5BO3RsvfB0RpU3ZtI3AZmLmMfoaQ41QtkLoFEhF0XnSBNhH .... ARFfAR39B4hHAnA+/+scVFdGT8i9kuYxk8Ocb7zHgULrOBfKEPEQ5XLmdiqAD+DN jO2IFPM= -----END PGP PUBLIC KEY BLOCK----- NOTE: private key 是鑰匙，public key 是鎖頭，永遠不要把 private key 傳出去，而是把 public key 傳出去讓別人加密，用 public key 加密過的東西，只能用手上這把 private key 解開，也就是專屬你的\n在 keybase.io 上面註冊後，上傳你的 pgp public key\n依照指示完成步驟 (optional) 認證 github / domain / 其他網站讓其他人確定這個是你本人 實際操作：password policy \u0026amp; pgp key NOTE: 如果不啟用 pgp-key，admin terraform apply 新 user 後拿到的 password 就會是明碼，可以透過 gpg 工具加密，在透過網路傳給對方\n這邊我們先使用 Accounting 先做測試，啟用 login profile\nNOTE: 你知道自己是 admin，就請不要拿自己的 User 做測試，壞了很麻煩\n將 terragrunt.hcl 改成\nAccounting = { groups = [\u0026quot;billing\u0026quot;] pgp_key = \u0026quot;keybase:chechiachang\u0026quot; create_login_profile = true create_access_keys = false # accounting always use web console, won't use access key } } 接著要改 module 中的 code，將 module 的 output 接出來到上層的 root module\noutput.tf\n# https://github.com/terraform-aws-modules/terraform-aws-iam/blob/master/modules/iam-user/outputs.tf output \u0026quot;keybase_password_pgp_message\u0026quot; { description = \u0026quot;Encrypted password\u0026quot; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_password_pgp_message } } output \u0026quot;keybase_password_decrypt_command\u0026quot; { description = \u0026quot;Decrypt user password command\u0026quot; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_password_decrypt_command } } 然後我們試著 plan\n預期：產生 accounting login profile，產生 pgp encrypted password text (而不是明碼 password) aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform will perform the following actions: # module.iam_user[\u0026quot;Accounting\u0026quot;].aws_iam_user_login_profile.this[0] will be created + resource \u0026quot;aws_iam_user_login_profile\u0026quot; \u0026quot;this\u0026quot; { + encrypted_password = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 + password_reset_required = false + pgp_key = \u0026quot;keybase:chechiachang\u0026quot; + user = \u0026quot;Accounting\u0026quot; } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + keybase_password_decrypt_command = { + Accounting = (known after apply) + Administrator = null } + keybase_password_pgp_message = { + Accounting = (known after apply) + Administrator = null } 符合預期！\nOutput 中 keybase_password_pgp_message 的 Accounting = (known after apply) 應該是加密過的 output 應該要加密給 keybase:chechiachang，也就是用我的 gpg key 可以解開 那直接來 apply 試試看\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt apply Outputs: keybase_password_decrypt_command = { \u0026quot;Accounting\u0026quot; = \u0026lt;\u0026lt;-EOT echo \u0026quot;wcFMA4U4ylYlNYTfAQ/+KLtNeJqOqK/V3pNdcRh9nvyDw/huJfaYF7Ab/YdKqLKNVySBqc6HY8xedmER5Wn78RlG962/f772u8UlQN4XCOjOLjyOaNL8K0tkeoSVg+XXPx2qJO9Myc1d+74rG57biUD26XpwfS7+ryIjaHf+NzjisExZy2mgiMIzzAlfqTzRAc+jYP11wZcyFGwOe9pMq6BJy7FH5935ndVgNLCgYtSjgIV5b5kWQ3SDr0E9egAHnoAcHs8mni71x7OL7OCc/bS3nJSLz1jRkMoukWyQQ3+lM7Nwi36NSJneIKxW5f7lSxr7Bx+W3mx6gZgb18yRVIwbVW5iDFsAQOFb0zOHUf8tm3tAQ0F79Yeqy9sfLCfUUjVpCENEJe5FtwJkI8EbHKe8mGnYA4dbHaCkIDgVa8TZ4mdXl7yVZx0adlkKFIS48niwPHxMErZYqnRBitUCGdaH0bHcpPcnSVolLljZmXZUrntMjIEb9FGYbgl6hOiRF9MR3RQL5DNmpS7uSqU7V6TyhVbAOJO2oR01Z7hXdAMxZ54O+h06jN7fzvLrUSMWvUz/wp+JJnIDYNwu75Dvicn6NmXrP3fH3M8xmYIEPhpSvOldNzB4DMqQdD6tN+DphFFyvFwuLnWKWKOY+pbNXkd5q0H3A69owxgsONoOL1JqjZgOdcLtWTq+g9XqU5XSRQGo1ei5Ctv2f5yAHZuI2smwJkTA88SuyZqJNwRixw5OPpUDOuWxhK6xftsN7M1/TnioX8Ch3RTow7H0dvMFFD3ocvjG4A==\u0026quot; | base64 --decode | keybase pgp decrypt EOT \u0026quot;Administrator\u0026quot; = tostring(null) } keybase_password_pgp_message = { \u0026quot;Accounting\u0026quot; = \u0026lt;\u0026lt;-EOT -----BEGIN PGP MESSAGE----- Version: Keybase OpenPGP v2.0.76 Comment: https://keybase.io/crypto wcFMA4U4ylYlNYTfAQ/+KLtNeJqOqK/V3pNdcRh9nvyDw/huJfaYF7Ab/YdKqLKNVySBqc6HY8xedmER5Wn78RlG962/f772u8UlQN4XCOjOLjyOaNL8K0tkeoSVg+XXPx2qJO9Myc1d+74rG57biUD26XpwfS7+ryIjaHf+NzjisExZy2mgiMIzzAlfqTzRAc+jYP11wZcyFGwOe9pMq6BJy7FH5935ndVgNLCgYtSjgIV5b5kWQ3SDr0E9egAHnoAcHs8mni71x7OL7OCc/bS3nJSLz1jRkMoukWyQQ3+lM7Nwi36NSJneIKxW5f7lSxr7Bx+W3mx6gZgb18yRVIwbVW5iDFsAQOFb0zOHUf8tm3tAQ0F79Yeqy9sfLCfUUjVpCENEJe5FtwJkI8EbHKe8mGnYA4dbHaCkIDgVa8TZ4mdXl7yVZx0adlkKFIS48niwPHxMErZYqnRBitUCGdaH0bHcpPcnSVolLljZmXZUrntMjIEb9FGYbgl6hOiRF9MR3RQL5DNmpS7uSqU7V6TyhVbAOJO2oR01Z7hXdAMxZ54O+h06jN7fzvLrUSMWvUz/wp+JJnIDYNwu75Dvicn6NmXrP3fH3M8xmYIEPhpSvOldNzB4DMqQdD6tN+DphFFyvFwuLnWKWKOY+pbNXkd5q0H3A69owxgsONoOL1JqjZgOdcLtWTq+g9XqU5XSRQGo1ei5Ctv2f5yAHZuI2smwJkTA88SuyZqJNwRixw5OPpUDOuWxhK6xftsN7M1/TnioX8Ch3RTow7H0dvMFFD3ocvjG4A== -----END PGP MESSAGE----- EOT \u0026quot;Administrator\u0026quot; = tostring(null) } 上面的 PGP message 是加密過的 text，可以放心的傳遞\n只有我的 private key 能解開，所以就算擴散也沒有什麼風險 沒有 private key 的人已經證明無法在有效的時間內解開 附上解密的 keybase command 很貼心，有使用 keybase tool 人可以直接解 但我是使用 gpg 而不是 keybase，也是可以解開\necho \u0026quot;wcFMA4U4ylYlNYTfAQ/+KLtNeJqOqK/V3pNdcRh9nvyDw/huJfaYF7Ab/YdKqLKNVySBqc6HY8xedmER5Wn78RlG962/f772u8UlQN4XCOjOLjyOaNL8K0tkeoSVg+XXPx2qJO9Myc1d+74rG57biUD26XpwfS7+ryIjaHf+NzjisExZy2mgiMIzzAlfqTzRAc+jYP11wZcyFGwOe9pMq6BJy7FH5935ndVgNLCgYtSjgIV5b5kWQ3SDr0E9egAHnoAcHs8mni71x7OL7OCc/bS3nJSLz1jRkMoukWyQQ3+lM7Nwi36NSJneIKxW5f7lSxr7Bx+W3mx6gZgb18yRVIwbVW5iDFsAQOFb0zOHUf8tm3tAQ0F79Yeqy9sfLCfUUjVpCENEJe5FtwJkI8EbHKe8mGnYA4dbHaCkIDgVa8TZ4mdXl7yVZx0adlkKFIS48niwPHxMErZYqnRBitUCGdaH0bHcpPcnSVolLljZmXZUrntMjIEb9FGYbgl6hOiRF9MR3RQL5DNmpS7uSqU7V6TyhVbAOJO2oR01Z7hXdAMxZ54O+h06jN7fzvLrUSMWvUz/wp+JJnIDYNwu75Dvicn6NmXrP3fH3M8xmYIEPhpSvOldNzB4DMqQdD6tN+DphFFyvFwuLnWKWKOY+pbNXkd5q0H3A69owxgsONoOL1JqjZgOdcLtWTq+g9XqU5XSRQGo1ei5Ctv2f5yAHZuI2smwJkTA88SuyZqJNwRixw5OPpUDOuWxhK6xftsN7M1/TnioX8Ch3RTow7H0dvMFFD3ocvjG4A==\u0026quot; | base64 --decode | gpg --decrypt 就取得 Accounting 的初次登入密碼的，趕快到 aws console 上面試試看\nNOTE: 想要在 browser 同時多開 aws login session 的話，可以參考 chrome plugin session box\n登入 aws web console\n注意 account ID 是 root account ID user 是 Accounting password 是剛剛解密的密碼 password 尾端如果有 % 不是密碼的一部分，是 EOL (End of line) indicator，記得去掉 前面的部分剛好 20 字元，符合我們 login profile 的密碼長度限制 順利登入拉 Accounting 的權限是不能動用 EC2 API 的，切到 EC2 頁面直接 API Error，也符合預期 上面的變更我們的 PR 如下\nterragrunt-infrastructure-modules PR terragrunt-infrastructure-live-example PR 明天還有 Administrator 帳戶需要處理，會再更麻煩一些\nTODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-18-14th-ithome-ironman-iac-aws-workshop-07-reset-iam-user/","summary":"\u003cp\u003e昨天我們建立 IAM Group 與 policy，並說明 policy 管理原則。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e然而昨天最後創建 user 時，我們關閉了 \u003ccode\u003ecreate_login_profile = false\u003c/code\u003e 的選項\n\u003cul\u003e\n\u003cli\u003e這是為了避免當前的登入機制被覆蓋掉，影響 root account Administrator 的使用\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e更改 login 方式，在某些極端的情形下，有可能讓 Administrator 自己覆蓋自己的 login 設定後，讓自己無法登入\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如果你是管理員，在更改 admin 帳號的權限與登入設定時，一定要多加注意\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e如果不幸改壞，無法登入，就只能再去找出 root account root user 帳號來解救了\n\u003cul\u003e\n\u003cli\u003e如果是 root account root user 把自己的 login 改壞，就會很痛苦，要請 aws support 來救你\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本日進度\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e root 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 將手動產生的 Administrator 的 IAM User import terraform 中\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Policy\u003c/li\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Group\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e reset root account IAM user login profile \u0026amp; pgp key\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e","title":"Reset Iam User"},{"content":"昨天我們將 root account IAM user import 到 terraform 中\n示範 terraform import 增加 iam user 的功能到 module repository 中 今天要完成 root account 中 IAM Group + Policy，順便聊聊 aws IAM policy 管理原則\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n承接昨天的 plan 結果，我們今天要把 IAM policy 與 group 開出來\nIam User 首先 review aws_iam_user 的 resource\n# module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] will be updated in-place ~ resource \u0026quot;aws_iam_user\u0026quot; \u0026quot;this\u0026quot; { + force_destroy = true id = \u0026quot;Administrator\u0026quot; name = \u0026quot;Administrator\u0026quot; tags = {} # (4 unchanged attributes hidden) }` force_destroy 這個參數我們需要嗎？可以一找以下的步驟查文件判斷\n直接 google \u0026ldquo;terraform aws iam user\u0026rdquo;，找到 Terraform registry 上 AWS iam user 的說明 這裡有說明，刪除 user 時，如果有 non-terraform-managed 的 access key, login profile, MFA devices，是否仍要強制刪除 這裡我們希望如果有 access key 或 MFA device 存在的話，不要直接刪除 User 將 https://github.com/chechiachang/terragrunt-infrastructure-modules/pull/1/files#diff-b15a741b1129d8f5451060653922d85c934792a1dd41c7fae1b02f3a6398094aR8 改成 false Iam Group \u0026amp; Policy 今天要來調整 Iam Group \u0026amp; Policy\nGruntwork 文件: Root Account 說明，root account 下應該有兩個 policy group group/full-access 給予 root account 超級管理員完整的控制權限 billing 給予 billing 的會計人員進來報帳 首先 full-access 的 iam_group plan 後的結果如下\n由於我們使用的 module 是 terraform-aws-iam 的 group with policies module.iam_group_with_policies_full_access 裡面有 group 也有 policy .aws_iam_group.this[0] 是主要的 group # module.iam_group_with_policies_full_access.aws_iam_group.this[0] will be created + resource \u0026quot;aws_iam_group\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + path = \u0026quot;/\u0026quot; + unique_id = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group_membership.this[0] will be created + resource \u0026quot;aws_iam_group_membership\u0026quot; \u0026quot;this\u0026quot; { + group = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + users = [ + \u0026quot;Administrator\u0026quot;, ] } 有了 group，接下來就是要配 policy\n一樣透過 group + policy -\u0026gt; attachment 的 resource，把 group 跟 policy 綁在一起 我們這邊的程式碼 直接使用 aws 預先定義好的 policy arn:aws:iam::aws:policy/AdministratorAccess # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;custom_arns\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026quot;arn:aws:iam::aws:policy/AdministratorAccess\u0026quot; } IAM AWS 預先定義的 policy 有哪些 aws 已經預先定義好的 policy 可以使用？\n可以上 aws web console -\u0026gt; iam -\u0026gt; policies 下查詢 這些預先定義的 policy，是 aws 依照最常出現的使用情境，事先建立的 policy\n使用者不用再建議 ex. AWS 覺得 administrator 大概會需要這些權限，都先開到這個 policy/admin 上 ex. AWS 覺得 billing 大概會需要這些權限，都先開到這個 policy/billing 上 由於是 AWS 依照大部分人的需求開的 policy，所以會多開許多權限 這也是用預先定義 policy 的缺點，就是為了滿足很多人的需求，權限太大 違反最小權限原則，給予權限過多也造成安全性的風險 不使用預先定義的 policy 的話，我們可以自己寫 policy\n一個 policy 開出來 default 沒有任何 permission 依據最小權限原則，一句一句增加 permission 到 policy 上 如此可以確保 policy 上的 permission 都是需要的，而不會有多開但是用不到的權限 權限愈大，安全性風險越高，所以最佳實踐會希望所有 policy 都是配得剛剛好夠用就好\n然而配 policy 很花時間，用 aws 已經寫好的 policy 馬上可以用 實務上可以考量專案時間與整體人力，以及需要的安全等級，來考慮要不要做到這麼細 AWS policy access advisor 通常寫 policy 很難配到非常完美剛剛好，通常都會多開 permission\n權限多開功能不會壞，少開直接 permission denied 那多開的權限沒有用到，之後要如何把沒有用到的權限收回來？ AWS web console 提供根據使用紀錄，提建議修改 policy\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html\naws 會統計各個 IAM 元件，存取 AWS API 使用/沒使用的權限\n例如這個 User 過去 90 天使用了哪些 permission 去打 API 以及 User 還有哪些 policy permission，是過去 90 天都沒有用到的 這些長時間都沒有用到的 policy，我們就應該定期 review，然後移除這些不必要的權限\nUser / Group / \u0026hellip; 等等都需要做一樣的事情 module 作為一個組成元件使用 由於我們在 module input 中開啟了 attach_iam_self_management_policy = true 參數，在 module 中便連帶產生\n.aws_iam_policy.iam_self_management[0]，裡面定義的要做 self management 所需要的 permission 再把 policy 透過 .aws_iam_group_policy_attachment.iam_self_management[0] 綁到 group 上 # module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management[0] will be created + resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;iam_self_management\u0026quot; { ... } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;iam_self_management\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } 上面就是我們的 root account group/full-access 與對應的 policy\nbilling account 接著是 root account group/full-access 與對應的 policy\n我們可以自己手寫 policy，依照最小權限原則，一條一條加入 permission 或是我們可以上 aws web console 找看看有無預先定義好的 policy 我們搜尋 billing 有看到四個 policy，每個 policy 都有附上 description 說明使用的目的\nBilling AWSBillingConductorReadOnlyAccess AWSBillingConductorFullAccess AWSBillingReadOnlyAccess 這裡就要依據使用的需求選擇\n給予 billing 管理員的權限，就會是 ConductorFullAccess 或 ConductorReadOnlyAccess 給予 billing 報帳人員，應該不用更改 aws 的其他設定，只需要讀取跟輸出，就會是 AWSBillingReadOnlyAccess 我們這邊選擇 AWSBillingConductorFullAccess，做個示範\n點擊 AWSBillingConductorFullAccess，跳到 Policies \u0026raquo; AWSBillingConductorFullAccess 頁面 可以看到 policy 的完整 arn，Amazon Resource Name (ARN) 唯一識別AWS 資源，類似於 ID 然後把 arn 填入 module.am_group_with_policies_billing plan \u0026amp; apply 上面的變更我們的 PR 如下\nterragrunt-infrastructure-modules PR terragrunt-infrastructure-live-example PR 由於我們 module 中有新增 module \u0026quot;iam_group_with_policies_billing\u0026quot;，記得要先執行 init\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan ╷ │ Error: Module not installed │ │ on group.tf line 29: │ 29: module \u0026quot;iam_group_with_policies_billing\u0026quot; { │ │ This module is not yet installed. Run \u0026quot;terraform init\u0026quot; to install all │ modules required by this configuration. ╵ aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt init aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.iam_group_with_policies_billing.aws_iam_group.this[0] will be created + resource \u0026quot;aws_iam_group\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = \u0026quot;billing\u0026quot; + path = \u0026quot;/\u0026quot; + unique_id = (known after apply) } # module.iam_group_with_policies_billing.aws_iam_group_membership.this[0] will be created + resource \u0026quot;aws_iam_group_membership\u0026quot; \u0026quot;this\u0026quot; { + group = (known after apply) + id = (known after apply) + name = \u0026quot;billing\u0026quot; + users = [ + \u0026quot;Accounting\u0026quot;, ] } # module.iam_group_with_policies_billing.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;custom_arns\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026quot;arn:aws:iam::aws:policy/AWSBillingConductorFullAccess\u0026quot; } # module.iam_group_with_policies_billing.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;iam_self_management\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } # module.iam_group_with_policies_billing.aws_iam_policy.iam_self_management[0] will be created + resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;iam_self_management\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = (known after apply) + name_prefix = \u0026quot;IAMSelfManagement-\u0026quot; + path = \u0026quot;/\u0026quot; + policy = jsonencode( { + Statement = [ + { + Action = [ + \u0026quot;iam:UploadSigningCertificate\u0026quot;, + \u0026quot;iam:UploadSSHPublicKey\u0026quot;, + \u0026quot;iam:UpdateUser\u0026quot;, + \u0026quot;iam:UpdateLoginProfile\u0026quot;, + \u0026quot;iam:UpdateAccessKey\u0026quot;, + \u0026quot;iam:ResyncMFADevice\u0026quot;, + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, + \u0026quot;iam:GenerateServiceLastAccessedDetails\u0026quot;, + \u0026quot;iam:GenerateCredentialReport\u0026quot;, + \u0026quot;iam:EnableMFADevice\u0026quot;, + \u0026quot;iam:DeleteVirtualMFADevice\u0026quot;, + \u0026quot;iam:DeleteLoginProfile\u0026quot;, + \u0026quot;iam:DeleteAccessKey\u0026quot;, + \u0026quot;iam:CreateVirtualMFADevice\u0026quot;, + \u0026quot;iam:CreateLoginProfile\u0026quot;, + \u0026quot;iam:CreateAccessKey\u0026quot;, + \u0026quot;iam:ChangePassword\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowSelfManagement\u0026quot; }, + { + Action = [ + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = \u0026quot;*\u0026quot; + Sid = \u0026quot;AllowIAMReadOnly\u0026quot; }, + { + Action = \u0026quot;iam:DeactivateMFADevice\u0026quot; + Condition = { + Bool = { + \u0026quot;aws:MultiFactorAuthPresent\u0026quot; = \u0026quot;true\u0026quot; } + NumericLessThan = { + \u0026quot;aws:MultiFactorAuthAge\u0026quot; = \u0026quot;3600\u0026quot; } } + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowDeactivateMFADevice\u0026quot; }, ] + Version = \u0026quot;2012-10-17\u0026quot; } ) + policy_id = (known after apply) + tags_all = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group.this[0] will be created + resource \u0026quot;aws_iam_group\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + path = \u0026quot;/\u0026quot; + unique_id = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group_membership.this[0] will be created + resource \u0026quot;aws_iam_group_membership\u0026quot; \u0026quot;this\u0026quot; { + group = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + users = [ + \u0026quot;Administrator\u0026quot;, ] } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;custom_arns\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026quot;arn:aws:iam::aws:policy/AdministratorAccess\u0026quot; } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;iam_self_management\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management[0] will be created + resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;iam_self_management\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = (known after apply) + name_prefix = \u0026quot;IAMSelfManagement-\u0026quot; + path = \u0026quot;/\u0026quot; + policy = jsonencode( { + Statement = [ + { + Action = [ + \u0026quot;iam:UploadSigningCertificate\u0026quot;, + \u0026quot;iam:UploadSSHPublicKey\u0026quot;, + \u0026quot;iam:UpdateUser\u0026quot;, + \u0026quot;iam:UpdateLoginProfile\u0026quot;, + \u0026quot;iam:UpdateAccessKey\u0026quot;, + \u0026quot;iam:ResyncMFADevice\u0026quot;, + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, + \u0026quot;iam:GenerateServiceLastAccessedDetails\u0026quot;, + \u0026quot;iam:GenerateCredentialReport\u0026quot;, + \u0026quot;iam:EnableMFADevice\u0026quot;, + \u0026quot;iam:DeleteVirtualMFADevice\u0026quot;, + \u0026quot;iam:DeleteLoginProfile\u0026quot;, + \u0026quot;iam:DeleteAccessKey\u0026quot;, + \u0026quot;iam:CreateVirtualMFADevice\u0026quot;, + \u0026quot;iam:CreateLoginProfile\u0026quot;, + \u0026quot;iam:CreateAccessKey\u0026quot;, + \u0026quot;iam:ChangePassword\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowSelfManagement\u0026quot; }, + { + Action = [ + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = \u0026quot;*\u0026quot; + Sid = \u0026quot;AllowIAMReadOnly\u0026quot; }, + { + Action = \u0026quot;iam:DeactivateMFADevice\u0026quot; + Condition = { + Bool = { + \u0026quot;aws:MultiFactorAuthPresent\u0026quot; = \u0026quot;true\u0026quot; } + NumericLessThan = { + \u0026quot;aws:MultiFactorAuthAge\u0026quot; = \u0026quot;3600\u0026quot; } } + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowDeactivateMFADevice\u0026quot; }, ] + Version = \u0026quot;2012-10-17\u0026quot; } ) + policy_id = (known after apply) + tags_all = (known after apply) } # module.iam_user[\u0026quot;Accounting\u0026quot;].aws_iam_user.this[0] will be created + resource \u0026quot;aws_iam_user\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + force_destroy = false + id = (known after apply) + name = \u0026quot;Accounting\u0026quot; + path = \u0026quot;/\u0026quot; + tags_all = (known after apply) + unique_id = (known after apply) } Plan: 11 to add, 0 to change, 0 to destroy. aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt apply 眼尖的同學應該有注意到我們把 create_login_profile = false 先關掉\n因為我們還沒有準備 pgp key，這個又要明天再來了 TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-17-14th-ithome-ironman-iac-aws-workshop-06-provision-iam-group-policy/","summary":"\u003cp\u003e昨天我們將 root account IAM user import 到 terraform 中\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e示範 terraform import\u003c/li\u003e\n\u003cli\u003e增加 iam user 的功能到 module repository 中\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e今天要完成 root account 中 IAM Group + Policy，順便聊聊 aws IAM policy 管理原則\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e root 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput checked=\"\" disabled=\"\" type=\"checkbox\"\u003e 將手動產生的 Administrator 的 IAM User import terraform 中\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Policy\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 補上 root account IAM Group\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e承接昨天的 plan 結果，我們今天要把 IAM policy 與 group 開出來\u003c/p\u003e","title":"Provision Iam Group Policy"},{"content":"昨天我們為每個環境(dev / stag / prod \u0026hellip;) 設定一個 aws organization account\n今天要使用 terraform 設定 AWS IAM User\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User 加到 terraform 中 security 中設定 IAM User security 設定 password policy security 設定 MFA policy iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nAccounts \u0026amp; IAM Users 今天要使用 Terraform 設定 IAM Users。\n未來所有的 User 都會透過 terraform 設定並管理 Day02 設定的 root account IAM User: Administrator 雖然是手動建立的，我們一樣需要把他匯入到 terraform 中 然而上個 PR 中，我們的 terraform module 中並沒有設定 IAM User 的功能，也就是這段 code 是沒有發揮功能 https://github.com/chechiachang/terragrunt-infrastructure-live-example/pull/1/files#diff-62920ff868733e1c625c23fe7ffd6c93bebd87ae16b865869bf682e29b082a99R54-R67\nusers = { alice = { groups = [\u0026quot;full-access\u0026quot;] pgp_key = \u0026quot;keybase:alice\u0026quot; create_login_profile = true create_access_keys = false }, bob = { groups = [\u0026quot;billing\u0026quot;] pgp_key = \u0026quot;keybase:bob\u0026quot; create_login_profile = true create_access_keys = false } } 我們這邊一樣嘗試把這個 users (map) 的功能補上\n使用開源 module 實作 IAM User 使用的開源 terraform module 是 terraform-aws-modules/terraform-aws-iam 是 aws 官方維護的開源 terraform module，品質很好，放心使用 需求整理\n要產生 IAM User input: 一個 map users = {} output: 多個 user 要產生 IAM Group 與 IAM Policy 增加 IAM User PR 的 commit 在此 https://github.com/chechiachang/terragrunt-infrastructure-modules/pull/1\n於是我們試著執行 terraform plan\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.iam_group_with_policies_full_access.aws_iam_group.this[0] will be created + resource \u0026quot;aws_iam_group\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + path = \u0026quot;/\u0026quot; + unique_id = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group_membership.this[0] will be created + resource \u0026quot;aws_iam_group_membership\u0026quot; \u0026quot;this\u0026quot; { + group = (known after apply) + id = (known after apply) + name = \u0026quot;full-access\u0026quot; + users = [ + \u0026quot;Administrator\u0026quot;, ] } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;custom_arns\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026quot;arn:aws:iam::aws:policy/AdministratorAccess\u0026quot; } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026quot;aws_iam_group_policy_attachment\u0026quot; \u0026quot;iam_self_management\u0026quot; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management[0] will be created + resource \u0026quot;aws_iam_policy\u0026quot; \u0026quot;iam_self_management\u0026quot; { + arn = (known after apply) + id = (known after apply) + name = (known after apply) + name_prefix = \u0026quot;IAMSelfManagement-\u0026quot; + path = \u0026quot;/\u0026quot; + policy = jsonencode( { + Statement = [ + { + Action = [ + \u0026quot;iam:UploadSigningCertificate\u0026quot;, + \u0026quot;iam:UploadSSHPublicKey\u0026quot;, + \u0026quot;iam:UpdateUser\u0026quot;, + \u0026quot;iam:UpdateLoginProfile\u0026quot;, + \u0026quot;iam:UpdateAccessKey\u0026quot;, + \u0026quot;iam:ResyncMFADevice\u0026quot;, + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, + \u0026quot;iam:GenerateServiceLastAccessedDetails\u0026quot;, + \u0026quot;iam:GenerateCredentialReport\u0026quot;, + \u0026quot;iam:EnableMFADevice\u0026quot;, + \u0026quot;iam:DeleteVirtualMFADevice\u0026quot;, + \u0026quot;iam:DeleteLoginProfile\u0026quot;, + \u0026quot;iam:DeleteAccessKey\u0026quot;, + \u0026quot;iam:CreateVirtualMFADevice\u0026quot;, + \u0026quot;iam:CreateLoginProfile\u0026quot;, + \u0026quot;iam:CreateAccessKey\u0026quot;, + \u0026quot;iam:ChangePassword\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowSelfManagement\u0026quot; }, + { + Action = [ + \u0026quot;iam:List*\u0026quot;, + \u0026quot;iam:Get*\u0026quot;, ] + Effect = \u0026quot;Allow\u0026quot; + Resource = \u0026quot;*\u0026quot; + Sid = \u0026quot;AllowIAMReadOnly\u0026quot; }, + { + Action = \u0026quot;iam:DeactivateMFADevice\u0026quot; + Condition = { + Bool = { + \u0026quot;aws:MultiFactorAuthPresent\u0026quot; = \u0026quot;true\u0026quot; } + NumericLessThan = { + \u0026quot;aws:MultiFactorAuthAge\u0026quot; = \u0026quot;3600\u0026quot; } } + Effect = \u0026quot;Allow\u0026quot; + Resource = [ + \u0026quot;arn:aws:iam::706136188012:user/*/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:user/${aws:username}\u0026quot;, + \u0026quot;arn:aws:iam::706136188012:mfa/${aws:username}\u0026quot;, ] + Sid = \u0026quot;AllowDeactivateMFADevice\u0026quot; }, ] + Version = \u0026quot;2012-10-17\u0026quot; } ) + policy_id = (known after apply) + tags_all = (known after apply) } # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] will be created + resource \u0026quot;aws_iam_user\u0026quot; \u0026quot;this\u0026quot; { + arn = (known after apply) + force_destroy = true + id = (known after apply) + name = \u0026quot;Administrator\u0026quot; + path = \u0026quot;/\u0026quot; + tags_all = (known after apply) + unique_id = (known after apply) } # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user_login_profile.this[0] will be created + resource \u0026quot;aws_iam_user_login_profile\u0026quot; \u0026quot;this\u0026quot; { + encrypted_password = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 + password_reset_required = false + pgp_key = \u0026quot;keybase:alice\u0026quot; + user = \u0026quot;Administrator\u0026quot; } Plan: 7 to add, 0 to change, 0 to destroy. 會產生幾個東西\nmodule.iam_user[\u0026quot;Administrator\u0026quot;] 是一個 module，裡頭產生 IAM User 與其他 resource module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user_login_profile 我們有開啟 create login file 的參數，所以 aws terraform module 便產生 module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management 我們有開啟 create self management policy，所以 aws terraform module 便產生 module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment 透過這個 attachment 將 IAM policy 關聯到 IAM group，也就是 group 中有 attach 此 full-access policy module.iam_group_with_policies_full_access.aws_iam_group 是 IAM Group: full-access module.iam_group_with_policies_full_access.aws_iam_group_membership 將 IAM User 關聯到 IAM group 也就是 user/Administrator 屬於 group/full-access AWS 許多資源的描述都用 attachment 的形式描述兩個元件的關聯\nuser + group -\u0026gt; group_membership group + policy -\u0026gt; group_policy_attachment 像是 RMDBS 的關聯 table，更改關聯時並不會影響到兩個元件本身的內容，調整關聯很彈性 Terraform Import 由於 Administrator 我們 Day02 已經透過 web console 建立，現在 terraform plan 出來的結果卻也是要 create 一個 new user，這個結果不是我們想要的\n因為 web console 產生的 User 並沒有在 terraform 中管理，也就是雖然 AWS 上 User 確實存在，但 terraform 中並沒有 state 來描述這個 User，所以 Terraform 不知道這個 plan create 的 User 其實就是 AWS console 上已經存在的 Administrator 不在 terraform state 的元件，terraform 便無法管理 要將已經存在的 AWS 元件，納入 terraform state 進行管理，這個行為我們稱作 import\n現在我們要試著 terraform import 已經存在的 User/Administrator\n首先，先到 terraform registry 去搜尋 aws provider import 的語法及參數 直接 google 元件的名稱 terraform aws iam user 通常就會找到 捲到頁面最底下就可以看到 import 語法 # terraform import aws_iam_user.lb loadbalancer terraform import \u0026lt;address\u0026gt; \u0026lt;username\u0026gt; 這邊的 address 需要填入 plan 時預計產生的 resource aws_iam_user\n也就是 module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] 這個 address 剛開始學 terraform 可能會還不太清楚 address 為何會長這樣，久了就會了解 可以先從找到 resource 主體為目標慢慢看\n所謂的 resource 的主體，其實就是 terraform 的基本單位 一個 terraform resource 可能就是對應一個 aws 元件 aws_iam_user -\u0026gt; iam_user aws_iam_policy -\u0026gt; iam_policy 去掉前面的 module 與後面的 index 就可以找到 resource\n# 其中 aws_iam_user 是 terraform resource module.\u0026lt;module_name\u0026gt;.aws_iam_user.\u0026lt;iam_user_name\u0026gt;.\u0026lt;iam_user_index\u0026gt; # 或是這個 address 中 aws_iam_policy 是 terraform resource module.\u0026lt;module_name\u0026gt;.aws_iam_policy.\u0026lt;iam_user_name\u0026gt;.\u0026lt;iam_user_index\u0026gt; # module 可以在包其他 module，所以架構複雜的 terraform module，resource address 就愈來越長 module.\u0026lt;module_name\u0026gt;.module.\u0026lt;module_name\u0026gt;.aws_iam_user.\u0026lt;iam_user_name\u0026gt;.\u0026lt;iam_user_index\u0026gt; 最上層的 resource 是 module/iam_user[*]，對應的 .tf code 是 user.tf\nmodule \u0026quot;iam_user\u0026quot; { source = \u0026quot;terraform-aws-modules/iam/aws//modules/iam-user\u0026quot; for_each = var.users name = each.key force_destroy = true create_iam_user_login_profile = each.value.create_login_profile create_iam_access_key = each.value.create_access_keys pgp_key = each.value.pgp_key password_reset_required = false } module/iam_user 中包含了許多 terraform resource aws_iam_user 是基本 resource aws_iam_user.this[0] 後面多了 index suffix，是因為可能在 module 中有使用 count 或 for_each，造成一個 list 的 aws_iam_user，而其中 index 為 0 的就是 aws_iam_user.this[0] terraform 支援許多好用的 build-in function 讓我們可以快速的使用，產生複雜的邏輯，這個之後有空會教大家。或是參考 2021 iThome 的發文: infrastructure 也可以 for each 之一\n回到 import，我們把 terraform import address name 數入後，發現出錯\n# Bash syntax error aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt import module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] Administrator zsh: no matches found: module.iam_user[Administrator].aws_iam_user.this[0] 這是因為有些 address 字元 bash 中有其他意義的特殊字元，bash 先看不懂了，就無法執行，還沒跑到 terragrunt。我們把 address 前後都增加單引號，讓 bash 把 address 當作字串處理而不要展開 (expension)\n# Add single quote to escape taws-vault exec terraform-30day-root-iam-user --no-session -- erragrunt import 'module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0]' Administrator module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0]: Importing from ID \u0026quot;Administrator\u0026quot;... module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0]: Import prepared! Prepared aws_iam_user for import module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0]: Refreshing state... [id=Administrator] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Releasing state lock. This may take a few moments... 顯示為 Import Success 了，就表示我們已經把 aws 存在的 user import 到 terraform state 中\n再次 plan，發現從 Plan: 7 to add，變成 Plan: 6 to add, 1 to change\n表示 terraform 知道 plan 中的 address 要直接對應到 aws 現存的 user/Administrator change 中顯示 + force_destroy = true 表示 aws 現存的 user/Administrator 沒有這個設定 .tf 的 module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] 有這行 code terraform plan 發現 .tf 比現存的 user/Administrator 多設定，於是要多加設定 我們可以依據需求，決定要 更改 .tf，拿掉 + force_destroy = true，這樣 plan 後 terraform 就會覺得兩邊的 user/Administrator 一模壹樣，不需要 change 或是就直接 apply ，讓現存的 user/Administrator 增加 + force_destroy = true aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan # module.iam_user[\u0026quot;Administrator\u0026quot;].aws_iam_user.this[0] will be updated in-place ~ resource \u0026quot;aws_iam_user\u0026quot; \u0026quot;this\u0026quot; { + force_destroy = true id = \u0026quot;Administrator\u0026quot; name = \u0026quot;Administrator\u0026quot; tags = {} # (4 unchanged attributes hidden) } Plan: 6 to add, 1 to change, 0 to destroy. TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-16-14th-ithome-ironman-iac-aws-workshop-05-provision-iam-user/","summary":"\u003cp\u003e昨天我們為每個環境(dev / stag / prod \u0026hellip;) 設定一個 aws organization account\u003c/p\u003e\n\u003cp\u003e今天要使用 terraform 設定 AWS IAM User\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e root 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 將手動產生的 Administrator 的 IAM User 加到 terraform 中\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 中設定 IAM User\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 設定 password policy\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e security 設定 MFA policy\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003ch1 id=\"accounts--iam-users\"\u003eAccounts \u0026amp; IAM Users\u003c/h1\u003e\n\u003cp\u003e今天要使用 Terraform 設定 IAM Users。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e未來所有的 User 都會透過 terraform 設定並管理\u003c/li\u003e\n\u003cli\u003eDay02 設定的 root account IAM User: Administrator 雖然是手動建立的，我們一樣需要把他匯入到 terraform 中\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e然而上個 PR 中，我們的 terraform module 中並沒有設定 IAM User 的功能，也就是這段 code 是沒有發揮功能\n\u003ca href=\"https://github.com/chechiachang/terragrunt-infrastructure-live-example/pull/1/files#diff-62920ff868733e1c625c23fe7ffd6c93bebd87ae16b865869bf682e29b082a99R54-R67\"\u003ehttps://github.com/chechiachang/terragrunt-infrastructure-live-example/pull/1/files#diff-62920ff868733e1c625c23fe7ffd6c93bebd87ae16b865869bf682e29b082a99R54-R67\u003c/a\u003e\u003c/p\u003e","title":"Provision Iam User"},{"content":"昨天設定了多個 aws organization accounts，但我們還沒有說明為什麼要這樣做\n今天會先講 Gruntwork 提出的 Production-Grade Design: multi-account security strategy\n拆分多個 account 的用意 多 account 下如何統一管理 IAM User access control 與安全性控管 iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nProduction-grade Design IAM Gruntwork 提出的 Production-Grade Design: multi-account security strategy，我們這邊一一講解\nRoot Account 的用途 位於 diagram 圖中的最上層 是整個架構的創世帳號，權限最大 root account 裡面沒有任何 infrastructure 元件， i.e. aws ec2 不會長在裡面，是一個空 account root account 的存取最嚴格 只有很少數的 admin 可以存取 i.e 只有 SRE 部門主管有自己的 IAM User，其他 SRE 沒有權限存取 root account root account 的工作最少 設定 child account 管理 billing IAM policy 管理，不要把 policy 直接綁在 User 上，應該要\n為不同權責的成員設定 IAM Group，ex. dev-admin / dev-user / stag-admin / qa / billing / \u0026hellip; 把 policy 綁在 Group 上，ex. ReadPermission -\u0026gt; dev-user / Read+Write -\u0026gt; dev-admin 把 User 加進 / 移除 Group，來管理人員權限 Group 通常會接近公司團隊的職責分配 root account 內部只會有 full-access 的管理員 group billing 的會計出帳 group Child Account 的用途 透過 root account 創建 child accounts 後，這裡說明各個 account 的設計用途\nSecurity 用來管理 authentication 與 authorization\n底下一樣沒有任何 infrastructure 元件 定義所有 IAM User 與 IAM Group，所有團隊成員的 User 在這邊設定 所有的 User 放在 Security 底下統一管理 其他的 child account 不會設定 IAM User，ex. dev / stag / prod 裡面都沒有 IAM User child account 中設定 IAM roles，讓 security/user assume role 存取其他 child account ex. security/dev-admin/chechia assume role dev/admin，透過 aws STS(Security Token Service) 暫時取得 dev/admin 這個 role 的權限來控制 dev 團隊主管也會有一個 security IAM User，平時開發 infrastructure 使用這個 IAM User，只有需要調整 root account 時才會動用 root account IAM User。反正 root account 是能不用就不用 這個做法有非常多好處，但又不會造成工作上的負擔\n使用方面：所有的團隊成員只會有一個 User，透過 security 登入。不會一個人有好幾個帳號密碼很煩 管理方面：管理員只要在 security 內管理 User，而不用一直切換 account 去調整 IAM User 安全方面：只要在 security 內落實安全性設定，就可以很好的控制所有 account 的存取權限 dev / stag / prod 用來存放 infrastructure 與跑 application\n開發時使用 dev / stag 環境，測試都完成後，才自動推倒 prod 公開給用戶使用 dev / stag 環境是內部環境，架構與 prod 類似，來模擬 prod 的環境方便開發與測試 可能機器數量或規格比 prod 小很多，來節省成本 有些團隊會有更多 account，例如 qa / uat / \u0026hellip;，都可以依照需求建立 prod 有些團隊會有複數 prod 環境，可以供災難發生時做備援切換 shared 裡面放跨 application account 共用的元件\n例如 AWS ECR 可能不想要每個 account 都開，想放一起的話可以放在 shared，然後讓其他 account 存取 shared 版本控制系統 Git / github 可能會只有一組大家共用 CI/CD pipeline / Jenkins \u0026hellip;等等跨環境共用的資源，可以放 shared 每個 account 都開也是有許多好處，例如更獨立更安全，這就看看各個團隊災安全與成本上的取捨 管理上由於 prod 會使用到 shared 的元件，建議要把 shared 的安全等級當作 prod 來管理，嚴格限制 shared 的存取，以保護 prod 環境的安全 很多個 sandbox account 讓工程師自由的測試功能\n如果有需求與預算，一個完全獨立的 sandbox 讓工程師可以做各種實驗，對於促進團隊內部的創新會有非常大的幫助 如果在 dev 上做這測試，有可能會影響到其他團隊成員，而覺得綁手綁腳的事情，都可以盡情在 sandbox 上操作 可以為 sandbox 的帳號設定更嚴格的 billing / cost 設定，避免工程師玩太嗨超過預算 黃金準則是一個工程師一個 sandbox 來達到 100% 開發獨立 testing account 用來測試 infrastructure 的測試\n如果有使用 Gruntwork/Terratest 來測試 terraform tf code 的朋友，應該知道 terratest 會把 terraform module 中的 infrastrcuture 真的在 aws 開出來，做功能測試，然後測試完成後再把 infrastrcture 全部刪掉 是的 IaC terraform module 也是需要單元測試 testing 就是用來讓 IaC 做自動化測試的環境，裡面的 infrastructure 都是常常 create + destroy，不會有常駐的服務 與 sandbox 不同的是 testing 是跑自動化 CI/CD pipeline 的測試 sandbox 是讓工程師做規模較小的手動測試與開發，因為我們也不希望一堆測試用 infrastructure 散落在 sandbox 裡面，感覺很貴 logs 用來收集 log 到單一 account 底下，方便查閱\n所有 child account 的 log 都收到這裡，而不用跑到各個 account 去查 log aws account 內部的 log，cloud trail 都可以透過客訂與工具轉發，集中收集 如果公司規模很大，有多個 business unit 的話，也可以多建幾個 organization，來對應公司組織\n透過 web console switch role 我是一個開發工程師（不是 admin），那在 multi-account 下的環境我應該如何工作？\n如何存取 aws\nIAM User 可以透過帳號密碼來存取 aws web console，登入之後會發現自己處在 security account 底下 第一次登入的話需要重設密碼，需要符合 IAM Password Policy 會需要輸入 MFA，會需要符合 IAM MFA Policy 兩面的安全性 policy 在實際公司帳號裡是必備，但在 workshop 中，我們還沒做，先留個坑之後來補 programatic access 如同 day03 的 root account IAM User 步驟，我們也是使用 aws-vault 搭配 aws access key，來讓 terraform 執行 web console 如何 switch role\n可以在 aws web console -\u0026gt; 右上角 user -\u0026gt; switch role 填入\n目標 account ID (ex dev: 123456789012) 目標的 role 給自己看的好辨識名稱 如果 admin 有設定 IAM role 與 assume role 權限，就可以切換到 dev account 下的 IAM role 身分 透過 web console switch role 如同 day03 使用 root account IAM User，我們也是使用 aws-vault 搭配 aws access key，來讓 terraform 執行) 來跑 terraform 建立 child accounts 一樣，我們也需要設定 aws-vault，讓我們可以在 local 機上 assume 不同 account role，來使用各個 child account\naws-vault Roles and MFA 需要調整 ~/.aws/config 的設定\nsecurity 管理員設定完 iam role + assume policy 後，會提供登入資訊給其他團隊成員 cat ~/.aws/config # 這是 root account IAM User 不要再拿來用了 # aws access key 收在本機的 aws-vault 內 [profile terraform-30day-root-iam-user] region=ap-northeast-1 # 這是 security account (111111111111) IAM User（還沒建立） # aws access key 收在本機的 aws-vault 內 [profile chechia-security-iam-user] region=ap-northeast-1 # 這是 dev account (333333333333) IAM Role（還沒建立） [profile dev-admin] source_profile = chechia-security-iam-user role_arn = arn:aws:iam::333333333333:role/dev-admin mfa_serial = arn:aws:iam::111111111111:mfa/chechia-security-iam-user 上面這些都還沒建立，所以都還不能用，只是先說一下之後要怎麼操作\nsecurity 管理員要設定一大堆東西，但工程師使用非常簡單，只要透過 aws-vault 切換就可以在不同 account 下操作 terraform\naws-vault exec dev-admin -- aws sts get-caller-identity { \u0026quot;UserId\u0026quot;: \u0026quot;xxxxxxxxxxxxxxxx\u0026quot;, \u0026quot;Account\u0026quot;: \u0026quot;333333333333\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:iam::333333333333:role/dev-admin\u0026quot; } aws-vault exec dev-admin -- terragrunt plan aws-vault exec stag-admin -- terragrunt plan ... 其他 Cloudtrail\n12 month free-tier 是可以免費使用的 log 存放在 s3 上，會需要收取 s3 的費用，12 month free-tier s3 是 5G，我們這個 workshop 不會超過 note: terraform state 也是放在 s3 上 TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","permalink":"https://chechia.net/posts/2022-09-15-14th-ithome-ironman-iac-aws-workshop-04-aws-multi-account-structure/","summary":"\u003cp\u003e昨天設定了多個 aws organization accounts，但我們還沒有說明為什麼要這樣做\u003c/p\u003e\n\u003cp\u003e今天會先講 Gruntwork 提出的 \u003ca href=\"https://docs.gruntwork.io/guides/build-it-yourself/landing-zone/production-grade-design/intro\"\u003eProduction-Grade Design: multi-account security strategy\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e拆分多個 account 的用意\u003c/li\u003e\n\u003cli\u003e多 account 下如何統一管理 IAM User\u003c/li\u003e\n\u003cli\u003eaccess control 與安全性控管\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003ch1 id=\"production-grade-design-iam\"\u003eProduction-grade Design IAM\u003c/h1\u003e\n\u003cp\u003eGruntwork 提出的 \u003ca href=\"https://docs.gruntwork.io/guides/build-it-yourself/landing-zone/production-grade-design/intro\"\u003eProduction-Grade Design: multi-account security strategy\u003c/a\u003e，我們這邊一一講解\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Multi-account security strategy\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20220918/20120327GL49e7CTD0.png\"\u003e\u003c/p\u003e\n\u003ch3 id=\"root-account-的用途\"\u003e\u003ca href=\"https://docs.gruntwork.io/guides/build-it-yourself/landing-zone/production-grade-design/the-root-account\"\u003eRoot Account\u003c/a\u003e 的用途\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e位於 diagram 圖中的最上層\u003c/li\u003e\n\u003cli\u003e是整個架構的創世帳號，權限最大\u003c/li\u003e\n\u003cli\u003eroot account 裡面沒有任何 infrastructure 元件， i.e. aws ec2 不會長在裡面，是一個空 account\u003c/li\u003e\n\u003cli\u003eroot account 的存取最嚴格\n\u003cul\u003e\n\u003cli\u003e只有很少數的 admin 可以存取\n\u003cul\u003e\n\u003cli\u003ei.e 只有 SRE 部門主管有自己的 IAM User，其他 SRE 沒有權限存取 root account\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eroot account 的工作最少\n\u003cul\u003e\n\u003cli\u003e設定 child account\u003c/li\u003e\n\u003cli\u003e管理 billing\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIAM policy 管理，不要把 policy 直接綁在 User 上，應該要\u003c/p\u003e","title":"AWS Multi-Accounts Structure"},{"content":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n\u0026ndash;\nProvision AWS Account\n今天要把 aws account 設定完成，並且開始使用 live-repository 來搭建 aws 元件\n我們自己的 module repository 開一個權限的 repository fork 我自幹的 modules repository terragrunt-infrastructure-modules https://github.com/chechiachang/terragrunt-infrastructure-modules 上面這個是放 terraform module 的 repository，是兩個 repository 不要跟執行 terragrunt repository 搞混，是兩個 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example 今天的進度與 code base\nterraform-live-example PR 在此 terragrunt-infrastructure-modules 請直接看 tag v0.0.1 因為有地方嚴重寫錯，我有 force push 東西 :tear:，如果已經提早下載的朋友可能會需要清掉 repository main branch + tag 重拉 orz 對，我們且戰且走常常會這樣，發現昨天寫的東西寫錯臨時改，請大家見諒\n為何使用 aws modules X) 因為我們沒有付錢，不能用 gruntwork 的 priviate repository (大誤\nO) 因為我們想要使用開源版本的 terraform module (XD)\n我們快速看一下 module 裡面的內容 https://github.com/chechiachang/terragrunt-infrastructure-modules/tree/v0.0.1/aws/modules/account-baseline-root\nresource \u0026quot;aws_organizations_organization\u0026quot; \u0026quot;org\u0026quot; { aws_service_access_principals = [ \u0026quot;cloudtrail.amazonaws.com\u0026quot;, \u0026quot;config.amazonaws.com\u0026quot;, ] feature_set = \u0026quot;ALL\u0026quot; } resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { for_each = var.child_accounts name = each.key email = each.value[\u0026quot;email\u0026quot;] depends_on = [ aws_organizations_organization.org ] } 我們的需求\n在 root account 下 provision 一個 aws organization 為每個環境 provision 一個 aws organization account 使用 terraform for_each function 來迭代 variabel 傳入的 organization accounts (型別是 map) each.key 會拿到每個 map element 的 key，例如 child_accounts = { logs = {}, security = {}, shared = {}, dev = {}, stage = {}, prod = {} } 跑出來 each.key 就會是 [logs, security, shared, dev, stage, prod] 正好是我們這次需要 provision 的六個 aws organization accounts，滿足目前的需求 each.value[\u0026ldquo;email\u0026rdquo;] 則依序帶入各自的 email 到 account 中 NOTE: 這些 email 請使用你收得到的 email\n在現實中這會是六組不同的 email，email 重複的話會被 aws api reject，所以需要使用不同的 email 或是跟 gruntwork guide 一樣，使用 gmail 帳號後+ 的字元會 ignore 如果設定了收不到信的 email 作為 account email 就會有點麻煩 不要像我一樣設了 6 個收不到 email，只好再多開一個 account XD，還好我們是在 workshop 這個 terraform module 的寫法完全忽略其他 each.value 內的值\n我們並不清楚 gruntwork 在 private 的 terraform module 中，遮些參數的確切用途，以及對 aws organization account 設定有何影響 但以筆者使用 aws 的經驗，大概猜得出來這些參數的用途，以及 terraform module 應該如何調整，為了課程進度安排，我們先暫時忽略。繼續照著 gruntwork 的文件做下去，之後有用到再來補 Terragrunt Plan \u0026amp; apply 在跑 terragrunt 之前，我們先要切換成 root account 的 iam User，昨天新建的 IAM User，存放在密碼企理，還記得嗎？\n使用 aws-vault 拿 IAM User 的 access key 做 programatic 登入，提供 terraform credential。而非透過 aws web console 記住：我們不能用 root account 做這些工作\naws-vault 由於我們現在要使用 IAM User 做 terraform 來創建新的 child accounts，未來會使用新的 child accounts 來做各個環境中的 terraform 元件操作，我們需要一個工具來協助切換這些 account\n我們使用的工具是 aws-vault\nhttps://github.com/99designs/aws-vault 支援很多安裝平台，選自己喜歡用的，或是直接在 github release 頁面下載 binary $ aws-vault --help usage: aws-vault [\u0026lt;flags\u0026gt;] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt; ...] A vault for securely storing and accessing AWS credentials in development environments. ... 使用 Root Account IAM User 身份執行 terragrunt\n在本機紀錄 access key (請好好保護本機的安全)\naws-vault add terraform-30day-root-iam-user Enter Access Key Id: XXXXXXXXXXXX Enter Secret Key: YYYYYYYYYYYY 透過 aws-cli 取得 caller identity，也就是現在的身份是誰，確定是 Administrator\n從 aws 的角度，所有操作都是 IAM User: Administrator 操作的 只能操作當初建立 IAM User 時 attach 的權限（蘇然也是很大）但已經比 root account 小很多 之後我們會進一步限縮每一個 IAM User 的 permission aws-vault exec terraform-30day-root-iam-user -- aws sts get-caller-identity { \u0026quot;UserId\u0026quot;: \u0026quot;AIDAxxxxxxxxxxxKGW\u0026quot;, \u0026quot;Account\u0026quot;: \u0026quot;706136188012\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:iam::706136188012:user/Administrator\u0026quot; } 然後透過 aws-vault 來執行 terragunt，這樣 terragrunt 就會使用 IAM User 的 credential 來操作 provider 去呼叫 aws api\naws-vault exec terraform-30day-root-iam-user -- terragrunt init aws-vault exec terraform-30day-root-iam-user -- terragrunt plan Terraform will perform the following actions: # aws_organizations_account.account[\u0026quot;dev\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;dev\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;logs\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;logs\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;prod\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;prod\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;security\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;security\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;shared\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;shared\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;stage\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;terraform30days@outlook.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;stage\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026quot;prod\u0026quot;] will be created + resource \u0026quot;aws_organizations_account\u0026quot; \u0026quot;account\u0026quot; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026quot;chechiachang999+terraform-test@gmail.com\u0026quot; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026quot;prod\u0026quot; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_organization.org will be created + resource \u0026quot;aws_organizations_organization\u0026quot; \u0026quot;org\u0026quot; { + accounts = (known after apply) + arn = (known after apply) + aws_service_access_principals = [ + \u0026quot;cloudtrail.amazonaws.com\u0026quot;, + \u0026quot;config.amazonaws.com\u0026quot;, ] + feature_set = \u0026quot;ALL\u0026quot; + id = (known after apply) + master_account_arn = (known after apply) + master_account_email = (known after apply) + master_account_id = (known after apply) + non_master_accounts = (known after apply) + roots = (known after apply) } Plan: 8 to add, 0 to change, 0 to destroy. Terragrunt plan 後產生 terraform plan，我們要仔細 review\n0 change / 0 delete，這邊很重要，確定我們不會改錯或是誤刪已經存在的 aws 元件 7 to add，新增了 7 個 aws 元件 一個 aws_organization 六個 aws_organization_account，對應未來的六個環境 一個 aws_organization_account，因為上面 email 設錯卡住，而多開一個 test 環境XD review 沒問題後，我們進行 apply\naws-vault exec terraform-30day-root-iam-user -- terragrunt apply aws_organizations_account.account[\u0026quot;logs\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;stage\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;prod\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;security\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;dev\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;shared\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;prod\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;logs\u0026quot;]: Creating... aws_organizations_account.account[\u0026quot;logs\u0026quot;]: Still creating... [10s elapsed] aws_organizations_account.account[\u0026quot;prod\u0026quot;]: Still creating... [10s elapsed] aws_organizations_account.account[\u0026quot;logs\u0026quot;]: Creation complete after 14s [id=557659608246] aws_organizations_account.account[\u0026quot;prod\u0026quot;]: Creation complete after 14s [id=420896464212] 創建完成 organization 與 organization accounts 後，我們可以進行檢查\n到 aws console -\u0026gt; organization 頁面，可以看到新建的 organization 與 accounts\n也可以使用 aws-cli 列出 organization 的 accounts\naws-vault exec terraform-30day-root-iam-user --no-session -- aws organizations list-accounts 使用 external module 二三事 使用開源的 terraform module 也是有很多好處\n首先我們使用的是 aws 維護的 terraform module，如果 aws api 有更新通常都跟得很快 開源 module 會有大量的社群幫忙發 issue 討論與 PR review 遇到問題可以直接開 issue 問 gruntwork enterprise 的 module 是 enterprise 等級的解決方案，有相當的 code 品質，但在上面幾點自然是不如開源 repository 順便提一下，如果之後要自己動手寫其他元件，應該如何找到適合的 external terraform module?\n有需要導入其他公有雲的元件 (ex. azure / gcp / \u0026hellip;)，可以在 registry.terraform.io 上面搜尋找到 通常比較大間的 Cloud provider 為了方便使用者，都會維護自家產品的 terraform module，方便使用這直接使用 IaC 的方式導入，可以直接使用自己家推出的 terraform module 因為 aws 最熟悉自己家的元件，比較不會出 bug 遇到問題可以直接發 issue 讓 aws 的工程師來看 如果 registry.terraform.io 上，某個元件找不到自己公司出的 terraform module，那可能就要找看看社群版本的 如果 registry.terraform.io 上搜尋不到，可以上 Github 搜尋 \u0026ldquo;terraform + 元件名稱\u0026rdquo; 可能找到沒有收錄在 registry 的 terraform module。或是直接 google，偶爾也會有意外收穫 沒有 terraform official validated module 就要特別注意安全性 一個好的 terraform module 通常有幾個特徵\n有良好的更新紀錄在 commit history 中 有活躍的社群 發 issue 討論 / PR / review 有完整的 changelog 紀錄變更如何時升版，也方便讓使用者升版時檢查 時常更新，能夠符合新版的 terraform core version 規格 會鎖定 dependency 版本，ex. terraform core version 與依賴的 provider version 反之，如果發現一個 terraform module 年久失修，很少人討論，也沒有跟上 terraform core 版本 (ex. 不支援 1.x) 可能就要考慮一下是否要使用 使用外部 module 應該注意的事情 有時候某些外部 module 會有雷，導致我們去引用時出現錯誤\n更改已經存在的 tag / 竄改 commit history / \u0026hellip; 等 或是 repository 移除變不見 / 改名 / 改 url 為了避免我們原本好好的 terraform code 受到影響，如果有使用外部 module，我們可以先 fork 一份外部 module 到 internal repository (clone 一份再 push 到自家的 cvs 上面），然後把 source url 改成 fork internal repository\n例如本來是 github.com/chechiachang/terragrunt-infrastructure-modules fork (clone \u0026amp; push) 變成 chechia.bitbucket.com/chechia/terragrunt-infrastructure-modules source url 中使用 git tag reference，明確指定 internal module (不用 github.com 而用 chechia.bitchecket) 需要更新時，在從 github 上 pull 最新的 code，push 到 bitbucket 上，修改 source url 就完成升版 這樣可以將對 external module 的依賴轉乘 loose coupling，降低耦合程度，如果 external module 整個壞掉，我們的 terraform module 使用不受影響。但又能 trace 原先的 external module remote，方便接收更新。 缺點是會花一些人力整理這些 forked module repositories 使用 external module 也要注意資訊安全\n要小心來路不明的 terraform module，terraform module 可以取得很多遠端的私密訊息，可能會有安群性的隱憂 沒有 terraform validated 的 terraform module，使用前自己要看過內容 看到使用不熟悉的 terraform provider，也應該去檢查 provider 的出處與內容 避免用自己的 aws account 身份執行後，發現被偷埋東西，或是資料洩漏出去 如果發現自己使用的元件比較冷門，符合自己需求的 external module 也年久失修，也可以考慮 hard fork 一版，由團隊自己維護\n可以參考現有的架構與內容，根據內部的需求去修改 terraform module，最後在自己引用 內部維護，自己控制升版與發佈 如果不是太複雜的 module 其實可以做 做完記得用安全性工具檢查，確保沒有安全隱憂（這個我們稍後幾天的內容會提） ","permalink":"https://chechia.net/posts/2022-09-14-14th-ithome-ironman-iac-aws-workshop-03-provision-aws-account/","summary":"\u003cp\u003e今天要使用 terraform 設定 AWS account structure\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e\u0026ndash;\u003c/p\u003e\n\u003cp\u003eProvision AWS Account\u003c/p\u003e\n\u003cp\u003e今天要把 aws account 設定完成，並且開始使用 live-repository 來搭建 aws 元件\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"我們自己的-module-repository\"\u003e我們自己的 module repository\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e開一個權限的 repository\u003c/li\u003e\n\u003cli\u003efork 我自幹的 modules repository terragrunt-infrastructure-modules \u003ca href=\"https://github.com/chechiachang/terragrunt-infrastructure-modules\"\u003ehttps://github.com/chechiachang/terragrunt-infrastructure-modules\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e上面這個是放 terraform module 的 repository，是兩個 repository\u003c/li\u003e\n\u003cli\u003e不要跟執行 terragrunt repository 搞混，是兩個 repository \u003ca href=\"https://github.com/chechiachang/terragrunt-infrastructure-live-example\"\u003ehttps://github.com/chechiachang/terragrunt-infrastructure-live-example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e今天的進度與 code base\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terragrunt-infrastructure-live-example/pull/1\"\u003eterraform-live-example PR 在此\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"\"\u003eterragrunt-infrastructure-modules 請直接看 tag v0.0.1\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e因為有地方嚴重寫錯，我有 force push 東西 :tear:，如果已經提早下載的朋友可能會需要清掉 repository main branch + tag 重拉 orz\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e對，我們且戰且走常常會這樣，發現昨天寫的東西寫錯臨時改，請大家見諒\u003c/p\u003e\n\u003ch3 id=\"為何使用-aws-modules\"\u003e為何使用 aws modules\u003c/h3\u003e\n\u003cp\u003eX) 因為我們沒有付錢，不能用 gruntwork 的 priviate repository (大誤\u003c/p\u003e","title":"Provision Child AWS Accounts"},{"content":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n\u0026ndash;\nAWS Account structure AWS account structure 是 AWS IAM 提供的 feature 之一，可以讓用戶從一個 account login，然後透過授權操作其他 account 的元件。\n然而有這個 feature 不代表用戶就要買單，有實際需求我們才來用 feature。使用 account structure 的理由是什麼？\nAccount structure 的需求 AWS 官方 whitepaper: 使用 multiple aws account 的好處\n依據職務需求將 workload 分組 為不同 environment 設定獨立的 security control 限制敏感資料的存取 提升創新與開發的敏捷 控制事故的影響程度與衝擊 impact 精細的拆分 IT 的工作 成本控管 分散 aws quotas 使用限額與 API rate limit AWS 官方的 organization best practice AWS 官方 OU 管理的 best practice\nGruntwork Doc 建議的 IaC best practice 中的 aws account structure，提到三個主要的目的\nIsolation (AKA compartmentalization) 不同環境，事故發生時，影響範圍不會太大 (ex. 工程師 terraform 誤刪 infra) 安全性，dev / stag 有安全性漏洞時，attacker 不會攻擊到 prod 環境 Authentication and authorization 把 user 都放在一起管理，透過更精細的 access control 去控制其他 account 內部的元件 Auditing and reporting 依據環境與用途拆分 account，讓監測與成本控制更精細 從實務上的經驗，有以下狀況就考慮要拆分 aws account\n如果 IT 部門很大人數很多，那將所有工作內容不同的 user 都放在同一個 aws account 下導致管理麻煩 大多數的 incident 都是工程師 change 造成的，拆分 dev / stag 的 change 曾經影響到 prod 不同團隊對於負責的元件範圍區分不清 工作掉球，該做的事沒人處理 工作重功，做了一樣的事情導致衝突 小故事：技術問題解決人為問題（？） iThome DevOpsDay 有一個聽眾聽完2022 DevOpsDay PaC for IaC 演講 後跑來找我，他問說： 講師你好，我們 terraform 常常誤刪東西，deploy dev / stag 結果 prod 壞掉（汗 如果不是技術的問題，是人的問題，不知道有沒有方法解？\n要治本還是要加強內部教育訓練，很基礎的重大錯誤是很難用技術擋的 但短期要治標，可以\n先區隔 dev stag 與 prod 的 terraform repo 跟 aws account 把 prod 權限鎖起來，先不要讓 junior 的成員去觸碰 所有 prod apply 都要兩到三個 senior 去 approve 也許你們的生活會快樂一點(好血淚)\nworkshop 我們會根據 Gruntwork Landing Zone Guide 的文字敘述來進行今天的 workshop\nCAUTION You must be a Gruntwork subscriber to access the Gruntwork Infrastructure as Code Library. 是的，這份 Gruntwork Landing Zone Guide 內部的範例有使用許多 gruntwork-io 的 private repository，這部分是需要付費。gruntwork subscription 一個月 $795 起跳，有興趣的朋友可以參考 Gruntwork subscription checkout\n我們這篇 workshop 並不使用 Gruntwork 的付費功能，所有 private repository 我們就會使用其他開源的 modules 來替代，例如使用 aws + terraform team 官方維護的 aws modules\n由於缺少部分 repository 的內容，這份 Gruntwork Landing Zone Guide 會常常看到很多 github external link 是 404 not found，表示是 private repository。看起來會有點不舒服，但沒關係我們就見招拆招自由發揮。\n建立 workspace repository repository 的內容可以直接參照 Gruntwork Guide 的內容\n本次 workshop 我們使用的 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example\nterragrunt 都會在這個 repository 裡面運作 各位可以直接 copy 來改 Root account root account 是整個 account structure 最上層的 account，創世 account。由於是創世 account，權限也是最大，因此會需要加上許多限制:\n使用高強度的密碼 使用密碼儲存器保管密碼，ex 1password / lastpass / \u0026hellip;，這些密碼儲存器都有自動加密，且有通過安全風險測試 不要存在電腦上 / email / google drive / 紙上 / 或是其他沒有經過安全驗證工具上 務必開啟 MFA 刪除 root account 的 aws access key，只允許透過 aws web console 存取 自己再也不要使用這個帳號，降低洩漏的風險 AWS 官方建議，只有這些工作需要使用 root account ，其他工作都不應該使用 root account Root Account IAM user 我們使用 root account 的最後一件工作，就是建立一個 IAM User 作為 admin，之後使用這個 IAM User 操作。文件與步驟在此\nName: Administrator 勾選: programmatic access AWS Management Console access Permission 頁面選擇 Attach existing policies to user directly 勾選 AdministratorAccess policy. 點下一步，直到產生出 IAM User 將登入資訊存在密碼儲存器 login url name password Access key ID Secret access key 接著也是要鎖住 root account 的 IAM User\n使用高強度的密碼 使用密碼儲存器保管密碼 務必開啟 MFA 為 Root account 設定 Security baseline 依照 gruntwork guide 接下來需要設定底下的元件\n建立所有的 child accounts 設定 AWS Organization IAM ROles IAM Users IAM Groups IAM Password Policies Amazon GuardDuty AWS CloudTrail AWS Config. CAUTION You must be a Gruntwork subscriber to access terraform-aws-service-catalog. 由於 terraform-aws-service-catalog 是 private repository，需要付費訂閱才能使用，所以我們無法取得裡面的內容\n為了避免這個 workshop 卡在這裡，我們就來自幹這些 module 吧ＸＤ\n直接使用其他的開源 modules 替代 依照 guide 把元件生出來 然後依照自己的需求去客製化 NOTE: Gruntwork 的 security module 有其 enterprise solution 的經驗在裡面，其實公司有預算還是相當值得參考的。只是我們 workshop 為了壁面大家破費，先當一回免費仔。\n我們自己的 module repository 開一個權限的 repository fork 我自幹的 modules repository terragrunt-infrastructure-modules 上面這個是放 terraform module 的 repository，是兩個 repository 不要跟執行 terragrunt repository 搞混，是兩個 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example 寫到這邊就 6000 字了，還沒開始碰 terragrunt\naws account structure 與 terragrunt directory structure 的概念非常重要，有好的基礎會讓後面的工作快樂很多 明天會繼續今天的工作，把 aws account / iam policy / permission 配好\n","permalink":"https://chechia.net/posts/2022-09-13-14th-ithome-ironman-iac-aws-workshop-02-aws-account-structure/","summary":"\u003cp\u003e今天要使用 terraform 設定 AWS account structure\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e\u0026ndash;\u003c/p\u003e\n\u003ch1 id=\"aws-account-structure\"\u003eAWS Account structure\u003c/h1\u003e\n\u003cp\u003eAWS account structure 是 AWS IAM 提供的 feature 之一，可以讓用戶從一個 account login，然後透過授權操作其他 account 的元件。\u003c/p\u003e\n\u003cp\u003e然而有這個 feature 不代表用戶就要買單，有實際需求我們才來用 feature。使用 account structure 的理由是什麼？\u003c/p\u003e\n\u003ch1 id=\"account-structure-的需求\"\u003eAccount structure 的需求\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/benefits-of-using-multiple-aws-accounts.html\"\u003eAWS 官方 whitepaper: 使用 multiple aws account 的好處\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e依據職務需求將 workload 分組\u003c/li\u003e\n\u003cli\u003e為不同 environment 設定獨立的 security control\u003c/li\u003e\n\u003cli\u003e限制敏感資料的存取\u003c/li\u003e\n\u003cli\u003e提升創新與開發的敏捷\u003c/li\u003e\n\u003cli\u003e控制事故的影響程度與衝擊 impact\u003c/li\u003e\n\u003cli\u003e精細的拆分 IT 的工作\u003c/li\u003e\n\u003cli\u003e成本控管\u003c/li\u003e\n\u003cli\u003e分散 aws quotas 使用限額與 API rate limit\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://aws.amazon.com/organizations/getting-started/best-practices/\"\u003eAWS 官方的 organization best practice\u003c/a\u003e\n\u003ca href=\"https://aws.amazon.com/blogs/mt/best-practices-for-organizational-units-with-aws-organizations/\"\u003eAWS 官方 OU 管理的 best practice\u003c/a\u003e\u003c/p\u003e","title":"AWS Account Struture"},{"content":"iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n\u0026ndash;\nAWS 為例，實現 production framework 的最佳實踐的 30 天 workshop\n準備 Production environmtn Devops 面臨的問題 DevOps 的工作內容繁雜\n今天想要 Deploy 一個 backend API deployment 到 k8s 上 一轉眼發現自己在修 port / servie / ingress / alb 一轉眼發現自己在修 tls certificate 一轉眼發現自己在修 node linux 上的一個 bug 一轉眼發現自己在修 monitoring / metrics + alert / log collection \u0026hellip; 我只是想 deploy 一個 backend deployment 修infra 花了好幾天，而寫 deployment 只要 2 hr Devops 的工作，是處理 developer 與 operator 中間的繁雜的細節 換個角度思考：上面這些元件全都是存在許久的功能，應該有一套統一介面隨叫即用這些服務，ex.\n今天想要 Deploy 一個 backend API deployment 到 k8s 上 承上，同值性高，coding 講求 DRY 原則 (Don\u0026rsquo;t Repeat Youself)，infra 管理上卻會有大量重複的工作\n開一個 Restful API deployment 開一個 GRPC deployment 開一個 Kafka message Queue / redis / RDS / \u0026hellip; 希望這些元件都有寫好的 unit 可以重複使用 但實務上還是會有細節需要調整 自動化\n標準化功能後，應該要可以自動部署 甚至是當 Backend API deployment 部署上去時，自動調用產生對應的元件 Backend API deployment 有變化時應該要跟據 deployment 需求去做 reconcile (k8s controller) 缺乏測試\nVM / alb / RDS / \u0026hellip; 等公有雲服務需要測試嗎 A: AWS / Google / Azure 都測過了，我們不用再測 然而 SRE 卻發現時常需要花時間 debug 這些元件 不是 functional issue，而是 configuration 問題，或是不同元件設定造成交互作用 B: 要上 production 的元件都要測試 使用 terraform provision 一台 EC2 後，在使用 terratest (Golang) 打 aws API 進行測試 標準化，自動化後，應該有大量的自動化測試，來確保每一個元件的 configuration 都是正確的，而且有信心維持穩定度 infra 時常改變\n技術元件時常改變：terraform 升級，aws 升級，k8s 升級，linux 升級與 patch infra 沒有射後不理這種事，都需要時常且穩定的更新 為了滿足上游服務(前後端/DB/\u0026hellip;)的需求 調整 VM spec / networking / routing / security rules / 不斷的改變耗費大量的人工時間 production ready 需要準備的\nproduction -\u0026gt; 真金白銀，公司的業務與名聲 production ready 應該是有明確的表準 True / False，而不是『我覺得現在是 production ready 了』 DevOps / SRE 都知道 production ready 的標準，只是沒有把他明確寫出來 production ready 應該就跟 unit test 一樣，一個 checklist 跑下去檢查，通過就是 production-ready，不通過就是 not-ready，且會說明不通過的原因 Gruntwork production ready checklist 人工介入工作太多\n新增元件需要人工處裡 定期的 patch \u0026amp; update / 安全性更新 / deploy 都需要人工介入的話就太浪費時間了 人工工作太多容易造成人為錯誤(human error) 多環境\nSRE 需要提供大量的環境 dev / qa / stag / prod \u0026hellip; 或是提早 scan 到更前面的工作流程，例如在 code base / reposiroty 中就先 scan Gruntwork solution 為何要使用 gruntwork framework\nWhy you need a framework\nGruntwork 提出兩個 solution\nReference Architecture Build your own architecture Prerequisites 由於本次分享不希望有多餘的費用，所以內容依據 gruntwork 公開網站與開源的 repository 內容講解使用\n不會涉及付費版的 terraform code library 內容與教材(subscription $795/month) aws 服務也盡量使用 free tier 中的額度 Prerequisites\n基本 Terraform 經驗，或是參考2021 年的鐵人賽系列文章: 30 天學 Terraform on Azure 基本 AWS 服務的觀念 都沒有也沒關係，會沿路在幫各位複習 30 天的目標 使用 terraform 設定所有 aws account 的 resource 參考 gruntwork-io 的開源 repository，設定 如果是測試或學習，可以參考 https://github.com/gruntwork-io/terraform-aws-service-catalog 學習目標\n熟悉 terraform 基本觀念 學會使用 terraform IaC 控制所有 aws service Repository NOTE: 如果是有購買 gruntwork subscription 的朋友，請使用私有的 IaC library repository，有更完整且經過測試的架構\n本課程使用 gruntwork 開源的 repository 架構\n請 copy 需要的部分（而不要 fork，因為這裡不需要過去的 commit history 與未來的 update)\n去年iThome 鐵人賽使用的範例 repository 我自己使用的 repo 今年 iThome 會使用的 repository 今天要做的事 請準備好以下 prerequisite\n設定 / 註冊 aws account 併取得 aws 12-month free-tier (預期是免費的預期 12-month free tier 即可包含所有內容) 然而成本控制也是課程的一環，各位要注意使用的資源，不要超過 aws free tier 太多，理論上也是最多幾美金的花費 安裝與設定 terraform tools terraform 1.2.9 terragrunt 0.38.9 使用舊版本的朋友請盡量使用 terraform \u0026gt;1.x 與 terragrunt \u0026gt;0.35.8 的版本 Aws account\n如果已經有 aws cloud service account 可以直接使用，本課程會使用全新的 aws account 使用 email 註冊 aws cloud service email 認證 信用卡登記 手機認證 Install terraform\nterraform 1.2.9 terragrunt 0.38.9 設定 Github repository 準備一個 github repository 來存放ㄏ的 terraform code\n可以使用我的範例 repository，也是 fork 底下 gruntwork 的 repository 稍作整理 或參考 gruntwork Github repository example git clone git@github.com:chechiachang/terragrunt-infrastructure-live-example.git 明天 帶著大家在 terraform code 中設定 aws account 與 IAM 權限\n參考資源 不熟悉 terraform 的朋友不妨參考去年的佳作 Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天，雖然是在 Azure 上 ，但許多基本概念都是相通的\n過去的鐵人賽文章\n13th 佳作 Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天 12th 佳作 Kubernetes X DevOps X 從零開始導入工具 X 需求分析＊從底層開始研究到懷疑人生的體悟＊ 11th 優勝 其實我真的沒想過只是把服務丟上 kubernetes 就有這麼多問題只好來參加30天分享那些年我怎麼在 kubernetes 上踩雷各項服務 kkk 推薦一下 Gruntwork 他們家的 blog: Gruntwork devops as a service\nQ\u0026amp;A 過去參賽都會有許多朋友提出問題，今年若有疑問可以集中在第一篇文章，我會跟大家一起討論。有想要看的題目也可以留言，我們看時間做安排。\nreferences https://docs.gruntwork.io/intro/overview/how-it-works ","permalink":"https://chechia.net/posts/2022-09-12-14th-ithome-ironman-iac-aws-workshop-01-introduction/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10290931\"\u003eiThome 鐵人賽好讀版\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e\u0026ndash;\u003c/p\u003e\n\u003cp\u003eAWS 為例，實現 production framework 的最佳實踐的 30 天 workshop\u003c/p\u003e\n\u003ch1 id=\"準備-production-environmtn-devops-面臨的問題\"\u003e準備 Production environmtn Devops 面臨的問題\u003c/h1\u003e\n\u003cp\u003eDevOps 的工作內容繁雜\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e今天想要 Deploy 一個 backend API deployment 到 k8s 上\u003c/li\u003e\n\u003cli\u003e一轉眼發現自己在修 port / servie / ingress / alb\u003c/li\u003e\n\u003cli\u003e一轉眼發現自己在修 tls certificate\u003c/li\u003e\n\u003cli\u003e一轉眼發現自己在修 node linux 上的一個 bug\u003c/li\u003e\n\u003cli\u003e一轉眼發現自己在修 monitoring / metrics + alert / log collection\n\u0026hellip;\u003c/li\u003e\n\u003cli\u003e我只是想 deploy 一個 backend deployment\n\u003cul\u003e\n\u003cli\u003e修infra 花了好幾天，而寫 deployment 只要 2 hr\u003c/li\u003e\n\u003cli\u003eDevops 的工作，是處理 developer 與 operator 中間的繁雜的細節\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e換個角度思考：上面這些元件全都是存在許久的功能，應該有一套統一介面隨叫即用這些服務，ex.\u003c/p\u003e","title":"IaC best practice workshop on aws"},{"content":"Titleq 從零導入 Policy as Code 到 terraform 甘苦談 - Introducing policy as code for terraform\nPresentation Google Slides\nDescription Terraform 是一個很棒的 Infrastructure as Code 的工具，能夠以現代的軟體工程流程來穩定的建構 infrastructure，並隨著需求變更自動化迭代，將新的 feature 安全地更新到既有的 infrastructure 上。只要是 Infrastructure 有關的問題，我一律推薦 Terraform。\n這也意味 Terraform 的品質就等於 infrastructure 的品質，好的 terraform 帶你上天堂，不好的 terraform 全 team 火葬場。\nInfrastructure 有太多資安的考量，新的風險不斷被檢查出來，連帶要不斷的 patch terraform code 來避免潛在的資安風險 Infrastructure 會不斷地更新，例如公有雲的 api 不斷更新，去年的 code 已經跟不上今年的最佳實踐了 好的工程師寫出的 terraform code 屌打菜鳥工程師，要如何讓團隊依循更好的實踐，避免寫出爛 code 維護 code 有 clean code / best practice，Terraform 也有 clean code 與 best practice，工程師要如何工具來輔助，是 Policy as Code 的一大課題。 本演講聚焦於實際經驗，從一大堆 terraform modules 開始，沒有 Policy as Code，到一步步評估、導入、實作、改進與迭代，逐漸的提升團隊 Terraform code 品質，提升 infrastructure 交付品質，並避免到未來潛在的風險。\n本次演講的 Terraform 版本為從 0.12, 0.13, 一路推進 1.0+。\nContent 從零導入 Policy as Code 到 terraform 甘苦談\n簡介 Terraform 與 Policy as Code 當你手上有數不完的 terraform modules，管理 terraform 品質是個大問題 如何管理 Terraform 自動化: 改變從 Jenkins pipeline 開始，gitflow 改進，pre-commit hook Policy as Code 校驗 使用工具量化 terraform code 品質 完整導入 Policy as Code 小結: 玩 infra 必用 terraform，玩 terraform 必做 policy as code About me Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit 2019 Ithome Cloud Summit 2020 Ithome Cloud Summit 2020/12/18\tCloud Native Taiwan 年末聚會 2020/8/17\tDevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2021 Ithome Cloud Summit 2022 Ithome Cloud Summit 2022 COSCUP\nSuggested Audience 推薦有 Terraform 使用經驗，特別是公有雲經驗，對 Policy as Code 有興趣的人 想要寫出 Terraform clean code 的人 ","permalink":"https://chechia.net/posts/2022-06-09-ithome-devopsday-2022-proposal-introduce-policy-as-code-for-terraform-/","summary":"\u003ch3 id=\"titleq\"\u003eTitleq\u003c/h3\u003e\n\u003cp\u003e從零導入 Policy as Code 到 terraform 甘苦談 - Introducing policy as code for terraform\u003c/p\u003e\n\u003ch3 id=\"presentation\"\u003ePresentation\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://docs.google.com/presentation/d/1yawazO1B_sP5Yiav-XLGJXW3ZS2JTV0wGuJwhrUKQ3A/edit?usp=sharing\"\u003eGoogle Slides\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eTerraform 是一個很棒的 Infrastructure as Code 的工具，能夠以現代的軟體工程流程來穩定的建構 infrastructure，並隨著需求變更自動化迭代，將新的 feature 安全地更新到既有的 infrastructure 上。只要是 Infrastructure 有關的問題，我一律推薦 Terraform。\u003c/p\u003e\n\u003cp\u003e這也意味 Terraform 的品質就等於 infrastructure 的品質，好的 terraform 帶你上天堂，不好的 terraform 全 team 火葬場。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInfrastructure 有太多資安的考量，新的風險不斷被檢查出來，連帶要不斷的 patch terraform code 來避免潛在的資安風險\u003c/li\u003e\n\u003cli\u003eInfrastructure 會不斷地更新，例如公有雲的 api 不斷更新，去年的 code 已經跟不上今年的最佳實踐了\u003c/li\u003e\n\u003cli\u003e好的工程師寫出的 terraform code 屌打菜鳥工程師，要如何讓團隊依循更好的實踐，避免寫出爛 code\n維護 code 有 clean code / best practice，Terraform 也有 clean code 與 best practice，工程師要如何工具來輔助，是 Policy as Code 的一大課題。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e本演講聚焦於實際經驗，從一大堆 terraform modules 開始，沒有 Policy as Code，到一步步評估、導入、實作、改進與迭代，逐漸的提升團隊 Terraform code 品質，提升 infrastructure 交付品質，並避免到未來潛在的風險。\u003c/p\u003e","title":"IThome 2022 DevOpsDay Introducing policy as code for terraform"},{"content":"Titleq 在 k8s 上跑 time series database 甘苦談 - Operating Time Series Database in K8s\nGoogle Slides Youtube live record: https://youtu.be/YexUnVOZC8M?t=9421\nDescription Influxdb 為市占最高的 time series DBMS 之一，使用上與 RDBMS 有不同優劣勢。\n在維運方面，database 有許多相似需求：穩定性、高可用性、備份、還原、資源管理、調度、災難復原\u0026hellip;等。社群常聽到有人問：可不可以在 K8s 上跑 database。\n本演講會分享在 k8s 中維運，實務上所遇到的問題，提供一些思考方向。\n本次演講的 influxdb 版本為 Influxdb OSS / enterprise 1.9+\nInfluxDB is one the the most popular time series database management system (DBMS) and is a powerful platform when dealing with time series data.\nDBMSs, including RDBMS, have many similar requirements on aspect of infrastructure operation. They all require stability, high availability, backup and restore utilities, cpu / memory resource management, disaster recovery\u0026hellip;etc. People often ask about that whether is it ok to run DBMS in kubernetes cluster or not. This lecture try to provide some points from our experiences dealing with real-world issues.\nThe version of InfluxDB discussed in the lecture is InfluxDB OSS / Enterprise 1.9+\nContent 在 k8s 上跑 time series database 甘苦談\n簡介 Influxdb，time series 與 RDBMS 的差異 使用 time series 的幾個情境: 在 k8s 上跑 DB 穩定性 Availability、HA (enterprise)、faillure recovery 資源管理 OOMKilled、cpu throttling、OutOfDisk DB management: data migration、backup \u0026amp; restore、data retention 小結: 你該不該用 cloud service / VM / 在 K8s 上跑 database Operating Time Series Database in K8s\nA brief introduction about InfluxDB and the differences with RDBMS Some scenario to use a time series database (other than RDBMS) Operate InfluxDB in K8s Stability, availability, disaster recovery Resource management, OOMKilled, cpu throttling, out of disk DB management: data migration, retention, rotation, backup \u0026amp; restore summary: whether or not to run a database in k8s About me Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit 2019 Ithome Cloud Summit 2020 Ithome Cloud Summit 2020/12/18\tCloud Native Taiwan 年末聚會 2020/8/17\tDevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2021 Ithome Cloud Summit\nSuggested Audience 推薦有 k8s 使用經驗的從業人員，對 k8s 有上手經驗 問可不可以在 k8s 上面跑 database 的人 ","permalink":"https://chechia.net/posts/2022-05-21-coscup-proposal-operating-time-series-database-in-k8s/","summary":"\u003ch3 id=\"titleq\"\u003eTitleq\u003c/h3\u003e\n\u003cp\u003e在 k8s 上跑 time series database 甘苦談 - Operating Time Series Database in K8s\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://docs.google.com/presentation/d/1PZryGImYRALMJfbEMuf_jPd2P-qzIwDmVd5DgXSuqq0/edit\"\u003eGoogle Slides\u003c/a\u003e\nYoutube live record: \u003ca href=\"https://youtu.be/YexUnVOZC8M?t=9421\"\u003ehttps://youtu.be/YexUnVOZC8M?t=9421\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"description\"\u003eDescription\u003c/h3\u003e\n\u003cp\u003eInfluxdb 為市占最高的 time series DBMS 之一，使用上與 RDBMS 有不同優劣勢。\u003c/p\u003e\n\u003cp\u003e在維運方面，database 有許多相似需求：穩定性、高可用性、備份、還原、資源管理、調度、災難復原\u0026hellip;等。社群常聽到有人問：可不可以在 K8s 上跑 database。\u003c/p\u003e\n\u003cp\u003e本演講會分享在 k8s 中維運，實務上所遇到的問題，提供一些思考方向。\u003c/p\u003e\n\u003cp\u003e本次演講的 influxdb 版本為 Influxdb OSS / enterprise 1.9+\u003c/p\u003e\n\u003cp\u003eInfluxDB is one the the most popular time series database management system (DBMS) and is a powerful platform when dealing with time series data.\u003c/p\u003e\n\u003cp\u003eDBMSs, including RDBMS, have many similar requirements on aspect of infrastructure operation. They all require stability, high availability, backup and restore utilities, cpu / memory resource management, disaster recovery\u0026hellip;etc. People often ask about that whether is it ok to run DBMS in kubernetes cluster or not. This lecture try to provide some points from our experiences dealing with real-world issues.\u003c/p\u003e","title":"2022 05 21 COSCUP Operating Time Series Database in K8s"},{"content":"各位好\n關於這個 QRcode\n每次上台前，我都會想要帶什麼樣的內容給觀眾，讓觀眾值得花 30 分鐘在底下聽。後來就習慣先發表一篇文章，把對觀眾有幫助的資源包成一包：\n首先是完整投影片：https://slides.com/chechiac\u0026hellip;/terraform-introduction-a56697 逐字講稿：(最後校稿中） 然而只有本次演講內容，回去可能還是不太容易操作。所以這次附上使用 Terraform 一鍵部署 vault 的 Github Repository：https://github.com/\u0026hellip;/southe\u0026hellip;/chechia_net/vault/singleton 如果不熟 Terraform，再附上 30 天手把手 Terraform 教學文章，只要願意花時間，全篇中文一個月帶你上手 Terraform。 IThome 鐵人賽好讀版：https://ithelp.ithome.com.tw/users/20120327/ironman/4057 Github Repository 完整版：https://github.com/\u0026hellip;/terraform-30\u0026hellip;/tree/main/lecture/zh 如果遇到問題還是需要找人發問，所以再推薦兩個社群，可以來這邊發問，要找我本人也找得到。甚至只是加入潛水不講話，都可以被動吸收許多新知。 Cloud Native Taiwan User Group https://t.me/cntug https://fb.cloudnative.tw DevOps Taiwan Meetup Group https://t.me/devopstw 大家可以手機拍完這個就出去吃午餐了。 或是拍回家，然後傳給一個同事叫他花 30 天把 Terraform 跟 Vault 這些都學會。\n總之希望對各位有幫助，讓國內技術力能持續進步成長。\n回到本次演講。\n本次演講有三個關鍵字\nKubernetes Vault Terraform 這個是隱藏的 請問平時工作會用到這三個技術之一的朋友，請舉個手，好讓我知道一下觀眾的分布，等等分享的內容會照比例做一些調整。\n我們今天不會講太多 Kubernetes 的內容，重點放在 Vault，以及如何設定 Vault，所以 Terraform Infrastructure as Code 或是 configuration as Code 會在這邊跑出來。\n關於我，我是哲嘉。我在 Maicoin 當 SRE。常出現的社群是 CNTUG 與 DevOpsTW。\n我們今天談的更大的主題其實是 Key Management 私鑰管理，或是密碼管理。這是一個很大的題目，今天演講內容只是其中的一個實作案例。\n舉個例子，一邊是 API Server，另一邊 Database，或是第三方服務\nDatabase 來 Authenticate 合法的 Client 用戶端，可能是 username + password，或是 API Key + Secret，或是 Access Token，或是 Private Key，Client Certificate，都可以。\n在跨不同平台或是介面的服務，我們常用的 Auth 方法。認 Key 不認人。\n那中間這些 Key 要怎麼管理，就有很多學問 其中最基本的，是怎麼配置給 API Server 讓他使用 注意：讓 API Server 使用，隱含的意思是，其他人不管是其他微服務，或是開發工程師，閒雜人等都不能看到。\n這邊我們假設 API Server 是在 Kubernetes 裡面跑，微服務架構，所以 API Server 可能是一個 Pod，我們 SRE 要為這組 Pod 配置密碼。\n這邊列出的應該是 K8s 比較常見的幾種做法。\nplain text，直接寫進 file 讓 Pod 去讀取 k8s secret 做 base64 encode 安全一點的透過外部機制作加密解密 或是寫在 API Server 的 memory 中 事實上如果沒有使用 K8s，使用 VM 或是公有雲 Container Service，應該都是類似的原理，大家都是 Linux base，secret 放進來看要放在 disk 或是 memory 裏面。\n實際放到 K8s 大概會長長這些 yaml\n最簡單，就直接把 secret 壓到 Pod env 裏面，\n最簡單也最危險\n所有看得到 yaml 的人都明碼看見密碼 所有能進到 file system 提外話\nAPI Server 被從正面打穿，滲透到拿到這組密碼的機會多高？\n如果 API Server golang library 有在跟安全性更新 Kubernetes 用公有雲的 Kubernetes Service，有在更新 OS ami 跟 docker image 都有在更新 然後有功能正常的防火牆 打掛蠻有可能的，但打穿服務到拿到密碼，是有難度的。\n更多時候，至少在幣圈有被爆出來的資安事件，大多是是公司員工被釣魚信掉到，被植入惡意軟體在 local 電腦，然後他又看得到明碼的密碼，直接爆炸。\n明碼糟糕的地方，大家都看得到，一開始就是 exposed 的狀態，風險不可控，也無從管制。\n然後是 K8s secret，也是從 env 掛進去 Pod\n放在 k8s secret，是 base64 的格式\n看起來跟原先內容不一樣了，有人就跟我說，他們家的 k8s secret 有用 base64 加密。\nencode 跟 encrypt\nk8s secret 有什麼問題\n明碼 plain text 的問題，不該看到的人很容易就看到 根本的問題還是 RBAC 懶得設定，大家都用 default role 進來 要用可以，先看 k8s secret 後面的儲存實作是什麼？\n是 k8s etcd 透過 k8s API server 存取 etcd 跟 kube-api 一般來說是夠安全的。官方文件還建議\nsecret 要加密 encrypt 增強 RBAC 控制，只有特定 role 才看得到 secret，，而不是每個人都用 default role 近來 k8s，然後進到 namespace 全部 secret 就看光光 RBAC 有設定好，有加密，是可以做到安全。當然還是沒有專門 key Management 工具，如 Vault 有額外管理上的優化功能。\n加密範例可以看強者我朋友的文章\n加密完可能長這樣，還要額外透過其他機制才能進行解密，拿到原始資料\n例如透過 k8s controller 解密 或是透過 vault server 或是透過公有雲的 Key Management Service 做解密 其他的 k8s secret 加密解決方案\n今天我們使用 Vault，其中一個目的就是要坐中間這段 Safe Magic\n給這個 Pod secret 然後只給這個 Pod Secret 當然 Key Management 其他還有一堆事情要處理\n密碼洩漏的話有沒有 revoke 機制\n能不能定時 Rotate 汰換密碼？\n改架構，底下的設定好不好耕著動態調整，還是要跟著 rename / mv k8s secret\n怎麼做稽核，怎麼檢查內容。我看不到 vault 幫我看一下內容對不對，這個很容易發生。\n臆想頭就很痛\n今天的目的很單純\n密碼的露出盡量小 最好密碼是有期限的，逾期自動失效 暴露了可以 revoke Vault\n有人用過？這邊簡介一下\n在 CNCF 的 landscape 漱渝 Key Management，應該是裡面市佔最高的開源項目\n重點就是保護與管理 secret\nVault 的核心概念，這個影片是 Hashicorp 官方介紹的影片，講得很好，大家自己回去看一下\n這邊簡單 vault 101\n現在有一台 vault server 已經設定好了，我們可以使用 vault client 連線\n例如這邊\n使用 VAULT TOKEN 告訴 vault 你是什麼身份的用戶 或是進行 login，Vault 會呸發一組臨時的 TOKEN 給你 拿著組 TOKEN 去問 Vault，請問我可以要 /user/mysql 這個路徑下的資料嗎？\nVault 檢查 TOKEN 的 role 與 permission，可以就回傳值 不行就 permission denied\nVault 就是金庫，真正重要的 key 存在裡面，使用這要來問 Vault，要先過 Vault 這關\nVault 實際存放 Secret Engine，這邊也跳過，大家先把他當 key / value 存放好了 XD\n等等，這樣 vault Auth 有點怪\n本來拿 username / password 去控制 Mysql 先在多一步，先拿到 VAULT TOKEN，再拿 TOKEN 去跟 Vault 拿 Mysql password，再去連 MySQL\n咦這樣不是很怪？我如果 Vault token 暴露了，有心人士還是可以從 vault 拿到資料啊\n這樣有比較安全嗎？還是只是花式被駭\n對，只是做 token 交換的話，還是不夠，所以 Vault 有提供許多 Auth method\ntoken 只是其中一種 有很多認證方法不用 token 交換，但也能讓 vault 認得 k8s pod 與 api server 這是今天的重點之一，token / 密碼傳遞不安全，那就用其他手段 auth\n以 AWS IAM auth 為例\n如果今天是在 aws ec2 上跑，那可以透過 aws internal api 去取得身份認證資料，也就是 ec2 instance metadata\n拿這個 metadata 去問 vault，vault 再透過 aws api 去確認，這個 ec2 instance 真的是合法的\n然後依據 ec2 instance 身份，配發權限跟資料\napi server ec2 就給 api server secret frontend server ec2 就給 frontend secret 中間沒有多餘的密碼 / token 交換\n來往的對話大概是這樣\nAWS API 是可信的 Vault 自己維護是可信的 服務透過三方交握去認證，認 runtime 環境的 metadata，不再是認 key 不認人 至於 api server ec2 近來 vault 後，應該有什麼樣的權限，在 vault 內部透過 policy 配置\n設定 path 設定允許的 operation 例如 read write list delete \u0026hellip; 等 runtime 動態調整\n當然 Vault 還有提供很多更加安全的功能\n例如如何安全地存放 secret Storage 這邊跳過\nSecret + auth 搭配跳過\n以及 dynamic secret，來解認 key 不認人的問題\n例如 mysql 的靜態 username / password，透過 vault 設定可以變成動態的\n回到 k8s，使用 token auth，然後把 VAULT TOKEN 放在 k8s secret 裏面，只有比較安全一點點\n就是 VAULT TOKEN 可以快速 rotate 跟 revoke\n這邊可以搭配其他 auth 方式來解決\n剛剛不是讓 vault 去認 aws ec2？\n在 k8s 中，可以讓 vault 去認 k8s cluster / service account\n路徑圖長這樣\npod runtime 都會有 service account 使用 service account 的 metadata 去問 vault vault 去問 k8s，這個 service account 是真的假的 k8s 回答 vault，Pod 跟service account 是合法的 vault 再配權限給 Pod auth detail 的文字描述，跟上面講的一樣\n配合 k8s sidecar，可以把 vault 拿到的 key 寫到 memory mount 裏面\nPod init 時 init container 才去 invoke vault API key 不會透過 k8s api 傳遞，也不會在 etcd 內出現 main container 透過 memory mount 存取 key key 的 lifecycle 跟 pod 一樣， pod delete 掉這組 in-memory key 也自動清除 好處\n更少 expose，沒有過 kube-api 與 etcd in-memory vault 跟 pod 之間的 vault token 的 time-to-live 期限可以很短，幾十秒內，init 完即可拋棄的 token 壞處（？）\n每個 pod 起來會去打 vault api 實務經驗上 loading 很低 而且 vault server 可以做 HA 跟 horizontal scaling 最大的成本，其實是 vault 設定 讓 vault 認 k8s 根據 service account 去配權限 其他花俏功能都需要額外的設定 系統複雜，保證配到你頭昏眼花 想像一下，這張圖里的\npolicy 跟為服務數量呈正比 secret path 也耕服務數量呈正比 auth k8s 數量也跟服務數量呈正相關 用越久，內容隨時間增加 改架構時更刺激 建議使用 vault 務必 搭配 Terraform 管理\ninfrastructure as code configuration as code policy as code Terraform 非常適合管理複雜，但有常常需要細部調整的設定資料\n其他功能，跳過\n其他功能\nsecret debug 超麻煩\n內容正確性，valid key 還是 invalid key，還是根本是另一隻 key 權限正確性，是不是這個 username / password 的權限是正確的 跨團隊溝通更頭痛，不是可以貼在 slack 大家一起幫看的東西 terraform 可以幫助 vault 設定 debug\n至少跨環境復現問題很方邊 Vault 有 HA\nVault Performance 通常不是問題\n雖然 loading 隨 micro service 線性增加 但本身可以是無狀態 server，可以 horizontal scale 注意一下 auto-retry\npod fail restart 又再 init 一次，如果一萬個 pod 一直 retry 可能真的會把 vault 打爆 backup limit 要注意 使用 init container 的話，是 cache 在 Pod 層級，container fail restart 不會重新打 vault api 安全性\nvault 有專業資安團隊把關 不要怕 vault 被打穿，而是要怕同事被釣魚 結論\n要不要 vault\nqrcode 最後機會\n有問題可以來社群找我\nMaicoin 是間好公司 2014 服務上線到現在，在台灣已經邁入第八年。顧客數一直增加，我們也持續緩慢擴編。\n覺得自己有能力，歡迎來挑戰，等等私下找我聊\n謝謝\nQ \u0026amp; A\nTerraform vs Pulumi\n操作語言差異，有好有壞 vault 跟 terraform 的 hcl 是增強版 json，本質還是 json，有興趣可以去看我的文章 terraform 目前是站跟星星還是領先，未來繼續看 Kubernetes Service account token 要不要更新\n要，應該定期更新，官方文件有操作步驟 Vault / Terraform 實務上的工作負擔會很花時間跟人力嗎\n學習曲線比較奇怪，要花一點時間做中學 學熟了之後就很好改，效率很高 至少是屌打用 gui 改或是 client 直接下 cmd 拉 ","permalink":"https://chechia.net/posts/2021-11-16-ithome-cloud-summit-vault/","summary":"\u003cp\u003e各位好\u003c/p\u003e\n\u003cp\u003e關於這個 QRcode\u003c/p\u003e\n\u003cp\u003e每次上台前，我都會想要帶什麼樣的內容給觀眾，讓觀眾值得花 30 分鐘在底下聽。後來就習慣先發表一篇文章，把對觀眾有幫助的資源包成一包：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e首先是完整投影片：https://slides.com/chechiac\u0026hellip;/terraform-introduction-a56697\u003c/li\u003e\n\u003cli\u003e逐字講稿：(最後校稿中）\u003c/li\u003e\n\u003cli\u003e然而只有本次演講內容，回去可能還是不太容易操作。所以這次附上使用 Terraform 一鍵部署 vault 的 Github Repository：https://github.com/\u0026hellip;/southe\u0026hellip;/chechia_net/vault/singleton\u003c/li\u003e\n\u003cli\u003e如果不熟 Terraform，再附上 30 天手把手 Terraform 教學文章，只要願意花時間，全篇中文一個月帶你上手 Terraform。\n\u003cul\u003e\n\u003cli\u003eIThome 鐵人賽好讀版：https://ithelp.ithome.com.tw/users/20120327/ironman/4057\u003c/li\u003e\n\u003cli\u003eGithub Repository 完整版：https://github.com/\u0026hellip;/terraform-30\u0026hellip;/tree/main/lecture/zh\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e如果遇到問題還是需要找人發問，所以再推薦兩個社群，可以來這邊發問，要找我本人也找得到。甚至只是加入潛水不講話，都可以被動吸收許多新知。\n\u003cul\u003e\n\u003cli\u003eCloud Native Taiwan User Group\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://t.me/cntug\"\u003ehttps://t.me/cntug\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://fb.cloudnative.tw\"\u003ehttps://fb.cloudnative.tw\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDevOps Taiwan Meetup Group\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://t.me/devopstw\"\u003ehttps://t.me/devopstw\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e大家可以手機拍完這個就出去吃午餐了。\n或是拍回家，然後傳給一個同事叫他花 30 天把 Terraform 跟 Vault 這些都學會。\u003c/p\u003e\n\u003cp\u003e總之希望對各位有幫助，讓國內技術力能持續進步成長。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e回到本次演講。\u003c/p\u003e\n\u003cp\u003e本次演講有三個關鍵字\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eKubernetes\u003c/li\u003e\n\u003cli\u003eVault\u003c/li\u003e\n\u003cli\u003eTerraform 這個是隱藏的\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e請問平時工作會用到這三個技術之一的朋友，請舉個手，好讓我知道一下觀眾的分布，等等分享的內容會照比例做一些調整。\u003c/p\u003e\n\u003cp\u003e我們今天不會講太多 Kubernetes 的內容，重點放在 Vault，以及如何設定 Vault，所以 Terraform Infrastructure as Code 或是 configuration as Code 會在這邊跑出來。\u003c/p\u003e","title":"2021 11 16 Ithome Cloud Summit Vault"},{"content":"2021 ithome 鐵人賽開跑了～\n今年的題目是「Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天」，是一個長達 30 天的 terraform workshop，對 terraform 有興趣的朋友歡迎追縱\nDay 01 - 引言：Terraform 是個好東西。\n課程內容與代碼會放在 Github 上: https://github.com/chechiachang/terraform-30-days\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n","permalink":"https://chechia.net/posts/2021-09-01-ithome-ironman-go/","summary":"\u003cp\u003e2021 ithome 鐵人賽開跑了～\u003c/p\u003e\n\u003cp\u003e今年的題目是「Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天」，是一個長達 30 天的 terraform workshop，對 terraform 有興趣的朋友歡迎追縱\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/articles/10258904\"\u003eDay 01 - 引言：Terraform 是個好東西。\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/chechiachang/terraform-30-days\"\u003e課程內容與代碼會放在 Github 上: https://github.com/chechiachang/terraform-30-days\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://chechia.net/\"\u003e賽後文章會整理放到個人的部落格上 http://chechia.net/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003e追蹤粉專可以收到文章的主動推播\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\" loading=\"lazy\" src=\"https://ithelp.ithome.com.tw/upload/images/20210901/20120327NvpHVr2QC0.jpg\"\u003e\u003c/p\u003e","title":"2021 09 01 Ithome Ironman Go"},{"content":"Event https://devops.kktix.cc/events/meetup33-maicoin-devops-taiwan https://youtu.be/8hEifTAWnY0?t=3269 audience: 90/90 resources this presentation\nhttps://slides.com/chechiachang/terraform-introduction-a56697 github\nhttps://github.com/chechiachang/terraform-azure https://github.com/chechiachang/vault-playground/tree/master/deploy/v0.8.0 ","permalink":"https://chechia.net/posts/2021-05-13-devops-taiwan-33-hashicorp-vault-for-kubernetes-apps/","summary":"\u003ch1 id=\"event\"\u003eEvent\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://devops.kktix.cc/events/meetup33-maicoin-devops-taiwan\"\u003ehttps://devops.kktix.cc/events/meetup33-maicoin-devops-taiwan\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/8hEifTAWnY0?t=3269\"\u003ehttps://youtu.be/8hEifTAWnY0?t=3269\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eaudience: 90/90\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"resources\"\u003eresources\u003c/h1\u003e\n\u003cp\u003ethis presentation\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction-a56697\"\u003ehttps://slides.com/chechiachang/terraform-introduction-a56697\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003egithub\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-azure\"\u003ehttps://github.com/chechiachang/terraform-azure\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/vault-playground/tree/master/deploy/v0.8.0\"\u003ehttps://github.com/chechiachang/vault-playground/tree/master/deploy/v0.8.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"DevOps Taiwan Meetup #33: Hashicorp Vault for Kubernetes"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n需求與問題 隨著 terraform 在團隊內的規模持續成長，團隊需要讓工作流程更加順暢，來面對大量的 tf 變更查核與變更請求。想像幾十個工程師同時在修改幾十個不同的 terraform projects / modules，這時可能會有幾個問題\n需要一個穩定乾淨的環境執行 terraform 工程師的開發本機不是個好選擇 需要 24/7 的 terraform 執行中心 執行中心會有各個環境 (dev / stage / prod) 的存取權限，希望設置在內部 下列兩個工作會切換工作平台，例如 Github review，檢視 difference，與討論 PR review 完有時會忘記 merge，merge 完有時會忘記 apply repository 越多，忘得越多\u0026hellip; 團隊已經導入 Git-flow，希望把工作流程做得更完整自動化更加便利\nAtlantis 解決方案是與版本控制整合，提供 terraform 執行的 worker，病可以與 Git Host (e.g. Github) 整合做 PR + Review，來達成持續的 CI/CD 遠端執行，自動 plan 與自動 apply merge。也就是 Atlantis 幫我們處理以下幾件事\nGit-flow TODO 自動化 plan TODO Webhook 回傳 plan 結果 TODO 透過 bot 控制 apply TODO 自動 merge 需求與工作流程 以下的範例使用\nGithub 作為版本控管工具 是整個工作流程的控制中心 tf code，review，plan，apply 的結果都會在 Github 進一步整合到 Slack 上也非常方便 Atlantis 放在 Kubernetes 上 使用 helm chart 部署 如果需要其他的版本控制工具或是安裝方式，請見Atlantis 官方文件，Atlantis 支援非常多版本控制工具，例如 Github，Gitlab，Bitbucket，\u0026hellip;等\n整個工作流程\nGithub commit push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Github PR push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Github Merge event / main branch push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Atlantis acquire access to site (dev / stage) Atlantis apply to site Github release tag push (eg. 1.2.0-rc) Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Atlantis acquire access to site (dev/stage) Atlantis apply to site (release-candidate / prod) 依據上面的環境，安裝需要準備以下幾個東西\nGit Git Host 上設定 Atlantis 存取 Git Repository 的權限 為 Git Host 設定存取私鑰，讓 Atlantis 認證 webhook Terraform Backend State storage: Atlantis 需要存取外部的 state storage Terraform 使用的版本要注意一下。Atlantis 可以支援不同 repository / project 使用不同版本 Environment credential (provider credential) Atlantis 會需要存取不同的環境 (dev / stage / prod) 為這些環境獨立配置 credential 讓 Atlantis 存取 實際 Atlantis 的環境會有\nDocker 作為孤城師本機開發測試使用 Kubernetes Atlantis 我們的團隊會是一個環境一個獨立的 Atlantis 安全性：切分各個環境的權限與 Access 各個 Atlantis webhook 只接收屬於自己的 event Docker 使用 Docker 作為本地開發與測試的容器 runtime：\ndocker pull runatlantis/atlantis:v0.15.0 docker run runatlantis/atlantis:v0.15.0 atlantis \\ --gh-user=GITHUB_USERNAME \\ --gh-token=GITHUB_TOKEN Helm Chart Helm 的安裝十分簡易\nhelm repo add runatlantis https://runatlantis.github.io/helm-charts helm inspect values runatlantis/atlantis \u0026gt; values.yaml 更改 values.yaml\ngithub: user: chechiachang token: ... secret: ... orgWhitelist: github.com/chechiachang/* 安裝到 Kubernetes 上\nhelm install atlantis runatlantis/atlantis -f values.yaml 結果 工程師透過 Github 就可以完成所有工作 加上 Github Slack 整合，就完全不用離開 Slack，跟 bot 聊天就可以完成所有 terraform 工作 安全穩定的 terraform 執行環境 獨立的 in-cluster credential，集群的存取權限都控制在集群內，不會暴露到外部環境 自動 plan 與 apply，不會遺忘 優劣 優點\n可以 self-hosted，credential 不外洩，確保最高的安全性 已經整合 Kubernetes, helm chart webhook 整合許多版本控制庫，例如 github, gitlab,\u0026hellip; 實現安全的遠端執行 本地執行還是會有諸多問題 terraform cloud 提供的遠端執行 缺點，其實沒什麼缺點\n就需要多養一組 Atlantis 使用 stateless deployment，其實是應該不會有什麼負擔 Terraform 系列至此應該是全部完結，感謝各位\n","permalink":"https://chechia.net/posts/2020-10-07-terraform-infrastructure-as-code-atlantis/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"需求與問題\"\u003e需求與問題\u003c/h1\u003e\n\u003cp\u003e隨著 terraform 在團隊內的規模持續成長，團隊需要讓工作流程更加順暢，來面對大量的 tf 變更查核與變更請求。想像幾十個工程師同時在修改幾十個不同的 terraform projects / modules，這時可能會有幾個問題\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e需要一個穩定乾淨的環境執行 terraform\n\u003cul\u003e\n\u003cli\u003e工程師的開發本機不是個好選擇\u003c/li\u003e\n\u003cli\u003e需要 24/7 的 terraform 執行中心\n\u003cul\u003e\n\u003cli\u003e執行中心會有各個環境 (dev / stage / prod) 的存取權限，希望設置在內部\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e下列兩個工作會切換工作平台，例如 Github\n\u003cul\u003e\n\u003cli\u003ereview，檢視 difference，與討論\u003c/li\u003e\n\u003cli\u003ePR\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ereview 完有時會忘記 merge，merge 完有時會忘記 apply\n\u003cul\u003e\n\u003cli\u003erepository 越多，忘得越多\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e團隊已經導入 Git-flow，希望把工作流程做得更完整自動化更加便利\u003c/p\u003e","title":"Terraform Infrastructure as Code: Atlantis"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt 問題與需求 當團隊已經開始大規模使用 terraform，tf 檔案越來越多。我們為了增加程式碼的重複利用性，會使用 terraform module 將常用的 tf 檔案封裝。\n隨著導入的規模越來越大，這些 module 會越來越多，而且使用這些 module 的 project 也增加 隨著管理的 infrastructure 越來越複雜，為了描述這些本來就很複雜的 infrastructure，module 不只越來越多，還會出現 module 引用其他 module，大 module 使用小 module 的 nested module 這點在複雜的 provider 中常出現，例如 使用 terraform 描述網路架構 描述複雜的 IAM \u0026amp; Role terraform vault-provider 管理的環境越來越多，例如 dev, staging, prod,\u0026hellip;，每個環境都需要獨立的工作空間，造成大量重複的 tf code 使用一段時間 terraform ，很快就會發現的第一個問題是：不論怎麼從 tf 檔案中提取重複部分，做成 module，還是有很多部分的 tf code 是完全重複的，例如：\n每個 module 會定義的 variable (argument)，使用這個 module 的上層會需要提供傳入 variable 每個 module 都會需要提供 provider Terraform 的語法要求上面這些參數都明確的定義，這讓整個 module 或資料夾的描述非常清楚。但會處就是，這些描述的參數道出都是，而且不斷的重複，每個 module 或資料夾都要提供，不然在 terraform validate 時會因為應提供參數未提供，導致錯誤。雖然立意良好，但卻嚴重違反 DRY (Don\u0026rsquo;t Repeat Yourself) 的原則\nmodule 多層引用，project 引用大 module，大 module 又引用小 module 開始感覺 backend 與 provider 的 code 比其他功能 code 多了 有沒有可能用其他工具，避免這些重複的參數，backend，provider 等等？\nTerragrunt 推薦有大量使用的團隊，直接使用 Terragrunt 這款工具。\n他是一層 terraform 的 wrapper\ni.e. 執行 terraform plan -\u0026gt; terragrunt plan terragrunt 會先解析，產生重複的 code，然後再執行 terraform 編輯時只要寫一次，terragrunt 代為在其他地方產生重複的 code 產生的 code 會被隱藏與 cache Terragrunt 有許多功能，例如\n處理 Backend，provider，與其他不斷重複的 tf code 處理多環境下重複的 tf \u0026hellip;完整功能請見官方文件 導入 Terragrunt 可以避免重複代碼，降低維護成本，提升生產效率\n使用情境 上述說明可能不太清楚，我們實際看範例，以前幾篇我們使用的範例 repository 為例子，把 gcp 的資料夾目錄應該長這樣：\n可以很清楚見到底下幾個東西不斷重複 由於 my-new-project, gke-playground, national-team-5g 三個專案是獨立的專案，各自可以獨立運行。但其實同屬於同公司的專案，可能許多內容都是重複的。\ntree gcp gcp ├── README.md ├── Makefile ├── my-new-project/ │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── gke-playground/ │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── national-team-5g/ │ ├── dev │ │ ├── terraform.tf │ │ ├── terraform.tfvars │ │ ├── variable.tf │ │ └── provider.tf │ ├── stag │ │ ├── terraform.tf │ │ ├── terraform.tfvars │ │ ├── variable.tf │ │ └── provider.tf │ └── prod │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── templates/ └── modules/ 可以很清楚見到底下幾個東西不斷重複\nprovider.tf 這裡描述使用的 provider，這邊只有使用 gcp provider，所以是完全一樣重複 variable.tf 是定義的參數 terraform.tfvars 是傳入的參數，也就是實際各專案各自的執行參數 national-team-5g 的專案下，又各自拆分多個執行環境，每個環境獨立，所以又有許多重複的 code 這邊的目的，是有系統化的處理這些重複的代碼\ngcp ├── terraform.tfvars # gcp 共用的參數 ├── terragrunt.hcl # gcp 共用的程式碼 ├── national-team-5g/ │ ├── terraform.tfvars # national-team-5g 共用的參數 │ ├── terragrunt.hcl # national-team-5g 共用的程式碼 │ ├── dev │ │ ├── terraform.tfvars # dev 的參數 │ │ └── terragrunt.hcl # dev 環境自己的程式碼 │ ├── staging │ │ ├── terraform.tfvars # staging 的參數 │ │ └── terragrunt.hcl # staging 環境自己的程式碼 │ └── prod │ ├── terraform.tfvars # prod 的參數 │ └── terragrunt.hcl # prod 環境自己的程式碼 差異非常多是吧，但這邊只是從資料目錄結構看，其實如果從 tf 檔案內部的程式碼看，裡面的代碼更是精簡到不行，用起來非常的爽XD\n安裝 去 release 頁面找適合的執行檔案 下載 chmod u+x terragrunt mv terragrunt /usr/local/bin/terragrunt 詳細文件請見\n範例 Terragrunt 的 DRY feature，其實內容都大同小異\n├── national-team-5g/ │ ├── dev │ │ ├── terraform.tfvars # staging 的參數 │ │ └── provider.tf │ ├── stag │ │ ├── terraform.tfvars # staging 的參數 │ │ └── provider.tf │ └── prod │ ├── terraform.tfvars # prod 的參數 │ └── provider.tf\n例如\nnational-team-5g/dev/provider.tf\nprovider \u0026quot;google\u0026quot; { version = \u0026quot;~\u0026gt;v3.25.0\u0026quot; credentials = file(var.credential_json) project = var.project region = var.region } 改成\nnational-team-5g/dev/terragrunt.hcl\ngenerate \u0026quot;provider\u0026quot; { path = \u0026quot;provider.tf\u0026quot; if_exists = \u0026quot;overwrite\u0026quot; contents = \u0026lt;\u0026lt;EOF provider \u0026quot;google\u0026quot; { version = \u0026quot;~\u0026gt;v3.25.0\u0026quot; credentials = file(var.credential_json) project = var.project region = var.region } EOF } 執行\ncd national-team-5g/dev terragrunt plan 關於參數\nvar.region 是本地參數，可以依據 dev/staging/prod 參數各自設定 var.project 這個是整個 national-team-5g 都共用的參數，可以進一步提高到更上層 i.g. 放在 national-team-5g/terraform.tfvars 檔案裡面，透過 terragrunt.hcl 傳遞。 Functions tf 檔案，在許多 block 中禁止使用 variable，這層限制有其考量，但是缺點是造成許多 hard coded 的程式碼。\nTerragrunt 產生的程式碼，由於是在 terragrunt 執行後，terraform 執行前，因次沒有這層限制，可以做許多事情，例如 code generate 以及 function 運算，terragrunt 提供許多內建 function，這邊只介紹常用幾個。\nfind_in_parent_folder() path_relative_to_include() 範例兩個檔案\nproject/dev/terraform.hcl\ninclude { path = find_in_parent_folders() } project/terraform.hcl\nenv = path_relative_to_include() 最後 parse 後的結果\nproject/dev/terraform.hcl\nenv = \u0026quot;dev\u0026quot; 做了兩件事\n要求 project/dev 去上層尋找 terraform.hcl，並且引入其中的設定 (env=\u0026quot;\u0026quot;) 子專案可以 include 母資料夾的程式碼 匯入上層 project 時，寫入相對的路徑 (dev)，並且帶入 (env=\u0026ldquo;dev\u0026rdquo;) 母專案可以為各個子目錄配置相對路徑 其他內建 function 請見官方文件\n小結 Terragrunt 可以精簡程式碼，大福提升生產效率，然而在還不熟悉 terraform 核心功能之前，不建議過早導入，會導致多餘的學習成本。\n","permalink":"https://chechia.net/posts/2020-10-06-terraform-infrastructure-as-code-terragrunt/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\u003c/p\u003e\n\u003cp\u003e而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\u003c/p\u003e\n\u003cp\u003e接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\u003c/p\u003e\n\u003cp\u003e以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e心得\n\u003cul\u003e\n\u003cli\u003eCI/CD 全自動化\u003c/li\u003e\n\u003cli\u003eState backend 選擇\u003c/li\u003e\n\u003cli\u003e最佳實踐 \u003ca href=\"https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\"\u003ehttps://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e工具\n\u003cul\u003e\n\u003cli\u003eTerraform Atlantis\u003c/li\u003e\n\u003cli\u003eTerragrunt\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"問題與需求\"\u003e問題與需求\u003c/h1\u003e\n\u003cp\u003e當團隊已經開始大規模使用 terraform，tf 檔案越來越多。我們為了增加程式碼的重複利用性，會使用 terraform module 將常用的 tf 檔案封裝。\u003c/p\u003e","title":"Terraform Infrastructure as Code: Terragrunt"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 工具 Terraform Atlantis Terragrunt 工具與文化 新工具提供解決方案，然而單純導入工具後不是就一勞永逸，許多實務上的問題，還是要依賴改善工作流程，並且避免整體運作的錯誤。\n其次，不同團隊已有既有的團隊文化，整合新的工具後還是需要磨合，不一定要照單全收。換句話說，工作流程的不斷改進也是解決方案的一環。\n建議實踐 Recommended Practices Terraform 官網有許多建議的實作與導入流程，其中大部分的建議我們都已經在前面的幾篇文章中提到，這邊要來說明一下，並補充其他官方推薦的實踐。\n技術複雜度: 隨著架構的複雜增加，維護整體架構的困難逐漸增加 組織複雜度: 隨著團隊規模增長，分工與權責越顯複雜，團隊的協作困難逐漸增加 Terraform 的目的，在於減少上面兩者的複雜度。\n工作目錄結構 工作目錄 (workspace) 是 terraform 運作的基準點，容納 tf 檔案，通常會使用版本控制工具管理。\n一個環境一個工作目錄 這個我們在前面的範例已經實踐，基本上目錄為\nproject (git repository) ├── api-server/ │ ├── dev │ │ └── terraform.tf │ ├── stage │ │ └── terraform.tf │ └── prod │ └── terraform.tf ├── grpc-server/ │ 將產品或專案切割成各自獨立的環境，以某專案的某環境作為管理單位\nproject-api-server-dev project-api-server-stage project-api-server-prod project-grpc-server-dev \u0026hellip; 這麼做有幾個好處：\n確保產品與環境的獨立 彼此不互相影響 但又可確保彼此的關連性，例如架構相同 以環境的為管理單位，換句話說，一組完整的環境 直接對應到 一組完整的工作目錄 管理上非常直觀明確 使用工作目錄分配權限與權責 直接為不同團隊分配不同工作目錄的存取權限 權責與工作目錄的明確對應 持續評估與改進 自動化程度也是工作流程進步的指標：\n手動控制 半自動化 全自動化 導入 terraform 後的我們已經描述過了，內容詳情請見上上篇文章。下面描述官方推薦的從最開始，完全沒有 terraform 經驗開始導入流程，以筆者個人於公司從零開始導入的經驗，非常直得參考。\n如何從完全手動操作，變成半自動操作 如何從半自動，變成 IaC (infrastructure as code) 如何從 IaC，變成 IaC 多人協作 進階改進 IaC 多人協作 如何從完全手動操作，變成半自動操作 如果團隊應該是還在完全手動控制 infrastructure\n查驗 (audit) 困難 無法複現 (reproduce) 拓展 (scale) 困難 很難分享 infrastructure 的知識 第一步要執行的是：選擇少部分，可以控制的 infrastructure 開始導入 terraform。所以要做的就是\n開始使用 terraform 如果需要可以參考一些範例專案 Terraform: Get Started collection 開始進入半自動階段後，團隊中的少部分人員開始使用 terraform，手上也有了可運作的少部分 infrastructure as code，可以作為 demo ，或是其他團隊成員的教育訓練，這個可以幫助下一階段的演進。\n如何從半自動，變成 IaC (infrastructure as code) 目前半自動的工作專案內容應該是：\nTerraform code 手動操作的流程 一些輔助腳本 下個階段希望繼續推展 terraform 的使用，降低手動步驟與腳本執行。這個階段可以做的事情\n使用版本控制管理 tf code 這邊假設團隊本來就有使用版本控制，開始將 tf code 導入版本控制的工作流程 開始將先產生的 tf code 移入版本控制 所有團隊都能共享新 tf code 的知識 開始使用第一個 terraform module 一個簡單的方式是將重複使用的 infrastructure 抽出，減少重複的 tf code 這邊需要提醒，盡量以完整個 infrastructure 作為單位抽離 tf code 作為 module 完整的 infrastructure 也是在 provider 中間轉成的單位 持續推廣團隊內 terraform 的使用 開始設定工作準則 (Guidelines) 來描述並規範工作流程 在團隊上不完全熟悉 terraform 前，可以提升工作效率，推廣最佳實踐，並且降低錯誤風險 團隊的架構師可以依據團隊文化形塑工作流程，更符合團隊需求 開始導入 configuration management 例如 Chef cookbook 私鑰與隱秘資訊管理 導入 vault 來管理 terraform 整合 vault，可以使用 terraform 管理 vault 的結構，使用 vault 來管理 terraform 所需的 credentials 如何從 IaC，變成 IaC 多人協作 導入版本控制可以降低堆人協作的複雜度，下個階段需要\n統一跨團隊的工作流程 使用 terraform 管理團隊的 IAM 權限 可以執行的改進如下：\n官方推薦使用 Terraform Cloud 來做後台，我們稍候推薦的幾個免費工具也有相似功能，團隊可以參考使用。這邊專注在需求與方法。 開始設計整個組織間的工作目錄結構 工作目錄要反映 獨立的環境與獨立的專案 負責管理的團隊組織，已分配存取權限 這部分的實作後面的文章也會提到 官方推薦的 Terraform Cloud 可見官方文件 進階改進 IaC 多人協作 如今團隊已經有多人協作的介面，也有完整的工作流程，我們可以藉由以下改進，達成更堅固的框架\n官方建議大量導入 Terraform Cloud，但這會超出免費額度 我們之後的文章會提供開源版本的解法 簡化工作目錄的管理 明確的 review 與 audit 工作流程 增加 infrastructure 的監測與效能監控，這些都可以使用 terraform 設置 具體實作，請見下篇文章\n","permalink":"https://chechia.net/posts/2020-10-05-terraform-infrastructure-as-code-recommended-practices/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\u003c/p\u003e\n\u003cp\u003e以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e心得\n\u003cul\u003e\n\u003cli\u003eCI/CD 全自動化\u003c/li\u003e\n\u003cli\u003eState backend 選擇\u003c/li\u003e\n\u003cli\u003e最佳實踐\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e工具\n\u003cul\u003e\n\u003cli\u003eTerraform Atlantis\u003c/li\u003e\n\u003cli\u003eTerragrunt\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"工具與文化\"\u003e工具與文化\u003c/h1\u003e\n\u003cp\u003e新工具提供解決方案，然而單純導入工具後不是就一勞永逸，許多實務上的問題，還是要依賴改善工作流程，並且避免整體運作的錯誤。\u003c/p\u003e\n\u003cp\u003e其次，不同團隊已有既有的團隊文化，整合新的工具後還是需要磨合，不一定要照單全收。換句話說，工作流程的不斷改進也是解決方案的一環。\u003c/p\u003e\n\u003ch1 id=\"建議實踐-recommended-practices\"\u003e建議實踐 Recommended Practices\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\"\u003eTerraform 官網有許多建議的實作與導入流程\u003c/a\u003e，其中大部分的建議我們都已經在前面的幾篇文章中提到，這邊要來說明一下，並補充其他官方推薦的實踐。\u003c/p\u003e","title":"Terraform Infrastructure as Code: Recommended Practices"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt Terraform Backend 剛開始使用 terraform 的時候，大家的第一個範例應該都是 local backend 吧，就是直接在本地 terraform apply，在目前的工作目錄下產生 state 檔案。這個 state 檔案直接 cat 打開來看後，可以發現裡面一切都是明碼的。初學時筆者感覺所謂的 Terraform backend 只是一個存放中繼的 state 資料的 workspace，後來發現完全不是這麼回事，便立刻棄用了 local backend。\n之後依照官方推薦就使用了 Terraform Cloud，後來便出現許多問題，等等會分析。\n最後團隊選用了自家公有雲的 backend，例如\nAWS S3 作為 state storage，DynamoDB 作為中心化的 workflow lock GCP 完整 terraform backend 支援清單可以見官方網站。\n這篇文章要來仔細探討所謂的 terraform backend，backend 的重要性，與如選擇適合自己團隊的 backend。\n問題 如果使用 tf random 產生亂數密碼，直接去 cat state 檔案就可以看到明碼的 random 數值。\n多人協作 lock\n另外一個解法是，根本不使用 backend，terraform 也支援這樣的做法。雖然官方也明講 backend are completely optional，但依照筆者的經驗，強烈建議多人團隊務必去啟用，並找尋適合自己的 Backend。\n需求 資訊安全，希望 terraform 使用是安全的，不會暴露敏感資訊 多人協作，希望可以同時工作，但又不會互相衝突 遠端操作，避免本地操作 State 存放與鎖 預設的 backend 是 loal backend，也就是執行 terraform apply 後，本地會出現一個 JSON 格式的 state 檔案。然而 local state 會立刻遇到的問題，就是\n第一個是協作困難，apply 的結果別人看不到，不能接續著做 每個人都可以 apply 但不同人的 apply 沒有相依性 可是遠端的 infrastructure 有，A B 不同人一起 apply 到 infrastructure 上，可寧就會衝突，或是產生不可預期的錯誤 所以筆者強烈推薦使用外部的 backend 來取代 local backend。多半 backend 會多做許多事，透過控制 state 來確保 infrastrure 的完整性，例如對 state 存取有以下的限制:\nＡ透過鎖來控制，禁止多人同時存取，例如同時有兩個人 apply 相同或不同檔案，先取得 backend lock 的人執行，後來的人會被 terraform 阻擋 避免多頭馬車的問題 確保 state 的唯一性，使用另外一個 repo 的 state 檔案，遠端的 state 會拒絕存取 確保 state 的順序，每次 apply 的 state 都是依序產生 如果 state 是舊的，可能就會被遠端的 backend 拒絕，避免使用舊的 apply 覆蓋新的 infrastructure 這些限制，如果使用 local backend 才會容易遇到，使用外部的 backend 其實不容易會發生。Terraform 基於 Infrastructure as Code 實現，可以將整個 terraform repo 視為 code 一部分，這樣就可以想像為何這些 state 限制是重要的。\nstate 跟隨 tf 檔案，隨著 tf 檔案的 commit 推進，state 也跟著推進 commit 1 的 tf 檔案，apply 後產生 state 1 如果今天有團隊 force push 了一個 conflict 的 tf 檔案，硬要 apply 的結果也可能造成 infrastructure conflicts 如果今天團隊有多個 branch 同時開發，branch A apply 的 state 會與 branch B apply 的 state 也可能造成衝突 State 手動更改 在很特殊的狀況下，你也可以手動更改 state file，然後 push 上傳，但這點非常不建議，terraform 會自動維護 state 的完整性，手動更改可能會直接破壞正常的 state。什麼情形會用到？就是你的 state 因為某個原因被玩壞，大部分是人為弄壞的。這時候才被迫要手動更改 state。手動更改 state ，講白了不做死就不會死。\nvault state pull vim your-local-state-file # Increment Serial if needed vault state push # vault state push -force Hosted: Terraform Cloud Terraform Cloud，是官方提供的解決方案，有提供較多功能，例如在 terraform cloud 的網站上遠端 plan。有提供免費版，提供最多 5 人團隊使用。也有提供進階的方案 $20/user 或是 $70/user，以及 enterprise 版本，可以本地安裝。\n使用很簡單，我在前幾篇提供的範例 repository全部都是使用 terraform cloud 作為 Backend。提供\n遠端的 state 儲存庫 所有團隊成員使用各自帳號登入 plan 之前會 sync terraform cloud 上面的 state 遠端的 state lock 如果 lock 被人佔據，表示有其他人正在使用，會取消新的操作，避免衝突 也提供線上檢視 state ，或是線上修改 state 的功能 貼心小功能，但多半用不到 託管的 Terraform Cloud 作為 backend 他有幾個問題\n如果要啟用遠端 plan，需要綁定版本控制服務器，例如綁定 Github 或 Gitlab 基於安全性考量，這點很多公司就直接打槍了 暴露原始碼給第三方公司，可能會讓公司的安全性檢驗不過 加上 terraform 的 Git Code，基本上就是所有 infrastructure 的資訊 加上如果有使用 terraform 編輯 IAM 或是 provision vault，等於許多敏感資料都會出現在 terraform repository 中 如果是重視安全性的團隊，則至少要使用可以 self-hosted 的 Terraform Enterprise，而不要使用公有的 Terraform Cloud。然而 Enterprise 我是沒用過，請不要問我價錢。\nConsul Consul 嚴格來說不是單純的儲存庫，他是 Service Networking 設定與服務發現的解決方案，只是本身帶有 key value 的儲存功能，就被自家整合。換句話說，他不是專門拿來做 terraform backend 的，比較像是如果團隊本來就有使用 consul，可以考慮公用儲存庫，作為 terraform backend。\n這是相同公司 Hashicorp 提供的自家的 backend，整合的很完整。但就只是單純的儲存庫，沒有 terraform cloud 的遠端執行啊，或是線上檢視 state 檔案的功能。\n可以在公司內部自行架設一組 cluster，然後就可以作為 backend。\n問題是\nConsul 這麼大一包，結果只使用了裡面的 kv store Consul 是分散式的 Key-value store，不熟悉的話不太好養 換句話說，如果死了救得起來嗎 Etcd Etcd 跟 consul 類似，雖然是專業的儲存庫，但是使用這麼複雜的分散式儲存庫，只作為 terraform 的 backend，顯然很不經濟。\n可以在公司內部自行架設一組 cluster，然後就可以作為 backend。\n一樣只建議已經有在使用 etcd ，且熟悉維運 etcd 的團隊，才考慮使用 etcd 兼作為 terraform backend。\n這些分散式的儲存庫，不太容易死，然而萬一死了可能不太好救。\nPublic Cloud AWS S3 + DynamoDB GCP gcs + pg Azurerm + pg\n只有單純的 state storage 與 lock 的功能，沒有什麼花俏的線上執行或是快速 review。\n好處是\n使用非常單純 也是處在安全的內網環境中 有於是公有雲提供的服務，基本的 IAM 與權限控管可以直接應用 Postgresql Postgrel 也是一個不錯的選擇\nTerraform 並不會帶來大量的資料庫負擔，所以可能會把 terraform 與附載較低的應用，共用資料庫。\n使用上作為 state storage 與 lock 很單純沒什麼問題\n其他 官方還有提供所有支援的 backend\n如果 Kubernetes 還有做其他事情的話，請不要用 Kubernetes secret 作為 terraform 的 backend\n","permalink":"https://chechia.net/posts/2020-10-04-terraform-infrastructure-as-code-backend/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\u003c/p\u003e\n\u003cp\u003e而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\u003c/p\u003e\n\u003cp\u003e接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\u003c/p\u003e\n\u003cp\u003e以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e心得\n\u003cul\u003e\n\u003cli\u003eCI/CD 全自動化\u003c/li\u003e\n\u003cli\u003eState backend 選擇\u003c/li\u003e\n\u003cli\u003e最佳實踐 \u003ca href=\"https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\"\u003ehttps://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e工具\n\u003cul\u003e\n\u003cli\u003eTerraform Atlantis\u003c/li\u003e\n\u003cli\u003eTerragrunt\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"terraform-backend\"\u003eTerraform Backend\u003c/h1\u003e\n\u003cp\u003e剛開始使用 terraform 的時候，大家的第一個範例應該都是 local backend 吧，就是直接在本地 terraform apply，在目前的工作目錄下產生 state 檔案。這個 state 檔案直接 cat 打開來看後，可以發現裡面一切都是明碼的。初學時筆者感覺所謂的 Terraform backend 只是一個存放中繼的 state 資料的 workspace，後來發現完全不是這麼回事，便立刻棄用了 local backend。\u003c/p\u003e","title":"Terraform Infrastructure as Code: Backends"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt CI/CD 全自動化 當公司成功導入 terraform ，並且整合 Git-flow 的工作流程後，應該可以很明顯的感受到，整體的 infrastructure 產出穩定度有大幅提升，畢竟軟體工程中 code review 是穩定度的核心關鍵之一，而導入 terraform 與 infrastructure as code 讓 infrastructure 也能在合理的工作流程中被層層 review。\nstable master -\u0026gt; feature/add-new-infra -\u0026gt; PR feature/add-new-infra -\u0026gt; master\nfeature branch merge 進去 master 之後，就確定是 review 過而且穩定的程式碼，再由工程師 pull 最新的 master ，然後在本地執行 terraform apply，把 infrastructure 生出來。\n這樣做本質不會有什麼大問題，畢竟 code 已經是穩定。然而，實務上卻還是會出現偶發的問題。例如：使用錯誤的 credentail 或 context，導致 dev 或 staging 的 infrastructure apply 到 prod 上，直接 p0 issue 大爆發，SRE 都跪著上班。又例如：使用錯誤的 master 版本 apply，結果也是服務掉線，整個 team 跪著上班。\n關鍵：團隊整體的安全性，是由程度最菜的同事決定。\n對我就是在說你XD。然而人都會菜，而人事成本也是公司經營的關鍵考量，相信所有同事都大神是不切實際的，不如改進工作流程，近一步降低人為操作失誤的可能。人的問題，根本之道還是教育，然而我們可以試著用技術與工具降低風險。\n以下的做法，可能會協助避免這個問題。我們要做的就是 CI/CD 的全自動化。\n需求 避免 apply 失誤 避免愚蠢的錯誤 環境切換錯誤 apply 錯版本 (愚蠢的錯誤比你想的要多，出現後會讓你三觀大開) 加速工作流程 PR 結束後，在合適的時間自動 apply 自動回報結果 出錯自動 rollback 上個穩定版本 最小權限原則 (least privilege access) 原本工程師為了 apply ，會有 admin 權限的 credential 移轉到安全的 CI/CD server 上，在 server 上執行 工程師不再握有這些超級管理員權限，避免工作機被駭的安全隱憂 NOTE: 工程師被釣魚 (phishing) 或是社交工程攻擊 (social engineering attacks) 才是導致公司服務被害的主因，不可不防 解決方案 選擇安全的 CI/CD server，例如在內網的 self-hosted Jenkins server 將 terraform 的執行點，從工程師本機移轉到 CI/CD 服務器上 更改 CI/CD ，執行以下步驟 Terraform validate Terraform plan 的結果輸出到 Github / Slack plan 結束後停住 CI/CD，發送一個 apply request 到 Github comment，不再繼續執行 apply SRE 主管只要透過 Github comment 或是 Slack bot 就可以選擇合適的時間，approve apply 最後移除大多數工程師的 terraform 權限 範例 事實上，不同家的 CI/CD server，工作流程都是類似\ncheckout initialize tools / SDK (ex. az/aws/gcloud client) inject public cloud credential (ex. Azure/AWS/GCP key) terraform validete terraform plan terraform apply (option) require manual approve 如果是 Jenkins 的使用者，可以參考 Azure 提供的範例\n環境管理 在人工 apply 的工作流程中，工程師需要自行切換環境，例如 git repo 工作目錄如下\ntree project ├── dev ├── staging └── prod cd project/dev; terraform plan # plan staging cd project/prod; terraform plan # plan prod 這是相對比較安全的做法，在對的資料夾目錄下，就會 apply 到正確的環境，程式碼與環境有緊密的對映。除了以下幾種情形\n想要部署 dev，結果沒注意到自己在 staging 或是 prod 有一部分的 input variable 會影響結果，然後又輸入錯誤的 input 到錯誤的環境上 對很愚蠢，但我都見過（氣血攻心）。\n既然導入了自動化，環境的切換可以自動切換，例如\n所有 feature branch 執行 CI/CD server 上的本地測試 lint init validate 所有 PR 都會觸發新的 dev 環境部署 plan apply 測試腳本 所有 master merge / push 都會觸發 staginge 部署 QA 測試 壓力測試 (optional) 所有 release candidate tag 會部署到 release candinate release management 所有發布版本的 tag (ex. 1.1.0 / 1.2.0-release) 會部署 prod 當然要事先通知相關人士 stack holders 資深工程師只要控制 branch / tag 就可以控制發布。\n你說這樣，萬一天兵去自己打 tag 打錯 commit，或是推錯 branch 推到 master或是 release candidate，還不是依樣爆掉。那你可以把 master 與 release branch 鎖起來 (protected branch) ，然後把 tag push 權限鎖住。\n你說還是錯 那我\u0026hellip; \u0026hellip;們看下一段 orz\n安全性 雖然說是自動化改善工作流程，然而收回存取權限，對於服務的整體安全性大幅提升，畢竟是可以更改 infrastructure 的管理員帳戶。Terraform 既然能夠新增修改雲端的 infrastructure，這個帳號的權限是相當大的，萬一金鑰(GCP/AWS/Azure credentail) 流出或遭駭，後果都是毀滅性的，例如可以直接刪除服務的 infrastructure，或是修改防火牆的規則，偷埋其他金耀，\u0026hellip;等於是整座公有雲送給駭客。所以我們使用 Terraform 應該要慎重考慮存取權限的安全性。\n本來是每個 SRE 的本機電腦上，可能都會有這把帳戶權限。\n如果是 self-hosted Jenkins server，或是 Github enterprise server，把服務權限移轉到這些服務器，便可以確保金鑰永遠都在公司的防火牆內部網路，更加大幅度的提升整體的安全性。\n其他 可以進一步做金鑰權限分割，將底下四個權限透過公有雲的 IAM role 去切割。要是萬一金鑰還是外洩了，可以降低損失。\n讀取 新增 修改 刪除 你說這個不用自動化就可以做，我說如果分割金鑰然後人工自己切換操作，反而會增加操作的複雜度，增加錯誤的機會。然後工程師的痛苦程度，與手上的金鑰數量成正比。\n或是利用環境存取金耀分割，把金耀進一步切割成不同的權限，萬一掉了，損害也控制在一個環境之內。例如：\ndev staging prod 不同環境的 infrastructure ，在創建初期就是透過不同的帳號產生的，彼此不會有不乾淨殘留的帳號權限。\n小結 這邊就講兩件事\n自動化可以防呆 自動化可以增加安全 參考文件 Terraform Official doc: Running Terraform in Automation ","permalink":"https://chechia.net/posts/2020-10-03-terraform-infrastructure-as-code-automation/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\u003c/p\u003e\n\u003cp\u003e而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\u003c/p\u003e\n\u003cp\u003e接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\u003c/p\u003e\n\u003cp\u003e以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e心得\n\u003cul\u003e\n\u003cli\u003eCI/CD 全自動化\u003c/li\u003e\n\u003cli\u003eState backend 選擇\u003c/li\u003e\n\u003cli\u003e最佳實踐 \u003ca href=\"https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\"\u003ehttps://www.terraform.io/docs/cloud/guides/recommended-practices/index.html\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e工具\n\u003cul\u003e\n\u003cli\u003eTerraform Atlantis\u003c/li\u003e\n\u003cli\u003eTerragrunt\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"cicd-全自動化\"\u003eCI/CD 全自動化\u003c/h1\u003e\n\u003cp\u003e當公司成功導入 terraform ，並且整合 Git-flow 的工作流程後，應該可以很明顯的感受到，整體的 infrastructure 產出穩定度有大幅提升，畢竟軟體工程中 code review 是穩定度的核心關鍵之一，而導入 terraform 與 infrastructure as code 讓 infrastructure 也能在合理的工作流程中被層層 review。\u003c/p\u003e","title":"Terraform Infrastructure as Code: CI/CD automation"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講了很多 terraform 的操作範例，應該看到這裡，對於 terraform 基本上是什麼東西，應該有些概念了。然而這樣還不能算是學會 terraform，這種工具的東西一定要有實際操作過的經驗才算是學會。\n可以直接參考 Terraform 官方的 Get-started 文件來操作學習，我這邊也提供一個 Git repository 讓大家上手，當作初次操作的框架。\n提供做為範例的原始碼 這個 Github Repository 是我給社群演講所使用的範例，第一次使用的可以參考\nhttps://github.com/chechiachang/terraform-playground\ntree ├── README.md ├── SOP.md ├── aws/ ├── azure/ └── gcp/ TL;DR 選擇使用的雲平台，這邊提供三家範例，例如我這邊使用 gcp，當然你就要準備 GCP 的帳號，並且下載有執行權限的用戶 credential json key 等等。\n雖然我沒收 gcp 錢，這邊還是推廣一下 gcp 的 free credit 試用。阿要用 Azure Cloud 的 free credit 來執行這個範例也是完全沒問題，非常夠用。唯有 AWS 的試用方案跟剩下兩家不太一樣，這個 repository 起的服務可能會超過 AWS 的免費額度涵蓋範圍，總之請自己注意。\ngit clone https://github.com/chechiachang/terraform-playground cd gcp DIR=my-new-project make project cd my-new-project vim *tf terraform init terraform plan terraform apply 這樣應該就會跑完。然後我們講解幾個地方。\n工作目錄 Terraform 預設是以當下執行的目錄作為基準，掃描資料夾中的 .tf 檔案。所以可以把一個一個獨立的專案先用資料夾裝好，彼此內容互不干涉。\n我們這邊創建新的 subdirectory，這邊是以 my-new-project 為範例。這邊指的 project 只是一個 terraform resource 範圍，可以但不用是一個真實的 gcp project。terraform 執行是以一個 directory 為範圍，不同 project directory 可以透過不同 terraform 指令控制。如果是獨立的服務希望獨立管理也可以切開。\n我寫了一個簡單的 Makefile，進一步封裝基本的指令，基本上不需要 Makefile 以外的操作。 make project 是其中一個指令，幫忙創建資料夾、布置 Makefile 與基本的 terraform 設定等等。\n如果團隊有多人協作，非常推薦使用統一 Makefile / 或是 bash script 封裝，統一這些編輯的雜事，降低不同人編輯出錯的風險。\n目錄結構 總之我們現在 cd 到 my-new-project 的工作目錄下，這個目錄代表一個專案。其他的 gke-playgound 與 national-team-5g 也是其他的專案，先忽略他。\ngcp ├── README.md ├── Makefile ├── my-new-project/ ├── gke-playground/ ├── national-team-5g/ ├── templates/ └── modules/ cd my-new-project 進入 my-new-project 下，可以看到裡面已經有一些檔案，我們首先要編輯的是這個 terraform.tf\nmy-new-project ├── Makefile ├── provider.tf ├── terraform.tf └── variables.tf vim terraform.tf terraform.tf 是 terraform 本身的設定 這邊是 Terraform Backend 的設定，如果不知道什麼是 terraform backend 這個我們明天的文章會講。這邊使用的 backend 是 terraform 官方自家的 terraform cloud，可以在網站上\n註冊使用者，填到底下 organization 這裡 創建一個 workspace，填到 workspace.name 這裡 terraform { # Create a remote organization on https://app.terraform.io backend \u0026quot;remote\u0026quot; { # Provide terraform credential by # - terraform login (suggested) # - use User API Token #token = \u0026quot;\u0026quot; hostname = \u0026quot;app.terraform.io\u0026quot; organization = \u0026quot;chechia\u0026quot; workspaces { name = \u0026quot;terraform-playground\u0026quot; } } } provider.tf 是 provider 的設定 terraform client 會把 tf 檔案拿來運算，透過 Provider ，將需求實際轉化成 API call ，然後送到公有雲或是其他目標。這邊就只講到這樣。\nprovider 為了工作，可能會需要提供一些參數，例如 google 的 provider 會需要在這裡提供 credential_json 的路徑，請把它放在適合的地方，然後用絕對路徑指向 google\nNOTE: 不要 commit credential key 到 git repository 裡面。可以放到外層資料夾，或至少要 gitignore 掉。\nprovider \u0026quot;google\u0026quot; { version = \u0026quot;~\u0026gt;v3.25.0\u0026quot; credentials = file(var.credential_json) project = var.project region = var.region } variable.tf 做參數的存放點 雖然上面 tf 檔案使用了 terraform / provider / variable ，但 terraform 掃描檔案時，檔名本身並不會影響。也就是說，參數你想擺哪就擺哪。不過上面是常見的命名慣例，這樣擺人類比較容易找得到。\nvariable 這邊設定的參數，比較像是 arguments，也就是當其他位置的 tf 檔案，引用這個資料夾作為 module 的時候，作為參數輸入的 placeholder，其他 tf 檔案可以使用 variable 關鍵字定義的參數，例如: var.project，或是 provider.tf 裡的 var.credential_json。\nvariable 關鍵字也可以定義 default 預設值，如果沒有定義 default，也沒有從外部傳入 argument，會在 validate 時造成 error。\n# Global variable \u0026quot;project\u0026quot; { type = string default = \u0026quot;myproject\u0026quot; } variable \u0026quot;credential_json\u0026quot; { type = string default = \u0026quot;../credentials/gke-playground-0423-aacf6a39cc3f.json\u0026quot; } variable \u0026quot;region\u0026quot; { type = string default = \u0026quot;asia-east1\u0026quot; } Create 這裡我們試著創建一台 GCE，使用下列指令，會發現多了一個檔案 compute_instance.tf。\nNAME=my-new-gce make gce my-new-project ├── Makefile ├── compute_instance.tf ├── provider.tf ├── terraform.tf └── variables.tf 內容大概是\nmodule \u0026quot;my-new-gce\u0026quot; { source = \u0026quot;../modules/compute_instance\u0026quot; providers = { google = google } project = var.project name = \u0026quot;my-new-gce\u0026quot; image = \u0026quot;https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200429\u0026quot; machine_type = \u0026quot;n1-standard-1\u0026quot; network = \u0026quot;projects/${var.project}/global/networks/myproject\u0026quot; subnetwork = \u0026quot;projects/${var.project}/regions/asia-east1/subnetworks/myproject\u0026quot; } module 關鍵字定義一組資源，具體的內容是這裡，簡單來說可以把一堆 tf 檔案放在一塊，然後把需要的參數使用 variable.tf 拉出去，讓其他地方引用。\nsource = \u0026quot;\u0026quot; 是實際引用的 module 來源\n底下是這個 module 需要用到的參數，例如 project, name, image, machine_type,\u0026hellip; 等是 gce 這個 module 需要的參數。\nMakefile NAME=my-new-gce make gce 這行指令與 terraform 無關，只是一個快速生成 compute_instance.tf 的小腳本。使用這個腳本可以\n快速產生 tf 檔案 產生標準化的 tf 檔案，所有 project 的 compute_instance 都長一樣 抽換名子 name 使用 symbolic link 讓所有 project 資料夾使用同一個 Makefile，keep your code DRY。\n最後就是常規的 plan 與 apply，這邊沒有什麼特別的。\nterraform plan terraform apply 小結 有規律地整理 project 可以降低維護成本 善用 module 封裝，可以提高整體的重用性與易用姓，提高開發效率 使用 template tf 可以加速重複的資源產生步驟 問題: 此時的資料夾中還是充滿大量重複的 code，例如到處都需要 provider、重複的 module，一大堆重複的東西。有沒有可能再讓我們的程式碼更 DRY 一點呢?\nTerragrunt 幫我們實現這點，非常值得使用的工具。請見下篇分享。\n","permalink":"https://chechia.net/posts/2020-10-02-terraform-infrastructure-as-code-repository-example/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e上面講了很多 terraform 的操作範例，應該看到這裡，對於 terraform 基本上是什麼東西，應該有些概念了。然而這樣還不能算是學會 terraform，這種工具的東西一定要有實際操作過的經驗才算是學會。\u003c/p\u003e\n\u003cp\u003e可以直接參考 Terraform 官方的 Get-started 文件來操作學習，我這邊也提供一個 Git repository 讓大家上手，當作初次操作的框架。\u003c/p\u003e\n\u003ch3 id=\"提供做為範例的原始碼\"\u003e提供做為範例的原始碼\u003c/h3\u003e\n\u003cp\u003e這個 Github Repository 是我給社群演講所使用的範例，第一次使用的可以參考\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003ehttps://github.com/chechiachang/terraform-playground\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etree\n├── README.md\n├── SOP.md\n├── aws/\n├── azure/\n└── gcp/\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"tldr\"\u003eTL;DR\u003c/h3\u003e\n\u003cp\u003e選擇使用的雲平台，這邊提供三家範例，例如我這邊使用 gcp，當然你就要準備 GCP 的帳號，並且下載有執行權限的用戶 credential json key 等等。\u003c/p\u003e","title":"Terraform Infrastructure as Code: Example repository"},{"content":"先占虛擬機與 Kubernetes 在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制\n資料中心會 (可預期或不可預期地) 終止先占虛擬機 先占虛擬機不能自動重啟，而是會被資料中心終止後回收 GCP 不保證有足夠的先占虛擬機 節點的終止會造成額外的維運成本，例如\n管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機 管理多個應用複本，節點終止時，維護整體應用的可用性 將移除節點上的應用，重新排程到其他可用節點 動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints) 意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效 配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量 這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要\n補足被移除的 15 個節點 計算被移除的應用，補足移除的應用數量 移除失效的應用端點，補上新的應用端點 持續監控應用狀態 \u0026hellip; 沒有自動化管理工具，看了心已累 (貓爪掩面)\n我們使用 Kubernetes 協助維運自動化，在 GCP 上我們使用 GKE，除了上述提到的容器應用管理自動化外，GKE 還額外整合先占虛擬機的使用\n啟用先占虛擬機的節點池 (node-pool)，設定節點池的自動拓展，自動補足先占節點的數量 GKE 自動維護先占虛擬機的 labels 關於 GKE 的先占虛擬機的完整細節，請見GCP 官方文件。這份文件底下也提供了 GCP 官方建議的先占虛擬機最佳實踐\n架構設計需要假設，部分或是全部的先占虛擬機都不可用的情形 Pod 不一定有時間能優雅終止 (graceful shutdown) 同時使用隨選虛擬機與先占虛擬機，以維持先占虛擬機不可用時，服務依然可用 注意節點替換時的 IP 變更 避免使用有狀態的 Pod 在先占虛擬機上 (這點稍後的文章，我們會試圖超越) 使用 node taint 來協助排程到先占虛擬機，與非先占虛擬機 總之，由於有容器自動化管理，我們才能輕易的使用先占虛擬機。\nGKE 然而，決定使用 GKE 後，就有許多關於成本的事情需要討論\n先看 GKE 的計費方式 pricing\n每個 GKE 集群管理費用 $0.1/hr = $72/hr 這個費用是固定收費，只要開一個集群，不論集群的節點數量。所以在節點多、算力大的集群裡，這個費用會被稀釋，但在節點少的集群裡比例會被放大。\n然後 GKE 還是會有一些自己的毛，俗話說有一好沒兩好，我們使用它的好處同時，也要注意許多眉眉角角。再來爬文件。如同最前面宣導，用產品就要乖乖把文件看完，不過這裡先針對與先占虛擬機相關的議題\nAllocatable Resource Regional Cluster Cluster autoscaler Allocatable Resource 在網路上看到這篇好文 GKE 上的可使用的資源 Allocatable Resource。啥意思呢？難道還有不能使用的資源嗎？\n沒錯，GKE 會保留一定的機器資源 (e.g. cpu, memory, disk)，來維持節點的管理元件，例如 container runtime (e.g. Docker)、kubelet、cAdvisor。\n也就是說，就算我們跟 GCP 購買了算力，有一個比例的資源我們是使用不到的。細節請見 理解 GKE 集群架構。這會影響我們單一節點的規格，我們也需要一並計算，能實際使用的資源 (allocatable resource)。\nAllocatable = Capacity - Reserved - Eviction Threshold\nCapacity，是機器上實際裝載的資源，例如 n1-standard-4 提供 4 cpu 15 Gb memory Reserved，公有雲代管集群，預保留的資源 Eviction Threshold：Kubernetes 設定的 kubelet 驅逐門檻 驅逐門檻 (Eviction threshold) Kubelet 會主動監測節點上的資源使用狀況，當節點發生資源不足的狀況時，kubelet 會主動終止某些 Pod 的運行，並回收節點的資源，來避免整個節點資源不足導致的系統不穩定。被終止的 Pod 可以再次排程到其他資源足夠的節點上。細節請見 官方文件 Scheduling and Eviction\n在 Kubernetes 上，我們可以進一步設定驅逐門檻，當節點的可用資源低於驅逐的門檻，kubelet 會觸發 Pod 驅逐機制\nGKE 上每個節點會額外保留 100 MiB 的記憶體，作為驅逐門檻，意思是當節點耗盡資源，導致剩餘記憶體低於 100 MiB 的時候，會直接觸發 GKE 的 Pod Eviction，終止並回收部分的 Pod。換句話說，這 100 MiB 是不能被使用的資源。細節請見官方文件\n集群保留資源精算 資源的定義，使用雲平台的一般費用大多來自此\ncpu memory storage 然後是這個表，注意保留的資源是累進級距\n255 MiB of memory for machines with less than 1 GB of memory 25% of the first 4GB of memory 20% of the next 4GB of memory (up to 8GB) 10% of the next 8GB of memory (up to 16GB) 6% of the next 112GB of memory (up to 128GB) 2% of any memory above 128GB\n值計上能夠用到的資源，底下 GCP 也整理好了，例如 n1-standard-4 實際使用的是 memory 12.3/15，cpu 3.92/4。\n在維持合理的使用率下，開啟大的機器，可以降低被保留的資源比例，依照筆者公司過去經驗，GKE 起跳就是 n1-standard-4 或是以上規格，如果低於這個規格，可調度的資源比例真的太低，應該重新考慮一下這個解決方案是否合乎成本。\n但究竟什麼規格的機器適合我們的需求，說實在完全要看執行的應用而定。\n","permalink":"https://chechia.net/posts/2020-09-26-gcp-preemptible-instance-kubernetes/","summary":"\u003ch3 id=\"先占虛擬機與-kubernetes\"\u003e先占虛擬機與 Kubernetes\u003c/h3\u003e\n\u003cp\u003e在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e資料中心會 (可預期或不可預期地) 終止先占虛擬機\u003c/li\u003e\n\u003cli\u003e先占虛擬機不能自動重啟，而是會被資料中心終止後回收\u003c/li\u003e\n\u003cli\u003eGCP 不保證有足夠的先占虛擬機\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e節點的終止會造成額外的維運成本，例如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機\u003c/li\u003e\n\u003cli\u003e管理多個應用複本，節點終止時，維護整體應用的可用性\u003c/li\u003e\n\u003cli\u003e將移除節點上的應用，重新排程到其他可用節點\u003c/li\u003e\n\u003cli\u003e動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints)\n\u003cul\u003e\n\u003cli\u003e意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效\u003c/li\u003e\n\u003cli\u003e配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e補足被移除的 15 個節點\u003c/li\u003e\n\u003cli\u003e計算被移除的應用，補足移除的應用數量\u003c/li\u003e\n\u003cli\u003e移除失效的應用端點，補上新的應用端點\u003c/li\u003e\n\u003cli\u003e持續監控應用狀態\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e沒有自動化管理工具，看了心已累 (貓爪掩面)\u003c/p\u003e\n\u003cp\u003e我們使用 Kubernetes 協助維運自動化，在 GCP 上我們使用 GKE，除了上述提到的容器應用管理自動化外，GKE 還額外整合先占虛擬機的使用\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e啟用先占虛擬機的節點池 (node-pool)，設定節點池的自動拓展，自動補足先占節點的數量\u003c/li\u003e\n\u003cli\u003eGKE 自動維護先占虛擬機的 labels\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e關於 GKE 的先占虛擬機的完整細節，請見\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms\"\u003eGCP 官方文件\u003c/a\u003e。這份文件底下也提供了 GCP 官方建議的先占虛擬機最佳實踐\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e架構設計需要假設，部分或是全部的先占虛擬機都不可用的情形\u003c/li\u003e\n\u003cli\u003ePod 不一定有時間能優雅終止 (graceful shutdown)\u003c/li\u003e\n\u003cli\u003e同時使用隨選虛擬機與先占虛擬機，以維持先占虛擬機不可用時，服務依然可用\u003c/li\u003e\n\u003cli\u003e注意節點替換時的 IP 變更\u003c/li\u003e\n\u003cli\u003e避免使用有狀態的 Pod 在先占虛擬機上 (這點稍後的文章，我們會試圖超越)\u003c/li\u003e\n\u003cli\u003e使用 node taint 來協助排程到先占虛擬機，與非先占虛擬機\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e總之，由於有容器自動化管理，我們才能輕易的使用先占虛擬機。\u003c/p\u003e","title":"Gcp Preemptible Instance Kubernetes"},{"content":"先占虛擬機，技術文件二三事 第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。\nGoogle 先占虛擬機官方文件\n使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。\n以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要\n多作功課，仔細查閱官方文件，理解服務的規格 深入分析自身的需求 基於上面兩者，量化分析 什麼是先占虛擬機器(Preemptible Instance) 先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如\n目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用 目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go) 由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。\n先占機器的優缺點 除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點\n比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由 先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。\nGCP 不保證會有足夠的先占虛擬機 先占虛擬機不能直接轉換成普通虛擬機 資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉 先占機器排除在 GCP 的服務等級協議 (SLA)之外 先占虛擬機不適用GCP 免費額度 費用粗估試算 至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。\n以常用的 N1 standard 虛擬機：https://cloud.google.com/compute/vm-instance-pricing#n1_standard_machine_types\nHourly Machine type\tCPUs\tMemory\tPrice (USD)\tPreemptible price (USD) n1-standard-1\t1\t3.75GB\t$0.0550\t$0.0110 n1-standard-2\t2\t7.5GB\t$0.1100\t$0.0220 n1-standard-4\t4\t15GB\t$0.2200\t$0.0440 n1-standard-8\t8\t30GB\t$0.4400\t$0.0880 n1-standard-16\t16\t60GB\t$0.8800\t$0.1760 n1-standard-32\t32\t120GB\t$1.7600\t$0.3520 n1-standard-64\t64\t240GB\t$3.5200\t$0.7040\n如果是用 GPU 運算：https://cloud.google.com/compute/gpus-pricing\nModel\tGPUs\tGPU memory\tGPU price (USD)\tPreemptible GPU price (USD) NVIDIA® Tesla® T4\t1 GPU\t16 GB GDDR6\t$0.35 per GPU\t$0.11 per GPU NVIDIA® Tesla® V100\t1 GPU\t16 GB HBM2\t$2.48 per GPU\t$0.74 per GPU\n依據虛擬機規格的不同，先占虛擬機大約是隨選虛擬機價格的 2 到 3 折。在 AWS 與 Azure，由於計費方式不同，有可能拿到 1 折左右的浮動價格。從各種角度來說，都是非常高的折數。\n不妨說，這整系列文章，都是衝這著個折數來的 XD。畢竟成本是實實在在的花費，工作負載 (workload) 合適的話，應該盡量嘗試導入。\n這個折數還有另外一個效果是，可以在相同成本下，添增更多資源算力，作為解決方案。什麼意思呢？就是如果工作負載合適的話，可以使用更高規格的先占節點，整體成本反而會下降。\n至於究竟差多少，需要依據規格與定價詳細試算才知道。底下我們就來算算看。\n","permalink":"https://chechia.net/posts/2020-09-26-gcp-preemptible-instance-introduction/","summary":"\u003ch1 id=\"先占虛擬機技術文件二三事\"\u003e先占虛擬機，技術文件二三事\u003c/h1\u003e\n\u003cp\u003e第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.google.com/compute/docs/instances/preemptible\"\u003eGoogle 先占虛擬機官方文件\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。\u003c/p\u003e\n\u003cp\u003e以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e多作功課，仔細查閱官方文件，理解服務的規格\u003c/li\u003e\n\u003cli\u003e深入分析自身的需求\u003c/li\u003e\n\u003cli\u003e基於上面兩者，量化分析\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"什麼是先占虛擬機器preemptible-instance\"\u003e什麼是先占虛擬機器(Preemptible Instance)\u003c/h1\u003e\n\u003cp\u003e先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用\u003c/li\u003e\n\u003cli\u003e目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。\u003c/p\u003e\n\u003ch1 id=\"先占機器的優缺點\"\u003e先占機器的優缺點\u003c/h1\u003e\n\u003cp\u003e除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGCP 不保證會有足夠的先占虛擬機\u003c/li\u003e\n\u003cli\u003e先占虛擬機不能直接轉換成普通虛擬機\u003c/li\u003e\n\u003cli\u003e資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉\u003c/li\u003e\n\u003cli\u003e先占機器排除在 \u003ca href=\"https://cloud.google.com/compute/sla\"\u003eGCP 的服務等級協議 (SLA)\u003c/a\u003e之外\u003c/li\u003e\n\u003cli\u003e先占虛擬機不適用\u003ca href=\"https://cloud.google.com/free\"\u003eGCP 免費額度\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"費用粗估試算\"\u003e費用粗估試算\u003c/h1\u003e\n\u003cp\u003e至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。\u003c/p\u003e\n\u003cp\u003e以常用的 N1 standard 虛擬機：https://cloud.google.com/compute/vm-instance-pricing#n1_standard_machine_types\u003c/p\u003e\n\u003cp\u003eHourly\nMachine type\tCPUs\tMemory\tPrice (USD)\tPreemptible price (USD)\nn1-standard-1\t1\t3.75GB\t$0.0550\t\t$0.0110\nn1-standard-2\t2\t7.5GB\t$0.1100\t\t$0.0220\nn1-standard-4\t4\t15GB\t$0.2200\t\t$0.0440\nn1-standard-8\t8\t30GB\t$0.4400\t\t$0.0880\nn1-standard-16\t16\t60GB\t$0.8800\t\t$0.1760\nn1-standard-32\t32\t120GB\t$1.7600\t\t$0.3520\nn1-standard-64\t64\t240GB\t$3.5200\t\t$0.7040\u003c/p\u003e","title":"Gcp Preemptible Instance Introduction"},{"content":"關於資源評估 架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？\n以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。\n導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：\n每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載 如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載 此外還有機會因為上篇提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低 只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源 例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu 希望整體資源的使用率夠高，當然預留太多的資源會造成浪費 要控管 Kubernetes 的資源使用量也可設定資源需求與資源限制，延伸閱讀。\n估計得越準確，當然實際部署的資源掌握度就越高，然而筆者過去的經驗，團隊在交付源碼時未必就能夠做出有效的資源消耗評估，那有沒有什麼辦法可以幫助我們？\n資源需求估估看 如果應用開發團隊，有先作應用的 profiling，然後 release candidate 版本有在 staging 上作壓力測試的話，維運團隊這邊應該就取得的數據，做部署前的資源評估。\n應用在不同狀態或是工作階段，會消耗不同的資源\n例如：運算密集的 batch job 可能會有\n控制節點 (master node) 啟動後會佔有一定的資源，一般來說不會消耗太多，只是需要為控制節點優先保留資源 工作節點 (worker node) 啟動時會需要預留足夠的資源，接收工作後會逐漸增加資源使用，拉到滿載 例如：面向用戶的服務，可能會有\n啟動應用所需的資源 沒有大量請求，只維持基本應用運行所使用的資源 負載壓力灌進來時，消耗資源隨用戶請求數量的成長曲線，設定的安全上界 如果沒有這些數據，其實維運很難事前估計資源，變成要實際推上線後見招拆招，基於實際的資源消耗去做自動擴容，其實有可能會造成資源的浪費，因此我建議如果開發團隊沒有作 profiling，維運團隊可以在工作流程內簡單加一步 profiling，目前主流語言都有提供相關工具，簡單的執行就可以獲得很多資訊。\n至於壓力測試，也是可以使用基本的工具(例如 Artilery)簡單整到工作流程。特別是面對客戶的應用，務必要進行壓力測試。\n有了上述的資源需求數據，才能事先安排機器的規格。例如\n應用是面對客戶的 API server 基本資源是 200m cpu 1Gi memory，這部分直接寫進 Kubernetes resource request，在排程時就先結點上預留。 負載拉到 1,000 RPS，latency 95% 20ms 99% 30ms，這時的資源需求的上界大約 2 cpu 4 Gi memory 超過 1,000 RPS 就應該要透過水平擴展去增加更多 instamce 如果單跑這個 API server，就可以安排 memory 8，cpu 4 的 GKE node-pool，讓負載落在可用資源的 60%-70%，這樣還有餘裕可以承受大流量，給自動水平擴展做動的時間。\n資源調整參考 當然這些數據都可以依照時實際需求調整，資源要壓縮得更緊或更鬆都是可行的。\n如果應用有整合分析後台，例如 Real-time Uer Monitoring、或是基本的 Google Analytics，都可以觀察這些調整實際對用戶帶來的影響，用戶行為改變對公司營收的影響，全都可以量化。例如\n機器負載拉到 80%，cpu 的壓力，導致 API latency 增加到 95% 50ms 99% 100ms 此時用戶已經很有感了，會導致 0.1% 用戶跳轉離開 而這 0.1% 的用戶，以往的平均消費，換算成為公司營收，是 $1,000/month 把機器負載壓到 60%，只計算 cpu 的數量的話，需要多開 3 台 n1-standard-4 機器，共計 $337.26/month 提供老闆做參考，老闆可能會趨向加開機器 當然上面的例子都非常簡化，變成國中數學問題，這邊只是提供一個估算的例子。現實中的問題都會複雜百倍，例如機器規格拉上去出現新的瓶頸、例如依賴的服務，message queue、database 壓力上升，或是公司內部問題，就拿不到預算(血淚 SRE)。如果要減少機器，也可以參考，一般來說聽到關機器省錢的話，老闆都會接受的 XD。\n回到先占機器 根據上面的國中數學，把應用一個一個都計算清楚，需求逐漸明確了。假設，架構團隊拿到開機器的工單，掐指一算，決定\n1 GKE cluster 6 n1-standard-4 這些都是隨選虛擬機，價格大約是 $747/month (含集群管理費 $73/month)。今天有人腦動大開，那如果全部換成先占節點呢？變成 $265/month，虛擬機費用 $192 / $674 = 0.28 直接打超過三折。\n有人就擔心，這樣真的可以嗎？真的沒問題嗎？會不會影響用戶阿。\n答案是會，就是會影響用戶 XD\n聽到這邊很多人就怕了。但是怎麼個影響法呢，還需要看底下幾個段落，如果換成先占虛擬機，用戶會怎麼受到影響。我們也要試圖量化這個影響，當作要不要導入的判斷依據。\n句個反例，「我覺得可以」「你覺得不行」，或是「某某公司的某某團隊可以啊」「我們公司也來做吧」，這些都是很糟糕的理由。除了對內部毫無說服力之外，也沒有辦法作為導入成效的指標，會讓團隊陷入「導入了也不知道有比導入前好？」或是「具體導入後成效量化」，會影響團隊做出真正有效的判斷，應極力避免。\n此外，問行不行之前，其實需要知道團隊願意為了三折機器，付出多少成本。如果只是每月省個台幣 15,000，工程師薪水都超過了。但如果手下有三十台或三百台以上，也許就非常值得投資。成本不是絕對的，很多時候要與其他成本 (e.g. 開發人力時間成本) 一起考量。\n以上都是說明導入的動機，以下說明先占虛擬機的各種機制，以及對應用的實際影響。\n","permalink":"https://chechia.net/posts/2020-09-25-gcp-preemptible-instance-resource-calculation/","summary":"\u003ch3 id=\"關於資源評估\"\u003e關於資源評估\u003c/h3\u003e\n\u003cp\u003e架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？\u003c/p\u003e\n\u003cp\u003e以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。\u003c/p\u003e\n\u003cp\u003e導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載\n\u003cul\u003e\n\u003cli\u003e如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載\u003c/li\u003e\n\u003cli\u003e此外還有機會因為\u003ca href=\"\"\u003e上篇\u003c/a\u003e提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源\n\u003cul\u003e\n\u003cli\u003e例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e希望整體資源的使用率夠高，當然預留太多的資源會造成浪費\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e要控管 Kubernetes 的資源使用量也可設定\u003ca href=\"https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/\"\u003e資源需求與資源限制\u003c/a\u003e，延伸閱讀。\u003c/p\u003e\n\u003cp\u003e估計得越準確，當然實際部署的資源掌握度就越高，然而筆者過去的經驗，團隊在交付源碼時未必就能夠做出有效的資源消耗評估，那有沒有什麼辦法可以幫助我們？\u003c/p\u003e\n\u003ch3 id=\"資源需求估估看\"\u003e資源需求估估看\u003c/h3\u003e\n\u003cp\u003e如果應用開發團隊，有先作應用的 profiling，然後 release candidate 版本有在 staging 上作壓力測試的話，維運團隊這邊應該就取得的數據，做部署前的資源評估。\u003c/p\u003e\n\u003cp\u003e應用在不同狀態或是工作階段，會消耗不同的資源\u003c/p\u003e\n\u003cp\u003e例如：運算密集的 batch job 可能會有\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e控制節點 (master node) 啟動後會佔有一定的資源，一般來說不會消耗太多，只是需要為控制節點優先保留資源\u003c/li\u003e\n\u003cli\u003e工作節點 (worker node) 啟動時會需要預留足夠的資源，接收工作後會逐漸增加資源使用，拉到滿載\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e例如：面向用戶的服務，可能會有\u003c/p\u003e","title":"Gcp Preemptible Instance Resource Calculation"},{"content":"需求規劃 使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 Google Cloud Pricing Calculator 精算成本。\n另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。\n這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。\n所以這邊先定義我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 Batch Job 常見的範例，例如\n使用網路爬蟲 (crawler) 去抓取許多網站的所有內容 使用 GPU 進行機器學習的 Model Training 大數據計算 MapReduce 這些任務的核心需求，很簡單直接\n盡快完成整體工作 盡可能節省大量算力成本 例如：我手上的機器學習 Model 粗略估計 10000 小時*GPU 的算力需求，才能產出一個有效的Model。由於大量的算力需求，一般來說都會選擇分散式的運算框架 (ex. MapReduce) ，將真正消耗算力的工作，使用分而化之 (divide and conquer) 的架構設計，將分配任務的控制節點 (master)，與實際進行運算的工作節點(worker) 拆分。基於原本的分散式架構，幾乎可以無痛地將工作節點轉移到先占虛擬機上。\n根據上述的需求，這類的工作特性可能有\nCPU / GPU 算力需求高的運算節點 (Worker) Worker 本身是無狀態的 Stateless 可控的即時負載 將整體工作切分成任務單元 (task)，分配給工作節點 任務單元的狀態外部保留，工作節點可容錯 (fault-tolerent)，任務單元可復原 由於先占虛擬機可能是浮動價格，這類工作可以根據優先程度，調整合適的工作時間，例如在資料中心算力需求低，先占虛擬機的費用低廉時，啟用較多的工作節點加快運算，如果費用高時，可以降低先占虛擬機的使用，延後工作，甚至是調用不同區域，費用低的工作節點，來降低整體的成本。\n執得注意的是，這類任務的控制節點 (master)，也許是集中式的，也許是分散式的，需要根據性質考量，是否適合放在先占虛擬機上。有些架構控制節點可以容錯，然而錯誤發生後會需要復原狀態，這時會消耗額外的算力，可能會拖緩整體進度，造成算力的消耗。也許就可以考量使用隨選虛擬機配合使用。\nUser-facing services 常見的範例，例如\nRestful API server Websocket Server TCP/HTTP reverse proxy 這些工作的核心需求如下：\n整題服務的高可用性 (high availability) 承受不可預期的負載高峰 (load spikes) 整體表現需要低延遲 (low latency) 可以水平擴展 (horizontal scaling)，支撐用戶的成長 最終平衡效能呈現與成本 由於會面對使用者，需要能支持使用者的壓力，又同時需要有一定的服務效能，來維持使用者體驗。實務上設計可能採用無狀態應用 (stateless)，多副本 (replica) 部署到集群中。需要儲存的狀態（如用戶狀態），使用外部的共用儲存 (例如：Redis，RDBMS，或是 Non-SQL DB)。請求的流量，透過上游的負載均衡器 (Load Balancer)，送進多個後端，處理完成請求後，再返回給使用者。\n這樣的設計，使用先占虛擬機也不會有太多的問題\n現代的附載均衡器多半都能像後端做可用性檢測 (health check)，可以把流量導向工作正常的節點，如果後端的虛擬機被資料中心收回了，流量也會移轉到其他節點上，不會遺失用戶請求 配合自動水平拓展工具 (Auto horizontal scaler)，可以設定期望的服務節點數量，如果資料中心回收先占節點，拓展工具可以同時去取得新的先占節點，或是取得隨選隨用的虛擬機 配合流量監測，也可以動態調整期望的服務節點數量，例如：偵測到大量用戶連線數時，增加更多服務節點，待流量下降後，再降低服務節點數量 這樣的設計實務上有幾點注意\n雖然說應用後端本身是無狀態的，但面對用戶也許還是會有部分狀態存在應用外部，例如：User session，或是 websocket 的長連線。特別注意這些服務斷線的時候，對於使用者的影響，配合前端增強使用者體驗 後端水平拓展後，瓶頸會轉移到其他地方，例如 Database 成為效能瓶頸，應用這邊需要做一定程度的自律( ex. connection limit，rate limit)，避免不斷增長的應用壓垮依賴的服務，如 MessageQueue 或是 Database 分散式的儲存中心 (distributed DB)，如：cassandra 或是小強 DB。而這樣類型的服務，是否適合放在先占節點上？\n","permalink":"https://chechia.net/posts/2020-09-24-gcp-preemptible-instance-requirement/","summary":"\u003ch1 id=\"需求規劃\"\u003e需求規劃\u003c/h1\u003e\n\u003cp\u003e使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 \u003ca href=\"https://cloud.google.com/products/calculator?hl=zh-tw\"\u003eGoogle Cloud Pricing Calculator\u003c/a\u003e 精算成本。\u003c/p\u003e\n\u003cp\u003e另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。\u003c/p\u003e\n\u003cp\u003e這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。\u003c/p\u003e\n\u003cp\u003e所以這邊先定義我們以下幾個需求：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e執行短期的 batch job\u003c/li\u003e\n\u003cli\u003e執行長期的 user-facing API server\u003c/li\u003e\n\u003cli\u003e執行長期的 stateful 資料庫、儲存庫\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"batch-job\"\u003eBatch Job\u003c/h3\u003e\n\u003cp\u003e常見的範例，例如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e使用網路爬蟲 (crawler) 去抓取許多網站的所有內容\u003c/li\u003e\n\u003cli\u003e使用 GPU 進行機器學習的 Model Training\u003c/li\u003e\n\u003cli\u003e大數據計算\u003c/li\u003e\n\u003cli\u003eMapReduce\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e這些任務的核心需求，很簡單直接\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e盡快完成整體工作\u003c/li\u003e\n\u003cli\u003e盡可能節省大量算力成本\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e例如：我手上的機器學習 Model 粗略估計 10000 小時*GPU 的算力需求，才能產出一個有效的Model。由於大量的算力需求，一般來說都會選擇分散式的運算框架 (ex. MapReduce) ，將真正消耗算力的工作，使用分而化之 (divide and conquer) 的架構設計，將分配任務的控制節點 (master)，與實際進行運算的工作節點(worker) 拆分。基於原本的分散式架構，幾乎可以無痛地將工作節點轉移到先占虛擬機上。\u003c/p\u003e\n\u003cp\u003e根據上述的需求，這類的工作特性可能有\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCPU / GPU 算力需求高的運算節點 (Worker)\u003c/li\u003e\n\u003cli\u003eWorker 本身是無狀態的 Stateless\u003c/li\u003e\n\u003cli\u003e可控的即時負載\u003c/li\u003e\n\u003cli\u003e將整體工作切分成任務單元 (task)，分配給工作節點\u003c/li\u003e\n\u003cli\u003e任務單元的狀態外部保留，工作節點可容錯 (fault-tolerent)，任務單元可復原\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於先占虛擬機可能是浮動價格，這類工作可以根據優先程度，調整合適的工作時間，例如在資料中心算力需求低，先占虛擬機的費用低廉時，啟用較多的工作節點加快運算，如果費用高時，可以降低先占虛擬機的使用，延後工作，甚至是調用不同區域，費用低的工作節點，來降低整體的成本。\u003c/p\u003e\n\u003cp\u003e執得注意的是，這類任務的控制節點 (master)，也許是集中式的，也許是分散式的，需要根據性質考量，是否適合放在先占虛擬機上。有些架構控制節點可以容錯，然而錯誤發生後會需要復原狀態，這時會消耗額外的算力，可能會拖緩整體進度，造成算力的消耗。也許就可以考量使用隨選虛擬機配合使用。\u003c/p\u003e\n\u003ch3 id=\"user-facing-services\"\u003eUser-facing services\u003c/h3\u003e\n\u003cp\u003e常見的範例，例如\u003c/p\u003e","title":"Gcp Preemptible Instance Requirement"},{"content":"先占虛擬機終止流程 (Preemption process) 子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。\n這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。\n如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如\n應用重啟初始化時 cpu memory 突然拉高 承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance \u0026hellip;等等 如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。\n這邊有幾個面向要注意\nGCP 如何終止先占節點 GCP 移除節點對 GKE 、以及執行中應用的影響 GKE 集群如何應對的節點失效 GCP 自動調度補足新的先占節點 GKE 集群如何應對節點補足 三個重點\n先占虛擬機終止對集群的影響 Pod 隨之終止對應用的影響，是否能夠優雅終止 有沒有方法可以避免上面兩者的影響 劇透一下：有的，有一些招式可以處理。讓我們繼續看下去。\nGCP 如何終止虛擬機 先占虛擬機的硬體終止步驟與一般隨選虛擬機相同，所以我們要先理解虛擬機的停止流程\n這裡指的終止 (Stop) 是虛擬機生命週期 的 RUNNING -\u0026gt; instances.stop() -\u0026gt; STOPPING -\u0026gt; TERMINATED 的步驟。\ninstances.stop() ACPI shutdown OS 會進行 shutdown 流程，並嘗試執行各個服務的終止流程，以安全的終止服務。如果虛擬機有設定Shtudown Script 會在這步驟處理 等待至少 90 秒，讓 OS 完成終止的流程 逾時的終止流程，GCP 會直接強制終止，就算 shutdown script 還沒跑完 GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內 虛擬機變成 TERMINATED 狀態 GCP 如何終止先占虛擬機 與隨選虛擬機不同\n先占虛擬機的時間 30 秒 搭配 GKE 使用 Managed Instance Group，終止的虛擬機會被刪除，Autoscaler 會啟動新的虛擬機 一樣先看先占虛擬機的說明文件：終止流程\n資料中心開始回收先占虛擬機，選中我們專案其中的一台先占虛擬機 Compute Engine 傳送 ACPI G2 Soft Off，這裡 OS 會試圖安全關變服務，也會執行 shutdown script，可以做簡短的優雅終止 30秒後，ACPI G3 Mechanical off 但 30 秒能做什麼？只能快速的交代當前進度。如果應用需要花時間收尾，保存工作進度，可能會產生許多問題\nGCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內 在面對大量 IO 的工作，可能會導致者台虛擬機的大量應用一起進入優雅終止，先占虛擬機最後耗盡資源，來不及做完 如果可能會超時，或是沒完成會有資料遺失風險，就不能在這個階段處理 依賴 shutdown script 做收尾是危險的，我們之後要想辦法處理這個不保證做完的優雅終止。\n如果應用本身有容錯的框架，或是有容錯機制，我們這邊要額外做的工作就會少很多。例如許多程式框架提供自動重啟的功能，在外部保存 checkpoint，worker 只負責運算，終止信號一進來，也不用保存，直接拋棄未完成的工作進度，留待繼起的 worker 從 checkpoint 接手。\nPreemption selection 除了 24 小時的壽命限制會終止虛擬機，資料中心的事件也會觸發主動的虛擬機回收，由 GCP 主動觸發的回收機制機率很低，會根據每日每個區域 (zone) 的狀態而定。這裡描述資料中心啟動的臨時回收。\nGCP 不會把所你手上的 preemptible 機器都收走，而是依照一定的規則，選擇一個比例撤換的機器。\n先看文件敘述\nCompute Engine 會避免從單一客戶移除太多先占虛擬機 優先移除新的虛擬機，偏向保留舊的虛擬機 (但最多仍活不過 24hr) 開啟後馬上被移除的先占虛擬機不計費 機器尺寸較小的機器，可用性較高。例如：16 cpu 的先占虛擬機，比 128 cpu 的先占虛擬機容易取得 GCP 每天平均會移除一個專案中 5% - 15% 的虛擬機，從用戶的角度我們需要預期至少這個程度的回收。不過這個比例，GCP 也不給予任何保證。以筆者經驗，只能說絕大部分的時候，都不會擔心超過這個比例的回收，但是還是要做好最壞的打算，如果臨時無法取得足夠的先占虛擬機，要有方法暫時補足隨選虛擬機。\nGKE 使用先占虛擬機會違反 Kubernetes 的設計\nPod grace period 會被忽略 Pod disreuption budget，不會被遵守 (可能會超過) 對應用的影響 GCP 觸發的 Preemptible process，對應用的影響\ninvoluntary disruptions，GCP 送 ACPI G2 Soft Off OS 終止服務，包含執行 Kubernetes 的 container runtime 容器內的應用會收到 SIGTERM，啟動 graceful shutdown Kubernetes 提供的 Graceful-shutdown 可能會跑不完 實務上只是中斷當前工作 https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters\n大量節點同時回收 由於 GCP 並不保證收回的機器的數量，同時回收的機器量大，還是會衝擊到服務。例如：一次收回 15% 的算力，當然服務還是會受到衝擊。當然這樣事件的機率並不高，但我們仍是需要為此打算。這邊有幾個做法\n預留更多的算力 使用 regional cluster，在多個 zone 上分配先占虛擬機 我們自行控制，提前主動回收虛擬機 預留更多資源 這點很直觀，由於使用了更加便宜的機器，我們可以用同樣的成本，開更多的機器。\n退一步說，使用打三折的先占虛擬機，然後開原本兩倍的機器數量\n總成本是 0.3 * 2 = 0.6 倍 同時間可用資源是 2 倍 由於先占節點回收的單位是一個一個虛擬機\n安排合適的機器尺寸 尺寸較小的先占虛擬機，可用性較高。意思是零碎的先占虛擬機容易取得 但當然也不能都開太小的機器，這會嚴重影響應用的分配。至於具體需要開多大，可以根據預計在機器上運行的應用，做綜合考量，例如有以下影用需要執行：\napp A: 1 cpu 5 replica app B: 3 cpu 5 replica app C: 5 cpu 5 replica 總共至少 45 cpu ，預期機器負載8 成的話，需要總共 45 / 0.8 = 56 cpu。也許可以考慮\n8 cpu * 7 先占虛擬機 4 cpu * 14 也舉幾個極端不可行的例子\n56 cpu * 1 這樣的虛擬機回收時的影響範圍 (blast radius) 就是 100% 服務 1 cpu * 56 機器太瑣碎，可能超出 Qouta (節點數量，IP 數量\u0026hellip;) 應用會更分散，節點間的內部網路流量會增加 前幾篇提到的 reserved resource 比例高，會影響應用的部署 如果希望更保險，可以再補上隨選虛擬機混合搭配，例如\n8 cpu * 7 5 先占虛擬機 2 隨選虛擬機 4 cpu * 14 10 先占虛擬機 4 隨選虛擬機 虛擬機區域 虛擬機的回收觸發，也是會依據服務的區域 (zone) 回收。意思是節點回收不會同時觸發 asia-east1 中所有 zone 的節點回收，一般來說時間是錯開的 (不過GCP 也不保證這點 XD)。為了維持 GKE 的可用性，我們都會開多個 node-pool 在多個區域下。\n總之避免把機器都放在同個區域中。\n自行控制的虛擬機汰換 簡單來說我們在 24 hr 期限之前，先分批自盡 XD，打散個各個虛擬機的 24 小時限制。\n使用這個有趣的工具 estafette-gke-preemptible-killer，自動汰換先占虛擬機，讓整個繼起虛擬機都分散在 24 小時間。\nestafette-gke-preemptible-killer ，使用上簡單，大家自己看著辦 XD。如果大家有興趣，留言的人多的話，我再另外開一篇細講。\n小結 為了使用先占虛擬機，我們要多做以下幾件事\n為應用設計可容錯分散式架構，例如應用可以同時執行一樣的 API server 3 個 replica 分散 Pod 到合適的機器上，例如設置 PodAntiAffinity 設定合適的虛擬機大小，合適的分散應用 使用 estafette-gke-preemptible-killer，自動汰換先占虛擬機 不依賴應用的 Graceful-shutdown 流程 ","permalink":"https://chechia.net/posts/2020-09-23-gcp-preemptible-instance-speficication/","summary":"\u003ch1 id=\"先占虛擬機終止流程-preemption-process\"\u003e先占虛擬機終止流程 (Preemption process)\u003c/h1\u003e\n\u003cp\u003e子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。\u003c/p\u003e\n\u003cp\u003e這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。\u003c/p\u003e\n\u003cp\u003e如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e應用重啟初始化時 cpu memory 突然拉高\u003c/li\u003e\n\u003cli\u003e承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance \u0026hellip;等等\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。\u003c/p\u003e\n\u003cp\u003e這邊有幾個面向要注意\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGCP 如何終止先占節點\u003c/li\u003e\n\u003cli\u003eGCP 移除節點對 GKE 、以及執行中應用的影響\u003c/li\u003e\n\u003cli\u003eGKE 集群如何應對的節點失效\u003c/li\u003e\n\u003cli\u003eGCP 自動調度補足新的先占節點\u003c/li\u003e\n\u003cli\u003eGKE 集群如何應對節點補足\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e三個重點\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e先占虛擬機終止對集群的影響\u003c/li\u003e\n\u003cli\u003ePod 隨之終止對應用的影響，是否能夠優雅終止\u003c/li\u003e\n\u003cli\u003e有沒有方法可以避免上面兩者的影響\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e劇透一下：有的，有一些招式可以處理。讓我們繼續看下去。\u003c/p\u003e\n\u003ch3 id=\"gcp-如何終止虛擬機\"\u003eGCP 如何終止虛擬機\u003c/h3\u003e\n\u003cp\u003e先占虛擬機的硬體終止步驟與一般隨選虛擬機相同，所以我們要先理解\u003ca href=\"https://cloud.google.com/compute/docs/instances/stop-start-instance\"\u003e虛擬機的停止流程\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e這裡指的終止 (Stop) 是\u003ca href=\"https://cloud.google.com/compute/docs/instances/instance-life-cycle\"\u003e虛擬機生命週期\u003c/a\u003e 的 RUNNING -\u0026gt; instances.stop() -\u0026gt; STOPPING -\u0026gt; TERMINATED 的步驟。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einstances.stop()\u003c/li\u003e\n\u003cli\u003eACPI shutdown\u003c/li\u003e\n\u003cli\u003eOS 會進行 shutdown 流程，並嘗試執行各個服務的終止流程，以安全的終止服務。如果虛擬機有設定\u003ca href=\"https://cloud.google.com/compute/docs/shutdownscript\"\u003eShtudown Script\u003c/a\u003e 會在這步驟處理\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://cloud.google.com/compute/docs/instances/deleting-instance\"\u003e等待至少 90 秒\u003c/a\u003e，讓 OS 完成終止的流程\n\u003cul\u003e\n\u003cli\u003e逾時的終止流程，GCP 會直接強制終止，就算 shutdown script 還沒跑完\u003c/li\u003e\n\u003cli\u003eGCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e虛擬機變成 TERMINATED 狀態\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"gcp-如何終止先占虛擬機\"\u003eGCP 如何終止先占虛擬機\u003c/h3\u003e\n\u003cp\u003e與隨選虛擬機不同\u003c/p\u003e","title":"Gcp Preemptible Instance Speficication"},{"content":"我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 該不該在 Kubernetes 上面跑 database？\nTL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的\n等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。\n應用是否適合部署到 Kubernetes 上，可以看這篇 Google Blog: To run or not to run a database on Kubernetes: What to consider，如果大家有興趣，再留言告訴我，我再進行中文翻譯。\n文中針對三個可能的方案做分析，以 MySQL 為例：\nSass，GCP 的 Cloud SQL 最低的管理維運成本 自架 MySQL 在 GCP 的 VM 上，自行管理 自負完全的管理責任，包含可用性，備份 (backup)，以及容錯移轉 (failover) 自架 MySQL 在 Kubernetes 上 自負完全的管理責任 Kubernetes 的複雜抽象層，會加重維運工作的複雜程度 然而 RDBMS 的提供商，自家也提供 Operator\nOracle 自家提供的 MySQL Operator CrunchyData 也有提供的 Postgres Operator 你就想，所以這些人是想怎樣，RDBMS 放 Kubernetes 上到底是行不行 XD。Google 的文章說明：如果應用本身並不符合 Kubernetes 的工作流程 (Pod life-cycle)，可以透過上述的 Operator 來自動化許多維運的作業，降低維運的困難。\n然而 DB 有千百種，除了 RDBMS 以外，還有另外一批 Database 天生就具有分散式的架構，這些儲存庫部署到 Kubernetes 上，並不會太痛苦 (還是要付出一定的成本XD)，但是卻可以得益於 Kubernetes 的諸多功能。\n底下我們先根據分散式的儲存庫做概觀描述，本系列文的最後，會根據時間狀況，做實例分享：Cassandra 或是 CockroachDB。提供各位一點發想，並根據需求去選擇需要的儲存庫\n「行不行要問你自己了施主，技術上都可以，維運上要看看你的團隊有沒有那個屁股吃這份藥 XD」\nDistributed Database 底下非常粗淺的簡介分散式儲存庫的概念，提供一個基準點，幫助接下來討論是否可以使用先占虛擬機。這邊要強調，儲存庫的類型千百種，底層的各種實作差異都非常大，底下的模型是基於 cassandra 但不會走太多細節。 cassandra 的規格有機會再細聊。\n當後端應用已經順利水平拓展之後，整體服務的效能瓶頸往往都壓在後端 DB 上。這些不同的 DB 面向不同的需求，當需求符合時，可以考慮使用這些解決方法。\n這邊要強調，不是放棄現有的 RDBMS ，完全移轉到新的資料庫，這樣的成本太高，也沒有必要性。更好的做法，是搭配既有的關聯性資料庫，將不是核心業務的資料處理抽出，移轉到合適的資料庫上。讓不同需求的資料儲存到更合適的儲存庫，是這段話要強調的重點，關連式資料庫也不是唯一選擇。\n分散式的資料庫有以下特徵\n分散式節點集群 (Cluster)：資料庫是多個節點共存，而非 single master, multiple slaves 的架構 配合共識算法 (consensus algorithm) 溝通節點之間的資訊 無單點錯誤 (Single-point failure)：e.g. 不會因為 master 錯誤導致整個服務失效 高可用(High Availitility：可以承受集群中一定數量虛擬機故障，服務仍然可用 資料 sharding 到不同節點上 複本 (replica) 節點複本 (node replicas)：多個節點提供服務，提供流量的帶寬與可用性 資料複本 (data replicas)：在多個節點上儲存資料，提供資料的備份，同時也提供讀取帶寬與可用性 從以上特徵來說，使用此架構的服務可以承受先占虛擬機的不定時終止，或許可以使用。\n實務上有非常多需要注意，需要依據各自服務的性質，各自處理。常見的問題舉例如下：\n應用可以容錯 (fault-tolerent)，然而錯誤發生後，會需要消耗復原成本，例如重啟後需要花時間初始化，或是在多節點上進行 data rebalance。 可以承受突然的錯誤，使用先占虛擬機，變成每日固定會承受必然的錯誤。這裡犧牲了部分算力，甚至造成隱性的維護成本，最後是否符合節省成本的需求。 都是需要仔細了解解決方案，並且分析需求，來評估是否有合乎成本。\nGKE 以上分析了三種常見需求例子：從 batch job，user-facing service，與 distributed database。明天會實際搬出 GKE 與 GCP Preemptible Instance 的技術規格，與大家實際討論。\n","permalink":"https://chechia.net/posts/2020-09-22-gcp-preemptible-instance-requirement-distributed/","summary":"\u003cp\u003e我們以下幾個需求：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e執行短期的 batch job\u003c/li\u003e\n\u003cli\u003e執行長期的 user-facing API server\u003c/li\u003e\n\u003cli\u003e執行長期的 stateful 資料庫、儲存庫\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e該不該在 Kubernetes 上面跑 database？\u003c/p\u003e\n\u003cp\u003eTL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的\u003c/p\u003e\n\u003cp\u003e等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。\u003c/p\u003e\n\u003cp\u003e應用是否適合部署到 Kubernetes 上，可以看這篇 \u003ca href=\"https://cloud.google.com/blog/products/databases/to-run-or-not-to-run-a-database-on-kubernetes-what-to-consider\"\u003eGoogle Blog: To run or not to run a database on Kubernetes: What to consider\u003c/a\u003e，如果大家有興趣，再留言告訴我，我再進行中文翻譯。\u003c/p\u003e\n\u003cp\u003e文中針對三個可能的方案做分析，以 MySQL 為例：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSass，GCP 的 Cloud SQL\n\u003cul\u003e\n\u003cli\u003e最低的管理維運成本\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e自架 MySQL 在 GCP 的 VM 上，自行管理\n\u003cul\u003e\n\u003cli\u003e自負完全的管理責任，包含可用性，備份 (backup)，以及容錯移轉 (failover)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e自架 MySQL 在 Kubernetes 上\n\u003cul\u003e\n\u003cli\u003e自負完全的管理責任\u003c/li\u003e\n\u003cli\u003eKubernetes 的複雜抽象層，會加重維運工作的複雜程度\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e然而 RDBMS 的提供商，自家也提供 Operator\u003c/p\u003e","title":"Gcp Preemptible Instance Requirement Distributed"},{"content":"前言 鐵人賽的第二部分，要帶來公有雲省錢系列文章。\n架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。\n一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。\nKubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。\n「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。\n這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。\n這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。\n先占虛擬機 TL;DR 先占虛擬機為隨選虛擬機定價的 2-3 折，使用先占虛擬機可能可以節省 7 成的雲平台支出 先占虛擬機比起隨選虛擬機，外加有諸多限制，e.g. 最長壽命 24 hr、雲平台會主動終止先占虛擬機\u0026hellip;等 配合使用自動水平擴展 (auto-scaler)，讓舊的先占虛擬機回收的同時，去購買新的先占虛擬機 配合可容錯 (fault-tolerent) 的分散式應用，讓應用可以無痛在虛擬機切換轉移，不影響服務 要讓應用可以容錯，需要做非常多事情 搭配 kubernetes ，自動化管理來簡化工作 配合正確的設定，可以穩定的執行有狀態的分散式資料庫或儲存庫 或是看 Google 官方 Blog：Cutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs\n預計內容\n需求假設、釐清需求，並且精準計價 精準計價使用先占虛擬機的節省成本 先占虛擬機的規格、額外限制 額外限制，造成技術要多做很多額外的事情 實務經驗分享：API server 實務經驗：從使用隨選虛擬機，移轉到先占虛擬機，公司實際導入經驗 實務經驗：Elasticsearch 實務經驗分享：其他分散式資料庫，也許是 Cassandra 或是 cockroachDB 上面的內容不曉得會寫幾篇看感覺 XD\n有寫過鐵人賽的都知道 30 篇真的很漫長，一篇文章幾千字，都要花好幾個小時。我去年後半，真的都會看讀者的留言跟按讚，取暖一波，才有動力繼續寫。留言的人多就會多寫，留言的人少就會少寫，各位覺得文章還看得下去，請務必來我粉專按讚留個言，不管是推推、鞭鞭、或是有想看的文章來許願，都十分歡迎。有你們的支持，我才有動力繼續寫。\n請大家務必以實際行動支持好文章，不要讓劣幣驅逐良幣。不然 iThome 上面之後只剩洗觀看數的熱門文章了 XD\n當然，沒人留言我就會當作自己才是垃圾文 (自知之明XD)，就會收一收回家嚕貓睡覺，掰掰~\n-\u0026gt;我的粉專，等你來留言\n","permalink":"https://chechia.net/posts/2020-09-21-gcp-preemptible-instance/","summary":"\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e鐵人賽的第二部分，要帶來公有雲省錢系列文章。\u003c/p\u003e\n\u003cp\u003e架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。\u003c/p\u003e\n\u003cp\u003e一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。\u003c/p\u003e\n\u003cp\u003eKubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。\u003c/p\u003e\n\u003cp\u003e「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。\u003c/p\u003e\n\u003cp\u003e這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。\u003c/p\u003e\n\u003cp\u003e這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。\u003c/p\u003e\n\u003ch1 id=\"先占虛擬機-tldr\"\u003e先占虛擬機 TL;DR\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e先占虛擬機為隨選虛擬機定價的 2-3 折，使用先占虛擬機可能可以節省 7 成的雲平台支出\u003c/li\u003e\n\u003cli\u003e先占虛擬機比起隨選虛擬機，外加有諸多限制，e.g. 最長壽命 24 hr、雲平台會主動終止先占虛擬機\u0026hellip;等\u003c/li\u003e\n\u003cli\u003e配合使用自動水平擴展 (auto-scaler)，讓舊的先占虛擬機回收的同時，去購買新的先占虛擬機\u003c/li\u003e\n\u003cli\u003e配合可容錯 (fault-tolerent) 的分散式應用，讓應用可以無痛在虛擬機切換轉移，不影響服務\u003c/li\u003e\n\u003cli\u003e要讓應用可以容錯，需要做非常多事情\u003c/li\u003e\n\u003cli\u003e搭配 kubernetes ，自動化管理來簡化工作\u003c/li\u003e\n\u003cli\u003e配合正確的設定，可以穩定的執行有狀態的分散式資料庫或儲存庫\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e或是看 Google 官方 Blog：\u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms\"\u003eCutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e預計內容\u003c/p\u003e","title":"Gcp Preemptible Instance"},{"content":"前言 這是原文完整版本。太長不讀 (TL;DR) 請見Borg Omega and Kubernetes 前世今生摘要\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n摘要 在 container 技術夯起來前，Google 已經做了 container 十幾年，過程中發展出需三套容器管理系統。雖然每一代系統的開發需求不同，但每一代都深受上一代影響。這篇文章描述 Google 開發這些系統時，學到的經驗。\n第一套 container management 系統是 Borg，為了管理 1. 長期執行的服務 2. 批次的短期工作 (batch job)，原本分別是由 Babysitter 與 Global Work Queue 兩套系統分開管理。後者的架構深刻影響 Borg，但 Global Work Queue 專注於 batch job。兩套系統都在 Linux control groups 之前。Borg 將上述兩種應用放在共享的機器上，來增加資源的使用率，以節省成本。這種共享基於支援 container 的 Linux Kernel (Google 也貢獻許多 Linux kernel container 程式碼)，提供更好的隔離 (isolation) 給延遲敏感的使用者服務 (latency-sentitive user-facing services)，以及消耗大量 cpu 的 batch 程式。\n越來越多應用都在 Borg 上開發執行， Google 的應用與 infratructure 團隊開發許多工具與服務，形成 Borg 生態系。這些系統提供設定 (configure) 與更新 (update) 工作、預測資源需求、動態推送設定到執行中的工作、服務發現 (service discovery) 與負載均衡 (Load balancing)，等等功能。這些生態系的開發基於 Google 不同團隊的需求，產生一個不同起源 (heterogeneous)、只針對各別需求的 (ad-hoc) 一個堆不同系統，Borg 的使用者需要使用不同的程式語言與程序，來與這些系統互動。Borg 仍然是 Google 主要的容器管理系統，因為他規模 (scale) 巨大，功能多樣，而且極度堅固 (robustness)。\nOmega 是 Borg 的下一代，目的是改善 Borg 生態系的軟體工程。Omega 承襲許多 Borg 測試成功的模式，但不同於 Borg，Omega 有完整的架構設計，整體更加一致。Omega 將集群狀態 (cluster state) 存放在中心化 (centralized)、基於 Paxos 算法、交易導向 (transaction-oriented) 的儲存系統，讓集群的控制面板 (control panel) 存取，例如 scheduler。Omega 使用樂觀的併發控制 (optimistic concurrency control) 來處理偶發的衝突，這一層解藕 (decoupling) 的設計，使得原先的 Borgmaster 的功能可以拆分成多個元件，取代原本單一 (monolithic) 集中 (centralized) 的 master，被所有變更請求堵塞。許多 Omage 成功的創新也會被迭代回去 Borg 中。\n第三套 Google 開發的容器管理系統是 Kubernetes，這時外界工程師也開始對 Linux 容器有興趣，而 Google 同時在開發並推展自己的公有雲架構。Kubernetes 在這樣的背景下構思並開發。與 Borg 及 Omega 不同，Kubernetes 是開源軟體，不限於 Google 內部開發。Kubernetes 內部有共享的持久層儲存 (persistent store)，服務元件持續監測有關物件，與 Omega 類似。不同的是，Omega 允許信任的控制面板的元件直接存取儲存庫，Kubernetes 則透過 domain-specific 的 REST API 存取，來提供高階 (higher-level) 的 API 版本控制、驗證、語意處理 (semantics)、以及存取政策 (policy)，來支援更廣泛的用戶端。更重要的是 Kubernetes 著重工程師在 cluster 上開發與執行應用的體驗，簡化複雜分散式系統 (distributed system) 的管理與部屬 (deploy)，同時仍能透過容器來提升資源的使用率。\n這篇文章描述 Google 從 Borg 到 Kubernetes 獲得的知識與經驗。\n容器 (Containers) 回顧歷史，第一個容器只提供 root file system (透過 chroot)。FreeBSD jail 進一步延伸出額外的命名空間 (namespace) 例如 process ID。Solaris 大幅地探索相關的新功能。Linux control groups (cgroups) 吸收這些想法，直到今日仍持續開發。\n容器提供資源隔離 (resource isolation)，Google 得以大幅提升資源使用率 (utilization) 超出當時產業平均值，例如 Borg 使用容器，將批次暫時工作、與面對用戶需要注意延遲的應用，兩者放在同樣的物理機器上。用戶應用需要預留更多額外的資源，來處理突然產生的負載高峰 (load spikes) 以及錯誤處理 (fail-over) ，這些預留的資源常常都不會用到，可以轉讓批次工作使用。容器提供的資源管理工具實現此類需求，kernel-level 的資源隔離也確保程序之間不會互相干擾。Google 開發 Borg 中，同時也提交新功能給 Linux 容器，來滿足上述的需求。然而目前的隔離並不完整，如果 Linux kernel 不管理的資源，容器自然也無法格理，例如 level-3 的處理器快取 (level-3 processor cache) 與記憶體帶寬 (memory bandwith)，容器還需要增加一層安全保護層 (例如虛擬機器 Virtual Machine) 才能對付公有雲上出現的惡意行為。\n現代的容器不只提供隔離機制，更提供映像檔 (image)，在這個檔案上建構容器的應用。Google 使用 Midas Package Manager (MPM) 來建構並部屬容器映像檔，隔離機制與 MPM package 的關係，可以對比 Docker daemon 與 Docker image registry。這個章節描述的「容器」同時包含兩個概念：隔離、映像檔。\n應用導向的架構 (Application-oriented infrastructure) 隨著時間證明，容器不只能提供高階的資源使用率，容器化 (containerization) 使資料中心 (data center) 從原本機器導向 (machine-oriented) 變成應用導向 (application-oriented)，這個段落提供兩個例子：\n容器封裝 (encapsulate) 應用的環境 (environment)，在機器與作業系統的細節上，增加一層抽象層，解藕兩件事情：應用開發、部屬到架構。 良好設計的容器與映像檔只專注在單一個應用，管理容器意味管理應用，而不是管理機器。這點差異使得管理的 API，從機器導向變成影用導向，大幅度的改善應用的部屬與監控。 應用的環境 (Application environment) 原本 kernel 內的 cgroup、chroot、與命名空間，是用來保護應用不被旁邊其他應用的雜訊影響。將這些工具與容器映像檔一起使用，產生一層抽象層，分離應用、與底下的 (各種不同家的) 作業系統。解藕映像檔與作業系統，進一步提供相同的環境給開發環境 (development) 與生產環境 (production)，降低環境間的不一致性造成的問題，最終提升開發的可靠程度、並加速開發的流程。\n建構這層抽象的關鍵是，使用密閉的容器映象檔 (hermetic container image) ，將所有應用有關的依賴 (dependencies) 都打包，整包部屬到容器中。正確執行的話，容器對外部的依賴只剩下 Linux kernel 系統調用介面 (system-call interface)。這層介面大幅改善映象檔的部屬方便性 (protability)，但目前機制仍不完美，應用仍然暴露在某些作業系統的介面上，特別是 socket options、/proc、以及 ioctl 的調用參數。Google 希望透過推廣開放容器倡議 (Open Container Initiative) 來清楚界定，上述介面與容器的抽象層。\n次外，容器提供的隔離與最小依賴，在 Google 證明非常有效，容器是 Google 架構上唯一的可執行單位 (runnable entity)，進一步使 Google 只提供少數的作業系統版本給所有機器，只需要少數的維護人員來負責管理版本。\n密閉容器映想檔有很多方式達成，在 Borg 建構映想檔時，應用的執行檔 (binary) 靜態連結到可用的程序庫 (library) 版本，程式庫在公司內部存放。至此 Borg 容器映像檔還不是完全密閉，底下還有依賴一層基底映象檔 (base image)，直接事先安裝到機器上，而不是隨映像檔部屬，基底映象檔包含通用工具例如 tar 與 libc 程式庫，因此更新基底映象檔仍會影響應用，偶爾會產生大麻煩。\n現代的容器映像檔格式，如 Docker 與 ACI，都加強這層抽象、提供更緊密的封裝，移除特定作業系統的依賴性，並要求使用者在分享映象檔內容時，必須明確宣告。\n容器作為管理單位 (Container as the unit of management) 圍繞容器建構管理 API，而非圍繞機器，使得資料中心的「Primary key」從機器變成應用。帶來許多好處：\n應用工程師與維運團隊 (operation team) 不需再煩惱機器與作業系統的細節 架構團隊 (infrastructure team) 獲得更多升版或是調度硬體的彈性，實際對應用與開發團隊的影響非常小 管理系統收集應用的監測數據 (例如 CPU 與 Memory 使用 metrics)，而非機器的監測數據，直接提升應用的監測與自我檢查 (introspection)，特別是擴展 (scale-up)、機器錯誤、或是維護時，造成應用移到其他位置時，監測依然可用。 容器提供許多切入點給泛用 API (generic API)，泛用 API 使資訊在管理系統與應用之間流動，但其實兩者互相不清楚對方的實作細節。在 Borg 中有一系列的 API 連結到所有容器，例如 /healthz 端點回報應用的健康程度給協調管理者 (orchestrator)，當發現不健康的應用，自動終止並重啟應用。這個自動修復功能是可靠的分散式系統的基礎。(Kubernetes 也提供類似功能，透過 HTTP 端點或是 exec command 來檢查容器內部的應用)\n容器提供、或是提供給容器的資訊，可以透過許多使用者介面呈現。Borg 提供可以動態更新的文字狀態訊息，Kubernetes 提供 key-value 的 annotation 存放在給個物件的 metadata，這些都能溝通應用。annotation 可以是容器自行設定、或是由管理系統設定 (例如滾動更新版本時標註新版本)。\n容器管理系統可以取得容器內部訊息，例如資源使用狀況、容器的 metadata，並傳播給日誌 (logging) 或是監控 (monitoring)，例如使用者名稱、監控任務名稱、用戶身分。甚至進一步在節點維護時，提早提供安全終止 (graceful-termination) 的警示。\n容器有其他方式可以提供應用導向的監測，例如 Linux Kernel cgroups 會提供應用的資源使用率，這些資訊可以擴展，並將客製化的 metrics 構過 HTTP API 送出。基於這些資料，進一步開發出泛用工具如自動擴展 (auto-scaler) 與 cAdvisor，他們不需要知道應用的規格就可以記錄 metrics。由於容器本身是應用，因次不需要在實體或虛擬機器、與多個應用之間，做信號多工轉發或多路分配 (multiplex / demultiplex signals)。這樣更簡單、更堅固、並且可以產生精度更高的 metrics 與日誌，並提供更精細的控制操作。你可以對比使用 ssh 登入機器，並且使用 top 指令監測。雖然工程師可以 ssh 登入容器，但很少需要這樣做。\n應用導向變遷在管理架構上產生更多回響，監測只是其中一個。Google 的附載均衡器 (load balancer) 不再根據機器做附載均衡，而是根據應用實體 (application instances)。日誌會根據應用打上標籤 (key) 而不是機器，因此可以跨機器的收集所有相同應用的日誌，而不會影響其他的應用日誌或是影像作業系統。我們可以檢測應用失效，更容易的描述錯誤原因，不需要先拆解處理機器的各層信號。由於容器實體的身分資訊，從基礎上都是由容器管理系統控制，工程師可以明確的指定應用的執行身分，使得應用更容易建構、管理、除錯。\n最後，雖然上述都針對容器與應用在一對一的狀況下，但實務上我們需要套裝的容器群 (nested containers)，這群容器一起排程到相同機器上，最外層的容器提供資源，內層的容器提供部屬的隔離。Borg 中，最外層容器叫做資源分配 (resource allocation or alloc)，在 Kubernetes 上稱做 Pod。Borg 允許最上層的應用直接跑在外層的 alloc 上，但這點產生了一些不便，所以這點在 Kubernetes 做了調整，應用永遠跑在最外層 Pod 的內部，就算只有單一一個容器。\n一個常見的使用情境，是一個 Pod 內部跑一個複雜的應用，大部分的應用都是一個一個子容器 (child containers)，其他的子容器則執行輔助工具，例如 log rotation 或是將日誌移轉到分散式檔案系統。對比把上述功能都打包到 binary 執行檔中，拆分更容易讓不同團隊各自開發負責的功能，拆分還提供更好的耐用程度 (就算主要應用終止，子容器仍能成功將日誌移轉出去)，更容易組裝應用 (composability) (由於每個服務都是獨立運作在各自的容器中，因此可以直接增加新的支援服務)，提供更精細的資源隔離 (所有容器資源隔離，所以日誌服務不會搶占主要應用的資源，反之亦然)。\n協調管理只是開始，而不是終點 (Orchestration is the beginning, not the end) Borg 的初衷只是分配不同的工作附載 (workload) 到相同的機器上，來提升資源使用率。然而 Borg 生態系中，支援系統的快速演化，表明容器管理系統本身只是一個起點：通往一個新的分散式系統開發與管理環境。許多新的系統在 Borg 上打造、嵌入 Borg、或是圍繞 Borg 打造，提升 Borg 的基本容器管理服務，底下是部分服務清單：\n命名與服務發現 (Naming and service discovery) Borg Name Service (BNS) 主節點選舉 (master election) 使用 Chubby 應用感知的 (application-aware) 的負載均衡 自動水平擴展 (horizontal) 與垂直擴展 (vertical) 新的執行檔與設定檔的滾動升級工具 (rollout) 工作流程工具 (workflow) (例如在不同工作階段，執行多任務的分析 pipeline) 監控工具，可以收集容器資訊、整合統計、在 dashboard 上顯示、並可以觸發告警 這些服務都是用來處理開發團隊面臨的問題，Google 選出成功的服務並廣泛的應用，讓工程師工作更加輕鬆。然而這些工具大多各自需要特別的 (idiosyncratic) API，例如需要知道檔案的位置，也需要 Borg 的深度整合。產生一個不好的副作用，就是部屬 Borg 生態系變得更複雜了。\nKubernetes 試圖降低這些複雜度，因此導入一致性設計的 API (consistent API)，例如每個 Kubernetes 物件都會有三個基礎內容：ObjectMetadata、Specification (Spec)、以及 Status。\n所有物件的 ObjectMetadata 都是一樣的，包含物件的名稱、UID、物件的版本 (作為樂觀併發控制 optimistic concurrency control 時使用)、標籤 (key-value label)。Spec 與 Status 的內容因物件型別有所不同，但核心概念一致：Spec 是描述期望狀態 (desired state)，而 Status 是紀錄物件的當前狀態 (current state)。\n統一 API 帶來很多好處，從系統獲取資訊更加簡單：所有物件都有相同的資訊，才可以開發所有物件都適用的泛用工具，而這點更進一步，使工程師開發的使用體驗更加一致。從 Borg 與 Omega 中的經驗學習，所以 Kubernetes 是一堆組合的區塊打造而成，使用者還可以自行擴展。有統一的 API 以及 object-metadata 結構讓這件事更簡單。例如 Pod API 用戶可以使用、Kubernetes 內部元件也使用、外部的自動化工具也使用。一致性繼續延伸，Kubernetes 允許用戶動態增加客製化的 API，與 Kubernetes 核心的 API 一起工作。\n一致性的促成也仰賴 Kubernetes 自身 API 的解藕。把各自元件的面向拆開，讓更高階的服務共享相同的底層基礎實作。例如拆分 Kubernetes 副本控制器 (replication controller) 與水平自動擴展系統 (horizontal auto-scaling system)，副本控制器確保一個腳色的 (ex. 前端網頁) 存在的數量符合期望的數量，自動擴展器則基於副本控制器的功能，只單純調整 Pods 的期望狀態 (desired state)，而不需負責 Pod 的增減，讓自動擴展器可以實作更多使用上的需求，例如預測使用，並可以忽略底下執行的實作細節。\nAPI 解藕也讓許多關聯，但是不盡相同的元件構成近似。例如 Kubernetes 有三個形式的副本 Pod：\nReplicationController：持續執行的容器副本 (例如 web server) DaemonSet：每個節點都有一個實體 (例如日誌收集器) Job：執行到工作完成的控制器，知道如何 (平行的) 啟動工作到結束工作 雖然有各自的執行政策 (policy) ，三個控制器都有相同的 Pod 物件，描述希望執行的容器。\n一致性也仰賴 Kubernetes 內部元件的相同設計模式 (design patterns)。控制器調和迴圈 (reconciliation controller loop) 是 Borg、Omega、Kubernetes 都共享的設計理念，為了提升系統的彈性：迴圈不斷比對期望狀態 (要求多少符合 label-selector 的 Pod 存在)、與實際狀態 (控制器實際觀察到的數量)，然後控制器採取行動，盡量收斂 (converge) 實際狀況與期望狀況。由於所有行動都基於觀察結果，而非一個狀態圖，調和迴圈更能承受錯誤、更能抵抗擾亂、更加堅固：當一個控制器出錯，只要重啟，就可以繼續上次中斷的工作。\nKubernetes 設計成一堆微服務 (microservice) 的組合，並透過各個小型的控制迴圈 (control loop)，來達成整體的編排結果 (choreography) - 一個期望的狀態，由各個分散的自動元件，協作達成。這個意識的設計選擇，對比一個中心化協調管理系統 (centralized orchestration)，後者更容易建構，但面對不可預期的錯誤與變更時，更顯脆弱與僵化。\n前車之鑑 (Things to avoid) Google 在開發這些系統時，學到一些最好別做的事情，我們提供這些資訊，所以其他人可以避免重複的錯誤，而把犯錯的成本放在新的錯誤上。\n不要使用容器管理系統管理 Port number Borg 上所有容器都使用宿主機器的 IP，Borg 在分配時便指派給每個容器各自的 port number。每個容器移動到新機器時會取得一個新的 port number，有時在本機重啟也會拿到新的。這點表示既存的網路服務 (例如 DNS) Google 都必須自行改動並使用自維護的版本；服務的客戶端並不能主動獲得服務的 port number 資訊，需要被動告知；port number 也不能放在 URLs 中，還需要額外的轉址機制；所有基於 IP 運作的工具都需要改成使用 IP:Port。\n有鑑於此，Kubernetes 上每個 Pod 分配一個 IP，讓應用的身分符合網路身分 (IP address)，讓現成的應用更容易在 Kubernetes 上執行，應用可以直接使用靜態的常規 port number (例如 HTTP 流量使用 80 port)，現有的工具也可以直接使用，例如網段切分 (network segmentation)、帶寬節流 (bandwidth throttling) 與控制。所有的公有雲都提供每個 Pod 一個 IP 的網路基底，在實體機器 (bare metal)，也可以使用軟體定義網路 (Software defined network) 的 overlay 網路，或是設定 L3 路由來配置機器上的 Pod IPs。\n不要幫容器編號，使用 label 控制容器 容器建構變得方便後，使用者會產生大量容器，很快就需要一個群組管理的方法。Borg 提供 job 給一組相同的 tasks (task 作為容器的名稱)，一個 job 是一組向量集合 (compact vector) 可以指向一個或多個 tasks，將 task 從 0 開始遞增編號，這個做法很直接也很方便，但等稍後需要重啟容器時，Google 就後悔這個固定的設計，例如其中一個 task 死了需要重啟刀另外一台機器，原本的 task 向量的相同位置，現在需要做兩倍工作：找到新的副本，然後指向舊的副本以免需要除錯。當向量集合中的 task 離開後，這個向量上就有很多空洞，這讓 Borg 上層的管理系統分配，跨級群的 job 時變得很困難。這也造成 Borg 的 job 升級語法中出現許多危險且不可預期的交互 (當滾動升級時依照 index 順序重啟 task)，而應用仰賴 task index (例如應用使用 index 執行資料級的 sharding 或 paritioning)：如果應用使用 task index 來做 sharding，Borg 重啟時會移除鄰近的 tasks，導致資料不可用。Borg 也沒有好方法來增加來自應用的 metadata 到 task 上，例如角色 (e.g. 前端網頁)、或是滾動升級的狀態 (e.g. canary)，工程師只好將 metadata 編碼，壓在 jobs 的名稱上，再使用正規表達式 (regular expression) 解碼。\nKubernetes 直接使用 label 來辨識容器群，label 是鍵值資料對 (key-value pair)，包含可以辨識應用的資料，一個 Pod 可能有 role=frontend 與 state=production 表示 Pod 是屬於 production 環境的前端網頁。label 可以動態增加，並使用自動化工具更改，個團隊可以自行維護一組各自的 label。使用 label-selector 來取得一組物件 (e.g. stage==production \u0026amp;\u0026amp; role==frontend)。各組物件可以重複，一個物件也可以屬於多個物件組，所以 label 也比靜態的物件清單更有彈性。由於物件組都是動態查詢時產生的，任何時候都可以增加新的物件組。label-selector 是 Kubernetes 的群組機制，並藉此分界管理維運，又可以同時處理多個實體。\n雖然有些使用情形，事先明確知道一組內有哪些 task 很有用 (例如靜態腳色指派、工作 partitioning 或 sharding)，合理的 label 也能產生一樣的效果，只是應用 (或是其他外部管理系統) 就需要負責提供 labeling。label 與 label-selector 為雙邊提供最適的做法。\n注意 Pod 的所有權 Borg 上 task 依賴 job，不會獨自存在，產生 job 時產生 tasks，這些 tasks 永遠連結特定的 job，刪除 job 同時刪除 tasks。這點很方便，但有一個缺陷，這個單一的群組機制，需要應對所有使用需求。例如 一個 job 需要儲存參數，提供給服務或是提供給應用，但不會一起提供，使用者就需要開發取代方案來協助處理 (e.g. DaemonSet 將 Pod 複製到所有節點上)\nKubernetes 中 Pod 的生命週期管理元件，例如 replication controller，使用 label selector 決定那些 Pod 要負責管理，所以會有一個以上的 controller 認為他們都對某個 Pod 有管轄權，這樣的衝突需要透過明確設定避免的。由於 label 的彈性也有許多優點，例如將 controller 與 Pod 解藕，表示再也不會有以往的孤兒 Pod (orphan) 或是認領的 Pod (adopt)。考慮附載均衡服務，透過 label selector 選擇流量的端點，如果一個 Pod 行為有異，這個 Pod 只要移除對應的 label，Kubernetes service load balancer 就可以避免流量，輕易的被隔離開來，但 Pod 本身還會存在，提供原地除錯。同時，管理 Pod 的 replication controller 會增加一個 Pod 來取代行為有異的 Pod。\n不要暴露 raw state Borg、Omega、Kubernetes 的關鍵不同點是 API 的架構設計。Borgmaster 是一個集中單一的 (monolithic) 元件，可見所有的 API 行為，包含級群管理邏輯，例如 job 與 task 的狀態機制，並且使用基於 Paxos 算法的分散式儲存庫。Omega 除了儲存庫以外，沒有集中式的元件，儲存庫只儲存被動的狀態資訊，提供樂觀的平行控制 (optimistic concurrenty control)：所有邏輯與語法都推送到儲存庫的客戶端，所以所有客戶端都可以讀取 Omega 的所有邏輯。實務中 Omega 的元件都使用相同的客戶端程式庫，負責打包/解包資料結構、資料重送、並確保語法的一致性。\nKubernetes 選擇中間，類似 Omega 元件化架構的架構，確保全域的常數、政策控管、資料轉型，來提供彈性與擴容性。Kubernetes 確保所有的儲存庫存取都透過 API server，API server 隱藏儲存庫的實作、負責物件的驗證、除錯、版本控制。如同 Omega，Kubernetes 提供多種不同的客戶端 (特別給是開源的環境)，但是集中化的 API server，仍能確保相同的語法、恆量、與政策控管。\n其他困難的開放問題 多年的容器開發系統開發競豔，Google 仍然有一些問題還沒找到合適的答案，這裡分享一些問題，希望能帶來更多討論與解決方案。\n設定 (configuration) 所有面對的問題中，與設定 (configuration) 有關的問題，產生最多的腦力激盪、文件、與許多程式碼。一組數值提供給應用，但不是直接寫死在程式碼中，我們可以就此寫一大篇些不完，但這邊先聚焦幾個重點。\n第一，關於應用的設定，容器管理系統沒做，但已經有包羅萬象的實作，Borg 的歷史中包含：\n範版 (boilerplate) 的精簡 (e.g. 針對不同工作負載，例如服務或是批次任務，提供預設的 task 重啟政策) 調整並驗證應用的參數與命令列標籤 (command-line flags) 實作 workaround 給還沒有的 API 抽象，例如 package 管理 應用的設定樣本程式庫 發布管理工具 (release management) 映象檔版本規格 為了與上述需求協作，設定管理系統最終產生一套針對特定領域 (domain-specific) 的設定語言，最終變成 Turing 語言，為了在設定資料中進行運算 (e.g. 透過服務的 sharding 數量，還決定調整機器的 memory)，這會造成一種難以理解的設定程式碼 (configuration is code) ，而這是應用工程師，透過消滅原始碼中寫死的參數，盡力避免的。這點並沒有降低維運的複雜性，新的語言只是將設定的運算，從應用的程式語言中，移到另外一個新的特定語言，而這個語言往往更缺乏開發工具、除錯工具、與測試框架。\nGoogle 認為目前最有效的辦法，是接受一部分無法避免的程式設定，並且清楚的區分運算的程式與資料。呈現資料的語言應該簡單清楚，例如 JSON 或是 YAML，設定運算的邏輯則需要使用完整的程式語言，才有完整的語法、與好的工具。這點可見於 Angular 前端框架，劃分 markup 資料與 JavaScript 運算邏輯。\n依賴管理 (dependency management) 建立服務往往意味著建立一套相關服務 (監控、CI/CD、等等)，如果一個應用依賴其他應用，是否容器管理系統也能自動提供依賴的服務呢？\n讓事情更複雜，啟動依賴服務往往不只是啟動新的副本，例如服務啟動後還需要向使用者註冊 (例如 BigTable 服務)，然後傳送身分認證、權限認證、以及其他中介服務的計價資訊。沒有一個系統可以捕捉、管理、維護、或是暴露這樣的依賴系統資訊。所以開啟一個新服務時，用戶仍需要自己煩惱、部屬新服務仍然困難，而這往往進一步導致使用者沒有跟隨最佳實踐，進而降低服務的可靠性。\n一個標準問題是，如果服務是手動提供的，跟上依賴服務更新會很困難。試圖自動化判斷 (e.g. 使用 tracing accesses) 失敗，因為無法取得判讀結果所需要的語法資訊。一個可能的解決方法，是要求應用列舉所有依賴的服務，並且透過架構控制，拒絕其他應用存取 (如同編譯時匯入的程式庫)。這樣可以讓容器管理系統提供自動化的環境設定、提供自動身分認證、並寫自動連接。\n不幸的是，這樣描述、分析、使用依賴性的系統太過複雜，還沒被增加到主流的容器管理系統。我們希望有天 Kubernetes 能夠建立起類似的工具，單目前仍是一個尚未完成的挑戰。\n結論 十年的經驗打掃容器管理系統，給了我們許多經驗，我們將其實作 Kubernetes 上，Kubernetes 的目的是透過容器，提升工程師的生產力，並減輕手動與自動管理系統的負擔，我們希望你也能加入拓展與探索的行列。\n","permalink":"https://chechia.net/posts/2020-09-12-borg-omega-and-kubernetes/","summary":"\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cp\u003e這是原文完整版本。太長不讀 (TL;DR) 請見\u003ca href=\"https://chechia.net/posts/2020-08-26-borg-omega-and-kubernetes-tldr/\"\u003eBorg Omega and Kubernetes 前世今生摘要\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\u003c/p\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cp\u003e在 container 技術夯起來前，Google 已經做了 container 十幾年，過程中發展出需三套容器管理系統。雖然每一代系統的開發需求不同，但每一代都深受上一代影響。這篇文章描述 Google 開發這些系統時，學到的經驗。\u003c/p\u003e\n\u003cp\u003e第一套 container management 系統是 Borg，為了管理 1. 長期執行的服務 2. 批次的短期工作 (batch job)，原本分別是由 Babysitter 與 Global Work Queue 兩套系統分開管理。後者的架構深刻影響 Borg，但 Global Work Queue 專注於 batch job。兩套系統都在 Linux control groups 之前。Borg 將上述兩種應用放在共享的機器上，來增加資源的使用率，以節省成本。這種共享基於支援 container 的 Linux Kernel (Google 也貢獻許多 Linux kernel container 程式碼)，提供更好的隔離 (isolation) 給延遲敏感的使用者服務 (latency-sentitive user-facing services)，以及消耗大量 cpu 的 batch 程式。\u003c/p\u003e\n\u003cp\u003e越來越多應用都在 Borg 上開發執行， Google 的應用與 infratructure 團隊開發許多工具與服務，形成 Borg 生態系。這些系統提供設定 (configure) 與更新 (update) 工作、預測資源需求、動態推送設定到執行中的工作、服務發現 (service discovery) 與負載均衡 (Load balancing)，等等功能。這些生態系的開發基於 Google 不同團隊的需求，產生一個不同起源 (heterogeneous)、只針對各別需求的 (ad-hoc) 一個堆不同系統，Borg 的使用者需要使用不同的程式語言與程序，來與這些系統互動。Borg 仍然是 Google 主要的容器管理系統，因為他規模 (scale) 巨大，功能多樣，而且極度堅固 (robustness)。\u003c/p\u003e","title":"Borg Omega and Kubernetes Translation 全文翻譯"},{"content":"這是原文翻譯的太長不讀 (TL;DR) 版本。完整翻譯請見Borg Omega and Kubernetes 前世今生浩文完整翻譯\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n前言 Borg 以前就有應用管理系統，那時還沒有 Linux control group Borg 是第一套統一的 container-management system Borg 仍被大規模的使用，有許多功能而且非常堅固 Omega 繼承 Borg 上成功的設計，並希望改進 Borg 的生態系 Kubernetes 開源 透過 REST API 溝通 client 應用開發導向，著重於開發者的需求，希望能簡單的部署複雜的系統 Container Google 使用 Container 來提昇 utilization 把 batch jobs 跟預留資源的服務 (user-facing app) 放在一起，使用閒置時的資源跑 batch job 現代 container 的定義是 runtime-isolation 與 image Application-oriented infrastructure container 使用久了，不只滿足 utilization 的需求 資料中心從機器導向變成應用導向 Container 封裝環境，把機器與 OS 的依賴抽象化 應用不依賴 部署流程 runtime infrastrcture Container scope 在應用上，專注在應用管理而不是機器管理 Application environment cgroup, chroot, namespace 原本的目的是為了保護應用，不被其他應用影響 混合使用可以在應用與 OS 間產生抽象層，解耦 app 與 OS 提供完全相同的部署環境，避免切換環境(ex. dev, prod)時造成環境差異 進一步把 app 的依賴程式也打包 image container 對 OS 唯一的依賴只剩 Linux kernel system-call interface 大幅增加 app 調度的彈性 然而有些 interface 仍附著 OS 上，ex socket, /prod, ioctl calls 希望透過 Open Container Initiative，清楚定義 interface 與抽象 直接的好處，少數幾種 OS 與 OS Version 就可以跑所有應用，新版本也不影響 Container as the unit of management 資料中心的重心，從管理機器變成管理應用 提供彈性給 infrastructure team 提供統一的架構 收集統一的 metrics Container 統一的介面，讓 management system (ex. k8s) 可以提供 generic APIs REST API, HTTP, /healthz, exec\u0026hellip; 統一的 health check 介面，更方便的終止與重啟 一致性 容器提供統一的資訊，ex. status, text message, \u0026hellip; 管理平台提供統一設定 (ex. resource limits) ，並進行 logging 與 monitoring 提供更精細的功能 ex. graceful-termination cgroups 提供 app 的資源使用資訊，而不需要知道 app spec，因為 contaier 本身即是 app 提供更簡單，卻更精細且堅固的 logging 與 monitoring 應用導向的 monitoring ，而不是機器導向的 monitoring 可以收集跨 OS 的 app 狀態，進行整合分析，而不會有 OS 不同造成的雜訊 更容易對應用除錯 nested contaiers resource allocation (aka. alloc in Borg, Pod in Kubernetes) Orchestration is the beginning, not the end 原本 Borg 只是要把 workload 分配到共用的機器上，來改善 utilization 結果發現可以做更多事情，來幫助開發與部署 Naming, service discovery Application-aware load balancing Rollout tool Workflow tool Monitoring tool 成功的工具被留下 然而工具都需要各自的 API，副作用是增加部署的複雜度到 Borg 的生態系 Kubernetese 試圖降低複雜度 提供一致的 API ex. ObjectMetadata, Specification, Status Object metadata 是全域共通的 Spec 與 Status 根據 Object 有所不同，但是概念是一致的 Spec 描述 desired state of object Status 提供 read-only 的 current state of object Uniform API 有許多好處 降低學習成本 可以使用 generic 的工具讓所有 workflow 使用 統一使用者的開發流程與開發經驗 Kubernetes 本身模組化，可以使用延伸模組 ex. pod API 讓使用者使用，kubernetes 內部使用，外部自動化工具也使用 使用者可以自己增加 customized API 如何達到 Uniform API decoupling API 切分 API 關注的面向，變成不同 components API. ex. replication controller 確保 desired 數量的 Pod 存在 autoscaler 關注在需求與使用的預測，然後控制 replication controller API higher-level 服務都共用相同的 basic API 切分 API 而外的好處 有關聯但是用途不同的 API 的內容與使用方式十分相似. ex. ReplicationController: 控制長時間運行的 containers 與其複本 DeamonSet: 每個機器上都跑一個 container Job: 一次性執行完畢的 container Common design patterns ex. reconciliation controller loop 在 Borg, Omega, Kubernetes 中大量使用 需求(desired state) 觀察現況(current state) 執行動作，收斂需求與現況(reconcile) loop 由於狀態是基於實際觀測產生，reconciliation loop 非常堅固，可以承受相當的 failure Kubernetes 設計為一連串的為服務系統，以及許多小型的 control loop 對比大型的 centralized orchestration system Things to avoid Google 開發過程中，也發現許多不該做的事情\n不要使用 conainer system 來管理 port numbers Borg 會指定 unique port number 給每個 container 必須用其他方法取代 DNS port 也不易嵌入 URL 中，要另外處理轉址 需要而外的系統處理 ip:port Kubernetes 選擇指派 IP 給 Pod 可以直接使用常用 port (ex. 80,443) 可以使用內部 DNS，使用一般常用的工具 大部分公有雲都提供 networking underlays，達成 Ip-per-pod 可以使用 DNS overlay 或是 L3 routing，來控制一台機器上的多個 IPs 不要幫 container 編號，使用 label 來管理大量的 container Borg 會幫 job 從 0 開始編號 很直覺很直接，但稍後就後悔了 如果 job 死了，重啟新的 job 在機器上後，還需要去找上個死掉的 job task 中間會有很多洞 (死掉的 job) 更新版本，要更新 jobs 時會依序重啟 jobs 資料如果也是根據 index 做 sharding，重啟時要復原 index，不然會有資料遺失 Kubernetes 使用 label 可以透過 label 管理一組 container 一個 container 可使用多個 labels，更方便的調度 需要的資訊打在 label 上 (ex. role assignments, work-partitioning, sharding\u0026hellip;)，更容易管理 注意所有權 Borg 上，tasks 都綁定在 job 上，產生 job 也產生 tasks 很直覺方便 只剩下一種 group 控制機制 Kubernetes 的 pod-lifecycle management (ex. replication controller) 使用 label selector 來控制 pod 可以彈性控制大量 pod 可能有多個上層 controller 控制同一個 pod，要盡量避免這種情況 好處是保留彈性的同時，可以很清楚界定管理的 pod，不會有 orphan / adapt pod 透過 label 進行 service load balance 如果 pod 有問題，可以變更 label，讓流量不要進來，但又保留 Pod debug 不要暴露 raw state Borgmaster 是 monolithic，可見所有的 API Operation Omega 不是 centralized，只保留被動的資訊，使用 optimistic concurrent control state 存到 client store，並基於 state 進行 operation 所有 client 需要使用一樣的 client store library Kubernetes 走中間 所有 state 存取需要透過 centralized API server client components 可以獨立運作 Some open, hard problems configuration dependency management ","permalink":"https://chechia.net/posts/2020-08-26-borg-omega-and-kubernetes-tldr/","summary":"\u003cp\u003e這是原文翻譯的太長不讀 (TL;DR) 版本。完整翻譯請見\u003ca href=\"https://chechia.net/posts/2020-09-12-borg-omega-and-kubernetes/\"\u003eBorg Omega and Kubernetes 前世今生浩文完整翻譯\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\u003c/p\u003e\n\u003ch1 id=\"前言\"\u003e前言\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eBorg 以前就有應用管理系統，那時還沒有 Linux control group\u003c/li\u003e\n\u003cli\u003eBorg\n\u003cul\u003e\n\u003cli\u003e是第一套統一的 container-management system\u003c/li\u003e\n\u003cli\u003eBorg 仍被大規模的使用，有許多功能而且非常堅固\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOmega\n\u003cul\u003e\n\u003cli\u003e繼承 Borg 上成功的設計，並希望改進 Borg 的生態系\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes\n\u003cul\u003e\n\u003cli\u003e開源\u003c/li\u003e\n\u003cli\u003e透過 REST API 溝通 client\u003c/li\u003e\n\u003cli\u003e應用開發導向，著重於開發者的需求，希望能簡單的部署複雜的系統\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"container\"\u003eContainer\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle 使用 Container 來提昇 utilization\n\u003cul\u003e\n\u003cli\u003e把 batch jobs 跟預留資源的服務 (user-facing app) 放在一起，使用閒置時的資源跑 batch job\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e現代 container 的定義是 runtime-isolation 與 image\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"application-oriented-infrastructure\"\u003eApplication-oriented infrastructure\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003econtainer 使用久了，不只滿足 utilization 的需求\n\u003cul\u003e\n\u003cli\u003e資料中心從機器導向變成應用導向\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContainer 封裝環境，把機器與 OS 的依賴抽象化\n\u003cul\u003e\n\u003cli\u003e應用不依賴\n\u003cul\u003e\n\u003cli\u003e部署流程\u003c/li\u003e\n\u003cli\u003eruntime infrastrcture\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContainer scope 在應用上，專注在應用管理而不是機器管理\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"application-environment\"\u003eApplication environment\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ecgroup, chroot, namespace 原本的目的是為了保護應用，不被其他應用影響\n\u003cul\u003e\n\u003cli\u003e混合使用可以在應用與 OS 間產生抽象層，解耦 app 與 OS\n\u003cul\u003e\n\u003cli\u003e提供完全相同的部署環境，避免切換環境(ex. dev, prod)時造成環境差異\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e進一步把 app 的依賴程式也打包 image\n\u003cul\u003e\n\u003cli\u003econtainer 對 OS 唯一的依賴只剩 Linux kernel system-call interface\n\u003cul\u003e\n\u003cli\u003e大幅增加 app 調度的彈性\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e然而有些 interface 仍附著 OS 上，ex socket, /prod, ioctl calls\n\u003cul\u003e\n\u003cli\u003e希望透過 Open Container Initiative，清楚定義 interface 與抽象\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e直接的好處，少數幾種 OS 與 OS Version 就可以跑所有應用，新版本也不影響\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"container-as-the-unit-of-management\"\u003eContainer as the unit of management\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e資料中心的重心，從管理機器變成管理應用\n\u003cul\u003e\n\u003cli\u003e提供彈性給 infrastructure team\n\u003cul\u003e\n\u003cli\u003e提供統一的架構\u003c/li\u003e\n\u003cli\u003e收集統一的 metrics\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContainer 統一的介面，讓 management system (ex. k8s) 可以提供 generic APIs\n\u003cul\u003e\n\u003cli\u003eREST API, HTTP, /healthz, exec\u0026hellip;\u003c/li\u003e\n\u003cli\u003e統一的 health check 介面，更方便的終止與重啟\u003c/li\u003e\n\u003cli\u003e一致性\n\u003cul\u003e\n\u003cli\u003e容器提供統一的資訊，ex. status, text message, \u0026hellip;\u003c/li\u003e\n\u003cli\u003e管理平台提供統一設定 (ex. resource limits) ，並進行 logging 與 monitoring\n\u003cul\u003e\n\u003cli\u003e提供更精細的功能 ex. graceful-termination\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ecgroups 提供 app 的資源使用資訊，而不需要知道 app spec，因為 contaier 本身即是 app\n\u003cul\u003e\n\u003cli\u003e提供更簡單，卻更精細且堅固的 logging 與 monitoring\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e應用導向的 monitoring ，而不是機器導向的 monitoring\n\u003cul\u003e\n\u003cli\u003e可以收集跨 OS 的 app 狀態，進行整合分析，而不會有 OS 不同造成的雜訊\u003c/li\u003e\n\u003cli\u003e更容易對應用除錯\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003enested contaiers\n\u003cul\u003e\n\u003cli\u003eresource allocation (aka. alloc in Borg, Pod in Kubernetes)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"orchestration-is-the-beginning-not-the-end\"\u003eOrchestration is the beginning, not the end\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e原本 Borg 只是要把 workload 分配到共用的機器上，來改善 utilization\n\u003cul\u003e\n\u003cli\u003e結果發現可以做更多事情，來幫助開發與部署\n\u003cul\u003e\n\u003cli\u003eNaming, service discovery\u003c/li\u003e\n\u003cli\u003eApplication-aware load balancing\u003c/li\u003e\n\u003cli\u003eRollout tool\u003c/li\u003e\n\u003cli\u003eWorkflow tool\u003c/li\u003e\n\u003cli\u003eMonitoring tool\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e成功的工具被留下\n\u003cul\u003e\n\u003cli\u003e然而工具都需要各自的 API，副作用是增加部署的複雜度到 Borg 的生態系\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetese 試圖降低複雜度\n\u003cul\u003e\n\u003cli\u003e提供一致的 API\n\u003cul\u003e\n\u003cli\u003eex. ObjectMetadata, Specification, Status\u003c/li\u003e\n\u003cli\u003eObject metadata 是全域共通的\u003c/li\u003e\n\u003cli\u003eSpec 與 Status 根據 Object 有所不同，但是概念是一致的\n\u003cul\u003e\n\u003cli\u003eSpec 描述 desired state of object\u003c/li\u003e\n\u003cli\u003eStatus 提供 read-only 的 current state of object\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUniform API 有許多好處\n\u003cul\u003e\n\u003cli\u003e降低學習成本\u003c/li\u003e\n\u003cli\u003e可以使用 generic 的工具讓所有 workflow 使用\u003c/li\u003e\n\u003cli\u003e統一使用者的開發流程與開發經驗\u003c/li\u003e\n\u003cli\u003eKubernetes 本身模組化，可以使用延伸模組\n\u003cul\u003e\n\u003cli\u003eex. pod API 讓使用者使用，kubernetes 內部使用，外部自動化工具也使用\u003c/li\u003e\n\u003cli\u003e使用者可以自己增加 customized API\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e如何達到 Uniform API\n\u003cul\u003e\n\u003cli\u003edecoupling API\n\u003cul\u003e\n\u003cli\u003e切分 API 關注的面向，變成不同 components API. ex.\n\u003cul\u003e\n\u003cli\u003ereplication controller 確保 desired 數量的 Pod 存在\u003c/li\u003e\n\u003cli\u003eautoscaler 關注在需求與使用的預測，然後控制 replication controller API\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ehigher-level 服務都共用相同的 basic API\u003c/li\u003e\n\u003cli\u003e切分 API 而外的好處\n\u003cul\u003e\n\u003cli\u003e有關聯但是用途不同的 API 的內容與使用方式十分相似. ex.\n\u003cul\u003e\n\u003cli\u003eReplicationController: 控制長時間運行的 containers 與其複本\u003c/li\u003e\n\u003cli\u003eDeamonSet: 每個機器上都跑一個 container\u003c/li\u003e\n\u003cli\u003eJob: 一次性執行完畢的 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCommon design patterns\n\u003cul\u003e\n\u003cli\u003eex. reconciliation controller loop 在 Borg, Omega, Kubernetes 中大量使用\n\u003cul\u003e\n\u003cli\u003e需求(desired state)\u003c/li\u003e\n\u003cli\u003e觀察現況(current state)\u003c/li\u003e\n\u003cli\u003e執行動作，收斂需求與現況(reconcile)\u003c/li\u003e\n\u003cli\u003eloop\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e由於狀態是基於實際觀測產生，reconciliation loop 非常堅固，可以承受相當的 failure\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes 設計為一連串的為服務系統，以及許多小型的 control loop\n\u003cul\u003e\n\u003cli\u003e對比大型的 centralized orchestration system\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"things-to-avoid\"\u003eThings to avoid\u003c/h1\u003e\n\u003cp\u003eGoogle 開發過程中，也發現許多不該做的事情\u003c/p\u003e","title":"Borg Omega and Kubernetes TLDR 摘要翻譯"},{"content":"季中回顧\n這一季開了很多新坑，卻來不及寫文章填上\n完成的有 Terraform 的基礎與實務導入經驗\n正在趕工的有 hashicorp vault，Gitops，與 tls。\n其中 GitOps 已經有強者我朋友 Hwchiu 巨巨填坑了，我這邊就會偷懶跳過。友情連結\n剩下的希望本季結束前能填完(掩面)\n已填坑 從零開始導入 Terraform DevOps Taiwan Meetup iThome Cloud Summit 新坑，碼農正在耕田，挖坑自己跳 hashicorp vault install basic operation Sign \u0026amp; manage x509 certificate with pki secret engine 填坑中，文章尚未完成 aws\npost/play-aws-eks-with-low-cost: 精算小神童，如何用最少的 credits 玩 aws eks 服務 tls\npost/openssl-self-sign-tls-with-own-ca post/k8s-manage-tls-certificate gitOps\npost/gitops-with-argo-cd terraform\npost/terraform-infrastructure-as-code-module kubernetes\nborg, omega, and kubernetes 作者外出取材中\u0026hellip; MIT 6.824 Distributed System Learning Note ","permalink":"https://chechia.net/posts/2020-08-19-season-review/","summary":"\u003cp\u003e季中回顧\u003c/p\u003e\n\u003cp\u003e這一季開了很多新坑，卻來不及寫文章填上\u003c/p\u003e\n\u003cp\u003e完成的有 Terraform 的基礎與實務導入經驗\u003c/p\u003e\n\u003cp\u003e正在趕工的有 hashicorp vault，Gitops，與 tls。\u003c/p\u003e\n\u003cp\u003e其中 GitOps 已經有強者我朋友 Hwchiu 巨巨填坑了，我這邊就會偷懶跳過。\u003ca href=\"https://hwchiu.com/\"\u003e友情連結\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e剩下的希望本季結束前能填完(掩面)\u003c/p\u003e\n\u003ch1 id=\"已填坑\"\u003e已填坑\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch/posts/185733642920123\"\u003e從零開始導入 Terraform\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eDevOps Taiwan Meetup\u003c/li\u003e\n\u003cli\u003eiThome Cloud Summit\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"新坑碼農正在耕田挖坑自己跳\"\u003e新坑，碼農正在耕田，挖坑自己跳\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ehashicorp vault\n\u003cul\u003e\n\u003cli\u003einstall\u003c/li\u003e\n\u003cli\u003ebasic operation\u003c/li\u003e\n\u003cli\u003eSign \u0026amp; manage x509 certificate with pki secret engine\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"填坑中文章尚未完成\"\u003e填坑中，文章尚未完成\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eaws\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epost/play-aws-eks-with-low-cost: 精算小神童，如何用最少的 credits 玩 aws eks 服務\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etls\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epost/openssl-self-sign-tls-with-own-ca\u003c/li\u003e\n\u003cli\u003epost/k8s-manage-tls-certificate\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003egitOps\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epost/gitops-with-argo-cd\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eterraform\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epost/terraform-infrastructure-as-code-module\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ekubernetes\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eborg, omega, and kubernetes\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"作者外出取材中\"\u003e作者外出取材中\u0026hellip;\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMIT 6.824 Distributed System Learning Note\u003c/li\u003e\n\u003c/ul\u003e","title":"2020 08 Season Review"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n各位好\nAbout this presentation 開始之前，先分享一些資源\n投影片 講稿 程式碼 SOP 範本 Facebook 粉專 都放在這裡，因為有附逐字稿，所以如果很忙的朋友，掃了 QR code 就可以回家自己看了，不用客氣。\n然後有興趣在追這系列文章的，可以幫我 facebook 粉專按個讚跟追蹤，每周新文章出來，會推播通知。\n文章在 chechia.net 上，新文章通知靠 facebook 粉專這樣。也可以只追蹤不按讚。我自己看別人技術 blog 也很常這樣(XD\n需求 好，今天來講得這個 Terraform\n我會實際分享我們公司為雲團隊導入的經驗，前半部會有點像購物頻道的廣告\n以下的對話是不是常出現在日常工作中？\n以下這段對話很耳熟？ 「是誰改了這個設定？」\nby 週一上班的 DevOps 與週六值班的維運團隊 對啟用的環境的掌握如何 環境的變更能否永久保存 「這個環境怎麼少一個設定？」\n網站噴錯後，整個團隊一步步釐清問題，不是 code，不是變數config，是 infra 有一個小地方沒設定 環境複雜，以我們的例子是複雜得多雲網路，常出這種問題 環境設定的除錯很費時 完全可以避免 「會動就好，沒事不要改環境（抖）」\nLegacy site 調整 production 環境是否有信心 我們看不慣無人整理的舊架構，導入 terraform 後，（很不要命的）把 production site 搬過一遍 我有十成的信心跟你說新舊兩個 site 完全一樣 Our stories 我們是思華科技(ＸＤ) 開發團隊大概 100+ 個左右 專案很多，而且老闆很喜歡開新專案測試商業模型 環境也越開越多，大大小小幾百台 vm，幾十個資料庫，都是不同專案在跑，規模大概這樣 每週都有新環境要交付 交接缺口 我們公司東西多，但東西多不是問題，問題是什麼呢?\n問題 手動部屬本來不是問題，但漸漸成為問題 開 infrastructure 的方法，跟著 SOP 上去 GCP GUI 介面，點一點，填一填。公有雲開機器很方便。 可是不同人開的環境漸漸出現一些不同，可能差一個設定、一個參數、或是命名規則差一點。這些細節的不同，差一個 config 有時候就會雷到人。「這機器誰建的阿，根本有問題啊」，而且這種雷很多時候都是跑下去出事了，才發現「阿靠原來設定不一樣」 命名差一點不影響功能，但看久了就很煩，「阿就對不齊阿」，有強迫症就很痛苦。然後你維運的自動化腳本就爆掉，命名差一個字，regex 就要大改。突然增加維運成本 生產環境大家都不太敢動。架構調整很沒信心 誰知道當初環境設定了那些東西，開機器的人離職了，也不知道他為啥設定，「你知道他當初為什麼要設定這個嗎?」，你問我我是要去擲茭喔。 我們這一季把所有現有環境都搬到新的架構上，因為我們對舊架構不爽很久了(XD)，這個能做到當然有作法，後面細講 有實際需求才找解決方案，沒有需求就不用衝動導入新技術，導入過程中還是蠻累的\n需求 從維運的角度，需求大概長這樣\n提升穩定度\ninfra 交付標準化 交付自動化 測試環境 infra 提交要能夠 review\n提升效率\n老闆要的。超快部屬，腳本跑下去要快，還要更快 次要目標\n成本，效能最佳化，希望能在整理過程中，找到最適合的可行架構 新人好上手，Junior 同事也能「安全」的操作，看到這個安全兩個字了嗎? 安全第一，在訓練新的 op 時要注意安全，不然他上去 GUI 點一點，一個手起刀落 DB 就不見了，整個維運團隊一周不用睡覺。安全第一吼。 權限控管，IAM 也用 terraform 管理，權限管理人多手雜越用越亂，可以考慮使用 IaC，一覽無遺 Programatical approach for infra 啊不就是 Infrastructure as code XD\n導入前，大家都有聽過，大家都覺得很想導入，但沒人有經驗，每個人都超怕，但又不知道在怕三小 這表明了一件事，大家都知道要做對的事情，但不是每個人都能改變現況，讓團隊導向對的事情 計畫\n確定需求 開始 survey 「小心」導入 有經驗領頭羊很棒，但不是必須 技術細節 先簡單講一下 IaC (Yeah 終於要講技術了)\nIaC 上面都講概念跟心法，現在實際講用到的技術。\n首先是 Infrastructure as Code，這個概念很久了，但導入的公司好像不是那麼多。所以我今天要來傳教，洗腦大家(XD，跟你推薦這個配方保證快又有效(XD)\n簡單來說就是用程式來操作 infrastructure，今天主講的 terraform 是 IaC 工具中的一個 IaC 工具可以是宣告式，或是命令式，或是兩種都支援 一個是我告訴你結果，步驟我不管，請你幫我生出這樣的結果。 一個是我告訴你步驟，你一步一步幫我做完，就會得到我要的結果 terraform 是宣告式，說明邏輯跟結果，例如我要 1 2 3 台機器，terraform 自己去幫我打 Google API 這樣，把機器生出來 ansible 是命令式，我把步驟寫成一堆命令腳本 playbook ，ansible 幫我照著跑下去，理論上跑完後我的機器也準備好 Terraform 官網在這邊，自己看 https://www.terraform.io/ 宣告式的 Iac 工具 單一語法描述各家 API 透過 provider 轉換 tf 成為 API call Terraform Core Workflow https://www.terraform.io/guides/core-workflow.html\nWrite 撰寫期待狀態 tf file plan 計畫試算結果 apply 用期待狀態去更新遠端 tf file，就是宣告式的表達 infra ，描述期待的infra長這樣，ex. tf file 裡有這些機器 1 2 3 台這樣 resource 一個一個物件描述，後面可能是對映 provider 的 API Endpoint (ex. GCP GKE API) remote resources，是真實存在遠端的機器，例如 GCP 雲端實際上只有 1 2 兩台這樣。 terraform diff tf vs remote，算出 plan，少的生出來，多的上去砍掉 Demo 1 (empty project)\nadd my-gce.tf\ngit diff\nstate\ncheck GUI remote\nexisting my-gce\nremove my-gce.tf\nplan\n這邊不 apply 我 demo 還要用\n這邊這樣有理解 terraform 的基本流程嗎？編寫，計畫，apply 三步驟 很單純 然後講一點細節\nstate\nremove (out of scope)\nplan -\u0026gt; addd\napply (deplicated ID)\nmv state (Danger)\nrename state with the same ID -\u0026gt; destroy and recreate (Danger)\nState terraform 經手(apply) 過的 resource 會納入 state (scope)\n不在 scope 裡的 resource 不會納入 plan，不會被 destroy，但可能會 create duplicated ID\nterraform 允許直接操作 state\nimport remove 但我不允許XD 注意是 diff state 喔，所以每次 plan 時候會自動 refresh state\nstate 又是什麼? remote 是一個動態環境，可能會多會少，這樣沒辦法 diff，state 是把我執行當下，遠端相關資源的狀態快照存起來，然後根據這個 snapshop 去 diff\napply 只是拿你的期待去 diff state，terraform 幫你算出來差多少，例如我們這邊就是遠端少一台。terraform 透過 provider 去知道，喔這一台要去打那些 GCP API，把這台生出來。\nState，是核心概念，我當初自己卡觀念是卡這邊，所以我特別拉出來講\n雲端空蕩蕩，refresh state 也是空的，tf file 多加一個 VM，plan 覺得要 create 雲端有東西，refresh state 未必會 refresh 到 相同 ID 的資源之前 import 在 state 中，refresh state，tf file 沒東西，plan 覺得要 destroy 相同 ID 的資源不在 state 中，這些 resource 不在當前 state 的 scopor 中，refresh state 是空的，tf file 沒東西，plan 覺得沒增沒減 相同 ID 的資源不在 state 中，這些 resource 不在當前 state 的 scopor 中，refresh state 是空的，tf file 有相同 ID 的資源，plan 覺得要 create，但實際 apply，API error 遠端已經有相同 ID 的資源存在 小結 Write -\u0026gt; Plan -\u0026gt; Apply State 大家都會 terraform 惹 初步使用感想 IaC 地端跟雲端都能做，但雲端做起來效果超級好 完全展現雲端運算的特性，迅速、彈性、隨用隨叫，調度大量的虛擬化資源 新增東西很快，不要的資源，要刪掉也很快 不小心刪錯也很快(大誤)，所以我說新人一個手起刀落公司整個雲弄不見也是有可能的，「啊我的雲勒」「被 terraform 砍了」。不要笑，那個新人就是我，我自己剛學的時候就有把整個 db 變不見過，差點一到職就引咎辭職(XD。用這些技術還是有很多安全要注意，稍後會細講注意的安全事項。 總之，Iac 就是用程式化的語法，精準的描述雲端的狀態或是步驟，完全沒有模糊的地帶。帶來的好處，降低維運的錯誤風險，加快維運效率，最佳化節省成本。\n新手 state 的雷 多人協作，同時變更 state 會造成不可預期的錯誤 避免直接操作 state state 可能有 sensitive 資料 推薦使用外部帶有 lock 的 state storage 導入 導入工具之後 新工具導入時要做好風險評估，每個人都是第一次用 terraform ，用起來很快很爽的同時也要不斷宣導安全概念，雷在哪裡坑在哪裡。\n使用 terraform 的風險\n打 DELET API 超快，砍起來很方便，但很多時候方便 = 危險。眼看小明一個手起刀落，談笑間，公有雲灰飛煙滅(XD，通通變不見。現在在講故事很開心，實際發生的話大家都笑不出來，全公司 RD 都跑來維運部門排隊盯著你看，就算修好也要懲處。壓力超大。但小明砍錯東西不是小明的錯，是大環境的錯是 SOP 的錯(XD。認真的，團隊沒有提供 SOP，新人砍錯東西當然是團隊負責。所以我們 SOP 第一行就寫得很清楚。 看見 destroy 就雙手離開鍵盤，直接求救，這樣還能出事嗎 再來，給予特殊的 IAM 權限，例如只能新增不能刪除的權限 進一步導入 git-flow，push、review、PR，讓他連犯錯的機會都沒有 根本還是要給予新人足夠的訓練，然後同時保障公司安全。 給新人過大權限砍錯東西，或是工作流程一堆坑，根本是在誘導新人犯錯，團隊的資深成員要檢討。 逐漸導入 導入的過程不斷檢討跟修改，最後找出適合我們公司的流程 檢討透明，有錯就修 SOP，修工作流程。讓你的工作流程跟工作環境，固若金湯，成員很難在裡面犯錯，這才是 DevOps 在做這的事。 官方建議的最佳實作 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 至於我們是如何逐漸導入 IaC 到公司的開發流程中？具體的導入步驟如下 Introduction: IaC 舊架構保存。先把雲端上已經有的機器，terraform 裡面叫資源，import 成代碼 檢查舊架構。所有設定都變成代碼了，跟你看程式碼一樣，一拍兩瞪眼沒有任何模稜兩可。整理過程中找出合理跟不合理的設定。有可能會發現一些雷，只是還沒爆炸，也趁機修一修。 然後，依照這些現行的資源，去整理一份適合公司的環境範本，之後所有的新環境都這這個範本部屬，確定新的環境都有合理的規劃。 到此，所有新環境都是同一份範本生出來的，環境已經標準化了。不會再有零碎的小錯誤。聽起來超讚，但有時候出錯就是一起全錯，超慘(XD。當然有錯就修範本，修 SOP。同樣的錯永不再犯，不用再修第二次。 Introduction: Git-flow 然後，導入版本控管，整合 git-flow 的開發流程。寫 SOP，之後所有變更都要 先把 master 封起來，所有人都不准直接改架構 開新 branch，commit 開 PR 大家 review，大家都看過了吼，再 merge 進去，這樣有錯就不是一個人的鍋而是大家一起背鍋(XD。不是拉，review 能大幅降低錯誤，分享團隊經驗加速新人訓練，並且讓所有人 on the same page，不會再有「阿靠這機器誰開的」有人不知情的事情。 永遠只使用 master 來部屬雲端資源，也是確定所有架構都經過多人 review。 Introduction: Pipeline Automation 最後，整合 CICD，讓架構的部屬完全自動化。把人工降到最低，同時也把人工錯誤的機率降到最低，當然這個也是沒錯都沒錯，要錯一起錯的狀態(XD，使用時還是要注意。但如果執行的很穩定的話，自動化絕對是值得投資的。因為現在把架構當作產品做，部屬完要測試功能，網路設定是否正確，監控是否完整，proxy 是不是要打看看。這些都整合進 infra 自動 pipeline。部屬完就是測試，然後交付給其他團隊。 之後就是不斷調整 SOP，跟 CI/CD pipeline。把維運步驟轉成程式維護。 犯錯過一次，永不再犯。這個對於長期團隊經營非常重要，讓經驗跟知識累積，團隊質量才會成長。IaC 在這點幫助很大。\nDemo 2 NAME=terraform-devops make gke redis sql module git commit Review on github Merge request Repo 我使用的原碼都開源在 github 上，因為是真的拿來導入我們公司的架構，保證可以用。阿不能用的話，幫我發 issue 給我，或是你人更好發個 PR 給我都可以(XD。把 repo 拉下來，這邊有 gcp / azure / aws，雖然有三個但我們公司主要是用 gcp，剩下兩個我自己做興趣的。裡面 templates 跟 modules ，但你不用管，我 makefile 都寫好了\n我這邊要新增一個 kubernetes 集群 我直接進來我的專案， NAME=my-new-k8s make gke，東西就生出來，具體做的事情就是新增兩個程式區塊，每個區塊描述一個機器 git diff 看多了什麼，這邊多一個 k8s 跟多一個 node-pool 然後我 plan，讓 terraform 預測一下試跑結果，我們依據結果好好 review，例如這邊 2 to add 0 to destroy 我的想像是不是真的跟 terraform 計畫一樣。 然後 terraform apply，這邊要看清楚，我們是 2 to add 0 to destroy，如果看到有 destroy 就要雙手離開鍵盤，大家不要衝動，看清楚，因為她真的會上去把東西砍掉 P.S. 專案可以按照公司需求分，資源太多太擠就拆分成幾個資料夾好管理，然後分權責管理，例如館 iam 的、管網路、管應用機器的可以分開來\nGit-flow 然後因為我後面會講 git-flow 工作流程整合，所以我順便做完。\n新的變更 commit ，plan 但是還沒 apply。我要求所有新的 commit 推上去 發 PR，其他團隊成員來幫我 review。PR 用的 template ，描述一下新架構的目的，變更的地方，有沒有雷，然後幾個 checklist 檢查 其他隊員 review 都 lgtm 才 merge 回 master apply 永遠在最新的 master 上 apply，確保所有推到雲端的架構都是多人 review 過的。 有 review 才有品質可言，code 都要 review，infra 自然也需要 review。IaC + git-flow 是必要的。\nDemo 2 工具 + 流程 導入的成功與否，不是最佳實踐，而是各個階段，都給予團隊適合的挑戰與協助\nGit-flow SOP 範例 中文版，超長，上面操作過了，這邊不細講，大家自己上去看 但如果團隊是第一次導入 terraform，我強烈建議要有類似的東西 Provide template demo 時不是有 makefile，makefile 裡面寫的小腳本跟本身 IaC 沒有關係，提供一些而外的小腳本輔助，可以進一步降低人工操作，提升效率，又增加安全。工具不一定完全適合團隊吧，這時候就需要補足團隊文化跟工具間的落差，潤滑一下。\n再說一次，新人做錯，不是他做錯，而是團隊沒有提供他足夠的協助。如何讓新人也能有高產出同時又顧及安全，資深工程師是這邊在資深。提供一些一用性工具是必要的。\nTerraform module 又是一個 terraform 的功能\n簡單來說，GKE 也許定義了 2 個子物件(ex. Cluster，Node-pool)，總共有 30 個參數\n你其實不需要那麼多參數 XD 建立一個 my-gke-module，一個物件，5 個必填參數，5 個有預設值的選填參數 也許寫錯的機會只剩 5 個，也許工時只需要 5/30 需求變更就改 module，讓你的操作物件本身就是符合實際需求的 能手動改的地方就是能犯錯的地方，黑箱封裝可以保護整體架構，並提高易用性\n雜項 其他 上面是 IaC 在我們公司的流程 我們選 terraformㄨ 如果是用 terraform 以外的工具，可以參考流程，也許殊途同歸 https://www.terraform.io/intro/vs/index.html\n優缺點 好，跟團隊一步一步溝通改進，花了一兩個月，成功導入。是否有解決當初的問題？\n降低人工操作\n避免人工失誤 infra 交付標準化，沒有奇怪的設定，再也沒有「啊我機器開錯了」這回事 快，真低快。開一個機器就是我剛剛 demo 這樣，而且保證會動。這樣開出來的機器，對她超有信心，要複製完全一模一樣的環境也超有信心。如果再加上自動化測試就更敢保證。 準確\n大家都 review 過，比較不會有「啊我當時沒想到」的狀況，infra 出這種萬萬沒想到的問題，很有機率要幹掉重來。菜鳥跟著 review ，試著發 PR，這樣新人訓練才會有效率，他之後才能自己操作，資深工程師只要 review 就好。要給新人足夠的訓練，又要顧慮安全， review 花的時間非常值得。 保證開發、測試、staging、production 環境長的一模一樣。terraform 程式保證的不是我保證的(XD)。但他的保證是有根據的，讓團隊從開發到上限保重相同環境。「阿在我的機器上會跑怎麼上 production 就壞掉」不好意思沒這回事，壞掉就是你扣寫錯(兇。認真地說，排除一些 infra 的問題，可以大幅增加除錯的效率，只要檢查還沒自動化的地方就好。 自動化測試，扣要測試環境也要測試，這邊直接整進去，環境交出去保證是好的 生產環境變動\n因為已經轉成程式碼，要有什麼改動都很精確，大家也比較敢動環境，特別是 production 環境。再來因為保留所有環境產生的程式碼，要複製環境也很容易，而且有信心保證一樣。我們就把就架構的 production 複製，然後搬家。安全下庄沒出事。後面就搬上癮，整個公司服務大搬家，搬成團隊理想的架構。搬家已經上線的服務，這個需要多少信心跟勇氣你們知道嗎，維運真的是愛與勇氣的冒險。新架構我們也很滿意。 自動化\n自動化就是讓你用零倍的時間做十倍的事情嘛。聽起來怪怪的蛋是是真的。 因為我們目標是維運躺著上班嘛(XD)，我們才能把時間拿去做改進，不然以光是開機器，測試環境可用性，維運就飽了，根本沒時間改進跟提升。這樣對公司長期非常不好。 可讀性\nGUI 沒辦法打 comment 阿，誰知道這個機器當初為什是這個設定。IaC 後到處都可以寫 comment，怕你不寫而已。然後 code 的表達性還是很強大，比起 GUI，資深工程師可以把握整個公司的架構狀況，比起去雲平台下一堆搜索，手動比對，程式碼的可維護性真的超高。而 GCP 已經是 GUI 做得很好的公有雲了。 降低人工，快速，準確，自動化，有信心\nQ\u0026amp;A 還有空我們再來講 terraform 的細節\n有事歡迎透過粉專私敲，因為我也需要人討論\n還有時間再聊 terraform validate 既然是 code，這邊幫你做 lint、語法檢測、type check，過濾第一層錯誤 import 可以把遠端的資源匯入成 state，一個點讚 follow 追蹤的概念(XD)，不是所有的遠端資源都需要追蹤到 state，我們只需要在對的 scope 裡面關注需要的機器 module 可以自由撰寫，把有相依性的資源打包，依照團隊使用習慣調整使用 cloud 可以管理 state，terraform cloud 幫你維護全域同一份 state，有人在使用時會 lock state，避免多人同時修改，打亂 API 造成資源錯誤 state conflicts 如果有多份 state，你電腦上一份 local state，我電腦上一份 local state，其實會造成衝突 更怕同時多人憶起 apply，GCP API 直接被打亂，會有不可預期的錯誤 解法是使用 terraform remote backend，不要用 local state，使用 DB 、storage 或是 terraform cloud，透過一隻 lock 來保證 synchronized state 間接理解 API ex. GCP Load Balancer\n這個講下去就太多了，基本上透過爬 terraform google provider 的文件，然後去比對\n因為去點 GUI 其實感受不到 GCP API 的調用，但是使用 terraform 轉寫資源時候就很有感，打這個 API 跟打這個 API，tf 檔案上其實看得出來。 進一步去查，才發現 GCP Load Balancer 內網或外網、http 或 tcp、全球或區域，使用的 Load Balancer 行為不一樣，因為底下的實作不一樣。但之前使用 GUI 時其實不會去想為啥設定不一樣，使用 terraform 就會被迫去了解，強迫學習XD。 垃圾話 最佳實踐不是問題 ，如何導入才是問題 可以不用躺著上班，但是不能跪著上班 願意多幾個人來看我粉專比較實在 ","permalink":"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e各位好\u003c/p\u003e\n\u003ch1 id=\"about-this-presentation\"\u003eAbout this presentation\u003c/h1\u003e\n\u003cp\u003e開始之前，先分享一些資源\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003e投影片\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003e講稿\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003e程式碼\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground/blob/master/SOP.md\"\u003eSOP 範本\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.facebook.com/engineer.from.scratch/\"\u003eFacebook 粉專\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e都放在這裡，因為有附逐字稿，所以如果很忙的朋友，掃了 QR code 就可以回家自己看了，不用客氣。\u003c/p\u003e\n\u003cp\u003e然後有興趣在追這系列文章的，可以幫我 facebook 粉專按個讚跟追蹤，每周新文章出來，會推播通知。\u003c/p\u003e\n\u003cp\u003e文章在 chechia.net 上，新文章通知靠 facebook 粉專這樣。也可以只追蹤不按讚。我自己看別人技術 blog 也很常這樣(XD\u003c/p\u003e\n\u003ch2 id=\"需求\"\u003e需求\u003c/h2\u003e\n\u003cp\u003e好，今天來講得這個 Terraform\u003c/p\u003e","title":"Terraform Infrastructure as Code Transcript"},{"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\nOutlline our story: issues, steps, \u0026amp; results basics IaC, terraform benefits risks and 坑 to be or not to be experience oriented\nOur stories 100+ devs, many teams 25+ projects 50+ GKEs 80+ SQLs IAMs, redis, VPCs, load-balancers, \u0026hellip; Issues Ops manually create resources through GUI by SOP. We have many isolated, separeated resources, VPCs. It\u0026rsquo;s our culture, and we (devops) want to change. Some projects have short life-cycle. Rapid resources created \u0026amp; destroy. Our user story As a devops, I would like to introduce terraform (IaC) so that I can\nreview all existing resources minimize error from manual operation ASAP!! As a devops, I would like to fully enforce terraform (IaC) so that I can\nminimize efforts to operate infra delegate infra operations to junior team members minimize IAM privilges Introduction import existing resources review existing resources code plan best practice resource templates create new resources with templates introduce git workflow, plan, commit, PR, and review add wrapper handler automation pipeline repeat 2-4 IaC Programatic way to operate infra declarative (functional) vs. imperative (procedural) Perfect for public cloud, cloud native, virtualized resources Benefits: cost (reduction), speed (faster execution) and risk (remove errors and security violations) Terraform Terraform\nDeclarative (functional) IaC Invoke API delegation State management providers: azure / aws / gcp /alicloud / \u0026hellip; Demo https://github.com/chechiachang/terraform-playground\nScope Compute Instances\nKubernetes\nDatabases\nIAM\nNetworking\nLoad Balancer\nExpected benefits Minimize manual operation.\nZero manual operation error\nStandarized infra. Infra as a (stable) product. fast, really fast to duplicate envs\nInfra workflow with infra review\nEasy to create identical dev, staging, prod envs Reviewed infra. Better workflow. Code needs reviews, so do infra. Fully automized infra pipeline.\nOther Benefits Don\u0026rsquo;t afraid to change prod sites anymore We made a massive infra migration in this quater!! Better readability to GUI. Allow comment everywhere. Risks Incorrect usage could cause massive destruction. 如果看見 destroy 的提示，請雙手離開鍵盤。 ~ first line in our SOP If see \u0026ldquo;destroy\u0026rdquo;, cancel operation \u0026amp; call for help. State management A little latency between infra version and terraform provider version Reduce Risks Sufficient understanding to infra \u0026amp; terraform Sufficient training to juniors Minimize IAM privilege: remove update / delete permissions Git-flow Our SOP\nedit tf push new branch commit PR, review \u0026amp; discussion merge \u0026amp; apply revert to previous tag if necessary (Utility) Provide template wrap resources for better accesibility lower operation risks uniform naming convention best practice suggested default value About introducing new tool The hardest part is always people Focus on critical issues (痛點) instead of tool itself. \u0026ldquo;We introduce tool to solve\u0026hellip;\u0026rdquo; Put result into statistics \u0026ldquo;The outage due to misconfig is reduced by\u0026hellip;\u0026rdquo; Overall, my IaC experience is GREAT! IaC to automation. Comment (for infra) is important. You have to write doc anyway. Why not put in IaC? Q\u0026amp;A Full transcript Presentation file Source Code on Github chechia.net \u0026lt;- full contents Follow my page to get notification Like it if you really like it :) Appendix.I more about terraform terraform validate terraform import terraform module terraform cloud \u0026amp; state management\nAppendix.I understand State conflict Shared but synced watch out for state conflicts when colaborating state diff. could cause terraform mis-plan Solution: synced state lock Colatorative edit (git branch \u0026amp; PR), synchronized terraform plan \u0026amp; apply or better: automation Appendix.II understand resources from API aspect GCP Load Balancer\nGCP Load Balancing understand resources from API aspect\nhow terraform work with GCP API internal\nregional pass-through: tcp / udp -\u0026gt; internal TCP/UDP proxy: http / https -\u0026gt; internal HTTP(S) external\nregional pass-through: tcp / udp -\u0026gt; tcp/udp network global / effective regional proxy tcp -\u0026gt; TCP Proxy ssl -\u0026gt; SSL Proxy http / https -\u0026gt; External HTTP(S) Terraform Resource forwarding_rule\nforwarding_rule: tcp \u0026amp; http global_forwarding_rule: only http backend_service\nbackend_service health_check http_health_check https_health_check region_backend_service region_health_check region_http_health_check region_https_health_check Some ways to do IaC Cloud Formation bash script with API / client 引言 Infrastructure as Code 從字面上解釋，IaC 就是用程式碼描述 infrastructure。那為何會出現這個概念？\n如果不 IaC 是什麼狀況？我們還是可以透過 GUI 或是 API 操作。隨叫隨用\n雲端運算風行，工程師可以很在 GUI 介面上，很輕易的部署資料中心的架構。輸入基本資訊，滑鼠點個一兩下，就可以在遠端啟用運算機器，啟用資料庫，設置虛擬網路與路由，幾分鐘就可以完成架設服務的基礎建設(infrastructure)，開始運行服務。\n然而隨著\n雲平台提供更多新的（複雜的）服務 服務彼此可能是有相依性（dependency），服務需要仰賴其他服務 或是動態耦合，更改服務會連動其他服務，一髮動全身 需要縝密的存取控管（access control） 防火牆，路由規則 雲平台上，團隊成員的存取權限 專案的規模與複雜度增加 多環境的部署 多個備援副本設定 大量機器形成的集群 IaC 的實際需求 以下這些對話是不是很耳熟？\nIaC 的實際需求 沒有需求，就不需要找尋新的解決方案。\n有看上面目錄的朋友，應該知道這系列文章的後面，我會實際分享於公司內部導入 Terraform 與 IaC 方法的過程。\n各位讀者會找到這篇文，大概都是因為實際搜尋了 Terraform 或是 IaC 的關鍵字才找到這篇。\n如果沒有需求，自己因為覺得有趣而拉下來研究，\n如果沒有明確需求，就貿然導入 無謂增加亂度\nIaC 的實現工具 為何選擇 Terraform 建議 如果不熟，從 import 現有最好的資源開始。把 70 分保住，再向 80 90 邁進。 善用 module 封裝，只露出會用到的參數。 ","permalink":"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/","summary":"\u003cp\u003eThis article is part of \u003ca href=\"https://chechia.net/posts/2020-06-14-terraform-infrastructure-as-code/\"\u003e從零開始的 Infrastructu as Code: Terraform\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/chechiachang/terraform-playground\"\u003eGet-started examples / SOP on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2020-06-15-terraform-infrastructure-as-code-transcript/\"\u003eIntroducation to Terraform Iac: Speaker transcript\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://slides.com/chechiachang/terraform-introduction/edit\"\u003ePresentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCheck my website \u003ca href=\"https://chechia.net\"\u003echechia.net\u003c/a\u003e for other blog. \u003ca href=\"https://www.facebook.com/engineer.from.scratch\"\u003eFollow my page to get notification\u003c/a\u003e. Like my page if you really like it :)\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"outlline\"\u003eOutlline\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eour story: issues, steps, \u0026amp; results\u003c/li\u003e\n\u003cli\u003ebasics IaC, terraform\u003c/li\u003e\n\u003cli\u003ebenefits\u003c/li\u003e\n\u003cli\u003erisks and 坑\u003c/li\u003e\n\u003cli\u003eto be or not to be\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eexperience oriented\u003c/p\u003e\n\u003ch1 id=\"our-stories\"\u003eOur stories\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e100+ devs, many teams\u003c/li\u003e\n\u003cli\u003e25+ projects\u003c/li\u003e\n\u003cli\u003e50+ GKEs\u003c/li\u003e\n\u003cli\u003e80+ SQLs\u003c/li\u003e\n\u003cli\u003eIAMs, redis, VPCs, load-balancers, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"issues\"\u003eIssues\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eOps manually create resources through GUI by SOP.\u003c/li\u003e\n\u003cli\u003eWe have many isolated, separeated resources, VPCs. It\u0026rsquo;s our culture, and we (devops) want to change.\u003c/li\u003e\n\u003cli\u003eSome projects have short life-cycle. Rapid resources created \u0026amp; destroy.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"our-user-story\"\u003eOur user story\u003c/h1\u003e\n\u003cp\u003eAs a devops,\nI would like to introduce terraform (IaC)\nso that I can\u003c/p\u003e","title":"從零開始的 Infrastructure as Code: Terraform - 01"},{"content":"今年結束了，回顧一下今年做的事情\n\u0026ndash; 我是軟體工程師 \u0026ndash;\n6 場公開演講，並且踏出熟悉的社群舒適圈，南下進軍高雄XD\n37 篇技術文章 其中包含 30 篇 Ithome 30天(參賽就不用睡覺)鐵人賽參賽文章 結賽撿到賀優選狂賀?\n正職工作方面，進了幣圈，切身了解敝圈真亂後，又踏出了幣圈\n開坑翻譯麻省理工學院的課程『分散式系統』，好課揪團一起修 預計會有 22 篇文章，準備在可見的未來，犧牲無數個夜晚，邁向 2020\n\u0026ndash; 我是專業水肺潛水教練 \u0026ndash; 也是自由潛水員\n年末的幾天，正式開始執業，帶學生下海(?) 學習教導學生，也學習對學生的安全負責\n新年復工後，正職碼農，副業潛水 有人要潛請找我，保證優惠不藏私\n\u0026ndash; 我是數位行銷實習生 \u0026ndash;\n跟前公司 (雖然都不是MK但卻) 超強的行銷團隊\u0026lt;3學習數位行銷 從零開始大造個人品牌，邊學邊實習 開了兩個粉絲專頁 一個是技術文章分享，一個做潛水影片分享 打造個人品牌，自己推廣行銷，學習數位行銷\n","permalink":"https://chechia.net/posts/2019-12-31-say-goodbye-2019/","summary":"2019 年度回顧","title":"Say Goodbye 2019"},{"content":"https://github.com/chechiachang/mit-6.824-distributed-system\n課程講義中文翻譯 個人心得 跟著 MIT 6.824 學習分散式系統\n這個專案儲存 MIT 6.824 分散式系統編程的上課內容，我將內容翻譯程中文，加上個人學習筆記\n我會在我的學習過程中，持續翻譯課程內容 一方面深入個人學習 另一方面也回饋社群 依照課程的進度進行 若有餘力，會嘗試翻譯以下內容\n課堂 Q \u0026amp; A 論文 lab 實做 ","permalink":"https://chechia.net/posts/2019-12-16-mit-6.824-distributed-system/","summary":"跟著 MIT 6.824 深入淺出分散式系統","title":"MIT 6.824 Distributed System Learning Note"},{"content":"https://en.bitcoin.it/wiki/Atomic_swap\nAlgorithm 2 pay txs and 2 claim tx claim txs are singed at first, locked with time 2 pay txs are encrypted by x, affects only when x is reveal on the network Initialization A: random number x\ntx1: A pay B A Pay BTC to B\u0026rsquo;s public key if x known \u0026amp; singed by B or Signed by A \u0026amp; B\ntx2: A claim tx1 pay BTC to A\u0026rsquo;s public key locked 48 hours signed by A\nA -\u0026gt; B tx2 B -\u0026gt; A tx2 signed by A \u0026amp; B\nA -\u0026gt; submit tx1 tx3: B pay A alt-coin B Pay A alt-coin if x known \u0026amp; singed by A or signed by A \u0026amp; B\ntx4: B claim tx3 pay B alt-coins locked 48 hours signed by B\nB -\u0026gt; A tx4 A -\u0026gt; B tx4 signed by A \u0026amp; B\nB submit tx3 A spends tx3, reveal x B spends tx1 using x Specialized Alt-chain ","permalink":"https://chechia.net/posts/2019-11-08-blockchain-atomic-swap/","summary":"\u003cp\u003e\u003ca href=\"https://en.bitcoin.it/wiki/Atomic_swap\"\u003ehttps://en.bitcoin.it/wiki/Atomic_swap\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"algorithm\"\u003eAlgorithm\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e2 pay txs and 2 claim tx\u003c/li\u003e\n\u003cli\u003eclaim txs are singed at first, locked with time\u003c/li\u003e\n\u003cli\u003e2 pay txs are encrypted by x, affects only when x is reveal on the network\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"0\"\u003e\n\u003cli\u003eInitialization\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eA: random number x\u003c/p\u003e\n\u003cp\u003etx1: A pay B\nA Pay BTC to B\u0026rsquo;s public key\nif x known \u0026amp; singed by B\nor Signed by A \u0026amp; B\u003c/p\u003e\n\u003cp\u003etx2: A claim\ntx1 pay BTC to A\u0026rsquo;s public key\nlocked 48 hours\nsigned by A\u003c/p\u003e","title":"Ablockchain Atomic Swap"},{"content":"BEP3 Atomic Swap Binance 在 BEP3: HTLC and Atomic Peg 提到，BEP 即將在 binance chain 上支援原生的 Hash Timer Locked Transfer (HTLT) ，這使跨鏈的原子性交換 (atomic swap) 變得可行，透過 HTLC 在兩邊的鏈上鎖住 (peg) tokens，然後只有在執行交換的時候，透過 hash 交換，一次執行雙邊的交易。\n關於 Atomic Swap 網路有非常多的訊息，有興趣的話可以看這篇\n交易只有在雙邊完成後才完成，完成之前不能動用交換的資產 在任何階段失效都可以完全 fallback，並進行 refund 交易的認證是去中心化的 這邊有個但書，Ethereum 上是透過 smart contract 實現，但 Binance chain 上還是靠 Binance 認證 XD Binance 在 BEP3 中支援 HTLC，我們這邊主要的資訊來源是 binance.org 的官方說明文件，這邊針對文章進行驗證，並且補足文件缺漏的部分，提醒過程中可能會踩到的雷。\n跨鍊(Cross Chain) 交易 在部署 asset / token 的時候，我們會選擇合適的鏈作為發布資產並運行 block chain app。常用的應用鏈如 ethereum 與 binance chain 等等。不同的主鏈上有各自的優缺點，例如使用 ethereum ，可以與許多 token 與應用互動，也是最多人使用的應用主鏈。而在 binance 鏈上執行，則能夠快速的發生 transactions，並且可以與 binance 上的資產與交易所互動。\n在某些應用場景，我們會希望兩個獨立主鏈上的資產能後互動，例如在 binance chain 上執行快速的 transaction，然而也要使用 etheruem 上既有的 ERC-20 tokens，這時便需要一個溝通兩條鏈的機制。\n文章分為三個部分 在 Binance Chain 上互換兩個 address 的 binance asset 從 ethereum token 到 binance 從 binance chain 到 ethereum Atomic Swap on Binance Chain 我們今天會實作 Atomic Peg Swap，透過 HTLT 鎖住 Binance Chain 上兩個 address 的資產，並進行原子性的一次交易，來達成鏈上的資產互換。這邊直接使用 binance 提供的 bnbcli 來執行。\n使用情境 兩個在 Binance Chain 上的 address 想交換資產\nClient: HTLT 的發起方，擁有一部分 asset，發起 HTLT 希望執行資產互換 Recipient: HTLT 的收受方，收到 HTLT，需要於時限內 deposit 指定數量的資產到 swap 中 服務元件 HTLT transactions on binance chain: 來鎖住並 claim assets Client tooling: tbnbcli 讓客戶可以操作，監測鏈上 swap 的狀況 流程 Client 使用 tbnbcli 發起 HTLT Recipient 收到發起方送來的 swap info 與 asset (frozen) Recipient Deposit 指定數量的 asset 到 swap 中 Binance Chain 自動完成 swap，完成交換，解鎖兩邊交換的資產 取得 tbnbcli tbnbcli 的說明文件\n由於 bnbcli repo 中使用 Git Large File Storage 來存放 binary，這邊要啟用 git-lfs 來下載 binary\n# Mac port sudo port install git-lfs Git clone repo\ngit clone git@github.com:binance-chain/node-binary.git cd node-binary git chechout v0.6.2 git lfs pull --include cli/testnet/0.6.2/mac/tbnbcli sudo copy cli/testnet/0.6.2/mac/tbnbcli /usr/local/bin 這邊要注意使用 v0.6.2+ 的版本，不然會沒有 HTLT 的 subcommands\n測試 tbnbcli tbnbcli status --node http://data-seed-pre-0-s3.binance.org:80 { \u0026quot;node_info\u0026quot;: { \u0026quot;protocol_version\u0026quot;: { \u0026quot;p2p\u0026quot;: \u0026quot;7\u0026quot;, \u0026quot;block\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;app\u0026quot;: \u0026quot;0\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;34ac6eb6cd914014995b5929be8d7bc9c16f724d\u0026quot;, \u0026quot;listen_addr\u0026quot;: \u0026quot;aa13359cd244f11e988520ad55ba7f5a-c3963b80c9b991b7.elb.us-east-1.amazonaws.com:27146\u0026quot;, \u0026quot;network\u0026quot;: \u0026quot;Binance-Chain-Nile\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.31.5\u0026quot;, \u0026quot;channels\u0026quot;: \u0026quot;36402021222330380041\u0026quot;, \u0026quot;moniker\u0026quot;: \u0026quot;data-seed-0\u0026quot;, \u0026quot;other\u0026quot;: { \u0026quot;tx_index\u0026quot;: \u0026quot;on\u0026quot;, \u0026quot;rpc_address\u0026quot;: \u0026quot;tcp://0.0.0.0:27147\u0026quot; } }, \u0026quot;sync_info\u0026quot;: { \u0026quot;latest_block_hash\u0026quot;: \u0026quot;359AD9BF36B7DEEB069A86D53D3B65D9F4BB77A1A65E40E1289B5798D4C1094F\u0026quot;, \u0026quot;latest_app_hash\u0026quot;: \u0026quot;E748CFA5806B587D9678F55DFDDB336E3669CDF421191CDA6D2DF8AA7A3461F3\u0026quot;, \u0026quot;latest_block_height\u0026quot;: \u0026quot;45868456\u0026quot;, \u0026quot;latest_block_time\u0026quot;: \u0026quot;2019-10-23T07:36:38.176957281Z\u0026quot;, \u0026quot;catching_up\u0026quot;: false }, \u0026quot;validator_info\u0026quot;: { \u0026quot;address\u0026quot;: \u0026quot;1C360E22E04035E22A71A3765E4A8C5A6D586132\u0026quot;, \u0026quot;pub_key\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;tendermint/PubKeyEd25519\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;T56yDoH+B+OY8PP2tmeFtJtk+9ftnBUVHykKfLS45Es=\u0026quot; }, \u0026quot;voting_power\u0026quot;: \u0026quot;0\u0026quot; } } Acquire Valid Binance Testnet Account Check Testnet Doc\nGo to Binance Testnet Create a wallet Save address, mn, keystore, private key Use testnet faucet to fund testnet account Receive 200 BNB on testnet Client Create HTLT 這邊使用簡單的範例，鎖住兩個 BEP2 tokens 來進行交換，展示一下 tbnbcli 的 HTLT\n準備兩個 address，這邊是我自己的兩個 testnet address\ntbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p (179 BNB) Explorer 上查看 tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce (20 BNB) Exporler 上查看 目標：\nHTLT tbnb\u0026hellip;j9p -\u0026gt; 0.3 BNB -\u0026gt; tbnb\u0026hellip;7ce tbnb\u0026hellip;j9p \u0026lt;- 0.1 BNB \u0026lt;- tbnb\u0026hellip;7ce tbnbcli 執行 HTLT，從 from address 執行 HTLT，給 recipient-addr 0.3 BNB，並預期對方回 0.1 BNB，等待 height-span 個 block 時間(360 \u0026gt; 2 minutes)\ntbnbcli key 實際執行前，由於我們需要透過 tbnbcli 操作 from-address，要先透過 tbnbcli 把 address 的 key 加進到本地\ntbnbcli keys add tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce --recover tbnbcli keys list NAME:\tTYPE:\tADDRESS:\tPUBKEY: tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tlocal\ttbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tbnbp1addwnpepq0pw06d3y7ykg2j33pc604j3awgqgl5vhd88wdjhjg5sptnsfpqyx2rmhl4 實際執行 參數：\nFROM ADDR: tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\nRECIPIENT ADDR: tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\nhightspan: 3600，height-span 是發起 HTLT，受方 deposit，發起方去 claim 的時限。\namount: asset * 10^8\ntbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p 執行 HTLT\n給 tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce 0.3 BNB\n預期對方回 0.1 BNB\n等待 3600 個 block 時間\nFROM_ADDR=tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p RECIPIENT_ADDR=tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce HEIGHT_SPAN=3600 tbnbcli token HTLT \\ --recipient-addr ${RECIPIENT_ADDR} \\ --amount 30000000:BNB \\ --expected-income 10000000:BNB \\ --height-span ${HEIGHT_SPAN} \\ --from ${FROM_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 產生 swap 的結果，可以於Testnet Explorer 上看到\nCommitted at block 47218942 (tx hash: 8F865C5C9E5CD06239DE99746BCE73AACA2F3AD881C26765FB90C9465EF06EF0, response: {Code:0 Data:[77 138 29 51 186 65 213 125 105 217 5 102 170 194 248 149 189 188 56 208 166 93 48 159 188 196 143 111 31 66 151 249] Log:Msg 0: swapID: 4d8a1d33ba41d57d69d90566aac2f895bdbc38d0a65d309fbcc48f6f1f4297f9 Info: GasWanted:0 GasUsed:0 Tags:[{Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 104 113 54 118 52 57 97 110 51 119 119 104 114 100 56 110 121 55 113 106 51 101 120 103 102 109 118 112 118 117 101 108 107 99 97 106 57 112] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[97 99 116 105 111 110] Value:[72 84 76 84] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}] Codespace: XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}) Query Atomic Swap 產生了 atomic swap，這邊可以使用 tbnbcli 查詢 swap 的狀態\nSWAP_ID=4d8a1d33ba41d57d69d90566aac2f895bdbc38d0a65d309fbcc48f6f1f4297f9 tbnbcli token query-swap \\ --swap-id ${SWAP_ID} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 回傳 swap 的狀態\n{\u0026quot;from\u0026quot;:\u0026quot;tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\u0026quot;,\u0026quot;to\u0026quot;:\u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;,\u0026quot;out_amount\u0026quot;:[{\u0026quot;denom\u0026quot;:\u0026quot;BNB\u0026quot;,\u0026quot;amount\u0026quot;:\u0026quot;30000000\u0026quot;}],\u0026quot;in_amount\u0026quot;:null,\u0026quot;expected_income\u0026quot;:\u0026quot;10000000:BNB\u0026quot;,\u0026quot;recipient_other_chain\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;random_number_hash\u0026quot;:\u0026quot;4cf88f1acf8bcbc628609f3257406913f67e009e5c61f2671b601e40f4e5cc6a\u0026quot;,\u0026quot;random_number\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;1572506189\u0026quot;,\u0026quot;cross_chain\u0026quot;:false,\u0026quot;expire_height\u0026quot;:\u0026quot;47222542\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;2254\u0026quot;,\u0026quot;closed_time\u0026quot;:\u0026quot;0\u0026quot;,\u0026quot;status\u0026quot;:\u0026quot;Open\u0026quot;} Deposit HTLT 受方 reciept-address 這邊要把 1 BNB 打進去 swap 中\n注意這邊的 from-address 已經變成當初的 recipient-addr tbnb\u0026hellip;j9p\n當然這邊要存取，也要有 tbnb\u0026hellip;j9p 的 key，這樣我們本地就會有發受兩方的 key，但一般來說應該是兩個不同的人\ntbnbcli keys add tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p--recover tbnbcli keys list NAME:\tTYPE:\tADDRESS:\tPUBKEY: tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tlocal\ttbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tbnbp1addwnpepq0pw06d3y7ykg2j33pc604j3awgqgl5vhd88wdjhjg5sptnsfpqyx2rmhl4 tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\tlocal\ttbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\tbnbp1addwnpepqwk5zx3jnrq5guxc9tsgrte9aw9knla0ahunwynypkm0jvst6y7l2q83ueq 受方把約好的錢存進去\ntbnbcli token deposit \\ --swap-id ${SWAP_ID} \\ --amount 10000000:BNB \\ --from ${RECIPIENT_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 可以成功 deposit，檢查這次 tx Deposit swap 的內容\nPassword to sign with 'tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce': Committed at block 47219070 (tx hash: FDCC528B9F98E9CEEDCB113A398A747F440666061340535D44C526D79F9CD667, response: {Code:0 Data:[] Log:Msg 0: Info: GasWanted:0 GasUsed:0 Tags:[{Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 97 54 112 118 53 103 109 110 115 97 121 52 97 57 115 114 55 110 118 100 48 109 108 100 122 50 57 97 54 107 100 120 121 101 51 55 99 101] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[97 99 116 105 111 110] Value:[100 101 112 111 115 105 116 72 84 76 84] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}] Codespace: XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}) 發起方 Claim HTLT 針對每筆 HTLT ，解鎖鎖住的 assets。每個 HTLT 只能被解鎖一次。\ntbnbcli token claim \\ --swap-id ${SWAP_ID} \\ --random-number ${RANDOM_NUMBER} \\ --from ${FROM_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 可以成功 claim，檢查這次 tx Deposit swap 的內容\nPassword to sign with 'tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p': Committed at block 47219128 (tx hash: D08F81D0315F0A7B13E510782F6E56804803B5198F4914C8EB10E3A5084F2BAA, response: {Code:0 Data:[] Log:Msg 0: Info: GasWanted:0 GasUsed:0 Tags:[{Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 97 54 112 118 53 103 109 110 115 97 121 52 97 57 115 114 55 110 118 100 48 109 108 100 122 50 57 97 54 107 100 120 121 101 51 55 99 101] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 104 113 54 118 52 57 97 110 51 119 119 104 114 100 56 110 121 55 113 106 51 101 120 103 102 109 118 112 118 117 101 108 107 99 97 106 57 112] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[97 99 116 105 111 110] Value:[99 108 97 105 109 72 84 76 84] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}] Codespace: XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}) 查詢 swap 狀態 從 open 變成 completed\ntbnbcli token query-swap \\ --swap-id ${SWAP_ID} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 {\u0026quot;from\u0026quot;:\u0026quot;tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\u0026quot;,\u0026quot;to\u0026quot;:\u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;,\u0026quot;out_amount\u0026quot;:[{\u0026quot;denom\u0026quot;:\u0026quot;BNB\u0026quot;,\u0026quot;amount\u0026quot;:\u0026quot;30000000\u0026quot;}],\u0026quot;in_amount\u0026quot;:[{\u0026quot;denom\u0026quot;:\u0026quot;BNB\u0026quot;,\u0026quot;amount\u0026quot;:\u0026quot;10000000\u0026quot;}],\u0026quot;expected_income\u0026quot;:\u0026quot;10000000:BNB\u0026quot;,\u0026quot;recipient_other_chain\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;random_number_hash\u0026quot;:\u0026quot;4cf88f1acf8bcbc628609f3257406913f67e009e5c61f2671b601e40f4e5cc6a\u0026quot;,\u0026quot;random_number\u0026quot;:\u0026quot;cb5f6296078fd73b86d03eb58bc8a6e0af8d4b60c1cd678b71f8c185e206db53\u0026quot;,\u0026quot;timestamp\u0026quot;:\u0026quot;1572506189\u0026quot;,\u0026quot;cross_chain\u0026quot;:false,\u0026quot;expire_height\u0026quot;:\u0026quot;47222542\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;2254\u0026quot;,\u0026quot;closed_time\u0026quot;:\u0026quot;1572506323\u0026quot;,\u0026quot;status\u0026quot;:\u0026quot;Completed\u0026quot;} 這樣 swap 就完成了\n超時 (Expired) 處理 由於 HTLT 是有時間限制，有可能會超時，會無法繼續操作，例如另外一筆 HTLT 在受方 deposit 時無法 deposit\ntbnbcli token deposit \\ --swap-id ${SWAP_ID} \\ --amount 1:BNB \\ --from ${RECIPIENT_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 ERROR: { \u0026quot;codespace\u0026quot;:8, \u0026quot;code\u0026quot;:14, \u0026quot;abci_code\u0026quot;:524302, \u0026quot;message\u0026quot;:\u0026quot;Current block height is 46046996, the swap expire height(46043293) is passed\u0026quot; } 超過 swap block height 了，受方這邊逾時去存錢，導致整個 swap expired，不能在做什麼事\n發起方去透過 refund 解鎖\nRefund HTLT 若是已經 complete 或是 timeout expired，發起 HTLT 的 address 可以透過 refund 來取回資產\ntbnbcli token refund \\ --swap-id ${SWAP_ID} \\ --from ${FROM_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 回覆 tx 的狀態，可以在explorer 上看到，transaction type 為 refund swap\nCommitted at block 46047771 ( tx hash: F0F5AB40EF7B1CCFE54EBAE0B8022E9B3C381D0029C55A8FE7C3C87E8ACF800D, response: { Code:0 Data:[] Log:Msg 0: Info: GasWanted:0 GasUsed:0 Tags:[{Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 97 54 112 118 53 103 109 110 115 97 121 52 97 57 115 114 55 110 118 100 48 109 108 100 122 50 57 97 54 107 100 120 121 101 51 55 99 101] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[97 99 116 105 111 110] Value:[114 101 102 117 110 100 72 84 76 84] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}] Codespace: XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0 } ) 重新查詢 swap 狀態，已經變成 expired\ntbnbcli token query-swap \\ --swap-id ${SWAP_ID} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 { \u0026quot;from\u0026quot;: \u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;, \u0026quot;to\u0026quot;: \u0026quot;tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\u0026quot;, \u0026quot;out_amount\u0026quot;: [{ \u0026quot;denom\u0026quot;: \u0026quot;BNB\u0026quot;, \u0026quot;amount\u0026quot;: \u0026quot;3\u0026quot; }], \u0026quot;in_amount\u0026quot;: null, \u0026quot;expected_income\u0026quot;: \u0026quot;1:BNB\u0026quot;, \u0026quot;recipient_other_chain\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;random_number_hash\u0026quot;: \u0026quot;2c2588c81a5f08bf55a183cc2d61a123368405638741169933e200c90f4532e5\u0026quot;, \u0026quot;random_number\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;timestamp\u0026quot;: \u0026quot;1571905650\u0026quot;, \u0026quot;cross_chain\u0026quot;: false, \u0026quot;expire_height\u0026quot;: \u0026quot;46043293\u0026quot;, \u0026quot;index\u0026quot;: \u0026quot;2239\u0026quot;, \u0026quot;closed_time\u0026quot;: \u0026quot;1571908256\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;Expired\u0026quot; } 從 Ethereum swap 到 Binance Chain 這邊我們要從 ethereum 上，將 ERC-20 token 與 binance chain 上的 BNB 做交換，先準備需要用到的東西\nbinance testnet 準備好 address 與 BNB，這邊沿用我們上篇使用的 address tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p ethereum testnet (ropsten) Admin Address [ropsten etherscan] 部署 ERC20 token [ropsten etherscan]，我自己發行的 Party Parrot Token (PPT) 部署 ERC20 Atomic Swapper 智能合約 binance-chain/bep3-smartcontract 提供 Node/VM 用來執行 bep3 deputy process https://github.com/binance-chain/bep3-deputy 提供 workflow 我們看一下官方提供的這張圖\nWallet Address 在 ERC20 Token (PPToken) approve() Swap Contract 一部分 Token 初始化 Swap Transaction(tx) Deputy 監測到 Ethereum 鏈上的 swap tx，向 Binance Chain 發起一個對應的 HTLT tx，等待 Binance 上的 claim tx Wallet Address 向 Binance Chain 執行 HTLT claim() Deputy 監測到 Binance Chain 上的 claim tx 與 swap complete，代為向 Ethereum claim ERC-20 執行 deputy 我們可以先將 deputy 程序跑起來，這邊使用的是 Binance Chaing/bep3-deputy\ngo get github.com/binance-chain/bep3-deputy cd ${GOPATH}/src/github.com/binance-chain/bep3-deputy $ go mod download $ make build go build -o build/deputy main.go 啟動本地 MySQL，這邊直接用 docker 起一個無密碼的\n$ docker run \\ --name some-mysql \\ -e MYSQL_ALLOW_EMPTY_PASSWORD=yes \\ -e MYSQL_DATABASE=deputy \\ -d \\ mysql:5.7.27 設定 config/confg.json\n{ \u0026quot;db_config\u0026quot;: { \u0026quot;dialect\u0026quot;: \u0026quot;mysql\u0026quot;, \u0026quot;db_path\u0026quot;: \u0026quot;root:@(localhost:3306)/deputy?charset=utf8\u0026amp;parseTime=True\u0026amp;loc=Local\u0026quot;, }, \u0026quot;chain_config\u0026quot;: { \u0026quot;bnb_start_height\u0026quot;: 42516056, \u0026quot;other_chain\u0026quot;: \u0026quot;ETH\u0026quot;, \u0026quot;other_chain_start_height\u0026quot;: 6495598 }, \u0026quot;admin_config\u0026quot;: { \u0026quot;listen_addr\u0026quot;: \u0026quot;127.0.0.1:8080\u0026quot; }, \u0026quot;bnb_config\u0026quot;: { \u0026quot;key_type\u0026quot;: \u0026quot;mnemonic\u0026quot;, \u0026quot;mnemonic\u0026quot;: \u0026quot;\u0026lt;my-mnemonic\u0026gt;\u0026quot;, \u0026quot;rpc_addr\u0026quot;: \u0026quot;tcp://data-seed-pre-0-s1.binance.org:80\u0026quot;, \u0026quot;symbol\u0026quot;: \u0026quot;BNB\u0026quot;, \u0026quot;deputy_addr\u0026quot;: \u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;, \u0026quot;fetch_interval\u0026quot;: 2, }, \u0026quot;eth_config\u0026quot;: { \u0026quot;swap_type\u0026quot;: \u0026quot;erc20_swap\u0026quot;, \u0026quot;key_type\u0026quot;: \u0026quot;private_key\u0026quot;, \u0026quot;private_key\u0026quot;: \u0026quot;\u0026lt;my-private-key\u0026gt;\u0026quot;, \u0026quot;provider\u0026quot;: \u0026quot;https://ropsten.infura.io/v3/cd9643b1870b489b93477921cb767882\u0026quot;, \u0026quot;swap_contract_addr\u0026quot;: \u0026quot;0xA08E0F38462eCd107adE62Ee3004850f2448f3d6\u0026quot;, \u0026quot;token_contract_addr\u0026quot;: \u0026quot;0xDec348688B060fB44144971461b3BAaC8BD1e571\u0026quot;, \u0026quot;deputy_addr\u0026quot;: \u0026quot;0x938a452d293c23C2CDEae0Bf01760D8ECC4F826b\u0026quot;, \u0026quot;gas_limit\u0026quot;: 300000, \u0026quot;gas_price\u0026quot;: 20000000000, } } 這樣注意幾個地方\ndb_config 填上 mysql url，有設密碼的話一併帶入 chain_config.bnb_start_height 可以先追到目前的 block height，畢竟我們的 swap tx 都在 deputy 起來之後才會產生，可以跳過啟動時 sync block 的時間。如果是要追過去的 tx，就要調整 block 的高度，並且給予足夠的時間上 deputy 去 sync block。可以到 testnet-explorer 查目前的 block height。 chain_config.other_chain_start_height 也追到最新的 eth block height，可以到 Etherscan 查目前的 block height bnb_config 填上 bnb addr 與助記祠 eth_config 填上 eth addr 與 private key eth_config.deupty_addr 使用 Admin addr，並填上 private key 把 deputy 以 testnet 為目標執行起來\n./build/deputy --help ./build/deputy \\ --bnb-network 0 \\ --config-type local \\ --config-path config/config.json 2019-10-25 17:16:48 DEBUG Debug sent a request 2019-10-25 17:16:48 INFO fetch ETH cur height: 6641795 2019-10-25 17:16:48 DEBUG Debug sent a request 2019-10-25 17:16:48 DEBUG Debug sent a request 2019-10-25 17:16:48 INFO fetch BNB cur height: 46215517 2019-10-25 17:16:48 DEBUG Debug sent a request 2019-10-25 17:16:48 INFO fetch try to get ahead block, chain=BNB, height=46215518 deputy 啟動後，就會依據 db 中的 block 資料，開始一路追 block，發現是相關的 addr 就把 tx 拉下來處理。\n由於我們這邊是新 db，我們又直接跳到最新的 block，應該不會需要太多時間就能追上。\ndeputy 有 admin api 可以使用\ncurl http://localhost:8080 curl http://localhost:8080/failed_swaps/1 the number of total failed swaps is 0, the offset of query is 0 curl http://localhost:8080/status { \u0026quot;mode\u0026quot;: \u0026quot;NormalMode\u0026quot;, \u0026quot;bnb_chain_height\u0026quot;: 46215719, \u0026quot;bnb_sync_height\u0026quot;: 46215718, \u0026quot;other_chain_height\u0026quot;: 6641804, \u0026quot;other_chain_sync_height\u0026quot;: 6641804, \u0026quot;bnb_chain_last_block_fetched_at\u0026quot;: \u0026quot;2019-10-25T17:18:52+08:00\u0026quot;, \u0026quot;other_chain_last_block_fetched_at\u0026quot;: \u0026quot;2019-10-25T17:18:40+08:00\u0026quot;, \u0026quot;bnb_status\u0026quot;: { \u0026quot;balance\u0026quot;: [ { \u0026quot;symbol\u0026quot;: \u0026quot;BNB\u0026quot;, \u0026quot;free\u0026quot;: \u0026quot;18.99812504\u0026quot;, \u0026quot;locked\u0026quot;: \u0026quot;0.00000000\u0026quot;, \u0026quot;frozen\u0026quot;: \u0026quot;0.00000000\u0026quot; } ] }, \u0026quot;other_chain_status\u0026quot;: { \u0026quot;allowance\u0026quot;: \u0026quot;9.8e+13\u0026quot;, \u0026quot;erc20_balance\u0026quot;: \u0026quot;9.999999969e+20\u0026quot;, \u0026quot;eth_balance\u0026quot;: \u0026quot;6.982922764\u0026quot; } } 可以看到 deputy 上設定的 bnb_addr, eth_addr 的狀態\nother_chain_status.allowance: 9.8e+13 開始 Swap 複習一下流程\n部署好 swap contract \u0026amp; deputy，啟動 deputy process Wallet Address 在 ERC20 Token (PPToken) approve()，給 Swap Contract 一部分 Token 初始化 Swap Transaction(tx) Deputy 監測到 Ethereum 鏈上的 swap tx，向 Binance Chain 發起一個對應的 HTLT tx，等待 Binance 上的 claim tx Wallet Address 向 Binance Chain 執行 HTLT claim() Deputy 監測到 Binance Chain 上的 claim tx 與 swap complete，代為向 Ethereum claim ERC-20 ERC20 contract Approve() 到 ERC20 contract 的頁面，選擇 approve\nspender: swap contract address value: 10000 PPToken (乘上 10^18)\n到 etherscan 上查看\nCall HTLT function Go to swap contract，call htlt()\n用 etherscan 送上去，然後就壞掉了，Etherscan 上壞掉的 tx 不知為什麼 QAQ\n開 remix IDE，重新嘗試參數\nrandomNumberHash: SHA256(randomNumber||timestamp), randomNumber is 32-length random byte array. 0x0000000000000000000000000000000000000000000000000000000000000000 timestamp: it should be about 10 mins span around current timestamp. unix timestamp 1572250902 (now + 60 sec * 10 min) heightSpan: it\u0026rsquo;s a customized filed for deputy operator. it should be more than 200 for this deputy. 10000 recipientAddr: deputy address on Ethereum. 0x938a452d293c23C2CDEae0Bf01760D8ECC4F826b bep2SenderAddr: omit this field with 0x0 0x0000000000000000000000000000000000000000 bep2RecipientAddr: Decode your testnet address from bech32 encoded to hex 0xee82ca237387495e9603f4d8d7efed128bdd59a6 outAmount: approved amount, should be bumped by e^10. 10 0000000000 bep2Amount: outAmount * exchange rate, the default rate is 1. 10 0000000000 Deputy Call HTLT on Binance Chain Deputy 監測 Ethereum 上的 block 狀態，特別是會取得 Swap contract address 的 swap tx，並在 Binance Chain 上產生對應的 HTLT。\nClaim HTLT on Binance Chain Binance Chain 上產生 HTLT 後，客戶端這邊可以使用 tbnb 以 recipient addr 查詢 Binance Chain 上的 Swap ID\ntbnbcli token query-swapIDs-by-recipient \\ --recipient-addr tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 獲得過去產生的 swap list\n[ \u0026quot;c2be98ac3b9ee7153e5ba84edfefca1917b6e2ec72d2576bf6cce584cbd6095e\u0026quot;, \u0026quot;18c938b994c62bcce9e8cedcb426a603863d95565a246c323a1df89d5c4226c1\u0026quot;, \u0026quot;5c4fdd60ce44fa4be6de70e65df3f8295df88178fd381b4242a8c2d047663a1b\u0026quot;, \u0026quot;a47c89dfca910cbb34dec92acebebb59d2c62e7f90bf216a87c2c23c84e48d4f\u0026quot; ] 都是過去使用的 swap id，如果都沒有新的 swap 出來，可能是 height span 太高，導致一直都爬不到\nSWAP_ID=c2be98ac3b9ee7153e5ba84edfefca1917b6e2ec72d2576bf6cce584cbd6095e tbnbcli token query-swap \\ --swap-id ${SWAP_ID} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 這邊要對一下 random number, to wallet addr, out amount 等參數，如果 HTLT 符合，客戶就可以執行 Claim HTLT\ntbnbcli token claim \\ --swap-id ${SWAP_ID} \\ --random-number ${RANDOM_NUMBER} \\ --from ${FROM_KEY} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 Deputy Claim ERC20 Token 客戶端在 Binance Chain 上 Claim HTLT，Deputy 在 Ethereuem 上 Claim HTLT，至此完成 Atomic Swap 兩邊的流程。客戶端從 Binance Chain Claim，Deputy 從 Ethereuem 上 Claim。完整流程 如下：\nClient Call HTLT on Ethereum -\u0026gt; Deputy Call HTLT on Binance Chain Client Check HTLT Status on Binance Chain Client Call Claim HTLT on Binance Chain -\u0026gt; Deputy Call Claim HTLT on Ethereum Client APP javascript Demo 希望直接寫成 javascript app 可以參考這篇\nSwap Tokens from Binance Chain to Ethereum 這邊進行反向操作，客戶發起從 Binance Chain 換到 Ethereum 上的請求，Deputy 做對應的處理，把 Token Swap 到 Ethereum。我們依樣依照這份文件操作。\n客戶端在 Binance Chain 上 Call HTLT() Deputy 在 Ethereum 上 Init Swap tx Deputy Call Approve() 到 ethereum swap contract 客戶端取得 swap 資訊 客戶端在 Ethereum 上 Call Claim()，取得 ERC-20 Token Deputy 在 Binance 上 Call Claim()，取得 Binance Chain Token 在 Binance Chain 上 HTLT 客戶端發起 HTLT 請求，需要從客戶端 Wallet 送出（但因為我們這邊只使用一個 Binance Address，所以發起的 Addr 跟 Deputy Binance addr 是一樣的。\n這邊鎖進 10:BNB，等待 100:PPT 從 ethereum 近來\nDEPUTY_BNB_WALLET_ADDR=tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce CLIENT_BNB_WALLET_ADDR=${DEPUTY_BNB_WALLET_ADDR} CLIENT_ETHEREUM_ADDR=0x938a452d293c23C2CDEae0Bf01760D8ECC4F826b tbnbcli token HTLT \\ --from ${CLIENT_BNB_WALLET_ADDR} \\ --recipient-addr ${DEPUTY_BNB_WALLET_ADDR} \\ --chain-id Binance-Chain-Nile \\ --height-span 500 \\ --amount 10:BNB \\ --expected-income 100:PPT \\ --recipient-other-chain ${CLIENT_ETHEREUM_ADDR} \\ --cross-chain \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 HTLT 回復的結果如下，可以在Binance Testnet Explorer\nRandom number: 75267ba4cc4f2d9ddbf9f90dc1ea813ae2a4d2114eb2ef2cb7ff0a5d285c7396 Timestamp: 1572337686 Random number hash: dabd990af86582969d47218012ecdb09899b9ad2b069c05be94ef82bea889a1b Password to sign with 'tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce': Committed at block 46889130 (tx hash: D653FC7B5D2048A2A165F49426CDCAD733703CF534367133819091892E3A1F14, response: {Code:0 Data:[119 24 196 176 181 1 29 177 60 107 100 166 55 16 253 136 159 3 204 56 109 46 63 87 93 9 239 158 138 172 21 129] Log:Msg 0: swapID: 7718c4b0b5011db13c6b64a63710fd889f03cc386d2e3f575d09ef9e8aac1581 Info: GasWanted:0 GasUsed:0 Tags:[{Key:[115 101 110 100 101 114] Value:[116 98 110 98 49 97 54 112 118 53 103 109 110 115 97 121 52 97 57 115 114 55 110 118 100 48 109 108 100 122 50 57 97 54 107 100 120 121 101 51 55 99 101] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[114 101 99 105 112 105 101 110 116] Value:[116 98 110 98 49 119 120 101 112 108 121 119 55 120 56 97 97 104 121 57 51 119 57 54 121 104 119 109 55 120 99 113 51 107 101 52 102 102 97 115 112 51 100] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0} {Key:[97 99 116 105 111 110] Value:[72 84 76 84] XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}] Codespace: XXX_NoUnkeyedLiteral:{} XXX_unrecognized:[] XXX_sizecache:0}) 取得 swap id 與 random number，使用 swap ip 查詢\nSWAP_ID=7718c4b0b5011db13c6b64a63710fd889f03cc386d2e3f575d09ef9e8aac1581 tbnbcli token query-swap \\ --swap-id ${SWAP_ID} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 回復當前的狀態\n{ \u0026quot;from\u0026quot;:\u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;, \u0026quot;to\u0026quot;:\u0026quot;tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\u0026quot;, \u0026quot;out_amount\u0026quot;:[ { \u0026quot;denom\u0026quot;:\u0026quot;BNB\u0026quot;, \u0026quot;amount\u0026quot;:\u0026quot;10\u0026quot; } ], \u0026quot;in_amount\u0026quot;:null, \u0026quot;expected_income\u0026quot;:\u0026quot;100:PPT\u0026quot;, \u0026quot;recipient_other_chain\u0026quot;:\u0026quot;0x938a452d293c23C2CDEae0Bf01760D8ECC4F826b\u0026quot;, \u0026quot;random_number_hash\u0026quot;:\u0026quot;dabd990af86582969d47218012ecdb09899b9ad2b069c05be94ef82bea889a1b\u0026quot;, \u0026quot;random_number\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;timestamp\u0026quot;:\u0026quot;1572337686\u0026quot;, \u0026quot;cross_chain\u0026quot;:true, \u0026quot;expire_height\u0026quot;:\u0026quot;46889630\u0026quot;, \u0026quot;index\u0026quot;:\u0026quot;2245\u0026quot;, \u0026quot;closed_time\u0026quot;:\u0026quot;0\u0026quot;, \u0026quot;status\u0026quot;:\u0026quot;Open\u0026quot; } Deputy Appove Token Deputy 已經抓到 Binance Chain 上的 tx，會記錄在 Mysql 內\nuse deputy; select * from tx_log limit 10; | id | chain | swap_id | tx_type | tx_hash | contract_addr | sender_addr | receiver_addr | sender_other_chain | other_chain_addr | in_amount | out_amount | out_coin | random_number_hash | expire_height | timestamp | random_number | block_hash | height | status | confirmed_num | create_time | update_time | | 2 | BNB | 7718c4b0b5011db13c6b64a63710fd889f03cc386d2e3f575d09ef9e8aac1581 | BEP2_HTLT | d653fc7b5d2048a2a165f49426cdcad733703cf534367133819091892e3a1f14 | | tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce | tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce | | 0x938a452d293c23C2CDEae0Bf01760D8ECC4F826b | 100:PPT | 10 | BNB | dabd990af86582969d47218012ecdb09899b9ad2b069c05be94ef82bea889a1b | 46889630 | 1572337686 | | 6948f34e4013da29c9922306b60b2c12f174925c63ff2a46d6b2fc3acd7a3774 | 46889130 | CONFIRMED | 6 | 1572337694 | 1572337695 | Deputy Send HTLT on Ethereum 查詢 Ethereum 上的 HTLT\n客戶端 Call Claim 取得 ERC-20 Token 客戶端取得 100:PPT\nDeputy Call HTLT Claim 取得 Binance Token Deputy 取得 10:BNB\nGCE gcloud compute ssh dep3-deputy sudo apt-get update sudo apt-get install mysql-server sudo cat /etc/mysql/debian.cnf wget https://dl.google.com/go/go1.13.3.linux-amd64.tar.gz tar -C /usr/local -xzf go1.13.3.linux-amd64.tar.gz export PATH=$PATH:/usr/local/go/bin go get github.com/binance-chain/bep3-deputy go mod tidy go build -o build/deputy main.go References Binace Chain Doc Binance BEP3 spec ","permalink":"https://chechia.net/posts/2019-10-22-blockchain-bep3-atomic-swap/","summary":"\u003ch1 id=\"bep3-atomic-swap\"\u003eBEP3 Atomic Swap\u003c/h1\u003e\n\u003cp\u003eBinance 在 \u003ca href=\"https://github.com/binance-chain/BEPs/blob/master/BEP3.md\"\u003eBEP3: HTLC and Atomic Peg\u003c/a\u003e 提到，BEP 即將在 binance chain 上支援原生的 Hash Timer Locked Transfer (HTLT) ，這使跨鏈的原子性交換 (atomic swap) 變得可行，透過 HTLC 在兩邊的鏈上鎖住 (peg) tokens，然後只有在執行交換的時候，透過 hash 交換，一次執行雙邊的交易。\u003c/p\u003e\n\u003cp\u003e關於 Atomic Swap 網路有非常多的訊息，有興趣的話可以看\u003ca href=\"https://en.bitcoin.it/wiki/Atomic_swap\"\u003e這篇\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e交易只有在雙邊完成後才完成，完成之前不能動用交換的資產\u003c/li\u003e\n\u003cli\u003e在任何階段失效都可以完全 fallback，並進行 refund\u003c/li\u003e\n\u003cli\u003e交易的認證是去中心化的\n\u003cul\u003e\n\u003cli\u003e這邊有個但書，Ethereum 上是透過 smart contract 實現，但 Binance chain 上還是靠 Binance 認證 XD\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBinance 在 BEP3 中支援 HTLC，我們這邊主要的資訊來源是 binance.org 的\u003ca href=\"https://docs.binance.org/atomic-swap.html\"\u003e官方說明文件\u003c/a\u003e，這邊針對文章進行驗證，並且補足文件缺漏的部分，提醒過程中可能會踩到的雷。\u003c/p\u003e\n\u003ch3 id=\"跨鍊cross-chain-交易\"\u003e跨鍊(Cross Chain) 交易\u003c/h3\u003e\n\u003cp\u003e在部署 asset / token 的時候，我們會選擇合適的鏈作為發布資產並運行 block chain app。常用的應用鏈如 ethereum 與 binance chain 等等。不同的主鏈上有各自的優缺點，例如使用 ethereum ，可以與許多 token 與應用互動，也是最多人使用的應用主鏈。而在 binance 鏈上執行，則能夠快速的發生 transactions，並且可以與 binance 上的資產與交易所互動。\u003c/p\u003e","title":"Blockchain Bep3 Atomic Swap"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 超簡短推坑 oeprator-sdk 鐵人賽心得 承上 上篇介紹了 crd 與 controller，然而沒有說明 controller 的編寫與操作，因為 controller 的部分比較複雜，我們鐵人挑戰賽尾聲，篇幅說實在是不太夠。\n有興趣詳細了解的大德，請參考相同鐵人挑戰團隊的隊友文章，裏頭對 controller 有詳細介紹，這邊就不贅述。直接提供個人使用覺得最簡單上手的 operator sdk\nOperator SDK Operator SDK 是 Operator framework 中的一部分，能有效且自動化的管理 kubernetes native apps, operator 的管理工具。\n複雜的 kubernetes application 是非常難管理的，寫 operator 也是很有挑戰，不僅要處理大量 kubernetes 底層的 API，要寫很多樣版。 operator SDK 使用 controller-runtime 的 library 讓編寫 native application 變得簡單許多\n可以使用上層的 API 與抽象來編寫 operator 邏輯 快速使用 code generation 有擴充套件 Workflow 這邊以 golang 為例說明\n安裝 operator sdk 定義新的 API resource (custom resource definition) 定義 controller 來監測 custom resource 編寫 reconciling 邏輯來 sync desired state 與 current state 使用 sdk cli 進行測試 使用 sdk cli 來 build，並產生部屬用的 manifests 安裝請依照 安裝說明 操作即可。\n這邊使用 sdk cli 來增加新的 crd\n# Add a new API for the custom resource AppService $ operator-sdk add api --api-version=app.example.com/v1alpha1 --kind=AppService 產生的 go 源碼會放在 pkg 中，可以依自己需求調整 crd 的結構\n這邊使用 sdk cli 產生對應 crd 的 controller，裏頭已經寫好大部分的 code gene 與 reconcile 的樣板，直接修改就可使用，非常方便\n# Add a new controller that watches for AppService $ operator-sdk add controller --api-version=app.example.com/v1alpha1 --kind=AppService 修改完，直接使用 sdk cli build 成 image，然後推到 image hub 上\n# Build and push the app-operator image to a public registry such as quay.io $ operator-sdk build quay.io/example/app-operator $ docker push quay.io/example/app-operator 部屬前檢查一下 manefests 檔案，特別是 crd.yaml 與 operator.yaml，如果源碼有調整記得做對應的修改。\n# Setup Service Account $ kubectl create -f deploy/service_account.yaml # Setup RBAC $ kubectl create -f deploy/role.yaml $ kubectl create -f deploy/role_binding.yaml # Setup the CRD $ kubectl create -f deploy/crds/app.example.com_appservices_crd.yaml # Deploy the app-operator $ kubectl create -f deploy/operator.yaml 這樣便部屬了 operator，operator 會監看指定的 custom resource，並依照 controller 的邏輯進行 reconcile。\n這邊以增加 custom resource 為例\n# Create an AppService CR # The default controller will watch for AppService objects and create a pod for each CR $ kubectl create -f deploy/crds/app.example.com_v1alpha1_appservice_cr.yaml 增加一個 cr 到 kubernetes 上，這時 operator 會偵測到 cr 的變化，並且依照 reconcile 的邏輯 sync\n檢查一下 cr 與 operator 的狀態\n# Verify that a pod is created $ kubectl get pod -l app=example-appservice NAME READY STATUS RESTARTS AGE example-appservice-pod 1/1 Running 0 1m 詳細的操作步驟可以看 這邊\n小結 事實上，operator sdk 的功能還有非常多，細講又要花好幾篇文章講，之後有機會會放在我的個人網站上。\n另外 operator sdk 也歡迎外部的 Issue 與 PR，團隊的人非常 nice 會願意花時間跟社群朋友溝通，有興趣請來 contribute。\n這系列鐵人文章，說實在沒有什麼很深入的技術討論，多半資料都是各個項目的官方文件翻譯，加上一些個人的經驗與解讀，並不是含金量很高的文章。然而我個人在接觸這些項目時，卻往往因為找不到細節操作的步驟分享文章，在許多小細節上撞牆很久，也因此才有了這系列文章。\n這系列文就只是踩雷之旅，讓後人如果有用到這些文章，生活能過得開心一點，這 30 天的時間就有了價值。\n鐵人挑戰賽的最後一天，感謝各路大德一路相隨，讓我在假日也能心甘情願地坐下來寫文章。游於藝天一篇真的很逼人，有幾天的文章品質是有蠻多問題的，也感謝大德們協助捉錯，給予很多建議。\n謝謝各位。\n","permalink":"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e超簡短推坑 oeprator-sdk\u003c/li\u003e\n\u003cli\u003e鐵人賽心得\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"承上\"\u003e承上\u003c/h1\u003e\n\u003cp\u003e上篇介紹了 crd 與 controller，然而沒有說明 controller 的編寫與操作，因為 controller 的部分比較複雜，我們鐵人挑戰賽尾聲，篇幅說實在是不太夠。\u003c/p\u003e","title":"Kubernetes Custom Resources with Operator SDK"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 CRD 內容 Deploy CRD Use custom resource Recap 在上次的 cert-manager 內容中我們走過 cert-manager 的安裝步驟，其中有一個步驟是 apply cert-manager 的 manigests 檔案 *.yaml)\nhttps://github.com/jetstack/cert-manager/tree/release-0.11/deploy/manifests\n$ git clone https://github.com/jetstack/cert-manager $ git checkout release-0.11 $ ls deploy/manifest 00-crds.yaml 01-namespace.yaml BUILD.bazel\tREADME.md\thelm-values.yaml 我們快速看一下這個 00-crds.yaml，這個 yaml 非常長，直接跳到 certificates.certmanager.k8s.io\n希望看 golang 源碼文件的話，可以搭配godoc.org/k8s.io/apiextensions 來閱讀，更能理解 definition。\n在看之前先注意幾件事，CRD 內除了 schema 外，還定義了許多不同情境的使用資料。\nCRD 內定義了 custom resource 的資料儲存 .spec.validation.openAPIV3Schema，使用 custom resource 會透過 validator 驗證 .openAPIV3Schema 內定義了 .spec，以及 rumtime 中紀錄 .status 的資料 controller 可以把狀態 sync 到 custom resource 的 .status 中紀錄 controller 可以比對 .spec 與 .status 來決定是否要 sync 以及如何 sync CRD 內定義了與 server 以及 client 互動的方式， names 中定義各種使用情境的 custom resource 名稱 additionalPrinterColumns 中添加 kubectl 中的顯示內容 // 這邊使用的是 v1beta1 的 API (deprecated at v1.16) ，新版開發建議使用 apiextension.k8s.io/v1 的 api apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: creationTimestamp: null name: certificates.cert-manager.io spec: // 使用 kubectl 會額外顯示的資訊內容，透過 jsonpath 去 parse 顯示 additionalPrinterColumns: - JSONPath: .status.conditions[?(@.type==\u0026quot;Ready\u0026quot;)].status name: Ready type: string - JSONPath: .spec.secretName name: Secret type: string - JSONPath: .spec.issuerRef.name name: Issuer priority: 1 type: string - JSONPath: .status.conditions[?(@.type==\u0026quot;Ready\u0026quot;)].message name: Status priority: 1 type: string - JSONPath: .metadata.creationTimestamp description: CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. name: Age type: date group: cert-manager.io // 定義 CRD 在不同情境下使用的名稱 names: kind: Certificate listKind: CertificateList plural: certificates shortNames: - cert - certs singular: certificate scope: Namespaced subresources: status: {} validation: // openAPIV3Schema 中是 custom resource 實際操作會使用的內容 // properties 使用 . .description .type ，分別定義名稱，描述，檢查型別 openAPIV3Schema: description: Certificate is a type to represent a Certificate from ACME properties: apiVersion: description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources' type: string kind: description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds' type: string // custom resource runtime 中的 metadata metadata: type: object // custom resource 使用時的 spec，定義 custom resoure 的 desired status spec: ... // custom controller 監測 custom resource 的 current status，這邊的資料完全視 controller 實作來產生，如果沒有實作 sync status，也可以沒有資料 status: ... type: object // 這個是 CRD 物件的 version，可以定義多個不同 version 的 CRD，調用時需要註明版本 version: v1alpha2 versions: - name: v1alpha2 served: true storage: true // 這個是 CRD 物件的 status，描述 CRD 部署到 API server 的狀態，例如 CRD 儲存適用 configmap 的儲存空間，這邊顯示在 API server 上的儲存狀態。不要跟 custom resource 的 status 弄混了 status: acceptedNames: kind: \u0026quot;\u0026quot; plural: \u0026quot;\u0026quot; conditions: [] storedVersions: [] helm-values.yaml 與 01-namespace.yaml 很單純，前者是使用 helm 部署的可設定參數，預設只有 kubernetes resources，後者則是為之後的 cert-manager 元件新增一個 kubernetes namespace。\n小結 CRD 內容 (apiextensions/v1beta1) CRD 顯示名稱，內容 CRD spec 驗證 custom resource custom resource schema CRD 自身部署狀態 部署 部署相較定義本身就非常簡單，直接 kubectl apply 到 kubernetes 上\n使用 custom resource 有了 CRD，我們便可以使用 CRUD API，互動模式與其他 build-in kubernetes resources 相同，只是內容會照 CRD 上的定義調整\nkubectl get certificates.certmanager.k8s.io kubectl get certificates kubectl get certs --all-namespaces kubectl get cert -n cert-manager NAMESPACE NAME READY SECRET AGE cert-manager ingress-nginx-tls True ingress-nginx-tls 221d 這邊看到的內容可能會有些落差，因為我當初用的版本比較舊，但內容大同小異。\n底下的 describe 內容已經跟上面的 CRD 版本差太多，對不起來了。但我也懶得再佈一組，還要重做 dnsName 與 authotization challenge\n直接讓大家感受一下舊版的內容XD\n$ kubectl describe cert ingress-nginx-tls Name: ingress-nginx-tls Namespace: cert-manager API Version: certmanager.k8s.io/v1alpha1 Kind: Certificate Metadata: Creation Timestamp: 2019-03-06T06:48:26Z Generation: 4 Owner References: API Version: extensions/v1beta1 Block Owner Deletion: true Controller: true Kind: Ingress Name: ingress-nginx Self Link: /apis/certmanager.k8s.io/v1alpha1/namespaces/default/certificates/ingress-nginx-tls Spec: Acme: Config: Domains: chechiachang.com Http 01: Ingress: Ingress Class: nginx Dns Names: chechiachang.com Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-prod Secret Name: ingress-nginx-tls // 當前的 status，controller sync 上來 // controller 會比對 .spec 與 .status，判斷是否需要做事，ex. renew Status: Acme: Order: URL: https://acme-v02.api.letsencrypt.org/acme/order/* Conditions: Last Transition Time: 2019-09-02T03:52:03Z Message: Certificate renewed successfully Reason: CertRenewed Status: True Type: Ready Last Transition Time: 2019-09-02T03:52:01Z Message: Order validated Reason: OrderValidated Status: False Type: ValidateFailed Events: \u0026lt;none\u0026gt; 想要新增，可以回去看 cert-manager tutorial，這個是新版的文件\n當然，不爽這個 cert resource 也可以幹掉\n$ kubectl delete cert ingress-nginx-tls -n cert-manager 以上\n小結 簡介 CRD 與 CRD 內容 操作 custom resource ","permalink":"https://chechia.net/posts/2019-10-13-kubernetes-custom-resource-deployment/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eCRD 內容\u003c/li\u003e\n\u003cli\u003eDeploy CRD\u003c/li\u003e\n\u003cli\u003eUse custom resource\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"recap\"\u003eRecap\u003c/h1\u003e\n\u003cp\u003e在上次的 cert-manager 內容中我們走過 cert-manager 的安裝步驟，其中有一個步驟是 apply cert-manager 的 manigests 檔案 \u003ccode\u003e*.yaml\u003c/code\u003e)\u003c/p\u003e","title":"Kubernetes Custom Resource Deployment"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 custom resources custom controllers 簡介 custom resources Kubernetes 預先定義許多 resource ，這些 resource 是 kubernetes API 預先設置的 API objects，例如 kubernetes pods resource 包含許多 pods 物件。\nCustom resoure 則是透過擴充 kubernetes API ，讓自定義的物件也可以在 kubernetes 上使用。上篇 cert-manager 就使用了許多 custom resource，這些 resource 在一般安裝的 kubernetes 上沒有安裝，需要安裝 custom resource difinition，向 kubernetes cluster 定義新的 custom resource。例如 certificates.certmanager.k8s.io 就是 cert-manager 自定義的資源，用來代表產生 x509 certificate 的內容。\n越來越多的 kubernetes core 方法，如今也使用 custom resources 來定義，讓 kubernetes 核心元件更加模組化。\ncustom resource 可以在運行中的 kubernetes 集群中註冊 (registration) ，也可以動態註銷，custom resource 並不會影響集群本身的運作。只要向 kubernetes 註冊完 custom resource，就可以透過 API 與 kubectl 控制 custom resource，就像操作 Pod resource 一樣。\nCustom controllers custom resource 一但註冊，就可以依據 resource 的 CRD (custom resource definition) 來操作，因次可以儲存客製化的資料內容。然而在很多情形，我們並不只要 custom resource 來讀寫，而是希望 custom resource 能執行定義的工作，如同 Pod resource 可以在 kubernetes 集群上控制 Pod，在 Pod resource 上描述的 desired state kubernetes 會透過定義在 Pod API 中的 sync 邏輯，來達到 current state 與 desired state 的平衡。\n我們希望 custom resource 也能做到上述的功能，提供 declarative API，讓使用者不需編寫完整的程式邏輯，只要透過控制 custom resource，就可以透過 controller 內定義的邏輯，來實現 desired state。使用者只需要專注在控制 custom resource 上的 desired state，讓 controller 處理細節實作。\n例如：我們在 cert-manager 中設定 certificates.certmanager.k8s.io 資源，來描述我們希望取得 x509 certificate 的 desired state，但我們在 certificates.certmanager 上面沒有寫『透過 Let\u0026rsquo;s Encrypt 取得 x509 certificate』的實現邏輯，仍然能透過 cert-manager 產生 x509 certiticate，因為 cert-manager 內部已經定義 certificates.certmanager.k8s.io 的 custom controller。\n基本的 custom resource 操作\n註冊 custom resource definition，讓 kubernetes API 看得懂 custom resource 不然 API 會回覆 error: the server doesn\u0026rsquo;t have a resource type 有 CRD 便可以 apply custom resource 到集群中 部署 custom controller，監測 custom resource 的 desired state 內容，並實現達到 desired state 的業務邏輯 沒有 custom controller，custom resource 就只是可以 apply 與 update 的資料儲存結構，沒有 cert-manager 中 controller 的邏輯，也還是生不出 x509 certificate。 kubectl get chechiachang error: the server doesn't have a resource type \u0026quot;chechiachang\u0026quot; custom controller 也可以跟其他的 kubernetes resource 互動，例如 cert-manager 在產生 certificate 的時候，會把產生的 certificate 檔案放在 secret 中，cert-manager 會依據 order 中定義的 lifecycle ，持續檢查 certificate 的有效性，如果接近過期，則會觸發新的一輪 order。\n我們也可以寫一個操作 Configmap 與 Deployment Resource 的 custom controller，來進行 deploymnet 的 Image 更新。\n我需要 custom resource 嗎 kubernetes 在should I add custom resource 有列表分析該不該使用 custom resource ，將你的 API 邏輯整合到 kubernetes API 上。幾個判斷參考:\nAPI 是 declarative model，如果不是可能不適合跟 kubernetes API 整合，獨立成為一個自己運行的服務即可 需要使用 kubernetes 需要使用 kubectl 控制 API 需要使用 kubernetes 支援的功能 正哉開發全新功能，因為整合舊的服務到 kubernetes API 工程浩大 也許 configmap/secret 就可以解決 如果只是需要將資料儲存在 kubernetes 上，有一個 build-int 的 kubernetes resource 很適合，就是 configmap。可以瘀考以下條件，判斷是否 configmap 搭配能監看 configmap 的 controller 就可以達成需求。\n已經有完整的 config file，例如 mysql.cnf, nginx.conf\u0026hellip; 主要用途是把檔案掛載到 Pod 中的 process 使用 使用時的格式，是整個檔案放在 Pod 中，或是使用環境變數塞到 Pod 裡面，而不是透過 kubernetes API 存取 (ex 使用 kubectl) 更新 configmpa 時更新 Pod，會比更新 custom resource 時更新 Pod 容易 如果使用 CRD 或 Aggregated kubernetes API，大多符合下列條件\n使用 kubernetes libraries 與客戶端 (ex kubectl) 操作 custom resource 需要 top level 的 kubernetes 支援，例如可以 kubectl get cheachiachang 自動化 kubernetes 物件 需要用到 .spec, .status, .metadata，這些比較 desired state 與 currenty state 的功能 需要抽象類別來管理一群 controlled resource Custom Resource Definition Custom Resoure Definition 讓使用者可以定義 custom resource，定義 custom resource 的格式包括名稱與 data schema，然後交給 kubernetes API 去處理 custom resource 的儲存。\n也就是說，透過 CRD 我們不用寫 custom resource 的 API，例如 cert-manager 不用寫 certificates.certmanager.k8s.io 的 API，而是向 kubernetes API server 註冊 CRD，讓 kubernetes API server 看得懂 custom source 的定義，並且直接使用 kubernetes API server，進行 custom resource 的 CRUD。\n我們可以透過 kubectl (API server) 操作 certificates.certmanager.k8s.io，這個請求也是送到 kubernetes API server。\nAPI server aggregation 能夠透過註冊 CRD ，就可以使用原來的 kubernetes API 來進行 CRUD ，是因為 kubernetes API 對於普通的 API 操作提供泛型 (generic) 介面，直接使用 CRUD 的邏輯。\n由於是 kubernetes Aggregated API ，所有 kubernetes 的 clients 都一起兼容新註冊的 custom resource，不用在 API 要定義，在冊戶端也要定義。註冊完的 custom resource definition，可以直接透過 kubectl 存取。\n小結 custom resource 簡介 custom resource 使用的情境與條件 custom resource definition 與 Aggregated API ","permalink":"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ecustom resources\u003c/li\u003e\n\u003cli\u003ecustom controllers\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"簡介-custom-resources\"\u003e簡介 custom resources\u003c/h1\u003e\n\u003cp\u003eKubernetes 預先定義許多 resource ，這些 resource 是 \u003ca href=\"https://kubernetes.io/docs/reference/using-api/api-overview/\"\u003ekubernetes API\u003c/a\u003e 預先設置的 \u003ca href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/\"\u003eAPI objects\u003c/a\u003e，例如 kubernetes pods resource 包含許多 pods 物件。\u003c/p\u003e","title":"Kubernetes Custom Resources Basic"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\nRecap 昨天我們實際使用 cert-manager，為 nginx ingress controller 產生 certificates，過程中我們做了幾件事\n設置 Let\u0026rsquo;s Encript prod site 的 Issuer 設置 certificates.certmanager.k8s.io 資源來定義 certificate 的取得方式 或是在 ingress 中配置 tls，讓 cert-manager 自動透過 ingress-shim 產生 certifcates.cert-manager，並且產生 certificate 以上是使用 cert-manager 產生 certificate 的基本操作，剩下的是由 cert-manager 完成。實際上 cert-manager 在產生出 certificate 之前還做了很多事情，我們今天就詳細走過完整流程，藉此了解 cert-manager 配合 issuing certificate 的流程\n使用者設置 Issuer\n使用者設定 certificate -\u0026gt; cert-manager 根據 certificate -\u0026gt; 產生 certificate\nCertificateRequests certificaterequests.certmanager 是 cert-manager 產生 certificate 過程中會使用的資源，不是設計來讓人類操作的資源。\n當 cert-manager 監測到 certificate 產生後，會產生 certificaterequests.certmanager.k8s.io 資源，來向 issuer request certificate，這個過程與使用其他客戶端 (ex. certbot) 來向 3rd party CA server request certificate 時的內容相同，只是這邊我們使用 kubernetes resource 來定義。\n包含的 certificate request，會以 pem encoded 的形式，再變成 base64 encoded 存放在 resource 中。這個 pem key 也會從到遠方的 CA sercer (Let\u0026rsquo;s Encrypt prod) 來 request certificate\n如果 issuance 成功，certificaterequest 資源應該會被 cert-manager 吃掉，不會被人類看到。\n一個 certificaterequests.certmanager 大概長這樣\napiVersion: cert-manager.io/v1alpha2 kind: CertificateRequest metadata: name: my-ca-cr spec: csr: LS0tLS1CRUdJTiBDRVJUSUZJQ0FUR .................................. LQo= isCA: false duraton: 90d issuerRef: name: ca-issuer # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: Issuer group: cert-manager.io 這個 certificaterequests.certmanager 會讓 cert-manager 嘗試向 Issuer (lets-encrypt-prod) request certificate。\nOrder orders.certmanager.k8s.io 被 ACME 的 Issuer 使用，用來管理 signed TLD certificate 的 ACME order，這個 resource 也是 cert-manager 自行產生管理的 resource，不需要人類來更改。\n當一個 certificates.certmanager 產生，且需要使勇 ACME isser 時，certmanager 會產生 orders.certmanager ，來取得 certificate。\nChallenges challenges.certmanager 資源是 ACME Issuer 管理 issuing lifecycle 時，用來完成單一個 DNS name/identifier authorization 時所使用的。用來確定 issue certiticate 的客戶端真的是 DNS name 的擁有者。\n當 cert-manager 產生 order 時，order controller 接到 order ，就會為每一個需要 DNS certificate 的 DNSname ，產生 challenges.certmanager。\n這段也是 order controller 自動產生，並不需要使用者參與。\nACME certificate issuing user -\u0026gt; 設定好 issuers.certmanager\nuser -\u0026gt; 產生 certificates.certmanager -\u0026gt; 選擇 Issuer -\u0026gt;\ncert-manager -\u0026gt; 產生 certificaterequest -\u0026gt;\ncert-manager 根據 certiticfates.certmanager 產生 orders.certmanager -\u0026gt;\norder controller 根據 order ，並且跟每一個 DNS name target，產生一個 challenges.certmanager\nchallenges.certmanager 產生後，會開啟這個 DNS name challenge 的 lifecycle\nchallenges 狀態為 queued for processing，在佇列中等待， 如果沒有別的 chellenges 在進行，challenges 狀態變成 scheduled，這樣可以避免多個 DNS challenge 同時發生，或是相同名稱的 DNS challenge 重複 challenges 與遠端的 ACME server \u0026lsquo;synced\u0026rsquo; 當前的狀態，是否 valid 如果 ACME 回應這個 DNS name 的 challenge 還是有效的，則直接把 challenges 的狀態改成 valid，然後移出排程佇列。 如果 challenges 狀態仍然為 pending，challenge controller 會依照設定 present 這個 challenge，使用 HTTP01 或是 DNS01，challenges 被標記為 presented challenges 先執行 self check，確定 challenge 狀態已經傳播給 dns servers，如果 self check 失敗，則會依照 interval retry ACME authorization 關聯到 challenge cert-manager 處理 \u0026lsquo;scheduled\u0026rsquo; challenges.certmanager -\u0026gt; ACME challenge\n","permalink":"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"recap\"\u003eRecap\u003c/h1\u003e\n\u003cp\u003e昨天我們實際使用 cert-manager，為 nginx ingress controller 產生 certificates，過程中我們做了幾件事\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e設置 Let\u0026rsquo;s Encript prod site 的 Issuer\u003c/li\u003e\n\u003cli\u003e設置 certificates.certmanager.k8s.io 資源來定義 certificate 的取得方式\u003c/li\u003e\n\u003cli\u003e或是在 ingress 中配置 tls，讓 cert-manager 自動透過 ingress-shim 產生 certifcates.cert-manager，並且產生 certificate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e以上是使用 cert-manager 產生 certificate 的基本操作，剩下的是由 cert-manager 完成。實際上 cert-manager 在產生出 certificate 之前還做了很多事情，我們今天就詳細走過完整流程，藉此了解 cert-manager 配合 issuing certificate 的流程\u003c/p\u003e","title":"Cert Manager Complete Workflow"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n今天我們來實際使用 cert-manager，為 nginx ingress controller 產生 certificates with ACME Issuer\nCA Terminology 先把實際執行 CA 簽發的名詞定義一下，以免跟 cert-manager 的資源搞混\nCertificate: 憑證，x509 certificate，cert-manager 自動管理的目標，透過 let\u0026rsquo;s encript 取得的 x509 certificates CA (Certificate Authority): issue signed certificate 的機構 issue: 頒發，指 CA 產生 certificate 與 key (今天的範例格式是 .crt 與 .key) Sign vs self-signed: 簽核，自己簽核，使用信任的 CA issue certificate，或是使用自己產生的 CA self-sign，然後把 CA 加到可以被信任的 CA 清單中。 Let\u0026rsquo;s Encript CA issues signed certificates\nKubernetes in-cluster CA issues self-signed certificates\ncert-manager 的 CRD 資源，使用來描述 cert-manager 如何執行上述操作，CRD 底下都會加上 ``*.certmanager.k8s.io` 方便辨識。\n設定 Issuer Issuer 要怎麼翻成中文XD，憑證頒發機構？\n總之在開始簽發 certificates 前，要先定義 issuers.certmanager.k8s.io ，代表一個能簽發 certificate CA，例如 Let\u0026rsquo;s Encript，或是 kubernetes 內部也有內部使用的憑證簽發，放在 secrets 中。\n這些 Issuer 會讓 certificates.certmanager.k8s.i8o 使用，定義如何取得 certificate 時，選擇 Issuer。\ncert-manager 上可以定義單一 namespace 的 issuers.certmanager 與集群都可使用的 clusterissuers.certmanager\ncert-manager 有支援幾種的 issuer type\nCA: 使用 x509 keypair 產生certificate，存在 kubernetes secret Self signed: 自簽 certificate ACME: 從 ACME (ex. Let\u0026rsquo;s Encrypt) server 取得 ceritificate Vault: 從 Vault PKI backend 頒發 certificate Venafi: Venafi Cloud Certificate 有了簽發憑證的單位，接下來要定義如何取得 certificate。certificates.certmanager.k8s.io 是 CRD，用來告訴 cert-manager 要如何取得 certificate\ncertifcates.certmanager.k8s.io 提供了簡單範例\napiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: acme-crt spec: secretName: acme-crt-secret duration: 90d renewBefore: 30d dnsNames: - foo.example.com - bar.example.com acme: config: - http01: ingressClass: nginx domains: - foo.example.com - bar.example.com issuerRef: name: letsencrypt-prod # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: Issuer 上面這個 certificate.certmanger 告訴 cert-manager\n針對 foo.example.com 與 bar.example.com 兩個 domainsc 使用 letsencript-prd Issuer 去取得 certificate key pair 成功後把 ceritifcate 與 key 存在 secret/acme-crt-secret 中(以 tls.key, tls.crt 的形式) 與 certificate.certmanager 都放在相同 namespace 中，產生 certificate.certmanager 的時候要注意才不會找不到 secret 這邊指定了 certificate 的有效期間與 renew 時間 (預設值)，有需要可以更改 配合 Ingress 設置 tls 有上述的設定，接下來可以請求 tls certificate\n記得我們上篇 Nginx Ingress Controller 提到的 ingreess 設定嗎？這邊準備了一個適合配合 nginx ingress 使用的 tls 設定\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-nginx-ingress annotations: kubernetes.io/ingress.class: \u0026quot;nginx\u0026quot; cert-manager.io/issuer: \u0026quot;letsencrypt-prod\u0026quot; spec: tls: - hosts: - foo.example.com secretName: my-nginx-ingrss-tls rules: - host: foo.example.com http: paths: - path: / backend: serviceName: chechiachang-backend servicePort: 80 這個 ingress apply 後，就會根據 spec.tls 的 hosts 設定，自動產生一個 certificate.certmanager 資源，並在這個資源使用 letsencryp-prod。\n不用我們手動 apply 新的 ceritificate，這邊是 cert-manager 使用了 annotation 來觸發 Ingress-shim，簡單來說，當 ingress 上有使用 cert-manager.io 的 annotation 時，cert-manager 就會根據 ingress 設定內容，抽出 spec.tls 與 isuer annotation，來產生同名的 certificates.certmanager，這個 certificateas.certmanager 會觸發接下的 certificate 頒發需求。\n只要部署 Issuer 與 Ingress 就可以自動產生 certificate。當然，希望手動 apply certificates.certmanager 也是行得通。\n把產生了 certificate.certmanager 拉出來看\nkubectl describe certificate my-nginx-ingress Name: my-nginx-ingress Namespace: default API Version: cert-manager.io/v1alpha2 Kind: Certificate Metadata: Cluster Name: Creation Timestamp: 2019-10-10T17:58:37Z Generation: 0 Owner References: API Version: extensions/v1beta1 Block Owner Deletion: true Controller: true Kind: Ingress Name: my-nginx-ingress Resource Version: 9295 Spec: Dns Names: example.your-domain.com Issuer Ref: Kind: Issuer Name: letsencrypt-prod Secret Name: my-nginx-ingress-tls Status: Acme: Order: URL: https://acme-prod-v02.api.letsencrypt.org/acme/order/7374163/13665676 Conditions: Last Transition Time: 2019-10-10T18:05:57Z Message: Certificate issued successfully Reason: CertIssued Status: True Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreateOrder 1d cert-manager Created new ACME order, attempting validation... Normal DomainVerified 1d cert-manager Domain \u0026quot;foo.example.com\u0026quot; verified with \u0026quot;http-01\u0026quot; validation Normal IssueCert 1d cert-manager Issuing certificate... Normal CertObtained 1d cert-manager Obtained certificate from ACME server Normal CertIssued 1d cert-manager Certificate issued Successfully 把 certificate 從 secret 撈出來看\n$ kubectl describe secret my-nginx-ingress-tls Name: my-nginx-ingress-tls Namespace: default Labels: cert-manager.io/certificate-name=my-nginx-ingrsss-tls Annotations: cert-manager.io/alt-names=foo.example.com cert-manager.io/common-name=foo.example.com cert-manager.io/issuer-kind=Issuer cert-manager.io/issuer-name=letsencrypt-prod Type: kubernetes.io/tls Data ==== tls.crt: 3566 bytes tls.key: 1675 bytes 如此便可以透過 ingress 設定 nginx 使用 https\n小結 了解 *.certmanager.k8s.io CRD 定義與意義 設定 Issuer 與 certificate 透過 ingress-shim 直接部署 ingress 來產生 certificate ","permalink":"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e今天我們來實際使用 cert-manager，為 nginx ingress controller 產生 certificates with ACME Issuer\u003c/p\u003e\n\u003ch1 id=\"ca-terminology\"\u003eCA Terminology\u003c/h1\u003e\n\u003cp\u003e先把實際執行 CA 簽發的名詞定義一下，以免跟 cert-manager 的資源搞混\u003c/p\u003e","title":"Cert Manager How It Work"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Cert-manager Introduction Deploy cert-manager 簡介 cert-manager TLS certificate 管理很重要，但在 kubernetes 上管理 TLS certificates 很麻煩。\n以往我們使用 Let\u0026rsquo;s Encrypt 提供的免費自動化憑證頒發，搭配 kube-lego 來自動處理 certificate issuing，然而隨著 kube-lego 已不再更新後，官方建議改使用 Cert-manager 來進行 kubernetes 上的憑證自動化管理。\ncert-manager 是 kubernetes 原生的憑證管理 controller。是的他的核心也是一個 controller，透過 kubernetes object 定義 desired state，監控集群上的實際狀態，然後根據 resource object 產生憑證。cert-manager 做幾件事情\n在 kubernetes 上 使用 CRD (Customized Resource Definition) 來定義 certificate issuing 的 desired state 向 let\u0026rsquo;s encrypt 取得公開的憑證 在 kubernetes 上自動檢查憑證的有效期限，並自動在有效時限內 renew certificate。 安裝 官方文件有提供 詳細步驟 可以直接使用 release 的 yaml 部屬，也可以透過 helm。\n使用 yaml 部屬 # Create a namespace to run cert-manager in kubectl create namespace cert-manager # Disable resource validation on the cert-manager namespace kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true 開一個獨立的 namespace 來管理 cert-manager resources\n取消 namespcae 中的 kubernetes validating webhook。由於 cert-manager 本身就會使用 ValidatingWebhookConfiguration 來為 cert-manager 定義的 Issuer, Certificate resource 做 validating。然而這會造成 cert-manager 與 webhook 的循環依賴 (circling dependency)\n# Install the CustomResourceDefinitions and cert-manager itself kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml # Install the CustomResourceDefinitions and cert-manager itself kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml 這個 yaml 裡面還有幾個元件\nCluster Role-bindings CustomResourceDefinition certificaterequests.certmanager.k8s.io certificates.certmanager.k8s.io challenges.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io orders.certmanager.k8s.io 這些元件的細節，留待運作原理分析時再詳解。\nhelm deployment 這邊也附上使用 helm 安裝的步驟\n#!/bin/bash # Install the CustomResourceDefinition resources separately kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.10.1 \\ jetstack/cert-managerNAMESPACE=cert-manager 部屬完檢查一下\nkubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m 這邊部屬完，會獲得完整的 cert-manager 與 cert-manager CRD，但 certificate 的 desired state object 還沒部屬。也就是關於我們要如何 issue certificate 的相關描述，都還沒有 deploy， cert-manager 自然不會工作。關於 issuing resources configuration，我們下次再聊。\n","permalink":"https://chechia.net/posts/2019-10-10-cert-manager-deployment/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eCert-manager Introduction\u003c/li\u003e\n\u003cli\u003eDeploy cert-manager\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"簡介-cert-manager\"\u003e簡介 cert-manager\u003c/h1\u003e\n\u003cp\u003eTLS certificate 管理很重要，但在 kubernetes 上管理 TLS certificates 很麻煩。\u003c/p\u003e","title":"Cert Manager Deployment on Kubernetes"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊該了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\nNginx Ingress Controller 簡介 nginx \u0026amp; Ingress Controller 部屬並設定 nginx ingress controller Nginx Introduction Nginx 是一款高效能、耐用、且功能強大的 load balancer 以及 web server，也是市占率最高的 web server 之一。\n高效能的 web server，遠勝傳統 apache server 的資源與效能 大量的模組與擴充功能 有充足的安全性功能與設定 輕量 容易水平擴展 Ingress \u0026amp; Ingress Controller 這邊簡單講一下 kubernetes ingress。當我們在使用 kubernetes 時需要將外部流量 route 到集群內部，這邊使用 Ingress 這個 api resource，來定義外部到內部的設定，例如:\nservice 連接 load balance 設定 SSL/TLS 終端 虛擬主機設定 一個簡單的 ingress 大概長這樣\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath backend: serviceName: test servicePort: 80 除了一般的 k8s 資源，nginx 主要的設定會落在 spec，以及依賴底下實作不同，額外設定的 annotation。\n這邊可以看到 spec.rule 定義了外部 http 流量，引導到 backend service 的路徑。\nannotations 下已經標註的 nginx.ingress 的 annotation，來快速增加額外的設定。\nIngress \u0026amp; Ingress Controller 雖然已經指定 nginx 的 annotation，但這邊要注意，ingress resource 本身是不指定底層的實現 (ingress controller)，也就是說，底下是 nginx 也好，traefik 也行，只要能夠實現 ingress 裏頭設定的 routing rules 就可以。\n只設定好 ingress，集群上是不會有任何作用的，還需要在集群上安裝 ingress controller 的實作，實作安裝完了以後，會依據 ingress 的設定，在 controller 裏頭實現，不管是 routing、ssl/tls termination、load balancing 等等功能。如同許多 Kubernetes resource 的設計理念一樣，這邊也很優雅的用 ingress 與 ingress controller，拆分的需求設定與實作實現兩邊的職責。\n例如以 nginx ingress controller，安裝完後會依據 ingress 的設定，在 nginx pod 裡設定對應的 routing rules，如果有 ssl/tls 設定，也一併載入。\nKubernetes 官方文件提供了許多不同的 controller 可以依照需求選擇。\n但如果不知道如何選擇，個人會推薦使用 nginx ingress controller，穩定、功能強大、設定又不至於太過複雜，基本的設定就能很好的支撐服務，不熟悉的大德們比較不容易被雷到。\n底下我們就要來開始使用 nginx ingress controller。\nDeployment 我們這邊使用的 ingress-nginx 是 kubernetes org 內維護的專案，專案內容主要是再 k8s 上執行 nginx，抽象與實作的整合，並透過 configmap 來設定 nginx。針對 nginx ingress kubernetes 官方有提供非常詳細的說明文件 ，剛接觸 nginx 的大德可以透過這份文件，快速的操作 nginx 的設定，而不用直接寫 nginx.conf 的設定檔案。\nrepo 版本是 nginx-0.26.1 Image 版本是 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 Helm 我們這邊用 helm 部屬，Nginx Ingress Controller Stable Chart，讓各位大德用最簡單的步驟，獲得一個功能完整的 nginx ingress controller。\n與前面幾個 helm chart 一樣，我們可以先取得 default values.yaml 設定檔，再進行更改。\n$ wget https://raw.githubusercontent.com/helm/charts/master/stable/nginx-ingress/values.yaml $ vim values.yaml 安裝時也可以使用 \u0026ndash;set 來變更安裝 chart 時的 parameters\n$ helm install stable/nginx-ingress \\ --set controller.metrics.enabled=true \\ -f values.yaml 安裝完後，resource 很快就起來。\nkubectl get all --selector app=nginx-ingress NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-7bbcbdcf7f-tx69n 1/1 Running 0 216d pod/nginx-ingress-default-backend-544cfb69fc-rnn6h 1/1 Running 0 216d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx-ingress-controller LoadBalancer 10.15.246.22 34.35.36.37 80:30782/TCP,443:31933/TCP 216d service/nginx-ingress-default-backend ClusterIP 10.15.243.19 \u0026lt;none\u0026gt; 80/TCP 216d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ingress-controller 1/1 1 1 216d deployment.apps/nginx-ingress-default-backend 1/1 1 1 216d NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ingress-controller-7bbcbdcf7f 1 1 1 216d replicaset.apps/nginx-ingress-default-backend-544cfb69fc 1 1 1 216d kubectl get configmap -l app=nginx-ingress NAME DATA AGE nginx-ingress-controller 2 216d kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress-nginx api.chechiachang.com 34.35.36.37 80, 443 216d 兩個 Pods\nNginx ingress controller 是主要的 nginx pod，裡面跑的是 nginx Nginx default backend 跑的是 default backend，nginx 看不懂了 route request 都往這邊送。 Service\nnginx-ingress-contrller 是我們在 GCP 上，在集群外部的 GCP 上的對外接口。如果在不同平台上，依據預設 service load balancer 有不同實作。 在 gcp 上，會需要時間來啟動 load balancer，等 load balancer 啟動完成，service 這邊就可以取得外部的 ip，接受 load balancer 來的流量 另外一個 service 就是 default backend 的 service 踩雷 第一個雷點是 helm chart install 帶入的 parameters，有些 parameter 是直接影響 deployment 的設定，如果沒注意到，安裝完後沒辦法透過 hot reload 來處理，只能幹掉重來。建議把這份表格都看過一次，再依照環境與需求補上。\n$ helm install stable/nginx-ingress \\ --set controller.metrics.enabled=true \\ --set controller.service.externalTrafficPolicy=Local \\ -f values.yaml 這邊開了 prometheus metrics exporter，以及 source IP preservation。\nNginx Config 再安裝完後，外部的 load balancer 啟用後，就可以透過 GCP 的 external ip 連入 nginx，nginx 依照設定的 rule 向後端服務做集群內的 load balancing 與 routing。\n如果在使用過程中，有需要執行更改設定，或是 hot reload config，在 kubernetes 上要如何做呢? 我們下回分解。\n","permalink":"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊該了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"nginx-ingress-controller\"\u003eNginx Ingress Controller\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e簡介 nginx \u0026amp; Ingress Controller\u003c/li\u003e\n\u003cli\u003e部屬並設定 nginx ingress controller\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"nginx-introduction\"\u003eNginx Introduction\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://nginx.org/en/docs/\"\u003eNginx\u003c/a\u003e 是一款高效能、耐用、且功能強大的 load balancer 以及 web server，也是市占率最高的 web server 之一。\u003c/p\u003e","title":"Kubernetes Nginx Ingress Controller"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Nginx Ingress Controller 運作原理 設定 Nginx Ingress Controller 運作原理 昨天講完 nginx ingress controller 部屬，今天來談談 controller 是如何運作的。\nNginx 使用 config file (nginx.conf) 做全域設定，為了讓 nginx 能隨 config file 更新，controller 要偵測 config file 變更，並且 reload nginx 針對 upstream (後端 app 的 endpoint) 變更，使用 lua-nginx-module 來更新。因為 kubernetes 上，service 後的服務常常會動態的變更，scaling，但 endpint ip list 又需要更新到 nginx，所以使用 lua 額外處理 在 kubernetes 上要如何做到上述兩件事呢?\n一般 controller 都使用同步 loop 來檢查 current state 是否與 desired state desired state 使用 k8s object 描述，例如 ingress, services, configmap 等等 object Nginx ingress controller 這邊使用的是 client-go 中的 Kubernetes Informer 的 SharedInformer，可以根據 object 的更新執行 callback 由於無法檢查每一次的 object 更動，是否對 config 產生影響，這邊直接每次更動都產生全新的 model 如果新產生的 model 與現有相同，就跳過 reload 如果 model 只影響 endpoint，使用 nginx 內部的 lua handler 產生新的 endpoint list，來避免因為 upstream 服務變更造成的頻繁 reload 如果新 Model 影響不只 endpoint，則取代現有 model，然後觸發 reload 具體會觸發 reload 的事件，請見官方文件\n除了監測 objects，build model，觸發 reload，之前 controller 還會將 ingress 送到 kubernetes validating admission webhook server 做驗證，避免描述 desired state 的 ingress 有 syntax error，導致整個 controller 爆炸。\nConfiguration 要透過 controller 更改 nginx 設定，有以下三種方式\n更改 configmap，對全域的 controller 設定 更改 ingress 上的 annotation，這些 annotation 針對獨立 ingress 生效 有更深入的客製化，是上述兩者達不到或尚未實作，可以使用 Custom Template 來做到，把 nginx.tmpl mount 進 controller Configmap 由於把全域設定放到 configmap 上，nginx ingress controller 非常好調度與擴展，controller 官方說明文件 除了列出目前已經支援的設定外，也直接附上 nginx 官方的文件說明連結，讓使用者查詢時方便比對。\n當需要更改需求，可以 google nginx 的關鍵字，找到 nginx 上設定的功能選項後，來 controller 的文件，找看看目前是否已經支援。有時候有需要對照 nginx 官方文件，來正確設定 controller。\nAnnotation 有很多 Nginx 的設定是根據 ingress 不同而有調整，例如針對這個 ingress 做白名單，設定 session，設定 ssl 等等，這些針對特定 ingress 所做的設定，可以直接寫在 ingress annotation 裡面。\n例如下面這個 Ingress\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-nginx annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: 'true' certmanager.k8s.io/cluster-issuer: letsencrypt-prod kubernetes.io/ingress.allow-http: \u0026quot;true\u0026quot; ingress.kubernetes.io/force-ssl-redirect: \u0026quot;true\u0026quot; nginx.ingress.kubernetes.io/whitelist-source-range: \u0026quot;34.35.36.37\u0026quot; nginx.ingress.kubernetes.io/proxy-body-size: \u0026quot;20m\u0026quot; ingress.kubernetes.io/proxy-body-size: \u0026quot;20m\u0026quot; # https://github.com/Shopify/ingress/blob/master/docs/user-guide/nginx-configuration/annotations.md#custom-nginx-upstream-hashing nginx.ingress.kubernetes.io/load-balance: \u0026quot;ip_hash\u0026quot; # https://kubernetes.github.io/ingress-nginx/examples/affinity/cookie/ nginx.org/server-snippets: gzip on; nginx.ingress.kubernetes.io/affinity: \u0026quot;cookie\u0026quot; nginx.ingress.kubernetes.io/session-cookie-name: \u0026quot;route\u0026quot; nginx.ingress.kubernetes.io/session-cookie-hash: \u0026quot;sha1\u0026quot; nginx.ingress.kubernetes.io/session-cookie-expires: \u0026quot;3600\u0026quot; nginx.ingress.kubernetes.io/session-cookie-max-age: \u0026quot;3600\u0026quot; nginx.ingress.kubernetes.io whitelist-source-range: 只允許白名單 ip load-balance: \u0026ldquo;ip_hash\u0026rdquo;: 更改預設 round_robin 的 load balance，為了做 session cookie affinity: \u0026ldquo;cookie\u0026rdquo;: 設定 upstream 的 session affinity session-cookie-name: \u0026ldquo;route\u0026rdquo; session-cookie-hash: \u0026ldquo;sha1\u0026rdquo; session-cookie-expires: \u0026ldquo;3600\u0026rdquo; session-cookie-max-age: \u0026ldquo;3600\u0026rdquo; 如果後端 server 有 session 需求，希望相同 source ip 來的 request 能持續到相同的 endpoint。才做了以上設定。\nhelm configuration helm 的 configuration 也是重要的設定，這裡在安裝時決定了 nginx ingress controller 的 topology、replicas、resource、k8s runtime 設定如 healthz \u0026amp; readiness、其實都會影響 nginx 具體的設定。這部分就會有很多考量。有機會我們再來分享。\n","permalink":"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cp\u003e這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-controller/\"\u003eDeploy Nginx Ingress Controller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-08-kubernetes-nginx-ingress-config/\"\u003eConfigure Nginx Ingress\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCert-manager (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-10-cert-manager-deployment/\"\u003eDeploy cert-manager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-11-cert-manager-how-it-work/\"\u003eHow cert-manager work\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-12-cert-manager-complete-workflow/\"\u003eCert-manager complete workflow\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKubernetes CRD \u0026amp; Operator-sdk (3)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eIntroduction about custom resource\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-13-kubernetes-custom-resources-basic/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-15-kubernetes-custom-resource-with-operator-sdk/\"\u003eDeployment \u0026amp; Usage\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eNginx Ingress Controller 運作原理\u003c/li\u003e\n\u003cli\u003e設定 Nginx Ingress Controller\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"運作原理\"\u003e運作原理\u003c/h1\u003e\n\u003cp\u003e昨天講完 nginx ingress controller 部屬，今天來談談 controller 是如何運作的。\u003c/p\u003e","title":"Kubernetes Nginx Ingress Controller Config"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n如果要透過 prometheus 來監控集群的運行狀況，有兩個 exporter 是必裝的，一個是把 node 狀態 export 出來的 node exporter，一個是把 kubernetes 集群狀態 export 出來的 kube state metrics exporter。\nNode Exporter 簡介 kube metrics exporter 安裝與設定 Node Exporter Node Exporter 是 prometheus 官方維護的一個子項目，主要在把類 unix 硬體 kernel 的 metrics 送出來。官方也支援 windows node 與 nvidia gpu metrics，可以說是功能強大。\n為了能夠監測 kubernetes node 的基礎設施狀態，通常都會使用 node exporter。\nnode exporter 安裝，我們在安裝 prometheus helm chart 時就一並安裝了。這邊看一下設定與運行。\nCollectors Node exporter 把不同位置收集到的不同類型的 metrics ，做成各自獨立的 colletor，使用者可以根據求需求來啟用或是不啟用 collector，完整的 collector 目錄 在這邊。\n如果有看我們第一部份的 ELK part，應該會覺得這裡的設定，跟 metricbeat 非常像，基本上這兩者做的事情是大同小異的，收集 metrics 來源都是同樣的類 unix 系統，只是往後送的目標不一樣 (雖然現在兩者都可以兼容混搭了)。如果有接觸過其他平台的 metrics collector，也會發現其實大家做的都差不多。\nTextfile Collector Prometheus 除了有 scrape 機制，讓 prometheus 去 exporter 撈資料外，還有另外一個機制，叫做 Pushgateway，這個我們在部屬 prometheus 時也部屬了一個。這邊簡單說明一下。\n經常性執行的服務(redis, kafka,\u0026hellip;)會一直運行，prometheus 透過這些服務的 metrics 取得 runtime metrics，作為監控資料。可是有一些 job 是暫時性的任務，例如果一個 batch job，這些服務不會有一直運行的 runtime metrics，也不會有 exporter。但這時又希望監控這些 job 的狀態，就可以使用 Pushgateway。\nPushgateway 的作用機制，就是指定收集的目標資料夾，需要監測的 batch job，只要把希望監測的資料，寫到該資料夾。Pushgateway 會依據寫入的資料，轉成 time series metrics，並且 export 出來。\n這種去 tail 指定目錄檔案，然後把 metrics 後送的機制，是否跟 filebeat 有一點類似? 只是 filebeat 一般取得資料後，會主動推送到 ELK 上，prometheus pushgateway 會暴露出 metrics 後，讓 prometheus server 來 scrape。\nPushgateway 也會在收集資料時打上需要的 label，方面後段處理資料。\nKubernetes State Metrics (Exporter) Node Exporter 將 kubernetes 集群底下的 Node 的硬體狀態，例如 cpu, memory, storage,\u0026hellip; expose 出來，然而我們在維運 kubernetes 還需要從 api server 獲得集群內部的資料，例如說 pod state, container state, endpoints, service, \u0026hellip;等，這邊可以使用 kube-state-metrics 來處理。\nkube-state-metrics 是 kubernetes 官方維護的專案，做的事情就是向 api server 詢問 kubernetes 的 state，例如 pod state, deployment state，然後跟 prometheus exporter 一，開放一個 http endpoint，讓需要的服務來 scrape metrics。\n工作雲裡也很單純，kubernetes api server 可以查詢 pod 當下的狀態，kube-state-metrics 則會把當下的狀態依照時間序，做成 time series 的 metrics，例如這個 pod 什麼時候是活著，什麼時候因為故障而 error。\nkube-state-metrics 預設的輸出格式是 plaintext，直接符合 Prometheus client endpoint 的格式\nDeployment 如果依照第一篇安裝 prometheus helm 的步驟，現在應該已經安裝完 kube-state-metrics 了。如果沒有安裝，也可以依照官方說明的基本範例安裝。\ngit clone git@github.com:kubernetes/kube-state-metrics.git cd kube-state-metrics kubectl apply -f examples/standard/*.yaml 安裝完可以看到\n$ kubectl get pods --selector 'app=prometheus,component=kube-state-metrics' NAME READY STATUS RESTARTS AGE prometheus-kube-state-metrics-85f6d75f8b-7vlkp 1/1 Running 0 201d $ kubectl get svc --selector 'app=prometheus,component=kube-state-metrics' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 201d 我們可以透過 service 打到 pod 的 /metrics 來取得 metrics。\nkubectl exec -it busybox sh curl prometheus-kube-state-metrics:8080 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Kube Metrics Server\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Kube Metrics\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='/metrics'\u0026gt;metrics\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='/healthz'\u0026gt;healthz\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; curl prometheus-kube-state-metrics:8081 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Kube-State-Metrics Metrics Server\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Kube-State-Metrics Metrics\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href='/metrics'\u0026gt;metrics\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 這邊有兩套 metrics，一個是 kube-state-metrics 自己自我監測的 metrics，在 8081，另外一個才是 kube metrics，在 8080，兩個都要收，記得不要收錯了。\n$ curl prometheus-kube-state-metrics:8080/metrics 打下去就可以看到超多 metrics 。 Metrics 的清單與說明文件，有用到的 metrics 使用前都可以來查一下定義解釋。\n理論上不用每個 metrics 都 expose 出來，有需要可以把不會用到的 metrics 關一關，可以節省 kube-state-metrics 的 cpu 消耗。\nResource Recommendation kube-state-metrics 很貼心的還附上建議的資源分配\nAs a general rule, you should allocate 200MiB memory 0.1 cores For clusters of more than 100 nodes, allocate at least 2MiB memory per node 0.001 cores per node Scaling kube-state-metrics 還有提供 horizontal scaling 的解決方案，如果你的集群很大，node 數量已經讓 kube-state-metrics 無法負荷，也可以使用 sharding 的機制，把 metrics 的工作散布到多個 kube-state-metrics，再讓 prometheus 去收集統整。這部分我覺得很有趣，但還沒實作過，我把文件 放在這邊，有緣大德有時做過請來討論分享。\nDashboard metrics 抓出來，當然要開一下 dashboard，這邊使用的是這個kubernetes cluster，支援\nnode exporter kube state metrics nginx ingress controller 三個願望一次滿足~\n小結 跑 kubernetes 務必使用這兩個 exporter kube-state-metrics 整理得很舒服，有時間可以多看看這個專案 ","permalink":"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus / Grafana (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/\"\u003eGKE 上自架 Prometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/\"\u003eGKE 上自架 Grafana\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-scrape/\"\u003escrape config \u0026amp; exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/\"\u003eDive into Redis Exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/\"\u003e輸出 kube-state 的監測數據\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e如果要透過 prometheus 來監控集群的運行狀況，有兩個 exporter 是必裝的，一個是把 node 狀態 export 出來的 node exporter，一個是把 kubernetes 集群狀態 export 出來的 kube state metrics exporter。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNode Exporter 簡介\u003c/li\u003e\n\u003cli\u003ekube metrics exporter 安裝與設定\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"node-exporter\"\u003eNode Exporter\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/prometheus/node_exporter\"\u003eNode Exporter\u003c/a\u003e 是 prometheus 官方維護的一個子項目，主要在把類 unix 硬體 kernel 的 metrics 送出來。官方也支援 windows node 與 nvidia gpu metrics，可以說是功能強大。\u003c/p\u003e","title":"Prometheus \u0026 Kubernetes State Metrics Exporter"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Exporter 工作原理簡介 Prometheus exporter library Exporter workflow 上次講到 exporter 可以從服務端把運行資料抽出來，並開成 http endpoint，讓 prometheus 來 scrape metrics。那 exporter 本身是如何取得服務內部的 metrics 呢? 我們今天就稍微看一下。\nRedis Exporter 我們今天以 Redis Exporter 為例，研究一下外部的 exporter 是如何取得 redis 內部的 metrcs。\nRedis exporter 是用 golang 寫的一個小程式，總共算算才 1000 行，而且很多都是對 redis 內部 metrics 的清單，以及轉化成 prometheus metrics 的 tool functions，主要的邏輯非常簡單。我們簡單看一下源碼。\nCollect 是主要的收集邏輯，就是執行 scrapeRedisHost(ch) ，然後把收集到的資訊，使用 Prometheus Go Client Library 的工具將資料註冊成 prometheus metrics\nfunc (e *Exporter) Collect(ch chan\u0026lt;- prometheus.Metric) { e.Lock() defer e.Unlock() e.totalScrapes.Inc() if e.redisAddr != \u0026quot;\u0026quot; { start := time.Now().UnixNano() var up float64 = 1 // 從 host scrape 資料，然後塞進 channel streaming 出來。 if err := e.scrapeRedisHost(ch); err != nil { up = 0 e.registerConstMetricGauge(ch, \u0026quot;exporter_last_scrape_error\u0026quot;, 1.0, fmt.Sprintf(\u0026quot;%s\u0026quot;, err)) } else { e.registerConstMetricGauge(ch, \u0026quot;exporter_last_scrape_error\u0026quot;, 0, \u0026quot;\u0026quot;) } e.registerConstMetricGauge(ch, \u0026quot;up\u0026quot;, up) e.registerConstMetricGauge(ch, \u0026quot;exporter_last_scrape_duration_seconds\u0026quot;, float64(time.Now().UnixNano()-start)/1000000000) } ch \u0026lt;- e.totalScrapes ch \u0026lt;- e.scrapeDuration ch \u0026lt;- e.targetScrapeRequestErrors } scrapeRedisHost 內部的主要邏輯，又集中在執行 Info\n// 執行 info infoAll, err := redis.String(doRedisCmd(c, \u0026quot;INFO\u0026quot;, \u0026quot;ALL\u0026quot;)) if err != nil { infoAll, err = redis.String(doRedisCmd(c, \u0026quot;INFO\u0026quot;)) if err != nil { log.Errorf(\u0026quot;Redis INFO err: %s\u0026quot;, err) return err } } 也就是說當我們在 redis-cli 連入 redis 時，可以執行 Info command，取得 redis 內部的資訊，包含節點設店與狀態，集群設定，資料的統計數據等等。然後 exporter 這邊維護持續去向 redis 更新 info ，並且把 info data 轉化成 time series 的 metrcs，再透過 Prometheus Client promhttp 提供的 http endpoint library，變成 http endpoint。\n首先看一下 redis info command 的文件，這邊有說明 info 的 option ，以及 option 各自提供的資料，包括 server 狀態，賀戶端連線狀況，系統資源，複本狀態等等。我們也可以自己透過 info 取得資料。\n$ kubectl get po | grep redis redis-2-redis-ha-server-0 3/3 Running 0 11d redis-2-redis-ha-server-1 3/3 Running 0 11d redis-2-redis-ha-server-2 3/3 Running 0 11d $ kubectl exec -it redis-2-redis-ha-server-0 sh $ redis-cli -h haproxy-service -a REDIS_PASSWORD $ haproxy-service:6379\u0026gt; $ haproxy-service:6379\u0026gt; info server # Server redis_version:5.0.5 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:4d072dc1c62d5672 redis_mode:standalone os:Linux 4.14.127+ x86_64 arch_bits:64 multiplexing_api:epoll atomicvar_api:atomic-builtin gcc_version:8.3.0 process_id:1 run_id:63a97460b7c3745577931dad406df9609c4e2464 tcp_port:6379 uptime_in_seconds:976082 uptime_in_days:11 ... $ haproxy-service:6379\u0026gt; info clients # Clients connected_clients:100 client_recent_max_input_buffer:2 client_recent_max_output_buffer:0 blocked_clients:1 Redis exporter 收集這些數據，透過 prometheus client library 把資料轉成 time series prometheus metrics。然後透過 library 放在 http enpoint 上。\n配合上次說過的 redis overview dashboard，可以直接在 Grafana 上使用\n這邊 dashboard 顯示幾個重要的 metrics\nUptime Memory Usage，要設定用量太高自動報警 Command 的執行狀況，回應時間 訊息的流量，以及超出 time-to-live 的資料清除。 都是需要好好加上 alert 的核心 metrics\n貢獻 exporter 其他服務的 exporter 工作原理也相似，如果服務本身有內部的 metrics，可以透過 client command 或是 API 取得，exporter 的工作就只是轉成 time series data。\n如果有比較特殊的 metrics 沒有匯出，例如說自家的 metrics ，但又希望能放到 prometheus 上監測，例如每秒收到多少 request count，回應速度，錯誤訊息的統計\u0026hellip;\u0026hellip;等，這點也可以使用 client library 自幹 exporter 然後 expose http endpoint，這樣在 prometheus 上也可以看到自家產品的 metrics，非常好用。有機會我們來聊自幹 exporter。\n","permalink":"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus / Grafana (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/\"\u003eGKE 上自架 Prometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/\"\u003eGKE 上自架 Grafana\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-scrape/\"\u003escrape config \u0026amp; exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/\"\u003eDive into Redis Exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/\"\u003e輸出 kube-state 的監測數據\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eExporter 工作原理簡介\u003c/li\u003e\n\u003cli\u003ePrometheus exporter library\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"exporter-workflow\"\u003eExporter workflow\u003c/h1\u003e\n\u003cp\u003e上次講到 exporter 可以從服務端把運行資料抽出來，並開成 http endpoint，讓 prometheus 來 scrape metrics。那 exporter 本身是如何取得服務內部的 metrics 呢? 我們今天就稍微看一下。\u003c/p\u003e\n\u003ch1 id=\"redis-exporter\"\u003eRedis Exporter\u003c/h1\u003e\n\u003cp\u003e我們今天以 \u003ca href=\"https://github.com/oliver006/redis_exporter\"\u003eRedis Exporter\u003c/a\u003e 為例，研究一下外部的 exporter 是如何取得 redis 內部的 metrcs。\u003c/p\u003e\n\u003cp\u003eRedis exporter 是用 golang 寫的一個小程式，總共算算才 1000 行，而且很多都是對 redis 內部 metrics 的清單，以及轉化成 prometheus metrics 的 tool functions，主要的邏輯非常簡單。我們簡單看一下源碼。\u003c/p\u003e","title":"Prometheus Exporter Library \u0026 Redis Exporter"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Prometheus Introduction Deploy Prometheus Prometheus Introduction 生產環境與非生產環境，其中的一指標就是有沒有足夠完整的服務監測系統，這句話可以看出服務監測對於產品化是多麼重要。而監控資料 (metrics) 的收集與可視化工具其實非常多，例如上周介紹的 ELK Stack，這次我們要來介紹另外一個很多人使用的 prometheus。\nPromethues 在官網上提到 是一個 Monitoring system and time series database\n可以收集高維度的資料 使用自己的 PromQL 做有效且精簡的資料查詢 內建資料瀏覽器，並且與 Grafana 高度整合 支援 sharding 與 federation，來達到水平擴展 有許多隨插即用的整合 exporter，例如 redis-exporter, kafka-exporter，kubernetes-exporter ，都可以直接取得資料 支援 alert，使用 PromQL 以及多功能的告警，可以設定精準的告警條件 與 ELK 做比較 基本上 Prometheus 跟 ELK 比，其實是很奇怪的一件事，但這也是最常被問的一個問題。兩者在本質上是完全不同的系統。\nPrometheus 是 based on time series database 的資料收集系統 ELK 是基於全文搜索引擎的資料查詢系統 是的，他們都能做 metrics 收集，在有限的尺度下，能達到一樣的效果。但這樣說的意思就等於是在說 mesos DC/OS 與 kubenetes 都能跑 container cluster 一樣，底下是完全不一樣的東西。\n兩者的差異使用上差非常多\nmetrics 結構: ELK 借助全文搜索引擎，基本上送什麼資料近來都可以查找。Prometheus metrics 拉進來是 time series 的 key-value pairs。 維護同樣的 metrics，prometheus 的使用的儲存空間遠小於 elasticsearch prometheus 針對 time based 的搜尋做了很多優化，效能很高 Prometheus 對於記憶體與 cpu 的消耗也少很多 Elasticsearch 資源上很貴，是因為在處理大量 text log 的時候，他能夠用後段的 pipeline 處理內容，再進行交叉比對，可以從 text 裡面提取很多未事先定義的資料 Elasticsearch 的維護工作也比較複雜困難 如果要收集服務運行資料，可以直接選 prometheus。如果有收集 log 進行交叉比對，可以考慮 elk。\nHelm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 prometheus。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nDeploy Prometheus 我把我的寶藏都放在這了https://github.com/chechiachang/prometheus-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash HELM_NAME=prometheus-1 helm upgrade --install ${HELM_NAME} stable/prometheus \\ --namespace default \\ --values values-staging.yaml Configuration Prometheus Stable Chart\nvalues.yaml 很長，但其實各個元件設定是重複的,設定好各自的 image, replicas, service, topology 等等\nalertmanager: enabled: true kubeStateMetrics: enabled: true nodeExporter: enabled: true server: enabled: true pushgateway: enabled: true 底下有更多 runtime 的設定檔\n定義好 global 的 scrape 間距，越短 metrics 維度就越精準 PersistenVolume 強謝建議開起來，維持歷史的資料 加上 storage usage 的 self monitoring（之後會講) 才不會滿出來 server 掛掉 server 的 scrapeConfigs 是 server 去收集的 job 設定。稍後再來細講。 server: global: ## How frequently to scrape targets by default ## scrape_interval: 10s ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 10s persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 80Gi alertmanagerFiles: serverFiles: 部屬完看一下\nkubectl get pods --selector='app=prometheus' NAME READY STATUS RESTARTS AGE prometheus-alertmanager-694d6694c6-dvkwd 2/2 Running 0 8d prometheus-kube-state-metrics-85f6d75f8b-7vlkp 1/1 Running 0 8d prometheus-node-exporter-2mpjc 1/1 Running 0 8d prometheus-node-exporter-kg7fj 1/1 Running 0 51d prometheus-node-exporter-snnn5 1/1 Running 0 8d prometheus-pushgateway-5cdfb4979c-dnmjn 1/1 Running 0 8d prometheus-server-59b8b8ccb4-bplkx 2/2 Running 0 8d kubectl get services --selector='app=prometheus' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-alertmanager ClusterIP 10.15.241.66 \u0026lt;none\u0026gt; 80/TCP 197d prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 197d prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 197d prometheus-pushgateway ClusterIP 10.15.254.0 \u0026lt;none\u0026gt; 9091/TCP 197d prometheus-server ClusterIP 10.15.245.10 \u0026lt;none\u0026gt; 80/TCP 197d kubectl get endpoints --selector='app=prometheus' NAME ENDPOINTS AGE prometheus-alertmanager 10.12.6.220:9093 197d prometheus-kube-state-metrics 10.12.6.222:8080 197d prometheus-node-exporter 10.140.0.30:9100,10.140.0.9:9100,10.140.15.212:9100 197d prometheus-pushgateway 10.12.6.211:9091 197d prometheus-server 10.12.3.14:9090 197d 簡單說明一下\nprometheus-server 是主要的 api-server 以及 time series database alertmanager 負責告警工作 pushgateway 提供 client 端主動推送 metrics 給 server 的 endpoint kube-state-metrics 是開來收集 cluster wide 的 metrics, 像是 pods running counts, deployment ready count, total pods number 等等 metrics node-exporter 是 daemonsets, 把每一個 node 的 metrics, 像是 memory, cpu, disk\u0026hellip;等資料,收集出來 主要服務存取就是透過 prometheus-server\nAccess Prometheus server 除了直接 exec -it 進去 prometheus-server 以外，由於 prometheus 本身有提供 web portal, 所以我們這邊透過 port forwarding 打到本機上\nPROMETHEUS_POD_NAME=$(kc get po -n default --selector='app=prometheus,component=server' -o=jsonpath='{.items[0].metadata.name}') kubectl --namespace default port-forward ${PROMETHEUS_POD_NAME} 9090 透過 browser 就可以連入操作\nhttp://localhost:9090 也可以透過 HTTP API 用程式接入控制\nPrometheus Web Prometheus 本慎提供的 UI 其實功能就很強大\n可以查到 (已經匯入存在) 的 metrics 可以在上面執行 PromQL 查詢語法 查詢運行的 status 查詢目前所有收集的 targets 的狀態,有收集器掛了也可以在這邊看到 小結 輕鬆自架 prometheus Prometheus 頁面有精簡，但是功能完整的 graph 製圖 但大家通常會使用 Grafana 搭配使用, 用過都說讚, 我們明天繼續 ","permalink":"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus / Grafana (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/\"\u003eGKE 上自架 Prometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/\"\u003eGKE 上自架 Grafana\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-scrape/\"\u003escrape config \u0026amp; exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/\"\u003eDive into Redis Exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/\"\u003e輸出 kube-state 的監測數據\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus Introduction\u003c/li\u003e\n\u003cli\u003eDeploy Prometheus\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"prometheus-introduction\"\u003ePrometheus Introduction\u003c/h1\u003e\n\u003cp\u003e生產環境與非生產環境，其中的一指標就是有沒有足夠完整的服務監測系統，這句話可以看出服務監測對於產品化是多麼重要。而監控資料 (metrics) 的收集與可視化工具其實非常多，例如上周介紹的 ELK Stack，這次我們要來介紹另外一個很多人使用的 prometheus。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://prometheus.io/\"\u003ePromethues 在官網上提到\u003c/a\u003e 是一個 Monitoring system and time series database\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e可以收集高維度的資料\u003c/li\u003e\n\u003cli\u003e使用自己的 PromQL 做有效且精簡的資料查詢\u003c/li\u003e\n\u003cli\u003e內建資料瀏覽器，並且與 Grafana 高度整合\u003c/li\u003e\n\u003cli\u003e支援 sharding 與 federation，來達到水平擴展\u003c/li\u003e\n\u003cli\u003e有許多隨插即用的整合 exporter，例如 redis-exporter, kafka-exporter，kubernetes-exporter ，都可以直接取得資料\u003c/li\u003e\n\u003cli\u003e支援 alert，使用 PromQL 以及多功能的告警，可以設定精準的告警條件\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"與-elk-做比較\"\u003e與 ELK 做比較\u003c/h1\u003e\n\u003cp\u003e基本上 Prometheus 跟 ELK 比，其實是很奇怪的一件事，但這也是最常被問的一個問題。兩者在本質上是完全不同的系統。\u003c/p\u003e","title":"Prometheus Deployment on Kubernetes"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Grafana Introduction Deploy Grafana Grafana Introduction 上偏我們簡單介紹了 Prometheus，prometheus 的 Web Portol 已經附上簡單的 Query 與 Graph 工具，但一般我們在使用時，還是會搭配 Grafana 來使用。\nGrafana 在官網上提到 是一個 Analytics system，可以協助了解運行資料，建立完整的 dashboard。\n支援許多圖表，直線圖，長條圖，區域分析，基本上需要的都有 在圖表上定義 alter，並且主動告警，整合其他通訊軟體 對後端 data source 的整合，可以同時使用 ELK, prometheus, influxdb 等 30 多種的資料來源 有許多公開的 plugin 與 dashboard 可以匯入使用 總之功能強大，至於用起來的感覺，個人是非常推薦。如果有大得想要試玩看看，可以直接到 Grafana Live Demo 上面試玩\n一般使用都會圍繞 dashboard 為核心，透過單一畫面，一覽目前使用者需要讀取的資料 左上角的下拉選單，可以選擇不同的 dashboards 與 Kibana 做比較 雖然大部分使用上，我們都會使用 ELK 一套，而 Prometheus + Grafana 另一套。但其實兩邊的 data source 都可以互接。例如 grafana 可以吃 elasticsearch 的 data source，而 kibana 有 prometheus module。\n我們這邊基於兩款前端分析工具，稍微做個比較，底層的 data source 差異這邊先不提。\n都是開源: 兩者的開源社群都非常強大 兩者內建的 dashboard 都非常完整，而且不斷推出新功能 Log vs Metrics: Kibana 的 metrics 也是像 log 一樣的 key value pairs，能夠 explore 未定義的 log Grafana 的 UI 專注於呈現 time series 的 metrics，並沒有提供 data 的欄位搜尋，而是使用語法 Query 來取得數據 Data source: Grafana 可以收集各種不同的後端資料來源 ELK 主要核心還是 ELK stack，用其他 Module 輔助其他資料源 Deploy Grafana 我把我的寶藏都放在這了https://github.com/chechiachang/prometheus-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncd grafana cat install.sh #!/bin/bash HELM_NAME=grafana-1 helm upgrade --install grafana stable/grafana \\ --namespace default \\ --values values-staging.yaml Helm 我們這邊用 helm 部屬，Grafana Stable Chart\nConfiguration 簡單看一下設定檔\nvim values-staging.yaml replicas: 1 deploymentStrategy: RollingUpdate Grafana 是支援 Grafana HA ，其實也非常簡單，就是把 grafana 本身的 dashboard database 從每個 grafana 一台 SQLite，變成外部統一的 MySQL，統一讀取後端資料，前端就可水平擴展。\nreadinessProbe: httpGet: path: /api/health port: 3000 livenessProbe: httpGet: path: /api/health port: 3000 initialDelaySeconds: 60 timeoutSeconds: 30 failureThreshold: 10 image: repository: grafana/grafana tag: 6.0.0 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName 一些 Pod 的基本配置， health check 使用內建的 api，有需要也可以直接打 api\nsecurityContext: runAsUser: 472 fsGroup: 472 extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /etc/grafana/ssl/ # configMap: certs-configmap # readOnly: true 有要開外部 ingress，需要 ssl 的話可以從這邊掛進去\n## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: type: LoadBalancer port: 80 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer annotations: {} labels: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026quot;true\u0026quot; labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local 這邊可以開 service load balancer, 以及 ingress，看實際使用的需求\npersistence: enabled: true initChownData: true # storageClassName: default accessModes: - ReadWriteOnce size: 10Gi # annotations: {} # subPath: \u0026quot;\u0026quot; # existingClaim: Persistent Volume 作為本地儲存建議都開起來，\n# Administrator credentials when not using an existing secret (see below) adminUser: admin # adminPassword: strongpassword # Use an existing secret for the admin user. admin: existingSecret: \u0026quot;\u0026quot; userKey: admin-user passwordKey: admin-password 帳號密碼建議使用 secret 掛進去\ndatasources: {} # datasources.yaml: # apiVersion: 1 # datasources: # - name: Prometheus # type: prometheus # url: http://prometheus-prometheus-server # access: proxy # isDefault: true ## Configure grafana dashboard providers ## ref: http://docs.grafana.org/administration/provisioning/#dashboards ## ## `path` must be /var/lib/grafana/dashboards/\u0026lt;provider_name\u0026gt; ## dashboardProviders: {} # dashboardproviders.yaml: # apiVersion: 1 # providers: # - name: 'default' # orgId: 1 # folder: '' # type: file # disableDeletion: false # editable: true # options: # path: /var/lib/grafana/dashboards/default ## Configure grafana dashboard to import ## NOTE: To use dashboards you must also enable/configure dashboardProviders ## ref: https://grafana.com/dashboards ## ## dashboards per provider, use provider name as key. ## dashboards: {} # default: # some-dashboard: # json: | # $RAW_JSON # custom-dashboard: # file: dashboards/custom-dashboard.json # prometheus-stats: # gnetId: 2 # revision: 2 # datasource: Prometheus # local-dashboard: # url: https://example.com/repository/test.json # local-dashboard-base64: # url: https://example.com/repository/test-b64.json # b64content: true Data source, Dashboard 想要直接載入，可以在這邊設定，或是 grafana 起來後，透過 Web UI 進去新增也可以\n## Grafana's primary configuration ## NOTE: values in map will be converted to ini format ## ref: http://docs.grafana.org/installation/configuration/ ## grafana.ini: paths: data: /var/lib/grafana/data logs: /var/log/grafana plugins: /var/lib/grafana/plugins provisioning: /etc/grafana/provisioning analytics: check_for_updates: true log: mode: console grafana_net: url: https://grafana.net 然後是 grafana.ini 核心 runtime 設定，更多設定可以參考官方文件\nDeployment 部屬完看一下\nkubectl get po --selector='app=grafana' Access 如果沒有透過 service load balancer 打出來，一樣可以使用 kubectl 做 port forwarding，權限就是 context 的權限，沒有 cluster context 的使用者就會進步來\nGRAFANA_POD_NAME=$(kc get po -n default --selector='app=grafana' -o=jsonpath='{.items[0].metadata.name}') kubectl --namespace default port-forward ${GRAFANA_POD_NAME} 3000 http://localhost:3000 由於我們透過 service load balancer，gcp 會在外部幫忙架一個 load balancer， 可以直接透過 load balancer ip 存取，如果想設定 dns，指向這個 ip 後記得去調整 grafana 的 server hostname。\n使用 secret 的密碼登入，username: grafana，這個是系統管理員\nkubectl get secret --namespace default grafana -o jsonpath=\u0026quot;{.data.admin-password}\u0026quot; | base64 --decode ; echo Configuration 近來畫面後先到左邊的Configuration 調整\n產生新的 user org 與 user，把 admin 權限控制在需要的人手上 把 prometheus data source 加進來，就可以直接看到 prometheus 裡面的資料。 切換到非管理員的 user 繼續操作 Import Dashboard Grafana 網站上已經有超多設置好的 Dashboard 可以直接 import，大部分的服務都已經有別人幫我們把視覺畫圖表拉好，使用社群主流的 exporter 的話，參數直接接好。我們匯入後再進行簡單的客製化調整即可。\n我們鐵人賽有用到的服務，都已經有 dashboard\nkubernetes Cluster: 6417 https://grafana.com/dashboards/6417 Kafka Exporter Overview: 7589 https://grafana.com/dashboards/7589 Prometheus Redis: 763 https://grafana.com/dashboards/763 Kubernetes Deployment Statefulset Daemonset metrics: 8588 https://grafana.com/dashboards/8588 Haproxy Metrics Servers: 367 https://grafana.com/dashboards/367 Go to grafana lab to find more dashboards Export Dashboard dashboard 會依照登入使用者的需求做調整，每個腳色需要看到的圖表都不同，基本上讓各個腳色都能一眼看到所需的表格即可\n自己的調整過的 dashboard 也可以匯出分享\n小結 到這邊就可以正常使用 grafana了，資料來源的 exporter 我們會搭配前幾周分享過的服務，一起來講\n","permalink":"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus / Grafana (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/\"\u003eGKE 上自架 Prometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/\"\u003eGKE 上自架 Grafana\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-scrape/\"\u003escrape config \u0026amp; exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/\"\u003eDive into Redis Exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/\"\u003e輸出 kube-state 的監測數據\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eGrafana Introduction\u003c/li\u003e\n\u003cli\u003eDeploy Grafana\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"grafana-introduction\"\u003eGrafana Introduction\u003c/h1\u003e\n\u003cp\u003e上偏我們簡單介紹了 Prometheus，prometheus 的 Web Portol 已經附上簡單的 Query 與 Graph 工具，但一般我們在使用時，還是會搭配 Grafana 來使用。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://grafana.com/grafana/\"\u003eGrafana 在官網上提到\u003c/a\u003e 是一個 Analytics system，可以協助了解運行資料，建立完整的 dashboard。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e支援許多圖表，直線圖，長條圖，區域分析，基本上需要的都有\u003c/li\u003e\n\u003cli\u003e在圖表上定義 alter，並且主動告警，整合其他通訊軟體\u003c/li\u003e\n\u003cli\u003e對後端 data source 的整合，可以同時使用 ELK, prometheus, influxdb 等 30 多種的資料來源\u003c/li\u003e\n\u003cli\u003e有許多公開的 plugin 與 dashboard 可以匯入使用\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e總之功能強大，至於用起來的感覺，個人是非常推薦。如果有大得想要試玩看看，可以直接到 \u003ca href=\"https://play.grafana.org/d/000000029/prometheus-demo-dashboard?orgId=1\u0026amp;refresh=5m\"\u003eGrafana Live Demo\u003c/a\u003e 上面試玩\u003c/p\u003e","title":"Prometheus Deploy Grafana"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Prometheus scrape scrape_configs Node exporter Scrape Prometheus 收集 metrics 的方式，是從被監測的目標的 http endpoints 收集 (scrape) metrics，目標服務有提供 export metrics 的 endpoint 的話，稱作 exporter。例如 kafka-exporter 就會收集 kafka 運行的 metrics，變成 http endpoint instance，prometheus 從 instance 上面收集資料。\nPromethesu 自己也是也提供 metrics endpoint，並且自己透過 scrape 自己的 metrics endpoint 來取得 self-monitoring 的 metrics。把自己當作外部服務監測。下面的設定就是直接透過 http://localhost:9090/metrics 取得。\nglobal: scrape_interval: 15s # By default, scrape targets every 15 seconds. # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: 'codelab-monitor' # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9090'] 透過 Grafana -\u0026gt; explore 就可以看到 Prometheus 的 metrics\n而使用 metrics 時最好先查到說明文件，確定 metrics 的定義與計算方法，才可以有效的製圖。關於 Prometheus Exporter 的 metrics 說明 可以到這裡來找。\nDashboard 收集到 metrics 之後就可以在 prometheus 中 query，但一般使用不會一直跑進來下 query，而是會直接搭配 dashboard 製圖呈現，讓資料一覽無遺。\n例如 prometheus 自身的 metrics 也已經有搭配好的 Prometheus overview dashboard 可以使用。\n使用方法非常簡單，直接透過 Grafana import dashboard，裡面就把重要的 prometheus metrics 都放在 dashboard 上了。不能更方便了。\nExporters Prometheus 支援超級多 exporter，包含 prometheus 自身直接維護的 exporter，還有非常多外部服務友也開源的 exporter 可以使用，清單可以到這裡看\n有希望自己公司的服務，也使用 prometheus\nNode Exporter prometheus/node_exporter 是 Prometheus 直接維護的 project，主要用途就是將 node / vm 的運行 metrics export 出來。有點類似 ELK 的 metricbeat。\n我們這邊是在 kubernetes 上執行，所以直接做成 daemonsets 在 k8s 上跑，部屬方面在 deploy prometheus-server 的 helm chart 中，就已經附帶整合，部屬到每一台 node 上。\n如果是在 kubernetes 外的環境，例如說 on premise server，或是 gcp instance，希望自己部屬 node exporter 的話，可以參考這篇教學文章。\n我們這邊可以看一下 config，以及 job 定義。\nvim values-staging.yaml # Enable nodeExporter nodeExporter: create: true prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: # Add kubernetes node job - job_name: 'kubernetes-nodes' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$1/proxy/metrics kubernetes_sd_config: 可以透過 kubernetes API 來取得 scrape target，以這邊的設定，是使用 node role 去集群取得 node，並且每一台 node 都當成一個 target，這樣就不用把所有 node 都手動加到 job 的 instance list 裡面。\n從 node role 取得的 instance 會使用 ip 標註或是 hostname 標註。node role 有提供 node 範圍的 meta labels，例如 __meta_kubernetes_node_name, _meta_kubernetes_node_address 等等，方便查找整理資料。\nrelabel_configs: 針對資料做額外標記，方便之後在 grafana 上面依據需求 query。\n","permalink":"https://chechia.net/posts/2019-10-04-prometheus-scrape/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus / Grafana (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deployment-on-kubernetes/\"\u003eGKE 上自架 Prometheus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-deploy-grafana/\"\u003eGKE 上自架 Grafana\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-04-prometheus-scrape/\"\u003escrape config \u0026amp; exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-06-prometheus-exporter-library-redis-exporter/\"\u003eDive into Redis Exporter\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-07-prometheus-kube-state-metrics-exporter/\"\u003e輸出 kube-state 的監測數據\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ePrometheus scrape\u003c/li\u003e\n\u003cli\u003escrape_configs\u003c/li\u003e\n\u003cli\u003eNode exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"scrape\"\u003eScrape\u003c/h1\u003e\n\u003cp\u003ePrometheus 收集 metrics 的方式，是從被監測的目標的 http endpoints 收集 (scrape) metrics，目標服務有提供 export metrics 的 endpoint 的話，稱作 exporter。例如 kafka-exporter 就會收集 kafka 運行的 metrics，變成 http endpoint instance，prometheus 從 instance 上面收集資料。\u003c/p\u003e","title":"Prometheus Scrape"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Failure Case Recovery Topology 上篇的例子完成應該是這樣\n+-------+ +--------+ +------------+ +---------+ |Clients|---|HAProxys|----|redis master|----|sentinels| +-------+ +--------+ +------------+ +---------+ HAproxy 作為後端 redis 的 gateway Client 透過 HAproxy 連入 redis master sentinel 負責監測 redis 狀態與 failover，只是 client 不再透過 sentinel 去取得 master，而是透過 HAProxy。 那現在就來聊聊這些服務可能怎麼死的，回復的機制又是如何\nFailure Recovery Redis master 故障 這個是目前我們這個 Redis HAProxy 配置主要想解決的問題，故障與回覆的流程大概是這樣\nRedis Master failing Sentinels detect master failure sentinel 等待 down-after-milliseconds，超過才判定 master failure sentinel 彼此取得 quorum，授權其中一台 sentinel 執行 failover sentinel 指派新的 master master 故障同時，HAProxy 也偵測 master failure HAProxy 發現沒有可用的 master tcp checklist 再次執行時，由於新的 master 尚未選出來，仍會顯示三台 server 都離線 直到 master 選出，role:master 的 tcp check 有回應後，才會將後端接到新的 master Client 由於 HAProxy 沒有可用的 master，所以連線斷掉 持續中斷到 HAProxy 回復 這邊的幾個重要的參數\nsentinel\ndown-after-milliseconds: 斷線多久才會覺得 master 死了需要 failover，可以盡量縮短，加速 failure 發生 failover 的時間 haproxy.cfg\nserver check inter 1s: 多久跑一次 tcp-check 越短，便能越早接受到 redis instance failure 的發生 從這個例子來看，這個配置的 HA 其實還是有離線時間\ndown-after-milliseconds 設定為 2s ，那從 failure 發生，到 sentinel 開始 failover 的時間就會超過 2s，這兩秒客戶端無法寫入。 事實上，這也是 redis master-slave 的模式的問題，並無法確保 zero downtime 能做到的是秒級的 auto-recovery Sentinel Failure 這個是很好解決的錯誤，如同我們在 topology 這篇提到的，原則上只要能維持 quorum 以上的 sentinel 正常運作，就可以容忍多個 sentinel 的錯誤\n例如 5 sentinel，quorum 3，就可以允許兩個 sentinel 錯誤 服務都正常 zero downtime 等待錯誤的 sentinels 復原 錯誤不一定是兩個 sentinel 死了，可能是網路斷開，把 3 sentinels 與 2 sentinels 隔開，無法溝通。 這時也不用會有複數 failover 產生，因為 quorum 只有 3 sentinels 的這端可以取得授權，正常執行 failover 2 sentinels 的這邊只會靜待網路回復。 HAProxy Failure 這個在 kubernetes 上也是很好解決\nHAProxy 不用知道彼此，只要能夠監測後端服務，並且 proxy request 即可\n我們啟動 HAProxy 時會一次啟動多個 HAProxy\nHAProxy 是無狀態的服務，可以直接水平擴展 (Horizontal Scale) 算是成本的地方，就是 HAProxy instance 會各自對後端 redis 做 tcp-check，頻繁的 check，還是會有成本，但相較於 client request 應該是比較輕 HAProxy 是高效能，而且只做 proxy，一奔來說只要維持有多餘的副本備用即可，不用開太多 Kubernetes 會自動透過 stats port，對 HAPRoxy 做 liveness check，check 失敗就不會把流量導近來\nHAproxy 前端的 kubernetes service 會自動 load balance client 到正常運作的 HAProxy 上\n例如起了 3 HAProxy\n3 HAProxy 都各自向 redis instance 做 tcp-check，每秒 3 * 3 組 check\n客戶端連入任一 HAProxy，都可以連入正確的 master\nHAProxy 只要至少有一個活著就可以，也就是可以死 2 個\nKubernetes service 會自動導向活著的 HAProxy\n2 HAPRoxy 回復的時候，就是 HAProxy 重啟後重新開始服務\n拆分 read write client 由於效能瓶頸還是在 redis master，為了能支撐夠多 client，最好把 client 需要讀寫的拆分開來\nHAProxy 的設定，就會需要\n把 frontend redis_gate 拆成 redis_slaves_gate: 接收讀取的 client redis_master_gate: 接收寫入的 client 把 redis_servers 拆成 redis_slaves: 更改 tcp-check 去找 role:slave 的 redis，應該有兩台 redis_master: 維持找尋 role:master 的 redis 這樣可以輕易地透過 scale slave 來擴大讀取的流量帶寬\nRedis Cluster Intro 的時候有提到，redis cluster 是另一個面向的 redis solution。\n使用 redis cluster 將資料做 sharding，分散到不同群組內，partitions 由複數的 master 來存取\n這部份我們下回待續\n","permalink":"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 GKE 上部署 Redis HA (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-28-redis-ha-deployment/\"\u003e使用 helm 部署 redis-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/\"\u003eRedis HA with sentinel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-30-redis-ha-topology/\"\u003eRedis sentinel topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/\"\u003eRedis HA with HAproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/\"\u003eRedis HA Failure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eFailure Case\u003c/li\u003e\n\u003cli\u003eRecovery\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"topology\"\u003eTopology\u003c/h1\u003e\n\u003cp\u003e上篇的例子完成應該是這樣\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\n   +-------+   +--------+    +------------+    +---------+\n   |Clients|---|HAProxys|----|redis master|----|sentinels|\n   +-------+   +--------+    +------------+    +---------+\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eHAproxy 作為後端 redis 的 gateway\u003c/li\u003e\n\u003cli\u003eClient 透過 HAproxy 連入 redis master\u003c/li\u003e\n\u003cli\u003esentinel 負責監測 redis 狀態與 failover，只是 client 不再透過 sentinel 去取得 master，而是透過 HAProxy。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e那現在就來聊聊這些服務可能怎麼死的，回復的機制又是如何\u003c/p\u003e","title":"Redis Ha Failure Recovery"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 HAProxy Introduction Redis Sentinel with HAProxy HAProxy Intro HAproxy 全名是 High Availability Proxy，是一款開源 TCP/HTTP load balancer，他可以\n聽 tcp socket，連 server，然後把 socket 接在一起讓雙向流通 可做 Http reverse-proxy (Http gateway)，自己作為代理 server，把接受到的 connection 傳到後端的 server。 SSL 終端，可支援 client-side 與 server-side 的 ssl/tls 當 tcp/http normalizer 更改 http 的 request 與 response 當 switch，決定 request 後送的目標 做 load balancer，為後端 server 做負載均衡 調節流量，設定 rate limit，或是根據內容調整流量 HAProxy 還有其他非常多的功能，想了解細節可以來看原理解說文件\nTopology 我們今天的範例是在後端的 redis 與 clients 中間多放一層 HAProxys\n+-------+ +--------+ +------------+ +---------+ |Clients|---|HAProxys|----|redis master|----|sentinels| +-------+ +--------+ +------------+ +---------+ 可能有人會問說，那前兩天講的 redis sentinel，跑去哪裡了。\nsentinel 還在正常運作，負責監測 redis 狀態與 failover，只是 client 不再透過 sentinel 去取得 master，而是透過 HAProxy。\nDeploy HAProxy 我把我的寶藏都在這了https://github.com/chechiachang/haproxy-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash # redis-db-credentials should already exists #kubectl create secret generic redis-db-credentials \\ --from-literal=REDIS_PASSWORD=123456 # Update haproxy.cfg as configmap kubectl create configmap haproxy-config \\ --from-file=haproxy.cfg \\ --output yaml \\ --dry-run | kubectl apply -f - kubectl apply -f deployment.yaml kubectl apply -f service.yaml 這邊做的事情有幾件\n取得 redis 的 auth REDIS_PASSWORD 放在 secret 中，如果前面是照我們的範例，那都已經設定了 把 haproxy.cfg 的設定檔，使用 configmap 的方式放到 kubernetes 上 部屬 HAProxy deployment 部屬 HAProxy service 簡單看一下 deployment\napiVersion: apps/v1beta1 kind: Deployment metadata: name: haproxy spec: replicas: 3 template: metadata: labels: app: haproxy app.kubernetes.io/name: haproxy component: haproxy spec: volumes: - name: haproxy-config configMap: name: haproxy-config affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026quot;app\u0026quot; operator: \u0026quot;In\u0026quot; values: - \u0026quot;haproxy\u0026quot; topologyKey: kubernetes.io/hostname containers: - name: haproxy image: haproxy:2.0.3-alpine command: [\u0026quot;haproxy\u0026quot;, \u0026quot;-f\u0026quot;, \u0026quot;/usr/local/etc/haproxy/config/haproxy.cfg\u0026quot;] readinessProbe: initialDelaySeconds: 15 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 2 failureThreshold: 2 tcpSocket: port: 26999 port: 6379 volumeMounts: - name: haproxy-config mountPath: /usr/local/etc/haproxy/config resources: requests: cpu: 10m memory: 30Mi env: - name: REDIS_PASSWORD valueFrom: secretKeyRef: name: redis-db-credentials key: REDIS_PASSWORD ports: - containerPort: 8000 name: http - containerPort: 9000 name: https - containerPort: 26999 name: stats - containerPort: 6379 name: redis Replicas: 3 ，開起來是三個 HAProxy podAntiAffinity，三個分布到不同 node 上，盡量維持 HA readinessProbe，等 tcpSocket 26999 (HAProxy Stats) 與 6370 (Redis Proxy) 通了才 READY 把 redis password 掛進去 把 haproxy.cfg 掛進去 開幾個 port 看一下 service\nkind: Service apiVersion: v1 metadata: name: haproxy-service spec: sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800 # 3 hr selector: app: haproxy ports: - name: http protocol: TCP port: 8000 - name: https protocol: TCP port: 9000 - name: stats protocol: TCP port: 26999 - name: redis protocol: TCP port: 6379 - name: redis-exporter protocol: TCP port: 8404 很單純，就是把幾個 port 接出來 把 sessionAffinity 開起來 這邊希望來自相同 clientIP (kubernetes 內部 app clients) 的 session 能持續走同一個 server 可以降低進到 service 往後送到一直重連浪費資源 但一直連著也不好，可能會 connection not closed 一直佔著 HAProxy1 HAProxy2 HAProxy3，上次 Client1 連 HAProxy1，service 也盡量讓你下個 request 也走 HAPRoxy1 kubectl get po | grep haproxy haproxy-56d94f857f-gmd4s 1/1 Running 0 47d haproxy-56d94f857f-p2vj6 1/1 Running 0 47d haproxy-56d94f857f-vhz8b 1/1 Running 0 47d HAProxy Config 看一下 haproxy.cfg\n# https://cbonte.github.io/haproxy-dconv/2.0/configuration.html # https://github.com/prometheus/haproxy_exporter # https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint/ # curl http://localhost:8404/metrics # curl http://localhost:8404/stats frontend stats mode http timeout client 30s bind *:8404 option http-use-htx http-request use-service prometheus-exporter if { path /metrics } stats enable stats uri /stats stats refresh 10s # Redis frontend redis_gate mode tcp timeout client 7d bind 0.0.0.0:6379 name redis default_backend redis_servers backend redis_servers mode tcp timeout connect 3s timeout server 7d option tcp-check tcp-check connect tcp-check send AUTH\\ \u0026quot;${REDIS_PASSWORD}\u0026quot;\\r\\n tcp-check send PING\\r\\n tcp-check expect string PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK server R1 redis-2-redis-ha-announce-0:6379 check inter 1s server R2 redis-2-redis-ha-announce-1:6379 check inter 1s server R3 redis-2-redis-ha-announce-2:6379 check inter 1s 兩個 frontend，吃前端 (client) 來的 request frontend stats 是 HAProxy 本身服務的 stats 把 prometheus-exporter 開起來，讓 prometheus 進來 scrape metrics frontend redis_gate 是用來服務 redis client 邏輯很簡單，進來的 request 往有效的 backend redis_server 送，這邊的有效指的是 redis master timeout 7d，因為我們的服務有長時間不間斷的 pubsub，可以視需求調整 一個 backend，HAProxy 會維護並監測狀態，然後把 frontend proxy 過去 mode tcp，使用 tcp 去 probe option tcp-check，下面是一串 tcp checklist，配合 redis 的 tcp auth protocol 去取得 tcp connect 連上 send AUTH 密碼 到 redis send ping，redis 要回 pong send info replication 直接打 redis tcp info API 預期 string 內有 role:master 意思是這台 redis 是 master 退出，redis 要回 ok server 有三台，透過 redis 各自的 ha-announce service 去打 HAProxy 會維護 backend 的 proxy stats，找到三台 redis 中，是 master 的這台\nRunning Log\nkubectl logs -f haproxy-123-123456789 [WARNING] 273/153936 (1) : Server redis_servers/R2 is DOWN, reason: Layer7 timeout, info: \u0026quot; at step 6 of tcp-check (expect string 'role:master')\u0026quot;, check duration: 1000ms. 2 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue. [WARNING] 273/153937 (1) : Server redis_servers/R3 is DOWN, reason: Layer7 timeout, info: \u0026quot; at step 6 of tcp-check (expect string 'role:master')\u0026quot;, check duration: 1001ms. 1 active and 0 backup servers left. 0 sessions active, 0 requeued, 0 remaining in queue. HAProxy 去 redis 問，你是 master 嗎，兩個人回不是，只有一個回 role:master，所以把 client 導過去\nHAProxy vs Sentinel Client 不用知道中間的 proxy，只要知道透過 HAproxy service 就會被 proxy 到 master HAproxy 是 stateless，非常好 scale Client 不用支援 sentinel，只要一般的 redis-cli 就可以連入 ","permalink":"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 GKE 上部署 Redis HA (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-28-redis-ha-deployment/\"\u003e使用 helm 部署 redis-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/\"\u003eRedis HA with sentinel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-30-redis-ha-topology/\"\u003eRedis sentinel topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/\"\u003eRedis HA with HAproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/\"\u003eRedis HA Failure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eHAProxy Introduction\u003c/li\u003e\n\u003cli\u003eRedis Sentinel with HAProxy\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"haproxy-intro\"\u003eHAProxy Intro\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://www.haproxy.org/#docs\"\u003eHAproxy\u003c/a\u003e 全名是 High Availability Proxy，是一款開源 TCP/HTTP load balancer，他可以\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e聽 tcp socket，連 server，然後把 socket 接在一起讓雙向流通\u003c/li\u003e\n\u003cli\u003e可做 Http reverse-proxy (Http gateway)，自己作為代理 server，把接受到的 connection 傳到後端的 server。\u003c/li\u003e\n\u003cli\u003eSSL 終端，可支援 client-side 與 server-side 的 ssl/tls\u003c/li\u003e\n\u003cli\u003e當 tcp/http normalizer\u003c/li\u003e\n\u003cli\u003e更改 http 的 request 與 response\u003c/li\u003e\n\u003cli\u003e當 switch，決定 request 後送的目標\u003c/li\u003e\n\u003cli\u003e做 load balancer，為後端 server 做負載均衡\u003c/li\u003e\n\u003cli\u003e調節流量，設定 rate limit，或是根據內容調整流量\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHAProxy 還有其他非常多的功能，想了解細節可以來看\u003ca href=\"http://cbonte.github.io/haproxy-dconv/1.9/intro.html#3\"\u003e原理解說文件\u003c/a\u003e\u003c/p\u003e","title":"Redis Ha HAProxy"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Redis Sentinel Topology Topology Masters: M1, M2, M3, \u0026hellip;, Mn. Slaves: R1, R2, R3, \u0026hellip;, Rn (R stands for replica). Sentinels: S1, S2, S3, \u0026hellip;, Sn. Clients: C1, C2, C3, \u0026hellip;, Cn. 每個方格代表一台機器或是 VM 2 Sentinels DON\u0026rsquo;T DO THIS\n+----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ Configuration: quorum = 1 這個設定下，如果 M1 掛了需要 failover，很有可能 S1 跟著機器一起掛了，S2 會沒有辦法取得多數來執行 failover，整個系統掛掉\n3 VM +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2 這是最基本的蛋又兼顧安全設定的設置\n如果 M1 死了 S1 跟著機器故障，S2 與 S3 還可以取得多數，順利 failover 到 R2 或是 R3。\n寫入資料遺失 +----+ | M1 | | S1 | \u0026lt;- C1 (writes will be lost) +----+ | / / +------+ | +----+ | [M2] |----+----| R3 | | S2 | | S3 | +------+ +----+ failover 之前，M1 是 master，Client 的寫入往 M1 寫 M1 網路故障，M2 failover 後成為新的 master，可是 Client 往 M1 寫入的資料並無法 sync 回 M2 等網路修復後，M1 回覆後會變成 R1 變成 slave，由 M2 去 sync R1，變成 R1 在 master 時收到的寫入資料遺失 為了避免這種情形，做額外的設定\nmin-slaves-to-write 1 min-slaves-max-lag 10 當 master 發現自己再也無法 sync 到足夠的 slave，表示 master 可能被孤立，這時主動拒絕客戶端的寫入請求。客戶端被拒絕後，會再向 sentinel 取得有效的 master，重新執行寫入請求，確保資料寫到有效的 master 上。\nSentinel 放在 Client 端 +----+ +----+ | M1 |----+----| R1 | | | | | | +----+ | +----+ | +------------+------------+ | | | | | | +----+ +----+ +----+ | C1 | | C2 | | C3 | | S1 | | S2 | | S3 | +----+ +----+ +----+ 有些情形，redis 這端只有兩台可用機器，這種情形可以考慮把 sentinel 放在客戶端的機器上\n仍然維持了獨立的 3 sentinels 的穩定 sentinel 與 client 所觀察到的 redis 狀態是相同的 如果 M1 死了，要 failover ，客戶端的 3 sentinel 可以正確地執行 failover，不受故障影響 客戶端又不足 3 個 +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | +------+-----+ | | | | +----+ +----+ | C1 | | C2 | | S3 | | S4 | +----+ +----+ Configuration: quorum = 3 +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | | | +----+ | C1 | | S3 | +----+ Configuration: quorum = 2 跟上個例子類似，但又額外確保 3 sentinels 如果 M1 死了，剩下的 sentinel 可以正確 failover ","permalink":"https://chechia.net/posts/2019-09-30-redis-ha-topology/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 GKE 上部署 Redis HA (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-28-redis-ha-deployment/\"\u003e使用 helm 部署 redis-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/\"\u003eRedis HA with sentinel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-30-redis-ha-topology/\"\u003eRedis sentinel topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/\"\u003eRedis HA with HAproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/\"\u003eRedis HA Failure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eRedis Sentinel Topology\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"topology\"\u003eTopology\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMasters: M1, M2, M3, \u0026hellip;, Mn.\u003c/li\u003e\n\u003cli\u003eSlaves: R1, R2, R3, \u0026hellip;, Rn (R stands for replica).\u003c/li\u003e\n\u003cli\u003eSentinels: S1, S2, S3, \u0026hellip;, Sn.\u003c/li\u003e\n\u003cli\u003eClients: C1, C2, C3, \u0026hellip;, Cn.\u003c/li\u003e\n\u003cli\u003e每個方格代表一台機器或是 VM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-sentinels\"\u003e2 Sentinels\u003c/h3\u003e\n\u003cp\u003eDON\u0026rsquo;T DO THIS\u003c/p\u003e","title":"Redis Ha Topology"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 redis-sentinel redis sentinel 與 redis 使用相容的 api，直接使用 redis-cli 透過 26479 port 連入，可以連到 sentinel，透過 sentinel 可以取得 redis master 的狀態與連線設定。\nredis-cli -h redis-redis-ha -p 26479 上篇我們的 redis-ha 安裝完變這樣\n$ kubectl get po | grep redis NAME READY STATUS RESTARTS AGE redis-1-redis-ha-server-0 3/3 Running 0 3d4h redis-1-redis-ha-server-1 3/3 Running 0 3d5h redis-1-redis-ha-server-2 3/3 Running 0 3d4h 有三個 Pod，裡面都是一個 redis, sentinel, 跟 exporter，這篇文章會專注講 sentinel 的功能與機制\nRedis Sentinel redis-sentinel 為 Redis 提供高可用服務，實務上可以透過 sentinel 在錯誤發生時，自動進行 failover。除此之外 sentinel 也提供監測，通知，與 redis 的設定。\nMonitoring: 持續檢測 master 與 slave instances 的狀態 Notification: 有事件發生可以發出通知 Automatic failover: 如果 master 失效自動啟動 failover 程序，將一個 slave 指排為 master，並設定其他 slave 使用新的 master Configuration provider: 為客戶端提供 service discovery，客戶可以通過 sentinel 取得 master 的連線資料。 Distributed Sentinel 本身是一個分散式系統，如我們的範例所示，三個 Pod 立面個含有一個 sentinel，組成 3 個 instace 的 sentinel cluster。\n錯誤檢測是由多個 sentinel 判定，要有多個 sentinel 都接收 master 已失效的訊息，才會判定成失效。這樣可以降低 false positive 的機率。 分散讓 sentinel 本身也具備高可用性，可以承受一定程度的錯誤。用來 fail over 的系統，不能因為自身的單點錯誤(single point failure) 而倒是整個 redis 失效。 Fundamental 一個耐用的 sentinel 需要至少三個 instance 最好把 instance 分散在多個獨立的隔離區域，意思是說，三個不會放在同一台機器上，或是放在同一個區域內，因為一個區域網路故障就全死。 app 使用 sentinel 的話，客戶端要支援 有時常測試的 HA 環境，才是有效的 HA Configuration Sentinel specific configuration options 在上篇我們跳過 sentinel 的設定，這邊說明一下\nsentinel: port: 26379 quorum: 2 config: ## Additional sentinel conf options can be added below. Only options that ## are expressed in the format simialar to 'sentinel xxx mymaster xxx' will ## be properly templated. ## For available options see http://download.redis.io/redis-stable/sentinel.conf down-after-milliseconds: 10000 ## Failover timeout value in milliseconds failover-timeout: 180000 parallel-syncs: 5 ## Custom sentinel.conf files used to override default settings. If this file is ## specified then the sentinel.config above will be ignored. # customConfig: |- # Define configuration here resources: {} # requests: # memory: 200Mi # cpu: 100m # limits: # memory: 200Mi Quorum quorum 是每次確定 master 失效時，需要達成共識的 sentinel 數量。 Quorum 使用在錯誤檢測，確定錯誤真的發生後，sentinel 會以多數決(majority) 的方式選出 sentinel leader，讓 leader 處理 failover。 以我們的例子為例，總共三個，確認 master 死掉只要兩個 sentinel 達成共識即可啟動 failover 程序。可以直接測試一下。\nkubectl logs -f redis-1-redis-ha-server-0 kubectl delete po redis-1-redis-ha-server-1 log 一個 Pod ，然後直接把另一個 Pod 幹掉 這樣會有 1/3 的機率砍到 master，砍中的話可以看到 redis failover ，選出新的 master 的過程。\n這邊要注意，由於我們的 sentinel 與 redis 是放在同樣一個 Pod，幹掉的同時也殺了一個 sentinel，只剩 2 個，剛好達成共識。如果 quorum 是三，就要等第三個 sentinel 回來才能取得 quorum。\nsentinel 與 redis 的配置位置，之後的 topology 會討論。\nConfigurations down-after-milliseconds: 超過多少時間沒回應 ping 或正確回應，才覺得 master 壞了 parallel-syncs: failover 時，要重新與新 master sync 的 slave 數量。數量越多 sync 時間就越久，數量少就有較多 slave 沒 sync 資料，可能會讓 client read 到舊的資料 雖然 sync 是 non-blocking ，但在 sync 大筆資料時，slave 可能會沒有回應。設定為 1 的話，最多只會有一個 slave 下線 sync。 這些參數也可以透過 redis-cli 直接連入更改，但我們是在 kubernetes 上跑，臨時的更改不易保存，所以盡可能把這些configurations 放在 configmap 裡面。\nSentinel command 6379 port 連入 redis，26379 連入 redis sentinel。都是使用 redis-cli，兩者兼容的 protocol。\n# 使用 kubectl 連入，多個 container 要明確指出連入的 container kubectl exec -it redis-1-redis-ha-server-0 --container redis sh redis-cli -h redis-redis-ha -p 26479 # 近來先 ping 一下 $ ping PONG # 列出所有 master 的資訊，以及設定資訊 sentinel master redis-2-redis-ha:26379\u0026gt; sentinel masters 1) 1) \u0026quot;name\u0026quot; 2) \u0026quot;mymaster\u0026quot; 3) \u0026quot;ip\u0026quot; 4) \u0026quot;10.15.242.245\u0026quot; 5) \u0026quot;port\u0026quot; 6) \u0026quot;6379\u0026quot; 7) \u0026quot;runid\u0026quot; 8) \u0026quot;63a97460b7c3745577931dad406df9609c4e2464\u0026quot; 9) \u0026quot;flags\u0026quot; 10) \u0026quot;master\u0026quot; 11) \u0026quot;link-pending-commands\u0026quot; 12) \u0026quot;0\u0026quot; 13) \u0026quot;link-refcount\u0026quot; 14) \u0026quot;1\u0026quot; 15) \u0026quot;last-ping-sent\u0026quot; 16) \u0026quot;0\u0026quot; 17) \u0026quot;last-ok-ping-reply\u0026quot; 18) \u0026quot;479\u0026quot; 19) \u0026quot;last-ping-reply\u0026quot; 20) \u0026quot;479\u0026quot; 21) \u0026quot;down-after-milliseconds\u0026quot; 22) \u0026quot;5000\u0026quot; 23) \u0026quot;info-refresh\u0026quot; 24) \u0026quot;5756\u0026quot; 25) \u0026quot;role-reported\u0026quot; 26) \u0026quot;master\u0026quot; 27) \u0026quot;role-reported-time\u0026quot; 28) \u0026quot;348144787\u0026quot; 29) \u0026quot;config-epoch\u0026quot; 30) \u0026quot;13\u0026quot; 31) \u0026quot;num-slaves\u0026quot; 32) \u0026quot;2\u0026quot; 33) \u0026quot;num-other-sentinels\u0026quot; 34) \u0026quot;2\u0026quot; 35) \u0026quot;quorum\u0026quot; 36) \u0026quot;2\u0026quot; 37) \u0026quot;failover-timeout\u0026quot; 38) \u0026quot;180000\u0026quot; 39) \u0026quot;parallel-syncs\u0026quot; 40) \u0026quot;5\u0026quot; # 取得集群中的 master 訊息，目前有一個 master $ sentinel master mymaster # 取得集群中的 slaves 訊息，目前有兩個 slave $ sentinel slaves mymaster # 取得集群中的 master 訊息 $ sentinel sentinels mymaster # 檢查 sentinel 的 quorum $ sentinel ckquorum mymaster OK 3 usable Sentinels. Quorum and failover authorization can be reached # 強迫觸發一次 failover sentinel failover mymaster Sentinel Connection 有支援的客戶端設定，以Golang FZambia/sentinel 為例，透過 sentinel 取得 redis-pool。\n# 使用獨立的 pod service 連入 sentinel，協助彼此識別 sntnl := \u0026amp;sentinel.Sentinel{ Addrs: []string{\u0026quot;redis-2-redis-ha-announce-0:26379\u0026quot;, \u0026quot;redis-2-redis-ha-announce-0:26379\u0026quot;, \u0026quot;redis-2-redis-ha-announce-0:26379\u0026quot;}, MasterName: \u0026quot;mymaster\u0026quot;, Dial: func(addr string) (redis.Conn, error) { timeout := 500 * time.Millisecond c, err := redis.DialTimeout(\u0026quot;tcp\u0026quot;, addr, timeout, timeout, timeout) if err != nil { return nil, err } return c, nil }, } # 產生 connection pool return \u0026amp;redis.Pool{ MaxIdle: 3, MaxActive: 64, Wait: true, IdleTimeout: 240 * time.Second, Dial: func() (redis.Conn, error) { # 透過 sentinel 取得 master address，如果 master 死了，再執行可以拿到新的 master masterAddr, err := sntnl.MasterAddr() if err != nil { return nil, err } c, err := redis.Dial(\u0026quot;tcp\u0026quot;, masterAddr) if err != nil { return nil, err } return c, nil }, TestOnBorrow: func(c redis.Conn, t time.Time) error { if !sentinel.TestRole(c, \u0026quot;master\u0026quot;) { return errors.New(\u0026quot;Role check failed\u0026quot;) } else { return nil } }, } 這邊要注意，客戶端 (golang) 處理 connection 的 exception，要記得重新執行 sntnl.MasterAddr() 來取得 failover 後新指派的 master。\nClient 測試 寫一個 golang redis 的 client 跑起來。這個部分我們在 kafka的章節做過類似的事情，可以簡單湊一個玩玩。\n延伸問題 使用上面的 golang 範例，確實是能透過 sentinel 取得 master，再向 master 取得連線。但這邊有兩個問題\n客戶端需要支援 sentinel 客戶端要感知 sentinel 的位址連線，才能知道所有 sentinel 的位置，設定又產生耦合 不能彈性的調度 sentinel，如果需要增加或是減少 sentinel，客戶端需要重新設定 雖然 sentinel 有 HA，可是客戶端對 sentinel 的設定沒有 HA，萬一已知的所有 sentinel 掛了就全掛 有沒有更優雅的方式使用 sentinel，我們下篇會討論使用 HAProxy 來完成\n","permalink":"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 GKE 上部署 Redis HA (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-28-redis-ha-deployment/\"\u003e使用 helm 部署 redis-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/\"\u003eRedis HA with sentinel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-30-redis-ha-topology/\"\u003eRedis sentinel topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/\"\u003eRedis HA with HAproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/\"\u003eRedis HA Failure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eredis-sentinel\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eredis sentinel 與 redis 使用相容的 api，直接使用 redis-cli 透過 26479 port 連入，可以連到 sentinel，透過 sentinel 可以取得 redis master 的狀態與連線設定。\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eredis-cli -h redis-redis-ha -p 26479\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e上篇我們的 redis-ha 安裝完變這樣\u003c/p\u003e","title":"Redis Ha Sentinel"},{"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n今天的文會比較短，因為我早上在綠島已經水肺潛水潛了三趟，有點累哈哈\nRedis introduction Redis 是常用的 in-memory 的資料儲存庫，可作為資料庫，快取，message broker 使用，都非常好用。Redis 官方支援 high availability，使用的是 redis-sentinel ，今天我們就來部署一個有完整 sentinel 的 redis-ha。\nRedis 另外提供了一個 solution Redis cluster (multiple writer solution)，作為增加資料輸出帶寬，與增加資料耐用度的分散式解決方案，與 redis sentinel 所處理的 ha 問題是不相同的。有機會我們也來談。\nDeploy 我把我的寶藏都在這了https://github.com/chechiachang/go-redis-ha\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash HELM_NAME=redis-1 # Stable: chart version: redis-ha-3.6.1\tapp version: 5.0.5 helm upgrade --install ${HELM_NAME} stable/redis-ha --version 3.6.1 -f values-staging.yaml Helm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 kafka。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nRedis-ha helm chart github\nInstall 這邊是用 upgrade \u0026ndash;install，已安裝就 upgrade，沒安裝就 install，之後可以用這個指令升版\nhelm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml values-staging 完整的 values.yaml 在 helm chart github\nimage: repository: redis tag: 5.0.5-alpine pullPolicy: IfNotPresent ## replicas number for each component replicas: 3 servers: serviceType: ClusterIP # [ClusterIP|LoadBalancer] annotations: {} auth: true ## Redis password ## Defaults to a random 10-character alphanumeric string if not set and auth is true ## ref: https://github.com/kubernetes/charts/blob/master/stable/redis-ha/templates/redis-auth-secret.yaml ## #redisPassword: ## Use existing secret containing key `authKey` (ignores redisPassword) existingSecret: redis-credentials ## Defines the key holding the redis password in existing secret. authKey: auth 這邊有準備 secret/redis-credentials 裡面的 key[auth] 存放 redis 密碼，要連入的 pod 需要掛載 secret 並把 auth 匯入。\nVersion 這邊使用的版本：\nchart version: redis-ha-3.6.1 app version: 5.0.5 Redis Image: redis:5.0.5-alpine Redis exporter: oliver006/redis_exporter:v0.31.0 安裝完變這樣\n$ kubectl get po | grep redis NAME READY STATUS RESTARTS AGE redis-1-redis-ha-server-0 3/3 Running 0 3d4h redis-1-redis-ha-server-1 3/3 Running 0 3d5h redis-1-redis-ha-server-2 3/3 Running 0 3d4h describe pod 可以看到裡面有三個 container\nredis: 主要的 redis sentinel: 維護 redis 可用性的服務，會監測 redis 狀態，並把連線指派到新的 master redis-exporter: 把 redis 的運行資料(metrics) 送出到 promethues Networking Service\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR redis-redis-ha ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,26379/TCP,9121/TCP 46m app=redis-ha,release=redis redis-redis-ha-announce-0 ClusterIP 10.3.243.81 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-0 redis-redis-ha-announce-1 ClusterIP 10.3.250.151 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-1 redis-redis-ha-announce-2 ClusterIP 10.3.242.59 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-2 nslookup redis-redis-ha Name: redis-redis-ha Address 1: 10.0.0.42 redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local Address 2: 10.0.1.13 redis-redis-ha-server-2.redis-redis-ha.default.svc.cluster.local Address 3: 10.0.2.8 redis-redis-ha-server-0.redis-redis-ha.default.svc.cluster.local Name: redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local Address 1: 10.0.0.43 redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local 連線 所有連線透過 redis-redis-ha service 連入\nredis-cli -h redis-redis-ha -p 6479 -a \u0026lt;password\u0026gt; 或是直接指定 redis instance 連入。\nredis-cli -h redis-redis-ha-announce-0 -p 6479 -a \u0026lt;password\u0026gt; redis-cli -h redis-redis-ha-announce-1 -p 6479 -a \u0026lt;password\u0026gt; redis-cli -h redis-redis-ha-announce-2 -p 6479 -a \u0026lt;password\u0026gt; 但上面兩者會有問題，redis 只有 master 是 writable，連入 slave 會變成 readonly，如果沒有任何 probe 機智，那就是每次連線時有 2/3 機率會連到 readonly 的 redis slave 。所以連線前要先找到正確的 master\nSentinel Sentinel 是 redis 官方提供的 HA solution，主要負責監控 redis 的狀態，並控制 redis master 的 failover 機制，一但超過 threshold，sentinel 就會把 master failover 到其他 slave 上。並把 master 連線指向新 master。\nredis sentinel 與 redis 使用相容的 api，直接使用 redis-cli 透過 26479 port 連入，可以連到 sentinel，透過 sentinel 可以取得 redis master 的狀態與連線設定。\nredis-cli -h redis-redis-ha -p 26479 App 端支援 sentinel 需要有支援 sentinel 的 redis client library，例如: python redis-py 有支援 sentinel 的設定。\n這邊就會比較麻煩，因為不是所有的語言對 redis-sentinel 的支援性都夠好，或是沒辦法設定到妮旺使用的情境上。\n如果你找得到支援性良好的套件，恭喜你。不然就像我們公司，與我們的需求有衝突，只好自己 fork library。\n所以說直接使用有支援 redis-sentinel 可能會遇到一些問題。那也沒有更好的解決方法？我們下次說明使用 HAproxy 的高可用方案。\nBenchmark 部署完後，可以跑一下 benchmark，看看在 kubernetes 上運行的效能有沒有符合需求。\nRun a redis pod with sleep command NOTE: CPU usage (rapidly) increasing during benchmark DON\u0026rsquo;T DO THIS on PRODUCTION\nkubectl run test-redis --image redis:5.0.5-alpine --command sleep 36000 kubectl exec -it test-redis-xxxxxxxxx-xxxxx sh Benchmark\nredis-benchmark --help redis-benchmark \\ -h haproxy-service.local \\ -p 6379 \\ -c 100 \\ -d 30 \\ -n 1000000 ====== MSET (10 keys) ====== 100000 requests completed in 2.32 seconds 50 parallel clients 3 bytes payload keep alive: 1 85.37% \u0026lt;= 1 milliseconds 98.06% \u0026lt;= 2 milliseconds 99.18% \u0026lt;= 3 milliseconds 99.62% \u0026lt;= 4 milliseconds 99.93% \u0026lt;= 5 milliseconds 100.00% \u0026lt;= 5 milliseconds 43066.32 requests per second ","permalink":"https://chechia.net/posts/2019-09-28-redis-ha-deployment/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在 GKE 上部署 Redis HA (5)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-28-redis-ha-deployment/\"\u003e使用 helm 部署 redis-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-29-redis-ha-sentinel/\"\u003eRedis HA with sentinel\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-30-redis-ha-topology/\"\u003eRedis sentinel topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-02-redis-ha-on-haproxy/\"\u003eRedis HA with HAproxy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-10-03-redis-ha-failure-recovery/\"\u003eRedis HA Failure Recovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e今天的文會比較短，因為我早上在綠島已經水肺潛水潛了三趟，有點累哈哈\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Exausted Cat Face\" loading=\"lazy\" src=\"https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"redis-introduction\"\u003eRedis introduction\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://redis.io/\"\u003eRedis\u003c/a\u003e 是常用的 in-memory 的資料儲存庫，可作為資料庫，快取，message broker 使用，都非常好用。Redis 官方支援 high availability，使用的是 \u003ca href=\"https://redis.io/topics/sentinel\"\u003eredis-sentinel\u003c/a\u003e\n，今天我們就來部署一個有完整 sentinel 的 redis-ha。\u003c/p\u003e\n\u003cp\u003eRedis 另外提供了一個 solution \u003ca href=\"https://redis.io/topics/cluster-tutorial\"\u003eRedis cluster (multiple writer solution)\u003c/a\u003e，作為增加資料輸出帶寬，與增加資料耐用度的分散式解決方案，與 redis sentinel  所處理的 ha 問題是不相同的。有機會我們也來談。\u003c/p\u003e","title":"Redis Ha Deployment"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Kafka\u0026rsquo;s quorum set Kafka 的 quorum set 這篇跟上篇其實再講 quorum，應該連在一起，但礙於篇幅（以及我個人的時間ＱＱ）拆成了兩篇。各位有需要可以回顧一下。\nReplicated log commit decision 上篇提到了兩個維持 replicated log 的 model\n有更新進來，leader 等待所有 follower 都 ack，才 commit。 有更新進來，leader 取得所有 node (2n+1) 中的多數 node 回應(n+1)，就 commit。而 leader election 時，必須比對 node 上的 log，決定誰是 electable leader(有最完整 log 的 follower)，這樣稱為共識(Quorum) 前者的好處是，所有 node 都有完整的 log 後，leader 才會 commit，回覆給客戶 commit 的資訊，所以每個 follower 都是 leader electable 人人都可以當 leader，leader 一故障就選擇新的 leader 即可。壞處就是 leader 在等待所有 follow ack 的時間會非常久，而且時間複雜度可能會隨 cluster size scale，或是變成要等最慢的 node 回應(worst case)。這樣在 node 數量多的時候非常不經濟。\n後者的好處是，n+1 node ack 後就 commit，leader commit 的速度是由前段班的回應速度決定。leader 出現故障，仍能維持多數 node 的資料正確。\nLeader election decision Leader Election 的問題也是類似，如果選擇 leader 時，所有的 follower 都比對過 log，這樣花的時間會很久。要知道，這是個分散式的架構，沒有中心化的 controller，也就是 follower 彼此需要交互比對。而且時間隨 follower 數量 scale。 造成topic partition 沒有 leader 的時間(downtime)太長。\n如果使用 majority，也就是當 leader 死掉，產生新的 leader election 時，只詢問 n+1 個 follower ，然後從選出 log 最完整的人當 leader， 這樣過程中每個 follower 彼此比對，確認，然後才選出 leader，確認 leader 的結果，所花的時間會大幅縮短。\n當然，這麼做產生的 tradeoff，就是萬一取得多數決的 n+1 個 follower 裡面，沒有最完整的 log ，那從裡頭選出來的 leader 自然也沒有完整 log，選出來的 leader 就會遺失資料。\n一個完整的 Quorum 機制 這不是 kafka 的機制，但我們順帶聊聊。\ncommit decision 使用多數決(majority) leader election 也使用多數決 總共有 2n+1 replicas，leader 取得 n+1 ack 才能 commit message。然後 leader election 時，從至少 n+1 個 follower 中取得多數決才能選出 leader。有過半的完整log，加上取得過半數的人確認，兩者產生 overlap。這樣的共識就確保有完整的 log 的 follower 一定會出現在 leader election 中，確保選出來的 leader 有完整 log。\n好處如前面描述，整體效能由前段班的速度決定。\n壞處是，很容易就沒有足夠的 electable leader。要容忍 1 個錯誤，需要 3 個完整備份，要容忍 2 個錯誤需要 5 個備份。在實務上，只靠依賴夠多的 redundency 容錯非常的不實際：每一次寫入需要 5 倍寫入跟硬碟空間，但整體效能只有 1/5。資料量大就直接ＧＧ。所以 quorum 才會只存在分散式集群(ex. zookeeper)，而不會直接用在儲存系統。\nKafka\u0026rsquo;s approach Kafka 不使用 majority vote，而是去動態維護一套 in-sync replicas(ISR) ，這些 ISR 會跟上 leader 的進度，而只有這些 ISR 才能是 leader eligible。一個 update 只有在所有 ISR 都 ack 後才會 commit。\nISR 的狀態不放在 kafka 而放在 zookeeper 上，也就是目前哪些 node 是 ISR 的記錄存在 zookeeper。這件事對維持 kafka 節點上，leader 能夠分散在各個 kafka node 上(leader rebalance)是很重要的。\nkafka\u0026rsquo;s approach 與 majority vote，在等待 message commit ack 上所花的成本是一樣的。 然而在 leader election 上，kafka 的 ISR 確保了更多個 eligiable leader 的數量，持續維持在合理的數量，而不會要維持大量個 redundency。ISR 放在外部，更方便 kafka 做 leader rebalance，增加穩定度。\nUnclean leader election 如果 leaders 都死光了會怎樣？\n只要有一個 replica in-sync，Kafka 就保證資料的完整性。然而所有可用的 leaders 都死了，這個就無法保證。\n如果這個情形發生了，kafka 會做以下處理\n等 ISR 中有人完全回復過來，然後選這個 node 作為 leader(有資料遺失的風險) 直接選擇第一個回覆的 node (不一定在 ISR 中)，先回覆的就指派為 leader 前者犧牲 availability （回覆前沒有 leader 可作讀寫）來確保資料是來自 ISR，雖然錯誤中無法讀寫(downtime)，但可以確定錯誤前跟錯誤後的資料都來自 ISR\n後者犧牲 consistency （來自非 ISR 的 leader 可能導致資料不正確），然而卻能更快的從錯誤中回覆，減少 downtime\n0.11.0.0 後的 kafka 預設是選擇前者，也就是 consistency over availability，當然這可以在設定更改。\nAvailability and Durability Guarantees 近一步考慮 client 的影響。\nProducer 在寫入時可以選擇 message 需要多少 acknowledge，0, 1 or all，ack=all 指的是 message 收到所有 in-sync replicas 的 ack\n如果 2 replicas 中有 1 個故障，這時寫入只要收到 1 個 ISR 的 ack，就達成 ack=all 但如果不幸剩下一個 replicas 也死了 (0/0 ack)，寫入的資料就會遺失 有些使用情境，會希望資料的耐用度(Durability)優先於可用性(Availability)，可以透過以下兩個方式設定\n禁用 unclean leader election，效果是如果所有的 replicas 都失效，則整個 partition 都失效，直到前一個 leader 回復正常。 指定可接受的最少 ISR，如果 partition 中的 ISR 低於這個數量，就停止寫入這個 partition，直到 ISR 的數量回覆。 這樣雖然犧牲了可用性，卻可以最大程度地確保資料的可靠性。\n複本管理 上面的討論都只是再說一個 topic，實務中 kafka 中會有大量的 topic ，乘上 partition number 與 replication factor，成千上萬的複本分散在集群中，kafka 會試圖分散 replicas 到集群中，並讓 leader 的數量平均在 node 上\n","permalink":"https://chechia.net/posts/2019-09-26-kafka-ha-continued/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e","title":"Kafka HA Continued"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Zookeeper Multi-server setup Kafka Multi-broker setup Zookeeper Multi-Server 為了維持 zookeeper 有效運作，cluster 必須維持 majority (多數)，也就是至少一半的機器在線。如果總共 3 台，便可以忍受 1 台故障仍保有 majority。如果是 5 台就可以容忍 2 台故障。一般來說都建議使用基數數量。Zookeeper Multi Server Setup\n普遍情況，3 台 zookeeper 已經是 production ready 的狀態，但如果為了更高的可用性，以方便進行單節點停機維護，可以增加節點數量。\nTopology 需要將 zookeeper 放在不同的機器上，不同的網路環境，甚至是不同的雲平台區域上，以承受不同程度的故障。例如單台機器故障，或是區域性的網路故障。\n我們這邊會使用 Kubernetes PodAntiAffinity，要求 scheduler 在部屬時，必須將 zookeeper 分散到不同的機器上。設定如下：\nvim values-staging.yaml zookeeper: enabled: true resources: ~ env: ZK_HEAP_SIZE: \u0026quot;1G\u0026quot; persistence: enabled: false image: PullPolicy: \u0026quot;IfNotPresent\u0026quot; url: \u0026quot;\u0026quot; port: 2181 ## Pod scheduling preferences (by default keep pods within a release on separate nodes). ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## By default we don't set affinity: affinity: # Criteria by which pod label-values influence scheduling for zookeeper pods. podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; labelSelector: matchLabels: release: zookeeper 使用 podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution，如果 topologyKey 已經有指定 label 的 pod 存在，則無法部署，需要數到其他台機器。\nkubectl get pods --output wide | grep zookeeper NAME READY STATUS RESTARTS AGE IP NODE kafka-0-zookeeper-0 1/1 Running 0 42d 10.8.12.4 gke-chechiachang-pool-1-e06e6d00-pc98 kafka-0-zookeeper-1 1/1 Running 0 42d 10.8.4.4 gke-chechiachang-pool-1-e06e6d00-c29q kafka-0-zookeeper-2 1/1 Running 0 42d 10.8.3.6 gke-chechiachang-pool-1-e06e6d00-krwc 效果是 zookeeper 都分配到不同的機器上。\nGuarantees Zookeeper 對於資料一致性，有這些保障 Consistency Guarantees\n順序一致性：資料更新的順序，與發送的順序一致 原子性：資料更新只有成功或失敗，沒有部份效果 系統一致性：可戶端連到 server 看到的東西都是一樣，無關連入哪個 server 可靠性： 客戶端的更新請求，一但收到 server 回覆更新成功，便會持續保存狀態。某些錯誤會造成客戶端收不到回覆， 可能是網路問題，或是 server 內部問題，這邊就無法確定 server 上的狀態，是否被更新了，或是請求已經遺失了。 從客戶讀取到的資料都是以確認的資料，不會因為 server 故障回滾(Roll back)而回到舊的狀態 Kafka 的設定 ## The StatefulSet installs 3 pods by default replicas: 3 resources: limits: cpu: 200m memory: 4096Mi requests: cpu: 100m memory: 1024Mi kafkaHeapOptions: \u0026quot;-Xmx4G -Xms1G\u0026quot; 設定 broker 的數量，以及 Pod 提供的 resource，並且透過 heapOption 把記憶體設定塞進 JVM\naffinity: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - kafka topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - zookeeper 這邊下了兩個 affinity\npodAntiAffinity 盡量讓 kafka-broker 分散到不同機器上 podAffinity 讓 broker prefer 跟 zookeeper 放在一起 分散的理由同上，不希望一台機器死了，就讓多個 broker 跟著死\n要和 zookeeper 放在一起，就要看需求與實際環境調整\nconfigurationOverrides: \u0026quot;default.replication.factor\u0026quot;: 3 \u0026quot;offsets.topic.replication.factor\u0026quot;: 2 # Increased from 1 to 2 for higher output \u0026quot;offsets.topic.num.partitions\u0026quot;: 3 \u0026quot;confluent.support.metrics.enable\u0026quot;: false # Disables confluent metric submission \u0026quot;auto.leader.rebalance.enable\u0026quot;: true \u0026quot;auto.create.topics.enable\u0026quot;: true \u0026quot;message.max.bytes\u0026quot;: \u0026quot;16000000\u0026quot; # Extend global topic max message bytes to 16 Mb 這邊再把 broker 運行的設定參數塞進去，參數的用途大多與複本與高可用機制有關下面都會提到。\nKafka 的複本機制 kafka 的副本機制 預設將各個 topic partition 的 log 分散到 server 上，如果其中一台 server 故障，資料仍然可用。\n兩個重要的設定\nnum.partitions=N default.replication.factor=M kafka 預設使用複本，所有機制與設計都圍繞著複本。如果（因為某些原因）不希望使用複本，可將 replication factor 設為 1。\nreplication 的單位是 topic partition，正常狀況下\n一個 partition 會有一個 leader，以及零個或以上個 follower leader + follower 總數是 replication factor 所有讀寫都是對 leader 讀寫 leader 的 log 會同步到 follower 上，leader 與 follower 狀態是一樣的 Election \u0026amp; Load balance 通常一個 topic 會有多個 partition，也就是說，每個 topic 會有多個 partition leader，分散負載\n通常 topic partition 的總數會比 broker 的數量多\n以上一篇範例，我們有三個 kafka-0-broker 各自是一個 Pod 有 topic: ticker 跟預設的 __consumer_offset__，乘上 partition number 的設定值(N)，會有 2N 個 partitions partitiion 會有各自的複本，kafka 會盡量將相同 topic 的複本分散到不同 broker 上 kafka 也會盡量維持 partition 的 leader 分散在不同的 broker 上，這個部分 kafka 會透過算法做 leader election，也可手動使用腳本做 Balancing leadership 總之，topic 的 partition 與 leader 會分散到 broker 上，維持 partition 的可用性。\nsync node 要能夠維持 zookeeper 的 session (zookeeper 有 heartbeat 機制) follower 不能落後 leader 太多 kafka 能保障資料不會遺失，只要至少一個 node 是在 sync 的狀態。例如本來有三個 partition，其中兩個 partition 不同步，只要其中一個 partition 是同步，便能作為 leader 持續提供正確的 message。\nReplicated Logs kafka 透過 replicated log 維持分散式的 partition\n複本間要維持共識(consensus)的最簡單機制，就是單一 leader 決定，其他 follower 跟隨。然而萬一 leader 死了，選出的新 leader 卻還沒跟上原先 leader 的資料。這時便使用 replicated log，來確保新的 leader 就算原先沒跟上，也能透過 replicated log 隨後跟上且不遺失資料。維持 log 一直都同步的前提，就是 leader 要一直確認 followers 的 log 都有跟上，這個其實就是變相的多 leader，效能消耗較大。\n另一個維持 log 機制，如果希望 follower 彼此的 log 應該先進行比對，讓資料交接過程有 overlap，這個過程稱為 Quorum。一個常用的方式是多數決(majority)\n如果總共有 2n+1 的 node，leader 要向 n+1 個 follower 取得共識，才確定這個 log 已經 commit 了 leader 總是維持 n+1 follower 的 log 有跟上，因此可以容忍最多 n 個 node 死了，集群整體能有 n+1 的 node 維持著正確的 commited log 不用向所有 node 確認才 commit ，節省了一半的 ack majarity 的另一個好處是，n+1 共識的速度是由前 1/2 快的 node 決定的。由於只要先取得 n+1 就可以 commit，速度快的 node 會先回應，讓整體速度提升。 ","permalink":"https://chechia.net/posts/2019-09-25-kafka-ha-topology/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e","title":"Kafka HA Topology"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 從 Zookeeper 獲取資訊 取得並處理 topic benchmark kafka zookeeper zookeeper 是 kafka 的分散式協調系統，在 kafka 上多個節點間需要協調的內容，例如：彼此節點的ID，位置與當前狀態，或是跨節點 topic 的設定與狀態。取名叫做 zookeeper 就是在協調混亂的分散式系統，,裡面各種不同種類的服務都要協調，象個動物園管理員。Zookeeper 的官方文件 有更詳細的說明。\nKafka 的節點資訊，與當前狀態，是放在 zookeeper 上，我們可以透過以下指令取得\n# 首先先取得 zkCli 的 cli，這個只有連進任何一台 zookeeper 內部都有 kubectl exec -it kafka-0-zookeeper-0 --container kafka-broker bash # 由於是在 Pod 內部，直接 localhost 詢問本地 /usr/bin/zkCli.sh -server localhost:2181 Connecting to localhost:2181 2019-09-25 15:02:36,089 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT 2019-09-25 15:02:36,096 [myid:] - INFO [main:Environment@100] - Client environment:host.name=kafka-0-zookeeper-0.kafka-0-zookeeper-headless.default.svc.cluster.local 2019-09-25 15:02:36,096 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_131 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.10.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=\u0026lt;NA\u0026gt; 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd64 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.version=4.14.127+ 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:user.name=zookeeper 2019-09-25 15:02:36,102 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/home/zookeeper 2019-09-25 15:02:36,102 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/ 2019-09-25 15:02:36,105 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@42110406 Welcome to ZooKeeper! 2019-09-25 15:02:36,160 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) JLine support is enabled 2019-09-25 15:02:36,374 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@876] - Socket connection established to localhost/127.0.0.1:2181, initiating session 2019-09-25 15:02:36,393 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x16d67baf1310001, negotiated timeout = 30000 WATCHER:: WatchedEvent state:SyncConnected type:None path:null [zk: localhost:2181(CONNECTED) 0] 取得 kafka broker 資料\n# List root Nodes $ ls / [cluster, controller, controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config] # Brokers 的資料節點 $ ls /brokers [ids, topics, seqid] # List /brokers/ids 得到三個 kafka broker $ ls /brokers/ids [0, 1, 2] # 列出所有 topic 名稱 ls /brokers/topics [ticker] ticker 是上篇範利用到的 topic\n簡單來說，zookeeper 存放這些狀態與 topic 的 metadata\n儲存核心的狀態與資料，特別是 broker 萬一掛掉，也還需要維持的資料 協調工作，例如協助 broker 處理 quorum，紀錄 partition master 等 # 離開 zkCli quit Kafka 這邊一樣先連線進去一台 broker，取得 kafka binary\nkubectl exec -it kafka-0-0 --container kafka-broker bash ls /usr/bin/ | grep kafka kafka-acls kafka-broker-api-versions kafka-configs kafka-console-consumer kafka-console-producer kafka-consumer-groups kafka-consumer-perf-test kafka-delegation-tokens kafka-delete-records kafka-dump-log kafka-log-dirs kafka-mirror-maker kafka-preferred-replica-election kafka-producer-perf-test kafka-reassign-partitions kafka-replica-verification kafka-run-class kafka-server-start kafka-server-stop kafka-streams-application-reset kafka-topics kafka-verifiable-consumer kafka-verifiable-producer 很多工具，我們這邊只會看其中幾個\ntopic 資訊 Topic 的資訊，跟 zookeeper 要\n# List topics /usr/bin/kafka-topics --list --zookeeper kafka-0-zookeeper ticker 操作 message 從 topic 取得 message\n# This will create a new console-consumer and start consuming message to stdout /usr/bin/kafka-console-consumer \\ --bootstrap-server localhost:9092 \\ --topic engine_topic_soundwave_USD \\ --timeout 0 \\ --from-beginning 如果 ticker 那個 example pod 還在執行，這邊就會收到 ticker 的每秒 message\n如果沒有，也可以開啟另一個 broker 的連線\nkubectl exec -it kafka-0-1 --container kafka-broker bash # 使用 producer 的 console 連入，topic 把 message 塞進去 /usr/bin/kafka-console-producer \\ --broker-list localhost:9092\\ --topic ticker tick [enter] tick [enter] kafka-console-consumer 那個 terminal 就會收到 message\ntick tick 當然也可以使用 consumer group\n# Use consumer to check ticker topics /usr/bin/kafka-console-consumer \\ --bootstrap-server localhost:9092 \\ --topic ticker \\ --group test 有做過上面的操作產生 consumer group，就可以透過 consumer API，取得 consumer group 狀態\n# Check consumer group /usr/bin/kafka-consumer-groups \\ --bootstrap-server localhost:9092 \\ --group ticker \\ --describe Consumer group 'test' has no active members. TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID ticker 0 23 23 0 - - - Topic 設定操作 Topic 設定文件 在此\n這邊透過 kafka-configs 從 zookeeper 取得 topic 設定，這邊的 max.message.bytes，是這個 topic 每個 message 的最大上限。\n/usr/bin/kafka-configs --zookeeper kafka-0-zookeeper:2181 --describe max.message.bytes --entity-type topics Configs for topic '__consumer_offsets' are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer Configs for topic 'ticker' are __consumer__offsets 是系統的 topic ，紀錄目前 consumer 讀取的位置。\nticker 沒有設定，就是 producer 當初產生 topic 時沒有指定，使用 default 值\n由於我們公司的使用情境常常會超過，所以可以檢查 producer app 那端送出的 message 大小，在比較這邊的設定。當然現在 ticker 的範例，只有一個 0-60 的數值，並不會超過。這個可以在 helm install 的時候，使用 value.yaml 傳入時更改。\n不喜歡這個值，可以更改，這邊增加到 16MB\nTOPIC=ticker /usr/bin/kafka-configs \\ --zookeeper kafka-3-zookeeper:2181 \\ --entity-type topics \\ --alter \\ --entity-name ${TOPIC} \\ --add-config max.message.bytes=16000000 Benchmark 使用內建工具跑 benchmark\nProducer\n/usr/bin/kafka-producer-perf-test \\ --num-records 100 \\ --record-size 100 \\ --topic performance-test \\ --throughput 100 \\ --producer-props bootstrap.servers=kafka:9092 max.in.flight.requests.per.connection=5 batch.size=100 compression.type=none 100 records sent, 99.108028 records/sec (0.01 MB/sec), 26.09 ms avg latency, 334.00 ms max latency, 5 ms 50th, 70 ms 95th, 334 ms 99th, 334 ms 99.9th. Consumer\n/usr/bin/kafka-consumer-perf-test \\ --messages 100 \\ --broker-list=kafka:9092 \\ --topic performance-test \\ --group performance-test \\ --num-fetch-threads 1 ","permalink":"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e","title":"Kafka Operation Scripts"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 在 Kubernetes 中連線 kafka 使用 golang library 連線到 Kafka 透過 kafka script 操作 kafka kubernetes 中連線 kafka 先看一看 kafka pods\n$ kubectl get pods --selector='app=kafka' NAME READY STATUS RESTARTS AGE kafka-1-0 1/1 Running 1 26d kafka-1-1 1/1 Running 0 26d kafka-1-2 1/1 Running 0 26d $ kubectl get pods -l 'app=zookeeper' NAME READY STATUS RESTARTS AGE kafka-1-zookeeper-0 1/1 Running 0 26d kafka-1-zookeeper-1 1/1 Running 0 26d kafka-1-zookeeper-2 1/1 Running 0 26d $ kubectl get pods -l 'app=kafka-exporter' NAME READY STATUS RESTARTS AGE kafka-1-exporter-88786d84b-z954z 1/1 Running 5 26d kubectl describe pods kafka-1-0 Name: kafka-1-0 Namespace: default Priority: 0 Node: gke-chechiachang-pool-1-e4622744-wcq0/10.140.15.212 Labels: app=kafka controller-revision-hash=kafka-1-69986d7477 release=kafka-1 statefulset.kubernetes.io/pod-name=kafka-1-0 Annotations: kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka-broker Status: Running IP: 10.12.6.178 Controlled By: StatefulSet/kafka-1 Containers: kafka-broker: Image: confluentinc/cp-kafka:5.0.1 Port: 9092/TCP Host Port: 0/TCP Command: sh -exc unset KAFKA_PORT \u0026amp;\u0026amp; \\ export KAFKA_BROKER_ID=${POD_NAME##*-} \u0026amp;\u0026amp; \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 \u0026amp;\u0026amp; \\ exec /etc/confluent/docker/run Requests: cpu: 100m Liveness: exec [sh -ec /usr/bin/jps | /bin/grep -q SupportedKafka] delay=30s timeout=5s period=10s #success=1 #failure=3 Readiness: tcp-socket :kafka delay=30s timeout=5s period=10s #success=1 #failure=3 Environment: POD_IP: (v1:status.podIP) POD_NAME: kafka-1-0 (v1:metadata.name) POD_NAMESPACE: default (v1:metadata.namespace) KAFKA_HEAP_OPTS: -Xmx4G -Xms1G KAFKA_ZOOKEEPER_CONNECT: kafka-1-zookeeper:2181 KAFKA_LOG_DIRS: /opt/kafka/data/logs KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: false KAFKA_DEFAULT_REPLICATION_FACTOR: 3 KAFKA_MESSAGE_MAX_BYTES: 16000000 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_JMX_PORT: 5555 Mounts: /opt/kafka/data from datadir (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2tm8c (ro) Conditions: Volumes: datadir: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: datadir-kafka-1-0 ReadOnly: false default-token-2tm8c: Type: Secret (a volume populated by a Secret) SecretName: default-token-2tm8c Optional: false 講幾個重點：\n這邊跑起來的是 kafka-broker，接收 producer 與 consumer 來的 request 這邊用的是 statefulsets，不是完全無狀態的 kafka broker，而把 message 記在 datadir 上，降低故障重啟時可能遺失資料的風險。 啟動時，把 kubernetes 指定的 pod name 塞進環境變數，然後作為當前 broker 的 ID 沒有設定 Pod antiAffinity，所以有可能會啟三個 kafka 結果三個跑在同一台 node 上，這樣 node 故障就全死，沒有HA Service \u0026amp; Endpoints 看一下 service 與 endpoints zookeeper 與 exporter 我們這邊先掠過不談，到專章講高可用性與服務監測時，再來討論。\n$ kubectl get service -l 'app=kafka' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-1 ClusterIP 10.15.242.178 \u0026lt;none\u0026gt; 9092/TCP 26d kafka-1-headless ClusterIP None \u0026lt;none\u0026gt; 9092/TCP 26d 兩個 services\n一個是 cluster-ip service，有 single cluster IP 與 load-balance，DNS 會過 kube-proxy。 一個是 headless service，DNS 沒有過 kube-proxy，而是由 endpoint controller 直接 address record，指向把符合 service selector 的 pod。適合做 service discovery，不會依賴於 kubernetes 的實現。 詳細說明在官方文件\n簡單來說，kafka broker 會做 auto service discovery，我們可以使用 headless service。\n客戶端(consumer \u0026amp; producer) 連入時，則使用 cluster-ip service，做 load balancing。\n$ kubectl get endpoints -l 'app=kafka' NAME ENDPOINTS AGE kafka-1 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d kafka-1-headless 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d Golang Example 附上簡單的 Golang 客戶端，完整 Github Repository 在這邊\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/segmentio/kafka-go\u0026quot; // 使用的套件 ) func main() { topic := \u0026quot;ticker\u0026quot; // 指定 message 要使用的 topic partition := 0 // 指定 partition，由於底下連線指定連線到 partition 的 leader，所以需要指定 partition kafkaURL := \u0026quot;kafka-0:9092\u0026quot; // 指定 kafkaURL，也可以透過 os.GetEnv() 從環境變數裡拿到。 // producer 對指定 topic, partition 的 leader 產生連線 producerConn, _ := kafka.DialLeader(context.Background(), \u0026quot;tcp\u0026quot;, kafkaURL, topic, partition) // 程式結束最後把 connection 關掉。不關會造成 broker 累積大量 connection，需要等待 broker 端 timeout 才會釋放。 defer producerConn.Close() //producerConn.SetWriteDeadline(time.Now().Add(10 * time.Second)) // 使用 go routine 跑一個 subprocess for loop，一直產生 message 到 kafka topic，這邊的範例是每秒推一個秒數。 go func() { for { producerConn.WriteMessages( kafka.Message{ Value: []byte(strconv.Itoa(time.Now().Second())), }, ) time.Sleep(1 * time.Second) } }() // make a new reader that consumes from topic-A, partition 0 r := kafka.NewReader(kafka.ReaderConfig{ Brokers: []string{kafkaURL}, Topic: topic, Partition: 0, MinBytes: 10e2, // 1KB MaxBytes: 10e3, // 10KB }) defer r.Close() //r.SetOffset(42) // 印出 reader 收到的 message for { m, err := r.ReadMessage(context.Background()) if err != nil { break } fmt.Printf(\u0026quot;%v message at offset %d: %s = %s\\n\u0026quot;, time.Now(), m.Offset, string(m.Key), string(m.Value)) } } 這邊可以使用 Dockerfile 包成一個 container image，然後丟上 kubernetes\n我稍晚補一下 docker image 跟 deployment 方便大家操作好了。\n或是攋人測試，直接 kubectl run 一個 golang base image 讓它 sleep，然後在連進去\nkubectl run DEPLOYMENT_NAME --image=golang:1.13.0-alpine3.10 sleep 3600 kubectl exec -it POD_NAME sh # 裡面沒有 Git 跟 vim 裝一下 apk add git vim go get github.com/chechiachang/kafka-on-kubernetes cd src/github.com/chechiachang/kafka-on-kubernetes/ vim main.go go build . ./kafka-on-kubernetes 2019-09-24 14:20:46.872554693 +0000 UTC m=+9.154112787 message at offset 1: = 46 2019-09-24 14:20:47.872563087 +0000 UTC m=+9.154121166 message at offset 2: = 47 2019-09-24 14:20:48.872568848 +0000 UTC m=+9.154126926 message at offset 3: = 48 2019-09-24 14:20:49.872574499 +0000 UTC m=+9.154132576 message at offset 4: = 49 2019-09-24 14:20:50.872579957 +0000 UTC m=+9.154138032 message at offset 5: = 50 2019-09-24 14:20:51.872588823 +0000 UTC m=+9.154146892 message at offset 6: = 51 2019-09-24 14:20:52.872594672 +0000 UTC m=+9.154152748 message at offset 7: = 52 2019-09-24 14:20:53.872599986 +0000 UTC m=+9.154158060 message at offset 8: = 53 這樣就連上了，完成一個最簡單的使用範例。\n這個例子太過簡單，上一篇講的 consumer group, partitions, offset 什麼設定全都沒用上。實務上這些都需要好好思考，並且依據需求做調整設定。\nClean up 把測試用的 deployment 幹掉\nkubectl delete deployment DEPLOYMENT_NAME 小結 簡述 kafka 在 kubernetes 上運行的狀況，連線方法 Demo 一個小程式 ","permalink":"https://chechia.net/posts/2019-09-24-kafka-basic-usage/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e","title":"Kafka-basic-usage"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n寫了部屬，本想談一下 kafka 的高可用性配置，看到大德的留言，才想到應該要先跟各位介紹一下 kafka，跟 kafka 的用途。也感謝大德路過發問，我也會順代調整內容。今天就說明何為 kafka，以及在什麼樣的狀況使用。\n摘要 簡介 kafka 基本元件 Kafka 的工作流程 簡介 Kafka Kafka 是分散式的 streaming platform，可以 subscribe \u0026amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。原版的說明在這邊\n這邊有幾個東西要解釋。\nMessage Queue System 當一個系統開始運作時，裡頭會有很多變數，這些變數其實就是在一定的範圍(scope）內，做訊息(message)的傳遞。例如在 app 寫了一個 function ，傳入一個變數的值給 function。\n在複雜的系統中，服務元件彼此也會有傳遞訊息的需求。例如我原本有一個 api-server，其中一段程式碼是效能瓶頸，我把它切出來獨立成一個 worker 的元件，讓它可以在更高效能地方執行，甚至 horizontal scaling。這種情境，辨可能歲需要把一部分的 message 從 api-server 傳到 worker，worker 把吃效能的工作做完，再把結果回傳給 api-server。這時就會需要一個穩定的 message queue system，來穩定，且高效能的傳遞這些 message。\nMessage Queue System 實做很多，ActiveMQ, RabbitMQ, \u0026hellip; 等，一些 database 做 message queue 在某些應用場景下也十分適合，例如 Redis 是 in-memory key-value database，內部也實做 pubsub，能夠在某些環境穩定的傳送 message。\nRequest-Response vs Publish-Subscribe 訊息的傳送有很多方式，例如 Http request-response 很適合 server 在無狀態(stateless) 下接受來自客戶端的訊息，每次傳送都重新建立新的 http connection，這樣做有很多好處也很多壞處。其中明顯的壞處是網路資源的浪費，以及訊息的不夠即時，指定特定收件人時發件人會造成額外負擔等。\n使用 Pub-sub pattern的好處，是 publisher 不需要額外處理『這個訊息要送給誰』的工作，而是讓 subscriber 來訂閱需要的訊息類別，一有新的 event 送到該訊息類別，直接透過 broker 推播給 subscriber。不僅即時，節省效能，而且訂閱的彈性很大。\nKafka producer \u0026amp; Consumer API Kafka 作為 client 與 server 兩邊的溝通平台，提供了許多 API 葛不同角色使用。Producer 產生 message 到特定 topic 上，consumer 訂閱特定 topics，kafak 把符合條件的訊息推播給 consumer。\nProducer API: 讓 app publish 一連的訊息 Consumer API: 讓 app subscribe 許多特定 topic，並處理訊息串流(stream) Stream API: 讓 app 作為串流中介處理(stream processor) Connect API: 與 producer 與 consumer 可以對外部服務連結 Topics \u0026amp; Logs Topic 是 kafka 為訊息串流提供的抽象，topic 是訊息傳送到 kafka 時賦予的類別(category)，作為 publish 與 consume 的判斷依據。\nPartition 訊息依據 topic 分類存放，並可以依據 replication factor 設定，在 kafka 中存放多個訊息分割(partition)。partition 可以想成是 message queue 的平行化 (parallel)，併發處理訊息可以大幅提昇訊息接收與發送的速度，並且多個副本也提高資料的可用性。\n由於訊息發送跟接收過程可能因為網路與環境而不穩定，這些相同 topic 的 partition 不一定會完全一樣。但 kafka 確保了以下幾點。\nGuarantees 良好配置的 kafka 有以下保證\n訊息在系統中送出跟被收到的時間不一定，但kafak中，從相同 producer 送出的訊息，送到 topic partition 會維持送出的順序 Consumer 看見的訊息是與 kafka 中的存放順序一致 有 replication factor 為 N 的 topic ，可以容忍(fault-tolerance) N-1 個 kafka-server 壞掉，而不影響資料。 當然，這邊的前提是有良好配置。錯誤的配置可能會導致訊息不穩定，效能低落，甚至遺失。\nProducer Producer 負責把訊息推向一個 topic，並指定訊息應該放在 topic 的哪個 partition。\nConsumer Consumer 會自行標記，形成 consumer group，透過 consumer group 來保障訊息傳遞的次序，容錯，以及擴展的效率。\nConsumer 透過 consumer group 共享一個 group.id。 Consumer group 去所有 partitions 裡拿訊息，所有 partitions 的訊息分配到 consumer group 中的 consumer。 app 在接收訊息時，設置正確的化，在一個 consumer group 中，可以容忍 consumer 失效，仍能確保訊息一指定的次序送達。在需要大流量時，也可調整 consumer 的數量提高負載。\n用例 kafka 的使用例子非常的多，使用範圍非常廣泛。\n基本上是訊息傳遞的使用例子，kafka 大多能勝任。\n小結 這邊只提了 kafka 的基本概念，基本元件，以及 consumer group 機制，為我們底下要談的 configuration 與 topology 鋪路。\n","permalink":"https://chechia.net/posts/2019-09-23-kafka-introduction/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e","title":"Kafka-introduction"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n","permalink":"https://chechia.net/posts/2019-09-23-kafka-helm-configuration/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e","title":"Kafka Helm Configuration"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n碎念 30 天每天一文真的蠻逼人的，每一篇都是新寫，還要盡可能顧及文章品質，下班趕文章，各位大德寫看看就知道\n這邊調整了當初想寫的文章，內容應該都會帶到 elk kafka-ha reids-ha prometheus kubernetes on gcp 但不會再一篇 10000 字了，逼死我吧\u0026hellip; 寫不完的部份 30 天候會在IT邦幫忙，或是我的 Github Page https://chechia.net/補完 寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n摘要 簡介 kafka 部屬 kafka 到 kubernetes 上 簡介 kafka Kafka 是分散式的 streaming platform，可以 subscribe \u0026amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。\n我們今天就來部屬一個 kafka。\nDeploy 我把我的寶藏都在這了https://github.com/chechiachang/kafka-on-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash # # https://github.com/helm/charts/tree/master/incubator/kafka #HELM_NAME=kafka HELM_NAME=kafka-1 helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator # Stable: chart version: kafka-0.16.2\tapp version: 5.0.1 helm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml Helm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 kafka。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nhelm init Helm Chart 一個 helm chart 可以當成一個獨立的專案，不同的 chart 可以在 kubernetes 上協助部屬不同的項目。\n這邊使用了還在 incubator 的chart，雖然是 prod ready，不過使用上還是要注意。\n使用前先把 incubator 的 helm repo 加進來\nhelm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator Install 這邊是用 upgrade \u0026ndash;install，已安裝就 upgrade，沒安裝就 install，之後可以用這個指令升版\nhelm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml Version 這邊使用的版本：\nchart version: kafka-0.16.2 app version: 5.0.1 kafka Image: confluentinc/cp-kafka:5.0.1 zookeeper Image: gcr.io/google_samples/k8szk:v3 kafka exporter: danielqsj/kafka-exporter:v1.2.0 values-staging 透過 helm chart，把啟動參數帶進去，這邊我們看幾個比較重要的，細節之後的文章在一起討論。\nhttps://github.com/chechiachang/kafka-on-kubernetes/blob/master/values-staging.yaml\nreplicas: 3 安裝三個 kafka，topology 的東西也是敬待下篇XD\n## The kafka image repository image: \u0026quot;confluentinc/cp-kafka\u0026quot; ## The kafka image tag 底層執行的 kafka 是 conluent kafka Configure resource requests and limits ref: http://kubernetes.io/docs/user-guide/compute-resources/ resources: {}\nlimits: cpu: 200m memory: 4096Mi requests: cpu: 100m memory: 1024Mi kafkaHeapOptions: \u0026ldquo;-Xmx4G -Xms1G\u0026rdquo;\n這邊可以調整在 kubernetes 上面的 limit 跟 request * Deploy 會先去跟 node 問夠不夠，夠的話要求 node 保留這些資源給 Pod * Runtime 超過 limit，Pod 會被 kubernetes 幹掉，不過我們是 JVM，外部 resource 爆掉前，應該會先因 heap 滿而死。一個施主自盡的感覺。 * CPU 蠻省的，吃比較多是 memory。但也要看你的使用情境 prometheus\n對我們有上 promethues，基本上就是 kafka-exporter 把 kafka metrics 倒出去 prometheus，這個也是詳見下回分解。 # 跑起來了 $ kubectl get po | grep kafka\nNAME READY STATUS RESTARTS AGE kafka-1-0 1/1 Running 0 224d kafka-1-1 1/1 Running 0 224d kafka-1-2 1/1 Running 0 224d kafka-1-exporter-88786d84b-z954z 1/1 Running 0 224d kafka-1-zookeeper-0 1/1 Running 0 224d kafka-1-zookeeper-1 1/1 Running 0 224d kafka-1-zookeeper-2 1/1 Running 0 224d\n","permalink":"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eELK Stack\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKafka HA on Kubernetes(6)\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/\"\u003eDeploy kafka-ha\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-23-kafka-introduction/\"\u003eKafka Introduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-24-kafka-basic-usage/\"\u003ekafka 基本使用\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-operation-scripts/\"\u003ekafka operation scripts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-25-kafka-ha-topology/\"\u003e集群內部的 HA topology\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-26-kafka-ha-continued/\"\u003e集群內部的 HA 細節\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePrometheus Metrics Exporter 很重要\u003c/li\u003e\n\u003cli\u003e效能調校\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"碎念\"\u003e碎念\u003c/h1\u003e\n\u003cp\u003e30 天每天一文真的蠻逼人的，每一篇都是新寫，還要盡可能顧及文章品質，下班趕文章，各位大德寫看看就知道\u003c/p\u003e","title":"Kafka Deployment on Kubernetes"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n摘要 簡介 logstash 將 logstash 部屬到 kubernetes 上 設定 logstash pipeline 處理 nginx access log 介紹 Logstash Logstash 是開元的資料處理引擎，可以動態的將輸入的資料做大量的處裡。原先的目的是處理 log ，但目前以不限於處理 log ，各種 ELK beat 或是其他來源的不同監測數據，都能處理。\nLogastash 內部的功能也大多模組化，因此可以組裝不同的 plugin，來快速處理不同來源資料。\n基本上常見的資料來源，logstash 都能夠處理，並且有寫好的 plugin 可以直接使用，細節請見logstash 官方文件\n後送資料庫與最終儲存庫 在開始架設 logstash 要先考慮 pipeline 處理過後送的資料庫，可使用的資料庫非常多，這邊會展示的有：\nELK Stack 標準配備送到 Elasticsearch 存放會時常查詢的熱資料，只存放一段時間前的資料 太舊的資料自動 Rollout 最終 archieving 的資料庫，這邊使用 GCP 的 Big Query 存放查找次數少，但非常大量的歷史紀錄。 Elasticsearch 在前幾篇已經架設好，GCP Big Query 的設定也事先開好。\n部屬 Logstash kubernetes resource 的 yaml 請參考 我的 github elk-kubernetes\nkubectl apply -f config-configmap.yaml kubectl apply -f pipelines-configmap.yam kubectl apply -f deployment.yaml kubectl apply -f service.yaml 放上去的 resource\nconfig-configmap: Logstash 服務本身啟動的設定參數 pipelines-configmap: Logstash 的 pipelines 設定檔案 Lostagh Deployment Logastash 的服務 instance 可以動態 scaling，也就是會有複數 Logstash instance 做負載均衡 Logstash service 可透過 kubernetes 內部的 kube-dns 服務 集群內的 filebeat 可以直接透過 logstash.default.svc.chechiachang-cluster.local 的 dns 連線 logstash 集群內的網路，直接使用 http（當然使用 https 也是可以，相關步驟請見前幾篇文章） 簡單講一下 kubernetes service 的負載均衡，關於 kubernetes service 細節這篇附上文件\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE logstash ClusterIP 10.15.254.47 \u0026lt;none\u0026gt; 5044/TCP 182d $ kubectl get endpoints NAME ENDPOINTS AGE logstash 10.12.0.132:5044,10.12.10.162:5044,10.12.9.167:5044 + 12 more... 182d 在 Kubernetes 內部每個 Pod 都能看到 logstash, logstash.default.svc.chechiachang-cluster.local 這兩個 dns DNS 直接指向複數的 logstash endpoints， 每一個 ip 都是 kubernetes 內部配置的一個 Pod 的 IP，開啟 5044 的 logstash port Service 的 load balance 機制視 service 設定，細節可以看這邊 講到最白，就是 filebeat LOGSTASH URL 設定為 http://logstash 就會打到其中一台 logstash\n更改 filebeat configmap\n$ kubectl edit configmap filebeat-configmap # Disable output to elasticsearch output.elasticsearch: enabled: false # Output to logstash output.logstash: hosts: [\u0026quot;logstash:5044\u0026quot;] protocol: \u0026quot;http\u0026quot; username: \u0026quot;elastic\u0026quot; password: 設定 logstash 這邊要先說，logstash 也支援 centralized configuration，如果你的 logstash 不是跑在 Kubernetes 上，沒辦法配置一套 configmap 就應用到全部的 instance，記的一定要使用。\nLogastash 的運行設定 logstash.yml，這邊我們沒有做設定，都是預設值，有需求可以自行更改\n當然之後要調整 batch size 或是 queue, cache 等等效能調校，也是來這邊改，改完 configmap ，rolling update logstash 就可以。\n這邊主要是來講 pipeline 設定。\n$ kubectl describe configmap pipelines-configmap apiVersion: v1 kind: ConfigMap metadata: name: logstash-pipelines namespace: elk labels: k8s-app: logstash data: # Nginx Template # https://www.elastic.co/guide/en/logstash/7.3/logstash-config-for-filebeat-modules.html#parsing-nginx nginx.conf: | ... Configmap 裡面只有一個 pipeline，就是 nginx.conf，我們這邊就只有一條，這邊一段一段看\nInput input { beats { # The lisening port of logstash port =\u0026gt; 5044 host =\u0026gt; \u0026quot;0.0.0.0\u0026quot; } } 設定 Input 來源，是 beat 從 5044 進來\nFilter 接下來一大段是 filter，每個 filter 中間的 block 都是一個 plugin，logstash 支援非常多有趣的 plugin ，處理不同來源的工作，細節請看這篇\nfilter { # Ignore data from other source in case filebeat input is incorrectly configured. if [kubernetes][container][name] == \u0026quot;nginx-ingress-controller\u0026quot; { # Parse message with grok # Use grok debugger in kibana -\u0026gt; dev_tools -\u0026gt; grok_debugger grok { match =\u0026gt; { \u0026quot;message\u0026quot; =\u0026gt; \u0026quot;%{IPORHOST:[nginx][access][remote_ip]} - \\[%{IPORHOST:[nginx][access][remote_ip_list]}\\] - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\u0026quot;%{WORD:[nginx][access][method]} %{DATA:[nginx][access][request_url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\u0026quot; %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\u0026quot;%{DATA:[nginx][access][referrer]}\\\u0026quot; \\\u0026quot;%{DATA:[nginx][access][agent]}\\\u0026quot; %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \\[%{DATA:[nginx][access][proxy_upstream_name]}\\] %{DATA:[nginx][access][upstream_addr]} %{NUMBER:[nginx][access][upstream_response_length]} %{NUMBER:[nginx][access][upstream_response_time]} %{NUMBER:[nginx][access][upstream_status]} %{DATA:[nginx][access][req_id]}\u0026quot; } } # Match url parameters if has params grok { match =\u0026gt; { \u0026quot;[nginx][access][request_url]\u0026quot; =\u0026gt; \u0026quot;%{DATA:[nginx][access][url]}\\?%{DATA:[nginx][access][url_params]}\u0026quot; } } # Remove and add fields mutate { remove_field =\u0026gt; \u0026quot;[nginx][access][request_url]\u0026quot; add_field =\u0026gt; { \u0026quot;read_timestamp\u0026quot; =\u0026gt; \u0026quot;%{@timestamp}\u0026quot; } # Add fileset.module:nginx to fit nginx dashboard add_field =\u0026gt; { \u0026quot;[fileset][module]\u0026quot; =\u0026gt; \u0026quot;nginx\u0026quot;} add_field =\u0026gt; { \u0026quot;[fileset][name]\u0026quot; =\u0026gt; \u0026quot;access\u0026quot;} } # Parse date string into timestamp date { match =\u0026gt; [ \u0026quot;[nginx][access][time]\u0026quot;, \u0026quot;dd/MMM/YYYY:H:m:s Z\u0026quot; ] remove_field =\u0026gt; \u0026quot;[nginx][access][time]\u0026quot; } # Split url_parameters with \u0026amp; # /api?uuid=123\u0026amp;query=456 # become # nginx.access.url_params.uuid=123 nginx.access.url_params.query=456 kv { source =\u0026gt; \u0026quot;[nginx][access][url_params]\u0026quot; field_split =\u0026gt; \u0026quot;\u0026amp;\u0026quot; } # Parse useragent useragent { source =\u0026gt; \u0026quot;[nginx][access][agent]\u0026quot; target =\u0026gt; \u0026quot;[nginx][access][user_agent]\u0026quot; remove_field =\u0026gt; \u0026quot;[nginx][access][agent]\u0026quot; } # Search remote_ip with GeoIP database, output geoip information for map drawing geoip { source =\u0026gt; \u0026quot;[nginx][access][remote_ip]\u0026quot; target =\u0026gt; \u0026quot;[nginx][access][geoip]\u0026quot; #fields =\u0026gt; [\u0026quot;country_name\u0026quot;,\u0026quot;city_name\u0026quot;,\u0026quot;real_region_name\u0026quot;,\u0026quot;latitude\u0026quot;,\u0026quot;longitude\u0026quot;,\u0026quot;ip\u0026quot;,\u0026quot;location\u0026quot;] } # ============== # Remove message to reduce data # ============== if [nginx][access][url] { mutate { # source:/var/lib/docker/containers/6e608bfc0a437c038a1dbdf2e3d28619648b58a1d1ac58635f8178fc5f871109/6e608bfc0a437c038a1dbdf2e3d28619648b58a1d1ac58635f8178fc5f871109-json.log remove_field =\u0026gt; \u0026quot;[source]\u0026quot; # Origin message remove_field =\u0026gt; \u0026quot;[message]\u0026quot; #add_field =\u0026gt; { \u0026quot;[nginx][access][message]\u0026quot; =\u0026gt; \u0026quot;[message]\u0026quot;} remove_field =\u0026gt; \u0026quot;[nginx][access][message]\u0026quot; # url_params:client_id=1d5ffd378296c154d3e32e5890d6f4eb\u0026amp;timestamp=1546849955\u0026amp;nonce=9a52e3e6283f2a9263e5301b6724e2c0d723def860c4724c9121470152a42318 remove_field =\u0026gt; \u0026quot;[nginx][access][url_params]\u0026quot; } } } # nginx-ingress-controller } # filter Grok Grok 本身的文件又是一大段，個人建議各路大德，如果要使用，請直接搜尋人家配置好的設定，不要自己寫\n真的要寫的話要善用工具\nKibana Grok Debugger YOUR_KIBANA_HOST/app/kibana#/dev_tools/grokdebugger 或是不知名大德貢獻線上 Debugger grok { match =\u0026gt; { \u0026quot;message\u0026quot; =\u0026gt; \u0026quot;%{IPORHOST:[nginx][access][remote_ip]} - \\[%{IPORHOST:[nginx][access][remote_ip_list]}\\] - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\u0026quot;%{WORD:[nginx][access][method]} %{DATA:[nginx][access][request_url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\u0026quot; %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\u0026quot;%{DATA:[nginx][access][referrer]}\\\u0026quot; \\\u0026quot;%{DATA:[nginx][access][agent]}\\\u0026quot; %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \\[%{DATA:[nginx][access][proxy_upstream_name]}\\] %{DATA:[nginx][access][upstream_addr]} %{NUMBER:[nginx][access][upstream_response_length]} %{NUMBER:[nginx][access][upstream_response_time]} %{NUMBER:[nginx][access][upstream_status]} %{DATA:[nginx][access][req_id]}\u0026quot; } } 其實就是 nginx 的 access log\n1.2.3.4 - [1.2.3.4] - - [21/Sep/2019:07:21:21 +0000] \u0026quot;GET /v1/core/api/list?type=queued\u0026amp;timestamp=1569050481\u0026amp;nonce=d1e80e00381e0ba6e42d4601912befcf03fbf291748e77b178230c19cd1fdbe2 HTTP/1.1\u0026quot; 200 3 \u0026quot;-\u0026quot; \u0026quot;python-requests/2.18.4\u0026quot; 425 0.031 [default-chechiachang-server-80] 10.12.10.124:8003 3 0.031 200 f43db228afe66da67b2c7417d0ad2c04 預設的 log 送件來，格式是 text，經過 pattern matching 後變成 json-like format，也就是可以從資料結構取得 .nginx.access.remote_ip 這樣的欄位，讓原本的 access log 從 text 變成可以查找的內容。\n原本的 text 送進 elasticsearch 當然也可以查找，但就會在 text 裡面做全文檢索，功能很侷限，效率很差。\nOutput logstash 支援的 output 以及設定在這邊\noutput { elasticsearch { hosts =\u0026gt; [\u0026quot;https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}\u0026quot;] user =\u0026gt; \u0026quot;${ELASTICSEARCH_USERNAME}\u0026quot; password =\u0026gt; \u0026quot;${ELASTICSEARCH_PASSWORD}\u0026quot; index =\u0026gt; \u0026quot;%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\u0026quot; manage_template =\u0026gt; false } } Elasticsearch 的配置很單純\noutput { google_bigquery { project_id =\u0026gt; ${GCP_PROJECT_ID} dataset =\u0026gt; ${GCP_BIG_QUERY_DATASET_NAME} csv_schema =\u0026gt; \u0026quot;path:STRING,status:INTEGER,score:FLOAT\u0026quot; json_key_file =\u0026gt; ${GCP_JSON_KEY_FILE_PATH} error_directory =\u0026gt; \u0026quot;/tmp/bigquery-errors\u0026quot; date_pattern =\u0026gt; \u0026quot;%Y-%m-%dT%H:00\u0026quot; flush_interval_secs =\u0026gt; 30 } } 其中的變數，我們全都用環境變數，在 deployment.yaml 配置，啟動 logstash pods 時代入\nGCP_JSON_KEY_FILE_PATH 這邊要配置一隻 GCP 的服務帳號金鑰，一個有 Big Query 寫入權限的 service account，把 json 使用 kubernetes secret 放到集群上，然後在 pod 上使用 volume from secret 掛載進來。 csv_schema =\u0026gt; \u0026quot;path:STRING,status:INTEGER,score:FLOAT\u0026quot; 這邊要配置之後會存入 Big Query 的 csv 結構\n小結 部屬 Logstash deployment 到 kubernetes 上 設定 pipeline，超多 plugin，族繁不及備載 Grok 配置 Big Query output 配置 ","permalink":"https://chechia.net/posts/2019-09-21-logstash-on-gke/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e作為範例的 ELK 的版本是當前的 stable release 7.3.1。\u003c/p\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e簡介 logstash\u003c/li\u003e\n\u003cli\u003e將 logstash 部屬到 kubernetes 上\u003c/li\u003e\n\u003cli\u003e設定 logstash pipeline 處理 nginx access log\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"介紹-logstash\"\u003e介紹 Logstash\u003c/h1\u003e\n\u003cp\u003eLogstash 是開元的資料處理引擎，可以動態的將輸入的資料做大量的處裡。原先的目的是處理 log ，但目前以不限於處理 log ，各種 ELK beat 或是其他來源的不同監測數據，都能處理。\u003c/p\u003e","title":"Logstash on GKE"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。\n官方文件 ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。\n這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：\nnode: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics) cluster: 處理 cluster 等級的 log, event 或是 metrics pod: 針對特定 pod 直接去掛一個 sidecar 上面的方法是可以混搭的，kubernetes 個個層級有log 處理流程，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。\n簡單來說，是去對的地方找對的 log。在架構上要注意 scalability 與 resource 分配，不要影響本身提供服務的 GKE ，但又能獲得盡量即時的 log。\n我們這邊直接進入 kubernetes resource 的設定，底下會附上在 GKE 找 log 的過程。\nNode level log harvest 為每一個 node 配置 filebeat，然後在 node 上面尋找 log，然後如我們上篇所敘述加到 input ，就可以把 log 倒出來。\n直覺想到就是透過 daemonsets 為每個 node 部署一個 filebeat pod，然後 mount node 的 log 資料夾，在設置 input。\nDeploy daemonsets kubernetes resource 的 yaml 請參考 我的 github elk-kubernetes\n給予足夠的 clusterrolebinding 到 elk\nkubectl apply -f filebeat/7.3.1/clusterrolebinding.yaml 先更改 filebeat 的設定，如何設定 elasticsearch 與 kibana，請參考上篇。至於 input 的部份已經配置好了。\nvim filebeat/7.3.1/daemonsets-config-configmap.yaml kubectl apply -f filebeat/7.3.1/daemonsets-config-configmap.yaml 部屬 filebeat daemonsets，會每一個 node 部屬一個 filebeat\nkubectl apply -f filebeat/7.3.1/daemonsets.yaml 取得 daemonsets 的狀態\nkubectl --namespcae elk get pods NAME READY STATUS RESTARTS AGE filebeat-bjfp9 1/1 Running 0 6m56s filebeat-fzr9n 1/1 Running 0 6m56s filebeat-vpkm7 1/1 Running 0 6m56s ... 有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\nlog havest for specific pods 由於 kubernetes 上我們可以便利的調度 filebeat 的部屬方式，這邊也可以也可以使用 deployment ，配合 pod affinity，把 filebeat 放到某個想要監測的 pod，這邊的例子是 nginx-ingress-controller。\nKubernetes 上有一個或多個 nginx ingress controller 部屬一個或多個 filebeat 到有 nginx 的 node 上 filebeat 去抓取 nginx 的 input， 並使用 filebeat 的 nginx module 做預處理 nginx module 預設路徑需要調整，這邊使用 filebeat autodiscover 來處理 一樣 apply 前記得先檢查跟設定\nvim filebeat/7.3.1/nginx-config-configmap.yaml kubectl apply -f filebeat/7.3.1/nginx-config-configmap.yaml 部屬 filebeat deployment 由於有設定 pod affinity ，這個 filebeat 只會被放到有 nginx ingress controller 的這個節點上，並且依照 autodiscover 設定的條件去蒐集 nginx 的 log\nkubectl apply -f filebeat/7.3.1/nginx-deployment.yaml 有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\n另外，由於有啟動 nginx module，logstash 收到的內容已經是處理過得內容。\nGCP fluentd 如果是使用 GKE 的朋友，可以投過開啟 stackdriver logging 的功能，把集群中服務的 log 倒到 stackdriver，基本上就是 node -\u0026gt; (daemonsets) fluentd -\u0026gt; stackdriver。\n這個 fluentd 是 GCP 如果有啟動 Stackdriver Logging 的話，自動幫你維護的 daemonsets，設定不可改，改了會被 overwrite 會去，所以不太方便從這邊動手腳。\nBtw stackdriver 最近好像改版，目前做 example 的版本已經變成 lagency （淚\n但我們先假設我們對這個 pod 的 log 很有興趣，然後把這邊的 log 透過 filebeat 送到 ELK 上XD\n因為 GKE 透過 fluentd 把 GKE 上面的 log 倒到 stackdriver，而我們是想把 log 倒到 ELK，既然這樣我們的 input 來源是相同的，而且很多處理步驟都可以在 ELK 上面互通，真的可以偷看一下 fluentd 是去哪收集 log ，怎麼處理 log pipeline，我們只要做相應設定就好。\n畢竟 google 都幫我們弄得妥妥的，不參考一下他的流程太可惜。\n偷看一下 GKE 上 fluentd 是去哪找 log ，這個是 fluentd gcp configmap，雖然看到這邊感覺扯遠了，但因為很有趣所有我就繼續看下去，各位大德可以跳過XD\nconfigmap 中的這個 input 設定檔，其中一個 source 就是一個資料來源，相當於 filebeat 的 input。這邊這個 source 就是去 /var/log/containers/*.log 收 log\n這邊還做了幾件事：\n打上 reform.* tag，讓下個 match 可以 收進去 pipeline 處理 附帶 parse 出 time containers.input.conf \u0026lt;source\u0026gt; @type tail path /var/log/containers/*.log pos_file /var/log/gcp-containers.log.pos # Tags at this point are in the format of: # reform.var.log.containers.\u0026lt;POD_NAME\u0026gt;_\u0026lt;NAMESPACE_NAME\u0026gt;_\u0026lt;CONTAINER_NAME\u0026gt;-\u0026lt;CONTAINER_ID\u0026gt;.log tag reform.* read_from_head true \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; 他這邊做一些 error handling，然後用 ruby (!) parse，這邊就真的太遠，細節大家可以 google ＸＤ。不過這邊使用的 pattern matching 我們後幾篇在 logstash pipeline 上，也會有機會提到，機制是類似的。\n\u0026lt;filter reform.**\u0026gt; @type parser format /^(?\u0026lt;severity\u0026gt;\\w)(?\u0026lt;time\u0026gt;\\d{4} [^\\s]*)\\s+(?\u0026lt;pid\u0026gt;\\d+)\\s+(?\u0026lt;source\u0026gt;[^ \\]]+)\\] (?\u0026lt;log\u0026gt;.*)/ reserve_data true suppress_parse_error_log true emit_invalid_record_to_error false key_name log \u0026lt;/filter\u0026gt; \u0026lt;match reform.**\u0026gt; @type record_reformer enable_ruby true \u0026lt;record\u0026gt; # Extract local_resource_id from tag for 'k8s_container' monitored # resource. The format is: # 'k8s_container.\u0026lt;namespace_name\u0026gt;.\u0026lt;pod_name\u0026gt;.\u0026lt;container_name\u0026gt;'. \u0026quot;logging.googleapis.com/local_resource_id\u0026quot; ${\u0026quot;k8s_container.#{tag_suffix[4].rpartition('.')[0].split('_')[1]}.#{tag_suffix[4].rpartition('.')[0].split('_')[0]}.#{tag_suffix[4].rpartition('.')[0].split('_')[2].rpartition('-')[0]}\u0026quot;} # Rename the field 'log' to a more generic field 'message'. This way the # fluent-plugin-google-cloud knows to flatten the field as textPayload # instead of jsonPayload after extracting 'time', 'severity' and # 'stream' from the record. message ${record['log']} # If 'severity' is not set, assume stderr is ERROR and stdout is INFO. severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end} \u0026lt;/record\u0026gt; tag ${if record['stream'] == 'stderr' then 'raw.stderr' else 'raw.stdout' end} remove_keys stream,log \u0026lt;/match\u0026gt; ssh 進去逛 想看機器上實際的 log 狀況，我們也可以直接 ssh 進去\n先透過 kubectl 看一下 pod\n$ kubectl get daemonsets --namespace kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-gcp-v3.2.0 7 7 7 7 7 beta.kubernetes.io/fluentd-ds-ready=true 196d $ kubectl get pods --output wide --namespace kube-system NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES fluentd-gcp-scaler-1234567890-vfbhc 1/1 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-5gqn \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-44tl7 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-wcq0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-5vc6l 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-tp05 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-6rqvc 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-5gqn \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-mmwk4 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-vxld \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 先透過 kubectl 看一下 node\n$ kubectl get node NAME STATUS ROLES AGE VERSION gke-chechaichang-pool-1-123456789-3bzp Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 gke-chechaichang-pool-1-123456789-5gqn Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 gke-chechaichang-pool-1-123456789-8n8z Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 ... gcloud compute ssh gke-chechaichang-pool-1-123456789-3bzp 如使用其他雲平台的 kubernetes service，或是 bare metal 的集群，請依照各自系統的方式連進去看看。\nssh node 找 log ssh 進去後就可以到處來探險，順便看看 GKE 跑在機器上到底做了什麼事情。\n如果官方有出文件，可能可以不用進來看。各位大德有發現文件請留言跟我說。我個人很喜歡自己架集群起來連就去看，面對照官方文件上寫的東西，當然大部份時候都是文件沒有帶到，有很多發現。\n$ ls /var/log gcp-*-log.pos kube-proxy.log containers/ metrics/ pods/ ... /var/log/containers 看一下，格式是 pod_namespace_container 這邊是 link 到 /var/log/pods/\n$ ls -al /var/log/containers lrwxrwxrwx 1 root root 105 Aug 12 07:42 fluentd-gcp-v3.2.0-st6cl_kube-system_fluentd-gcp-5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac.log -\u0026gt; /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/0.log 看到 pods 就覺得是你了，裡面有 pod 資料夾，格式是 namespace_pod_uuid\nls /var/log/pods default_pod-1-1234567890-fxxhp_uuid kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008 kube-system_heapster-v1.6.0-beta.1- kube-system_kube-proxy-gke- kube-system_l7-default-backend- kube-system_prometheus-to-sd- 再進去有 container log，格式是 pod_namespace_container.log，也是 link\nls -al /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/ lrwxrwxrwx 1 root root 165 Aug 12 07:42 0.log -\u0026gt; /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log 最終 link 到\nsudo su $ ls -alh /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/ total 3.9M drwx------ 4 root root 4.0K Aug 12 07:42 . drwx------ 92 root root 20K Sep 18 11:28 .. -rw-r----- 1 root root 3.8M Sep 18 11:29 5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log drwx------ 2 root root 4.0K Aug 12 07:42 checkpoints -rw------- 1 root root 7.8K Aug 12 07:42 config.v2.json -rw-r--r-- 1 root root 2.3K Aug 12 07:42 hostconfig.json drwx------ 2 root root 4.0K Aug 12 07:42 mounts 頭尾偷喵一下，確定是我們在找的東西\nhead /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log tail /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log 這樣就找到我們的 log 了\n小節 使用 filebeat 去查找 透過 kubernetes daemonsets 可以快速佈置一份 filebeat 到所有 node，且設定都是一起更新 透過 kubernetes deployment 可以指定 filebeat 的位置，去跟隨想要監測的服務 如果不熟 log 處理流程，可以直接看偷看大廠的服務，會有很多靈感 沒事可以多跑進 Kubernetes 服務節點逛逛，有很多有趣的東西 ","permalink":"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e作為範例的 ELK 的版本是當前的 stable release 7.3.1。\u003c/p\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.elastic.co/guide/en/beats/filebeat/7.3/running-on-kubernetes.html\"\u003e官方文件\u003c/a\u003e ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。\u003c/p\u003e\n\u003cp\u003e這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003enode: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics)\u003c/li\u003e\n\u003cli\u003ecluster: 處理 cluster 等級的 log,  event 或是 metrics\u003c/li\u003e\n\u003cli\u003epod: 針對特定 pod 直接去掛一個 sidecar\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e上面的方法是可以混搭的，kubernetes 個個層級有\u003ca href=\"https://kubernetes.io/docs/concepts/cluster-administration/logging/\"\u003elog 處理流程\u003c/a\u003e，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。\u003c/p\u003e","title":"Monitoring GKE With Elk"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\nELK 的 beats 是輕量級的系統監測收集器，beats 收集到的 data 經過 mapping 可以送到 Elasticsearch 後，進行彈性的搜尋比對。\nbeat 有許多種類，依據收集的 data 區別：\nAuditbeat: Audit data Filebeat: Log files Functionbeat: Cloud data Heartbeat: Availability Journalbeat: Systemd journals Metricbeat: Metrics Packetbeat: Network traffic Winlogbeat: Windows event logs 這邊先以 filebeat 為例，在 GCE 上收集圓端服務節點上的服務日誌與系統日誌，並在 ELK 中呈現。\nInstallation 安裝及 filebeat 安全性設定的步驟，在這篇Secure ELK Stack 中已經說明。這邊指附上連結，以及官方文件 提供參考。\nConfiguration 這邊談幾個使用方面的設定。\n首先，apt 安裝的 filebeat 預設的 /etc/filebeat/filebeat.yml 不夠完整，我們先到 github 把對應版本的完整載下來。\nwget https://raw.githubusercontent.com/elastic/beats/master/filebeat/filebeat.reference.yml sudo mv filebeat.reference.yml /etc/filebeat/filebeat.yml Beats central management beats 透過手動更改 config 都可以直接設定，但這邊不推薦在此設定，理由是\n系統中通常會有大量的 filebeat，每個都要設定，數量多時根本不可能 更改設定時，如果不一起更改，會造成資料格式不統一，之後清理也很麻煩 推薦的方式是透過 Kibana 對所有 filebeat 做集中式的的管理配置，只要初始設定連上 kibana，剩下的都透過 kibana 設定。文件在此，我們有空有可以分篇談這個主題。\n不過這邊還是待大家過一下幾個重要的設定。畢竟要在 kibana 上配置，filebeat 的設定概念還是要有。\nmodules filebeat 有許多模組，裡面已經包含許多預設的 template ，可以直接使用 default 的設定去系統預設的路徑抓取檔案，並且先進一步處理，減少我們輸出到 logstash 還要再做 pipeline 預處理，非常方便。\n例如這個 system module 會處理系統預設的 log 路徑，只要開啟 module ，就會自動處理對應的 input。\n- module: system syslog: enabled: true 剩下的就是照需求啟用 module ，並且給予對應的 input。\nELK 為自己的服務設定了不少 module ，直接啟用就可以獲取這協服務元件運行的 log 與監測數值。這也是 self-monitoring 監測數據的主要來源。\n- module: kibana - module: elasticsearch - module: logstash ... input filebeat 支援複數 inputs，每個 input 會啟動一個收集器，而 filebeat 收集目標是 log 檔案。基本上可以簡單理解為 filebeat 去讀取這些 log 檔案，並且在系統上紀錄讀取的進度，偵測到 log 有增加，變繼續讀取新的 log。\nfilebeat 具體的工作機制，可以看這篇How Filebeat works?\n這篇文件也提到 filebeat 是確保至少一次(at-least-once delivery)的數據讀取，使用時要特別注意重複獲取的可能。\n首先把 input 加上 ubuntu 預設的 log 路徑\nfilebeat.inputs: - type: log enabled: true paths: - /var/log/*.log 這邊注意 input 支援多種 type，參照完整設定檔案的說明配合自己的需求使用。\nProcessor 在 filebeat 端先進行資料的第一層處理，可以大幅講少不必要的資料，降低檔案傳輸，以及對 elasticsearch server 的負擔。\noutput output 也是 filebeat 十分重要的一環，好的 filebeat output 設定，可以大幅降低整體 ELK stack 的負擔。壞的設定也會直接塞爆 ELK stask。\noutput.elasticsearch: 直接向後送進 elasticsearch output.logstash: 先向後送到 logstash\n這邊非常推薦大家，所有的 beat 往後送進 elasticsearch 之前都先過一層 logstash，就算你的 logstash 內部完全不更改 data，沒有 pipeline mutation，還是不要省這一層。\nbeat 的數量會隨應用愈來越多而線性增加，elasticsearch 很難線性 scale，或是 scale 成本很大 filebeat 沒有好好調校的話，對於輸出端的網路負擔很大，不僅佔用大量連線，傳輸檔案的大小也很大。 logstash 的 queue 與後送的 batch 機制比 filebeat 好使用 filebeat 是收 log 的，通常 log 爆炸的時候，是應用出問題的時候，這時候需要 log 交叉比對，發現 elasticsearch 流量也爆衝，反應很應用 logstash 透過一些方法，可以很輕易的 scale，由於 pipeline 本身可以分散是平行處理，scale logstash 並不會影響資料最終狀態。 load balance 有網友留言詢問 logstash 前面的 load balance 如何處理比較好，我這邊也順便附上。不只是 logstash ，所有自身無狀態(stateless) 的服務都可以照這樣去 scale。\n在 kubernetes 上很好處理，使用 k8s 預設的 service 就輕易作到簡易的 load balance\n設置複數 logstash instances 使用 kubernetes 內部網路 service 實現 load balancing。 在 GCE 上實現的話，我說實話沒實作過，所以以下是鍵盤實現XD。\n官方文件 建議使用 beats 端設定多個 logstash url 來做 load balancing。\n但我不是很喜歡 beat 去配置多個 logstash url 的作法：beat 要感知 logstash 數量跟 url ，增加減少 logstash instance 還要更改 beats 配置，產生配置的依賴跟耦合。\n最好是在 logstash 前過一層 HAproxy 或是雲端服務的 Load balancer（ex. GCP https/tcp load balancer），beat 直接送進 load balance 的端點。\nautodiscover 如果有使用 container ，例如 docker 或 kubernetes，由於 container 內的 log 在主機上的位置是動態路徑，這邊可以使用 autodiscover 去尋找。\n在 kubernetes 上面的設定，之後會另開一天討論。\ndashboard kibana 預設是空的，沒有預先載入 dashboard，但我們會希望資料送進去，就有設定好的 dashboard ，圖像化把資料呈現出來。這部份需要從 beat 這邊向 kibana 寫入。\n在上面的部份設定好 kibana 的連線資料，沒有設定的話 beat 啟動會警告。\nsetup.dashboards.enabled: true 一起中就會檢查 kibana 是否有匯入 dashboard，沒有的話就匯入。\n也會一併匯入 modules 的 dashboard，例如如果有啟用 nginx module 處理 nginx 的 access log，nginx module 會處理 request source ip ，並透過 geoip database, 將 ip 轉會成經緯度座標。這時如果在 kibana 上有匯入 nginx dashboard，就可以看到圖像化的全球 request 分佈圖。\n小結 取得完整 filebeat 設定檔案並設定 filebeat 盡量透過 beat central management 來管理 beat 的設定檔 啟用對應 module 來更優雅的處理 log 後送到 elasticsearch 前的資料都必須經過精細的處理，送進去後就不好刪改了 ","permalink":"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e作為範例的 ELK 的版本是當前的 stable release 7.3.1。\u003c/p\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eELK 的 beats 是輕量級的系統監測收集器，beats 收集到的 data 經過 mapping 可以送到 Elasticsearch 後，進行彈性的搜尋比對。\u003c/p\u003e\n\u003cp\u003ebeat 有許多種類，依據收集的 data 區別：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAuditbeat: Audit data\u003c/li\u003e\n\u003cli\u003eFilebeat: Log files\u003c/li\u003e\n\u003cli\u003eFunctionbeat: Cloud data\u003c/li\u003e\n\u003cli\u003eHeartbeat: Availability\u003c/li\u003e\n\u003cli\u003eJournalbeat: Systemd journals\u003c/li\u003e\n\u003cli\u003eMetricbeat: Metrics\u003c/li\u003e\n\u003cli\u003ePacketbeat: Network traffic\u003c/li\u003e\n\u003cli\u003eWinlogbeat: Windows event logs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e這邊先以 filebeat 為例，在 GCE 上收集圓端服務節點上的服務日誌與系統日誌，並在 ELK 中呈現。\u003c/p\u003e","title":"Monitoring GCE With ELK"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n有板友問到，要如何選擇要不要用 ELK，其實也這是整篇 ELK 的初衷。這邊分享一下 ELK 與其他選擇，以及選擇解決方案應該考慮的事情。\n其他常用的服務 Prometheus: 開源的 time series metrics 收集系統\nStackdriver: GCP 的 log 與 metrics 平台\nElastic Cloud: ELK 的 Sass\nSelf-hosted ELK\n或是依照需求混搭，各個服務使用的各層套件是可以相容，例如\n在 GKE 上不用 beat 可以用 fluentd\nPrometheus -\u0026gt; Stackdriver\nELK -\u0026gt; Stackdriver\nFluentd -\u0026gt; Prometheus \u0026hellip;\nSass vs cloud self-hosted vs on-premised\nMetrics: ELK vs Prometheus vs Stackdriver\nLogging: ELK vs Stackdriver\n取捨原則 各個方法都各有利弊，完全取決於需求\n已知條件限制，例如安全性考量就是要放在私有網路防火牆內，或是預算 資料讀取方式，有沒有要交叉比對收集的資料，還是單純依照時間序查詢 或是資料量非常大，應用數量非常多 維護的團隊，有沒有想，或有沒有能力自己養 self-host 服務 Sass vs Self-hosted vs On-premised Sass: 指的是直接用 Elasitc Cloud，或是直接使用公有雲的服務(ex. 在 GCP 上使用 stackdriver)\nCloud Self-hosted: 在公有雲上使用 ELK\nOn-Premised: 自己在機房搭設\n安全性 看公司的安全政策，允許將日誌及監控數據，送到私有網路以外的地方嗎？如果在防火牆內，搞不好 port 根本就不開給你，根本不用考慮使用外部服務。\n要知道服務的 log 其實可以看出很多東西．如果有特別做資料分析，敏感的資料，金流相關數據，通常不會想要倒到第三方服務平台。\n可能有做金流的，光是安全性這點，就必須選擇自架。\n成本 金錢成本 + 維護成本\n金錢成本就看各個服務的計費方式\nElastic Cloud Pricing Self-hosted ELK \u0026amp; Prometheus：機器成本 公有雲服務(ex. GCP Stackdriver): 用量計費 維護成本: 工程師的月薪 * 每個月要花在維護服務的工時比例\n一般 Sass 代管的服務，會降低維護成本，基本上就是做到網頁點一點就可以用。\n如果公司有完整的維護團隊，有機房，服務的使用量也很大，當然 self-hosted 是比較省。 中小型企業以及新創，服務在公有雲上的，直接使用Sass 服務往往比較節省成本，服務直接由 Sass 維護，節省很多機器上管理跟日常維護。\n避免迷思，買外部服務的帳單是顯性的，報帳時看得到，而工程師維護的時間成本是隱性的。self-host 可能省下 Sass 費用，但工程因為分了時間去維護，而影響進度。這部分就看團隊如何取捨。\n易用性 如果應用都跑在公有雲上，可以考慮使用雲平台提供的監測服務，使用便利，而且整合度高。ex GCP 上，要啟用 Stackdriver 是非常輕鬆的事情，只是改一兩個選項，就可以開啟 / 關閉 logging 與 metrics\n如果是 On-premised 自家機房，也許 self-hosted 會更為適合。\n客製化程度 在大多數時候，沒有需要更改到服務的核心設定，都可以不可律客製化程度，直接使用 Sass 的設定，就能滿足大部分需求。可以等有有明確需求後再考慮這一點。短期內沒有特殊需求就可以從簡使用。\n使用GKE 到 Stackdriver 的話，對主機本身的機器是沒有控制權的，執行的 pipeline 也不太能更改 Elastic Cloud 有提供上傳 elasticsearch config 檔案的介面，也就是可以更改 server 運行的參數設定 Self-Hosted 除了上述的設定，還可以依照需求更改 ELK / prometheus 服務，在實體機器上的 topology，cpu 記憶體的資源配置，儲存空間配置等，可以最大化機器的效能。\nScalability 資料流量大，儲存空間消耗多，服務負擔大，可能就會需要擴展。\n一個是資料量的擴展。一個是為了應付服務的負擔，對 ELK 服務元件做水平擴展。\n除了 elasticsearch 以爲的元件，例如 kibana，apm-server, beats 都可以透過 kubernetes 輕易的擴展，唯有 elasticsearch ，由於又牽扯上述資料量的擴展，以及分佈，還有副本管理，index 本身的 lifecycle 管理。Elasticsearch 的 scaling 設定上是蠻複雜的，也有很多工要做。index 的 shards / replicas 設定都要注意到。否則一路 scale 上去，集群大的時候彼此 sharding sync 的效能消耗是否會太重。\nStackdriver 從使用者的角度，是不存在服務節點的擴展問題，節點的維護全都給 Sass 管理。資料量的擴展問題也不大，只要整理資料 pipeline，讓最後儲存的資料容易被查找。\nTimeseries vs non-timeseriese Prometheus 是自帶 time series database，stackdriver 也是 time series 的儲存。ELK 的 elasticsearch 是全文搜索引擎，用了 timestamp 做分析所以可以做到 time series 的資料紀錄與分析。這點在本質上是完全不同的。\n光只處理 time series data，Prometheus 的 query 效能是比 elasticsearch 好很多 Elasticsearch 有大量的 index 維護，需要較多系統資源處理，在沒有 query 壓力的情形下會有系統自動維護的效能消耗 ELK 的資料不需要預先建模，就可以做到非常彈性的搜尋查找。Stackdriver 的話，無法用未建模的資料欄位交叉查找。 Log 收集方面 Elasticsearch 中的資料欄位透過 tempalte 匯入後，都是有做 index ，所以交叉查找，例如可以從 log text 中包含特定字串的紀錄，在做 aggregate 算出其他欄位的資料分佈。會比較慢，但是是做得到的全文搜索 Stackdriver 可以做基本的 filter ，例如 filter 某個欄位，但不能做太複雜的交叉比對，也不能針對 text 內容作交互查找，需要換出來另外處理。 Metrics 收集方面 (同上) Elasticsearch 可以用全文搜索，做到很複雜的交叉比對，例如：從 metrics 數值，計算在時間範圍的分佈情形(cpu 超過 50% 落在一天 24 小時，各個小時的次數) Stackdriver 只能做基本的 time series 查找，然後透過預先定義好的 field filter 資料，再各自圖像化。 Prometheus 也是必須依照 time series 查找，語法上彈性比 stackdriver 多很多，但依樣不能搜尋沒有 index 的欄位 這邊要替別提，雖然 Elasticsearch 能用全文搜索輕易地做到複雜的查詢語法，但以 metrics 來說，其實沒有太多跳脫 time series 查找的需求。能做到，但有沒有必要這樣做，可以打個問號。 個人心得，如果驗證全新的 business model，或是還不確定的需求，可以使用 ELK 做各種複雜的查詢\n如果需求明確，收進來的 log 處理流程都很明確，也許不用使用 ELK。\n論系統資源 CP 值以及效能，time series 的 db 都會比 Elasticsearch 好上不少。 Elasticsearch 中也不太適合一直存放大量的資料在 hot 可寫可讀狀態，繪希好很多系統資源。 其他服務 Elastic 有出許多不同的增值服務\nApplication Performance Monitoring(APM) Realtime User Monitoring(RUM) Machine Learning(ELK ML) 而 ELK 以外也都有不同的解決方案，例如\nGCP 也出了自己的 APM Sass Google Analytics(GA) 不僅能做多樣的前端使用者行為分析，還能整合 Google 收集到的使用者行為，做更多維度的分析 相較之下 ELK 在這塊其實沒有特別優勢。\nElastic Cloud 我這邊要特別說 Elastic Cloud vs ELK\nElatic Cloud 的運行方式，是代為向公與恩平台(aaws, gcp,\u0026hellip;)，帶客戶向平台租用機器，然後把 ELK 服務部署到租用的機器上。用戶這邊無法直接存取機器，只能透過 ELK 介面或是 Kibana , API 進入 ELK。Elastic Cloud 會監控無誤節點的狀況，並做到一定程度的代管。\n這邊指的一定程度的代管，是 Elastic Cloud 只是代為部署服務，監控。有故障時並不負責排除，如果 ELK 故障，簡單的問題（ex. 記憶體資源不足）會代為重開機器，但如果是複雜的問題，還是要用戶自己處理．但是用戶又沒有主機節點的直接存取權限，所以可能會造成服務卡住無法啟動，只能透過 Elastic Cloud 的管理介面嘗試修復。\n使用服務除了把服務都架設完以外，還是需要定期要花時間處理 performance tuning，設定定期清理跟維護。包括 kafka, redis, mongoDB, cassandra, SQLs\u0026hellip;都是一樣，架構越複雜，效能要求越高，這部分的工都會更多。如果公司有 DBA，或是專職維護工程師，那恭喜就不用煩惱。\nElasticsearch server 目前用起來，算是是數服務中，維護上會花比較多時間的服務。\n因為引擎本身設計的架構，並不是很多人都熟悉。在使用ELK同時，對ELK底層引擎的運作流程有多熟悉，會直接影響穩定性跟跑出來的效能。 需要好好處理設計資料的儲存，如果使用上沒處理好，會直接讓整個ELK 掛掉。 然後產品本身的維護介面，目前只是在堪用，許多重要的功能也還在開發中。 如果公司有人會管 ELK，個人建議是可以 self-host\n小結 弄清楚需求，如果沒有特殊需求可以走 general solution Sass vs Self-hosted vs On-premised Time series vs non time series ","permalink":"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e有板友問到，要如何選擇要不要用 ELK，其實也這是整篇 ELK 的初衷。這邊分享一下 ELK 與其他選擇，以及選擇解決方案應該考慮的事情。\u003c/p\u003e\n\u003ch1 id=\"其他常用的服務\"\u003e其他常用的服務\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://prometheus.io/\"\u003ePrometheus\u003c/a\u003e: 開源的 time series metrics 收集系統\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.google.com/stackdriver/?hl=zh-tw\"\u003eStackdriver\u003c/a\u003e: GCP 的 log 與 metrics 平台\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.elastic.co/cloud/\"\u003eElastic Cloud\u003c/a\u003e: ELK 的 Sass\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://chechia.net/posts/self-host-elk-stack-on-gcp/\"\u003eSelf-hosted ELK\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e或是依照需求混搭，各個服務使用的各層套件是可以相容，例如\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e在 GKE 上不用 beat 可以用 fluentd\u003c/p\u003e","title":"ELK or Not ELK"},{"content":"簡單講一下 certificate X.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)\ncertificate 包含公鑰及識別資訊(hostname, organization, \u0026hellip;等資訊)\ncertificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)\n使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site\n在 elastic stack 上，如果有多個 elasticsearch server node 彼此連線，由於 node 彼此是 client 也是 server\n使用 self-signed CA 產出來的 certificate，連入時會檢查使用的 certificate 是否由同一組 CA 簽署 server 使用 certificate，確定連入 server 的 client 都帶有正確的私鑰與 public certificate，是 authenticated user 附帶說明，X.509 有多種檔案格式\n.pem .cer, .crt, .der .p12 .p7b, .p7c \u0026hellip; 另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證\nCA $ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys MAC: sha1, Iteration 100000 MAC length: 20, salt length: 20 PKCS7 Data Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000 PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000 Certificate bag Bag Attributes friendlyName: ca localKeyID: subject=CN = Elastic Certificate Tool Autogenerated CA issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- issuer command name 為 Elastic autogen CA subject command name 為 Elastic autogen CA\nhttps://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/\nopenssl s_client -connect google.com https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce\nopenssl verify -CAfile client-ca.cer client.cer openssl verify -show_chain -CAfile client-ca.cer client.cer Certificate 用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-certificates.p12 -info -nokeys MAC: sha1, Iteration 100000 MAC length: 20, salt length: 20 PKCS7 Data Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000 PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000 Certificate bag Bag Attributes friendlyName: elk.asia-east1-b.c.machi-x.internal localKeyID: subject=CN = elk.asia-east1-b.c.machi-x.internal issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Certificate bag Bag Attributes friendlyName: ca 2.16.840.1.113894.746875.1.1: \u0026lt;Unsupported tag 6\u0026gt; subject=CN = Elastic Certificate Tool Autogenerated CA issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- ","permalink":"https://chechia.net/posts/2020-09-17-x.509-certificate/","summary":"\u003ch1 id=\"簡單講一下-certificate\"\u003e簡單講一下 certificate\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eX.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecertificate 包含公鑰及識別資訊(hostname, organization, \u0026hellip;等資訊)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecertificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e在 elastic stack 上，如果有多個 elasticsearch server node 彼此連線，由於 node 彼此是 client 也是 server\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e使用 self-signed CA 產出來的 certificate，連入時會檢查使用的 certificate 是否由同一組 CA 簽署\u003c/li\u003e\n\u003cli\u003eserver 使用 certificate，確定連入 server 的 client 都帶有正確的私鑰與 public certificate，是 authenticated user\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e附帶說明，X.509 有多種檔案格式\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e.pem\u003c/li\u003e\n\u003cli\u003e.cer, .crt, .der\u003c/li\u003e\n\u003cli\u003e.p12\u003c/li\u003e\n\u003cli\u003e.p7b, .p7c\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證\u003c/p\u003e","title":"X.509 certificate"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n\u0026ndash;\n上篇Self-host ELK stack on GCP 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。\n官方的文件在這裡，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的Tutorial: Getting started with security 開始，過程比較不會這麼血尿。\n總之為了啟用 authentication \u0026amp; https，這篇要做的事情：\nenable x-pack \u0026amp; activate basic license Generate self-signed ca, server certificate, client certificate Configure Elasticsearch, Kibana, \u0026amp; other components to use server certificate when act as server use client certificate when connect to an ELK server 啟用 X-pack Elasticsearch 的安全性模組由 x-pack extension 提供，在 6.3.0 之後的版本，安裝 elasticsearch 的過程中就預設安裝 x-pack。\n附上啟用的官方文件\n然而，由於舊版的 x-pack 是付費內容，目前的 elasticsearch 安裝完後，elasticsearch.yml 設定預設不啟用 x-pack，也就是說沒看到這篇官方文件的話，很容易就獲得沒有任何 security 功能的 ELK。\n雖然目前已經可以使用免費的 basic license 使用 security 功能，還是希望官方可以 default 啟用 security。\n$ sudo vim /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: true xpack.license.self_generated.type: basic discovery.type: single-node 我們這邊啟用 xpack.security，同時將 self-generated license 生出來，我們這邊只使用基本的 basic subscription。若希望啟用更多功能，可以看官方subcription 方案介紹\n另外，如果不同時設定為 single-node 的話，預設會尋找其他elasticsearch node 來組成 cluster，而我們就必須要在所有 node 上啟用 security，這篇只帶大家做一個 single node cluster，簡化步驟。\n重啟 elasticsearch ，檢查 log，看啟動時有沒有載入 x-pack\nsudo systemctl restart elasticsearch $ tail -f /var/log/elasticsearch/elasticsearch.log [2019-09-16T07:39:49,467][INFO ][o.e.e.NodeEnvironment ] [elk] using [1] data paths, mounts [[/mnt/disks/elk (/dev/sdb)]], net usable_space [423.6gb], net total_space [491.1gb], types [ext4] [2019-09-16T07:39:49,474][INFO ][o.e.e.NodeEnvironment ] [elk] heap size [3.9gb], compressed ordinary object pointers [true] [2019-09-16T07:39:50,858][INFO ][o.e.n.Node ] [elk] node name [elk], node ID [pC22j9D4R6uiCM7oTc1Fiw], cluster name [elasticsearch] [2019-09-16T07:39:50,866][INFO ][o.e.n.Node ] [elk] version[7.3.1], pid[17189], build[default/deb/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/4.15.0-1040-gcp/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/12.0.2/12.0.2+10] [2019-09-16T07:39:50,878][INFO ][o.e.n.Node ] [elk] JVM home [/usr/share/elasticsearch/jdk] ... [2019-09-16T07:39:59,108][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-ccr] [2019-09-16T07:39:59,109][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-core] ... [2019-09-16T07:39:59,111][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-logstash] [2019-09-16T07:39:59,113][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-voting-only-node] [2019-09-16T07:39:59,114][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-watcher] [2019-09-16T07:39:59,115][INFO ][o.e.p.PluginsService ] [elk] no plugins loaded [2019-09-16T07:40:07,964][INFO ][o.e.x.s.a.s.FileRolesStore] [elk] parsed [0] roles from file [/etc/elasticsearch/roles.yml] [2019-09-16T07:40:10,369][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elk] [controller/17314] [Main.cc@110] controller (64 bit): Version 7.3.1 (Build 1d93901e09ef43) Copyright (c) 2019 Elasticsearch BV [2019-09-16T07:40:11,776][DEBUG][o.e.a.ActionModule ] [elk] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2019-09-16T07:40:14,396][INFO ][o.e.d.DiscoveryModule ] [elk] using discovery type [single-node] and seed hosts providers [settings] [2019-09-16T07:40:16,222][INFO ][o.e.n.Node ] [elk] initialized [2019-09-16T07:40:16,224][INFO ][o.e.n.Node ] [elk] starting ... [2019-09-16T07:40:16,821][INFO ][o.e.t.TransportService ] [elk] publish_address {10.140.0.10:9300}, bound_addresses {[::]:9300} [2019-09-16T07:40:16,872][INFO ][o.e.c.c.Coordinator ] [elk] cluster UUID [1CB6_Lt-TUWEmRoN9SE49w] [2019-09-16T07:40:17,088][INFO ][o.e.c.s.MasterService ] [elk] elected-as-master ([1] nodes joined)[{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 921, reason: master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]} [2019-09-16T07:40:17,819][INFO ][o.e.c.s.ClusterApplierService] [elk] master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}, term: 9, version: 921, reason: Publication{term=9, version=921} [2019-09-16T07:40:17,974][INFO ][o.e.h.AbstractHttpServerTransport] [elk] publish_address {10.140.0.10:9200}, bound_addresses {[::]:9200} [2019-09-16T07:40:17,975][INFO ][o.e.n.Node ] [elk] started [2019-09-16T07:40:18,455][INFO ][o.e.c.s.ClusterSettings ] [elk] updating [xpack.monitoring.collection.enabled] from [false] to [true] [2019-09-16T07:40:22,555][INFO ][o.e.l.LicenseService ] [elk] license [************************************] mode [basic] - valid [2019-09-16T07:40:22,557][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [elk] Active license is now [BASIC]; Security is enabled Enable user authentication 啟用 security 之前，我們直接連入 Kibana http://10.140.0.10:5601 ，不用任何使用者登入，便可以完整使用 Kibana 功能（包含 admin 管理介面）。\n啟用 security 後，便需要使用帳號密碼登入。在這邊先用工具把使用者密碼產生出來。\n# 互動式 /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive # 自動產生 /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto 密碼生出來後，就把帳號密碼收好，等等會用到。之後初次登入也是使用這些密碼。\nConfigure passwords on client-side 由於已經啟用 authentication，其他 ELK 元件 (Kibana, logstash, filebeat, apm-server,\u0026hellip;) 連入 Elasticsearch 也都會需要各自的帳號密碼驗證。\n以 Kibana 為例，可以直接在 kibana.yml 中直接設定帳號密碼\n$ sudo vim /etc/kibana/kibana.yml elasticsearch.hosts: [\u0026quot;http://localhost:9200\u0026quot;] xpack.security.enabled: true elasticsearch.username: \u0026quot;kibana\u0026quot; elasticsearch.password: \u0026quot;***********\u0026quot; 當然，這邊就是明碼的，看了不太安全。\n或是使用 keystore 把 built-in user 的密碼加密，存在 kibana 的 keystore 裡面，重啟 kibana 時便會載入。\n/usr/share/kibana/bin/kibana-keystore create /usr/share/kibana/bin/kibana-keystore add elasticsearch.username /usr/share/kibana/bin/kibana-keystore add elasticsearch.password 如果有啟用 Filebeat 功能，beat 元件連入 elasticsearch 一樣需要設定\n/usr/share/apm-server/bin/filebeat keystore create /usr/share/apm-server/bin/filebeat add elasticsearch.username /usr/share/apm-server/bin/filebeat add elasticsearch.password 如果有啟用 application performance monitoring(APM) 功能，apm-server 元件連入 elasticsearch 一樣需要設定\n/usr/share/apm-server/bin/apm-server keystore create /usr/share/apm-server/bin/apm-server add elasticsearch.username /usr/share/apm-server/bin/apm-server add elasticsearch.password Encrypting Communications 上面加了 username/password authentication，但如果沒 https/tls 基本上還是裸奔。接下來要處理連線加密。\n官方 tutorial\n一堆官方文件，我們先跳過XD\nelasticsearch security elastic stack ssl tls elasticsearch configuring tls certutil 分析一下需求跟規格 我們需要為每一個 node 生一組 node certificate，使用 node certificate 產生 client certificates 提供給其他 client，連入時會驗證 client 是否為 authenticated user。\n針對目前這個 single-node ELK stack，我們可能有幾種選擇\n簽署一個 localhost，當然這個只能在 localhost 上的客戶端元件使用，別的 node 無法用這個連入 簽署一個 public DNS elk.chechiachang.com，可以在公開網路上使用，別人也可以使用這個DNS嘗試連入 簽署一個私有網域的 DNS，例如在 GCP 上可以使用內部dns服務 長這樣 elk.asia-east1-b.c.chechiachang.internal [INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal 有需要也可以一份 server certificate 中簽署複數個 site 我們這邊選擇使用內部 dns，elk.asia-east1-b-c-chechaichang.internal，讓這個 single-node elk 只能透過內部網路存取。\nelasticsearch: elk.asia-east1-b.c.chechaichang.internal:9200 kibana: elk.asia-east1-b.c.chechaichang.internal:5601 外部要連近來 kibana，我們使用 vpn 服務連進私有網路 如果想使用外部 dns，讓 elk stack 在公開網路可以使用，ex. elk.chechiachang.com，可以\nGCP 的 load balancer掛進來，用 GCP 的 certificate manager 自動管理 certificate 或是在 node 上開一個 nginx server，再把 certificate 用 certbot 生出來 Generate certificates 先把 X.509 digital certificate 的 certificate authority(CA) 生出來。我們可以設定密碼保護這個檔案\nmkdir -p /etc/elasticsearch/config # CA generated with Elastic tool /usr/share/elasticsearch/bin/elasticsearch-certutil ca \\ -out /etc/elasticsearch/config/elastic-stack-ca.p12 生出來是 PKCS#12 格式的 keystore，包含：\nCA 的 public certificate CA 的基本資訊 簽署其他 node certificates 使用的私鑰(private key) 用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys 附帶說明，X.509 有多種檔案格式\n.pem .cer, .crt, .der .p12 .p7b, .p7c \u0026hellip; 另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證。裡面的內容也不同。\nELK 設定的過程中，由於不是所有的 ELK component 都支援使用 .p12 檔案，我們在設定過程中會互相專換，或是混用多種檔案格式。\nGenerate certificate \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD\n我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有\nnode certificate node key CA certificate 這邊只產生一組 server certificate 給 single-node cluster 的 node-1\n=======\n我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有\nnode certificate node key CA certificate 這邊只產生一組 server certificate 給 single-node cluster 的 node-1\n如果 cluster 中有多個 elasticsearch，為每個 node 產生 certificate 時都要使用同樣 CA 來簽署，讓 server 信任這組 CA。\n使用 elasticsearch-certutil 簡化簽署過程，從產生 CA ，到使用 CA 簽署 certificate。另外，再產生 certificate 中使用 Subject Alternative Name(SAN)，並輸入 ip 與 dns。\n# certificate for site: private dns with Elastic CA /usr/share/elasticsearch/bin/elasticsearch-certutil cert \\ --ca /etc/elasticsearch/config/elastic-stack-ca.p12 \\ --name elk.asia-east1-b.c.chechaichang.internal \\ --dns elk.asia-east1-b.c.chechaichang.internal \\ --ip 10.140.0.10 \\ -out /etc/elasticsearch/config/node-1.p12 用 openssl 看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -info -nokeys server 用這個 certificate ，啟用 ssl。\nclient 使用這個 certificate 產生出來的 client.cer 與 client.key 與 server 連線，server 才接受客戶端是安全的。\n25f5ab795b9e698333a36fde7ecf23a8ba9d4595 記得把所有權還給 elasticsearch 的使用者，避免 permission denied\n# Change owner to fix read permission chown -R elasticsearch:elasticsearch /etc/elasticsearch/config 有密碼記得也要用 keystore 把密碼加密後喂給 elasticsearch\n/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password 關於 X.509 Certifcate 之後有空我們來聊一下\n更新 elasticsearch 設定 Certificates 都生完了，接下來更改 elasticsearch 的參數，在 transport layer 啟用 ssl。啟用 security 後，在 transport layer 啟動 ssl 是必須的。\n$ sudo vim /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: true xpack.security.transport.ssl.enabled: true # use certificate. full will verify dns and ip xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12 xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12 啟用 security 與 transport layer 的 ssl，然後指定 keystore路徑，讓 server 執行 client authentication 由於這筆 p12 帶有 CA certificate 作為 trusted certificate entry，所以也可以順便當作 trustore，讓 client 信任這個 CA\nsecurity 這邊提供了 server side (elasticsearch) 在檢查客戶端連線時的檢查模式(vertification mode)，文件有說明，可以設定\ncertificate: 檢查 certificate 加密是否有效 full: 簽 node certificate 時可以指定 ip dns，啟用會檢查來源 node ip dns 是否也正確 (Optional) HTTP layer 啟動 ssl\nvim /etc/elasticsearch/elasticsearch.yml xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12 xpack.security.http.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12 /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password 重啟 elasticsearch，看一下 log\nsudo systemctl restart elasticsearch tail -f /var/log/elasticsearch/elasticsearch.log 然後你就發現，原來 kibana 連入 的 http 連線，不斷被 server 這端拒絕。所以以下要來設定 kibana\nKibana using kibana with security kibana configuring tls 使用剛剛簽的 server certificate，從裡面 parse 出 client-ca.cer，還有 client.cer 與 client.key\nmkdir -p /etc/kibana/config $ openssl pkcs12 --help Usage: pkcs12 [options] Valid options are: -chain Add certificate chain -nokeys Don't output private keys -nocerts Don't output certificates -clcerts Only output client certificates -cacerts Only output CA certificates -info Print info about PKCS#12 structure -nodes Don't encrypt private keys -in infile Input filename # no certs, no descript openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/kibana/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/kibana/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/kibana/config/client-ca.cer sudo chown -R kibana:kibana /etc/kibana/config/ 更改 kibana 連入 elasticsearch 的連線設定\nsudo vim /etc/kigana/kibana.yml elasticsearch.hosts: [\u0026quot;https://elk.asia-east1-b.c.chechaichang.internal:9200\u0026quot;] xpack.security.enabled: true elasticsearch.ssl.certificate: /etc/kibana/config/client.cer elasticsearch.ssl.key: /etc/kibana/config/client.key elasticsearch.ssl.certificateAuthorities: [ \u0026quot;/etc/kibana/config/client-ca.cer\u0026quot; ] elasticsearch.ssl.verificationMode: full 指定 ssl.certificate, ssl.key 做連線 elasticsearch server 時的 user authentication 由於我們是 self-signed CA，所以需要讓客戶端信任這個我們自簽的 CA 注意這邊 elasticsearch.hosts 我們已經從 http://localhost 換成 https 的內部 dns，原有的 localhost 已經無法使用（如果 elasicsearch 有 enforce https 的話）\n重啟 Kibana，看一下 log\nsudo systemctl restart kibana journalctl -fu kibana 如果沒有一直噴 ssl certificate error 的話，恭喜你成功了\n然而，除了 kibana 以外，我們還有其他的 client 需要連入 elasticsearch\n把上述步驟在 apm-server, filebeat, 其他的 beat 上也設定 如果在 k8s 上，要把 cer, key 等檔案用 volume 掛進去 \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定\nkibana server certificate filebeat client to kibana server =======\nKibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定\nkibana server certificate filebeat client to kibana server 就是他們彼此互打，都要有 ca, key, cert\n但基本上的設定都一樣，下面可以不用看下去了XD 如果有用到再查文件就好，這邊直接小結\n設定 security 前要先想號自己的需求，如何連入，安全性設定到哪邊 使用 utility 自簽 CA，然後產生 server certificate 使用 server certificate 再 parse 出 ca-certificate, client cers, key kibana 作為 server 工作路徑可能是這樣： app(apm-client library) -\u0026gt; apm-server -\u0026gt; kibana -\u0026gt; elasticsearch\nkibana 連入 elasticsearch時， kibana 是 client 吃 elasticsearch 的憑證 apm-server 連入 kibana時，kibana 是 server，apm-server 吃 kibana 的憑證 首先更改 kibana 設定\n$ sudo vim /etc/kibana/kibana.yml server.ssl.enabled: true server.ssl.certificate: /etc/kibana/config/client.cer server.ssl.key: /etc/kibana/config/client.key 重啟 kibana\njournalctl -fu kibana Apm-server https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html\n應用端的 apm-client (ex. apm-python-client)，連入 apm-server\n在 http 的狀況下，雖然有使用 secret-token，但還是裸奔 在 https 的狀況下，要把 certificates，然後餵給應用端的client library 更改 apm-server 的設定\nsudo vim /etc/apm-server/apm-server.yml host: \u0026quot;0.0.0.0:8200\u0026quot; secret_token: \u0026lt;設定一組夠安全的 token\u0026gt; rum: enabled: true kibana: protocol: \u0026quot;https\u0026quot; ssl.enabled: true output.kibana: enable: false # can only have 1 output output.elasticsearch: monitoring.elasticsearch: protocol: \u0026quot;https\u0026quot; username: \u0026quot;elastic\u0026quot; password: \u0026quot;*******************\u0026quot; hosts: [\u0026quot;elk.asia-east1-b.c.checahichang.internal:9200\u0026quot;] ssl.enabled: true ssl.verification_mode: full ssl.certificate_authorities: [\u0026quot;/etc/apm-server/config/client-ca.cer\u0026quot;] ssl.certificate: \u0026quot;/etc/apm-server/config/client.cer\u0026quot; ssl.key: \u0026quot;/etc/apm-server/config/client.key\u0026quot; 重啟 apm-server\nsystemctl restart apm-server journalctl -fu apm-server APM library 應用端的設定就需要依據 library 的實做設定，例如 flask-apmagent-python\nELASTIC_APM_SERVER_CERT=/etc/elk/certificates/client.cer apm agent python config server cert\nfilebeat 記得我們在 node 上有安裝 Self-monitoring filebeat，elasticsearch 改成 ssl 這邊當然也連不盡去了，再做同樣操作\nhttps://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html\nsudo apt-get install filebeat mkdir -p /etc/filebeat/config openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/filebeat/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/filebeat/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/filebeat/config/client-ca.cer Restart filebeat\nsystemctl restart filebeat journalctl -fu filebeat 如果你的應用在 kubernetes 上 可以使用下面方法拿到 client.cer ，然後用 secret 塞進 k8s，在用 volume from secrets，掛給監測應用的 filebeat\nmkdir -p /etc/beats/config openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/beats/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/beats/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/beats/config/client-ca.cer gcloud compute scp elk:/etc/beats/config/* . client-ca.cer client.cer client.key kubectl -n elk create secret generic elk-client-certificates \\ --from-file=client-ca.cer=client-ca.cer \\ --from-file=client.cer=client.cer \\ --from-file=client.key=client.key kubectl apply -f elk/gke/filebeat/ ","permalink":"https://chechia.net/posts/2019-09-15-secure-elk-stack/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u0026ndash;\u003c/p\u003e\n\u003cp\u003e上篇\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.elastic.co/guide/en/elastic-stack-overview/7.3/elasticsearch-security.html\"\u003e官方的文件在這裡\u003c/a\u003e，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的\u003ca href=\"https://www.elastic.co/guide/en/elastic-stack-overview/7.3/security-getting-started.html\"\u003eTutorial: Getting started with security\u003c/a\u003e 開始，過程比較不會這麼血尿。\u003c/p\u003e\n\u003cp\u003e總之為了啟用 authentication \u0026amp; https，這篇要做的事情：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eenable x-pack \u0026amp; activate basic license\u003c/li\u003e\n\u003cli\u003eGenerate self-signed ca, server certificate, client certificate\u003c/li\u003e\n\u003cli\u003eConfigure Elasticsearch, Kibana, \u0026amp; other components to\n\u003cul\u003e\n\u003cli\u003euse server certificate when act as server\u003c/li\u003e\n\u003cli\u003euse client certificate when connect to an ELK server\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"啟用-x-pack\"\u003e啟用 X-pack\u003c/h1\u003e\n\u003cp\u003eElasticsearch 的安全性模組由 x-pack extension 提供，在 \u003ca href=\"https://www.elastic.co/what-is/open-x-pack\"\u003e6.3.0 之後的版本\u003c/a\u003e，安裝 elasticsearch 的過程中就預設安裝 x-pack。\u003c/p\u003e","title":"Secure Elk Stack"},{"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n\u0026ndash;\n簡介 ELK stack 官方說明文件\nELK 的元件 Elasticsearch: 基於 Lucene 的分散式全文搜索引擎 Logstash: 數據處理 pipeline Kibana: ELK stack 的管理後台與數據視覺化工具 Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics) ELK 的工作流程 beats -\u0026gt; (logstash) -\u0026gt; elasticsearch -\u0026gt; kibana\n將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起 設定 beats 從指定的路徑收集 log 與 metrics 設定 beats 向後輸出的遠端目標 (Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據 透過 Kibana，將 elasticsearch 上的 log 顯示 官方不是有出文件嗎 Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。\n說穿了本篇的技術含量不高，就只是一個踩雷過程。\nLets get our hands dirty.\nWARNING 這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在v6.3.0 以前的版本是不包含安全性模組的，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。\n我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為\n另起一篇對安全性設定多加說明 在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯 雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。\n如果希望第一次安裝就有完整的 security 設定，請等待下篇 Secure ELK Stask\n討論需求與規格 這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。\nelk-node-standalone 10.140.0.10 app-node-1 10.140.0.11 ... ... 本機的 ELK stack 元件，彼此透過 localhost 連線\nElasticsearch: localhost:9200 Kibana: localhost:5601 Apm-server: localhost:8200 Self Monitoring Services 私有網路中的外部服務透過 10.140.0.10\nbeats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200 beats 從其他 node 輸出到 Apm-server: 10.140.0.10:8200 在內部網路中 透過 browser 存取 Kibana: 10.140.0.10:5601 standalone 的好處:\n方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰) 最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分 Standalone可能造成的問題:\nNo High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛 外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。 如果要有 production ready 的 ELK\nHA 開起來 把服務分散到不同 node 上, 方便之後 scale out 多開幾台 elasticsearch-1, elasticsearch-2, elasticsearch-3\u0026hellip; kibana-1 apm-server-1, apm-server-2, \u0026hellip; 如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇 主機設定 Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。\nGCP 上使用 Google Compote Engine 的朋友，可以照 Google 官方操作步驟操作\n完成後接近這樣\n$ df -h $ df --human-readable Filesystem Size Used Avail Use% Mounted on /dev/sda1 9.6G 8.9G 682M 93% / /dev/sdb 492G 63G 429G 13% /mnt/disks/elk $ ls /mnt/disks/elk /mnt/disks/elk/elasticsearch /mnt/disks/elk/apm-server /mnt/disks/elk/kibana 至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。\n開防火牆 需要開放 10.140.0.10 這台機器的幾個 port\nelasticsearch :9200 來源只開放私有網路其他 ip 10.140.0.0/9 apm-server :8200 (同上) kibana :5601 (同上)，如果想從外部透過 browser開，需要 whitelist ip GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線\ndefault-allow-internal: :all :10.140.0.0/9 tcp:0-65535 Install Elasticsearch Install Elasticsearch 官方文件 7.3\n我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝\nsudo apt-get install apt-transport-https wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - add-apt-repository \u0026quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026quot; sudo apt-get update sudo apt-get install elasticsearch 安裝完後路徑長這樣\n/etc/elasticsearch /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/jvm.options # Utility /usr/share/elasticsearch/bin/ # Log /var/log/elasticsearch/elasticsearch.log 有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs\n服務控制 透過 systemd 管理，我們可以用 systemctl 控制， 用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。\n但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。\nmkdir -p /mnt/disks/elk/elasticsearch chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch 設定檔案 ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案\nsudo vim /etc/elasticsearch/elasticsearch.yml # Change Network network.host: 0.0.0.0 # Change data path path.data: /mnt/disks/elk/elasticsearch vim /etc/elasticsearch/jvm-options # Adjust heap to 4G -Xms4g -Xmx4g # Enable xpack.security discovery.seed_hosts: [\u0026quot;10.140.0.10\u0026quot;] discovery.type: \u0026quot;single-node\u0026quot; xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.license.self_generated.type: basic 6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。\n有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。\n/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto # Keep your passwords safe 然後把啟動 Elasticsearch\nsudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service 看一下 log，確定服務有在正常工作\ntail -f /var/log/elasticsearch/elasticsearch.log 在 node 上試打 Elasticsearch API\n$ curl localhost:9200 { \u0026quot;name\u0026quot; : \u0026quot;elk\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;uiMZe7VETo-H6JLFLF4SZg\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;7.3.1\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;deb\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;4749ba6\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2019-08-19T20:19:25.651794Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.1.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;6.8.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;6.0.0-beta1\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } Kibana 有了正常工作的 Elasticsearch，接下來要安裝 kibana，由於 apt repository 已經匯入，這邊直接\nsudo apt-get update sudo apt-get install kibana 一樣快速設定一下\n$ vim /etc/kibana/kinana.yml # change server.host from localhost to 0.0.0.0 to allow outside requests server.host: \u0026quot;0.0.0.0\u0026quot; # Add elasticsearch password elasticsearch.username: \u0026quot;kibana\u0026quot; elasticsearch.password: sudo systemctl enable kibana.service sudo systemctl start kibana.service sudo systemctl status kibana.service 檢查 log 並試打一下\nsudo systemctl status kibana $ curl localhost:5601 透過內網 ip 也可以用 browser 存取 使用 elastic 這組帳號密碼登入，可以有管理員權限 可以檢視一下 kibana 的頁面，看一下是否系統功能都上常上線 http://10.140.0.10/app/monitoring#\nFilebeat 以上是 ELK 最基本架構: elasticsearch 引擎與前端視覺化管理工具 Kibana。當然現在進去 kibana 是沒有數據的，所以我們現在來安裝第一個 beat，收集第一筆數據。\n你可能會覺得奇怪: 我現在沒有任何需要監控的應用，去哪收集數據?\nELK 提供的自我監測 (self-monitoring) 的功能，也就是在 node 上部屬 filebeat 並啟用 modules，便可以把這台 node 上的 elasticsearch 運行的狀況，包含cpu 狀況、記憶體用量、儲存空間用量、安全性告警、\u0026hellip;都做為數據，傳到 elasticsearch 中，並在 Kibana monitoring 頁面製圖顯示。\n這邊也剛好做為我們 ELK stack 的第一筆數據收集。\nWARNING: 這邊一樣要提醒， production 環境多半會使用另外一組的 elasticsearch 來監控主要的這組 elastic stack，以維持 elk stack 的穩定性，才不會自己 monitoring 自己，結果 elastic 掛了，metrics 跟錯誤訊息都看不到。\n官方安裝文件\nsudo apt-get update sudo apt-get install filebeat 預設的 filebeat.yml 設定檔案不是完整的，請到官網下載完整版，但官網沒給檔案連結(慘)，只有網頁版 https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html\n我們上 github 把她載下來\n$ wget https://raw.githubusercontent.com/elastic/beats/v7.3.1/filebeat/filebeat.reference.yml $ sudo mv filebeat-reference-y $ sudo vim /etc/filebeat/filebeat.yml # Enable elasticsearch module and kibana module to process metrics of localhost elasticsearch \u0026amp; kibana filebeat.modules: - module: elasticsearch # Server log server: enabled: true - module: kibana # All logs log: enabled: true # The name will be added to metadata name: filebeat-elk fields: env: elk # Add additional cloud_metadata since we're on GCP processors: - add_cloud_metadata: ~ # Output to elasticsearch output.elasticsearch: enabled: true hosts: [\u0026quot;localhost:9200\u0026quot;] protocol: \u0026quot;http\u0026quot; username: \u0026quot;elastic\u0026quot; password: # Configure kibana with filebeat: add template, dashboards, etc... setup.kibana: host: \u0026quot;localhost:5601\u0026quot; protocol: \u0026quot;http\u0026quot; username: \u0026quot;elastic\u0026quot; password: 啟動 filebeat\nsudo systemctl start filebeat 看一下 log，filebeat 會開始收集 elasticsearch 的 log 與 metrics，可以在 log 上看到收集的狀況。\n$ sudo journalctl -fu filebeat Sep 15 06:28:50 elk filebeat[9143]: 2019-09-15T06:28:50.176Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\u0026quot;monitoring\u0026quot;: {\u0026quot;metrics\u0026quot;: {\u0026quot;beat\u0026quot;:{\u0026quot;cpu\u0026quot;:{\u0026quot;system\u0026quot;:{\u0026quot;ticks\u0026quot;:1670860,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:66}},\u0026quot;total\u0026quot;:{\u0026quot;ticks\u0026quot;:6964660,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:336},\u0026quot;value\u0026quot;:6964660},\u0026quot;user\u0026quot;:{\u0026quot;ticks\u0026quot;:5293800,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:270}}},\u0026quot;handles\u0026quot;:{\u0026quot;limit\u0026quot;:{\u0026quot;hard\u0026quot;:4096,\u0026quot;soft\u0026quot;:1024},\u0026quot;open\u0026quot;:11},\u0026quot;info\u0026quot;:{\u0026quot;ephemeral_id\u0026quot;:\u0026quot;62fd4bfa-1949-4356-9615-338ca6a95075\u0026quot;,\u0026quot;uptime\u0026quot;:{\u0026quot;ms\u0026quot;:786150373}},\u0026quot;memstats\u0026quot;:{\u0026quot;gc_next\u0026quot;:7681520,\u0026quot;memory_alloc\u0026quot;:4672576,\u0026quot;memory_total\u0026quot;:457564560376,\u0026quot;rss\u0026quot;:-32768},\u0026quot;runtime\u0026quot;:{\u0026quot;goroutines\u0026quot;:98}},\u0026quot;filebeat\u0026quot;:{\u0026quot;events\u0026quot;:{\u0026quot;active\u0026quot;:-29,\u0026quot;added\u0026quot;:1026,\u0026quot;done\u0026quot;:1055},\u0026quot;harvester\u0026quot;:{\u0026quot;open_files\u0026quot;:4,\u0026quot;running\u0026quot;:4}},\u0026quot;libbeat\u0026quot;:{\u0026quot;config\u0026quot;:{\u0026quot;module\u0026quot;:{\u0026quot;running\u0026quot;:0}},\u0026quot;output\u0026quot;:{\u0026quot;events\u0026quot;:{\u0026quot;acked\u0026quot;:1055,\u0026quot;active\u0026quot;:-50,\u0026quot;batches\u0026quot;:34,\u0026quot;total\u0026quot;:1005},\u0026quot;read\u0026quot;:{\u0026quot;bytes\u0026quot;:248606},\u0026quot;write\u0026quot;:{\u0026quot;bytes\u0026quot;:945393}},\u0026quot;pipeline\u0026quot;:{\u0026quot;clients\u0026quot;:9,\u0026quot;events\u0026quot;:{\u0026quot;active\u0026quot;:32,\u0026quot;published\u0026quot;:1026,\u0026quot;total\u0026quot;:1026},\u0026quot;queue\u0026quot;:{\u0026quot;acked\u0026quot;:1055}}},\u0026quot;registrar\u0026quot;:{\u0026quot;states\u0026quot;:{\u0026quot;current\u0026quot;:34,\u0026quot;update\u0026quot;:1055},\u0026quot;writes\u0026quot;:{\u0026quot;success\u0026quot;:35,\u0026quot;total\u0026quot;:35}},\u0026quot;system\u0026quot;:{\u0026quot;load\u0026quot;:{\u0026quot;1\u0026quot;:1.49,\u0026quot;15\u0026quot;:0.94,\u0026quot;5\u0026quot;:1.15,\u0026quot;norm\u0026quot;:{\u0026quot;1\u0026quot;:0.745,\u0026quot;15\u0026quot;:0.47,\u0026quot;5\u0026quot;:0.575}}}}}} 如果數據都有送出，就可以回到 kibana 的頁面，看一下目前這個 elasticsearch 集群，有開啟 monitoring 功能的元件們，是否都有正常工作。\nhttp://10.140.0.10/app/monitoring#\n頁面長得像這樣\nStandalone cluster 中的 filebeat，是還未跟 elasticsearch 配對完成的數據，會顯示在另外一個集群中，配對完後會歸到 elk cluster 中，就是我們的主要 cluster。\n點進去可以看各個元件的服務情形。\n小結 簡單思考 self-host ELK stack 搭建的架構 在單一 node 上安裝最簡易的 elastic stack 設定元件的 output 位置 設定 self-monitoring 恭喜各位獲得一個裸奔但是功能完整的 ELK, 我們下篇再向安全性邁進。\n","permalink":"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/","summary":"\u003cp\u003e\u003ca href=\"https://ithelp.ithome.com.tw/2020ironman\"\u003e2020 It邦幫忙鐵人賽\u003c/a\u003e 系列文章\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/\"\u003eSelf-host ELK stack on GCP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-15-secure-elk-stack/\"\u003eSecure ELK Stask\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/\"\u003e監測 Google Compute Engine 上服務的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/\"\u003e監測 Google Kubernetes Engine 的各項數據\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/\"\u003e是否選擇 ELK 作為解決方案\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://chechia.net/posts/2019-09-21-logstash-on-gke/\"\u003e使用 logstash pipeline 做數據前處理\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eElasticsearch 日常維護：數據清理，效能調校，永久儲存\u003c/li\u003e\n\u003cli\u003eDebug ELK stack on GCP\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e作為範例的 ELK 的版本是當前的 stable release 7.3.1。\u003c/p\u003e\n\u003cp\u003e由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\u003c/p\u003e\n\u003cp\u003e對我的文章有興趣，歡迎到我的網站上 \u003ca href=\"https://chechia.net\"\u003ehttps://chechia.net\u003c/a\u003e 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\u003c/p\u003e\n\u003cp\u003e\u0026ndash;\u003c/p\u003e\n\u003ch1 id=\"簡介-elk-stack\"\u003e簡介 ELK stack\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.elastic.co/guide/index.html\"\u003e官方說明文件\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"elk-的元件\"\u003eELK 的元件\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eElasticsearch: 基於 Lucene 的分散式全文搜索引擎\u003c/li\u003e\n\u003cli\u003eLogstash: 數據處理 pipeline\u003c/li\u003e\n\u003cli\u003eKibana: ELK stack 的管理後台與數據視覺化工具\u003c/li\u003e\n\u003cli\u003eBeats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"elk-的工作流程\"\u003eELK 的工作流程\u003c/h3\u003e\n\u003cp\u003ebeats -\u0026gt; (logstash) -\u0026gt; elasticsearch -\u0026gt; kibana\u003c/p\u003e","title":"Self-host ELK stack - Installation"},{"content":"各位好，我是Che-Chia Chang，社群上常用的名子是 David Chang。是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 Golang Taiwan Meetup 的 organizer。\n受到友人們邀請（推坑）參加了2020 It邦幫忙鐵人賽，挑戰在30天內，每天發一篇技術分享文章。一方面將工作上遇到的問題與解法分享給社群，另一方面也是給自己一點成長的壓力，把這段時間的心得沈澱下來，因此也了這系列文章。\n本系列文章重點有三：\n提供的解決方案，附上一步步的操作步驟。希望讓讀者可以重現完整操作步驟，直接使用，或是加以修改\n著重 Google Cloud Platform，特別是Google Compute Engine (GCE) 與Google Kubernetes Engine (GKE) 兩大服務。這也是我最熟悉的平台，順便推廣，並分享一些雷點。\n從維運的角度除錯，分析問題，提升穩定性。\n預定的主題如下（可能會依照實際撰寫狀況微調）\nELK Stask on GCP (8) Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter Prometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 Nginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 文章發表於鐵人挑戰頁面，同時發布與本站備份。有任何謬誤，還煩請各方大德\u0026lt;3透過底下的聯絡方式聯絡我，感激不盡。\nFeatures\nstep-by-step guide for deployment: guarentee a running deployment on GCP basic configuration, usage, monitoring, networking on GKE debugging, stability analysis in an aspect of devop Topics\nELK stack(8) Deploy self-hosted ELK stack on GCE instance Secure ELK stack with SSL and role-based authentication Monitoring services on Kubernetes with ELK beats Monitoring services on GCE instances Logstash pipelines and debugging walk through Elasticsearch operations: house-cleaning, tuning, pernament storage Elasticsearch maitainence, trouble shooting Get-Started with Elastic Cloud SASS General operations on Kubernetes(4) Kubernetes Debug SOP Kubectl cheat sheet Secure services with SSL by cert-manager Speed up container updating with operator My operator example Deploy Kafka HA on Kubernetes(4) deploy kafka-ha on Kubernertes with helm in-cluster networking configuration for high availability basic app-side usage, performance tuning Operate Kafka: update config, upgrade version, migrate data Promethus / grafana(5) Deploy Prometheus / Grafana stack on GCE instance Monitoring services on Kubernetes with exporters Export Kubernetes metrics to Prometheus Export Redis-ha metrics to Prometheus Export Kafka metrics to Prometheus GCP networking(4) Firewall basic concept for private network with GCE instances \u0026amp; Kubernetes Load balancer for Kubernetes service \u0026amp; ingress DNS on GCP from Kube-dns to GCP DNS service GCP log management(3) Basic usage about GCP logging \u0026amp; GCP Error Report Stackdriver, metrics, alerts Logging on GKE from gcp-fluentd to stackdriver ","permalink":"https://chechia.net/posts/2019-09-09-ithome-ironman-challenge/","summary":"2019 IT邦幫忙鐵人賽","title":"2019 IT邦幫忙鐵人賽"},{"content":"各位好，我是Che-Chia Chang，專長 DevOps \u0026amp; Kubernetes\n今年不保證不爛尾的 2021 鐵人挑戰頁面 第一手消息發布，免費線上諮詢，請見 FB 往年回顧 4 萬餘字的血淚 30 日 - 2020 鐵人挑戰 優等 回到 2021 年，本系列會寫得很隨興，讀者有幫助為目的撰寫，\n如果有興趣的題目，或是好奇想問問題，歡迎底下留言，或到我的 FB 私訊，我都會一對一回復。基於「取之社群回饋社群」的精神，諮詢聊天都是永遠免費。\n基於明確需求，討論解決方案 跟 Kubernetes 有關 提供手把手 SOP 喜歡也請幫我點個讚，或是留言讓我知道，讓我有點動力繼續寫，大幅降低我偷懶爛尾的機率(XD)\n預定的主題（可能會微調或大改XD）\n好文翻譯分享 Borg, Omega, Kubernetes - Google 容器化開發十年經驗，Kubernetes 的前日今生 公有雲省錢大作戰 - 我這邊有一批便宜的好 VM 打三折賣你，Preemptible / Spot Instance 先占節點實戰分享 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，原理簡介與規格 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，適用案例實戰分析 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，使用範例 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，經驗分享，雷點 公有雲省錢大作戰 - 我跑了一個 bot，24 小時上線， GCP 一天收你 4 元台幣 大家都來用 Terraform，Infrastructure as Code 演講全文分享 iThome Cloud Summit 講到時間超過，一對東西沒講，所以來這邊發文 真的好用，實戰經驗分享 解答粉專私訊問題與觀眾發問 分散式工具實驗室 - Scalable Database on Kubernetes Thanos - Scalable HA Prometheus 簡介 需求分析，Unlimited Retention 鐵一般的需求 一步步帶你架 Scalable DB 實驗- Cockroach DB - 耐用打不死又高效能的小強資料庫 前言，分散式系統下的困境，資料庫瓶頸 Cockroach DB 基本原理簡介 Cockroach DB Free Trial 玩起來 Cassandra - 支撐百萬級寫入的分散式資料庫 Cassandra 簡介 Cassandra 細部原理，分散式的資料庫設計超好玩 Cassandra 細部原理，Consistency Hashing Cassandra 細部原理，Data modeling Netflix case study - 偷學 Netflix 能學到幾成功力呢 有任何謬誤，還煩請各方大德\u0026lt;3聯絡我，感激不盡。\n","permalink":"https://chechia.net/posts/2020-09-09-ithome-ironman-challenge/","summary":"2020 IT邦幫忙鐵人賽","title":"2020 IT邦幫忙鐵人賽"},{"content":"This post is about my learning steps for quantum-computing.\nFor a quick-start tutorial, check my workshop project throught the project link above.\nResources Courses\nCoursera\non MIT x pro\nQuantum Information Processing from UW Madison\nQuantum Computation by John Preskill\nIBM Q Experience\nhttps://github.com/Qiskit/openqasm\nhttps://github.com/Qiskit/qiskit-tutorials\nIBM Q Experience Day 1 Getting Started with Circuit Composer\nHello Quantum World circuit transformed two qubits, from $ \\vert0\\rangle $ to $ \\frac{\\vert00\\rangle + \\vert11\\rangle}{\\sqrt{2}} $\nQuestions\n[] Hadamard Gate [] Bell states [] Annotations ","permalink":"https://chechia.net/posts/2019-06-02-journey-to-quantum-computing/","summary":"\u003cp\u003eThis post is about my learning steps for quantum-computing.\u003c/p\u003e\n\u003cp\u003eFor a quick-start tutorial, check my workshop project throught the project link above.\u003c/p\u003e\n\u003ch1 id=\"resources\"\u003eResources\u003c/h1\u003e\n\u003cp\u003eCourses\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.coursera.org/learn/quantum-computing-algorithms\"\u003eCoursera\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://mitxpro.mit.edu/courses/course-v1:MITxPRO\u0026#43;QCx0\u0026#43;1T2019/about\"\u003eon MIT x pro\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://pages.cs.wisc.edu/~dieter/Courses/2010f-CS880/lectures.html\"\u003eQuantum Information Processing from UW Madison\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.theory.caltech.edu/people/preskill/ph229/\"\u003eQuantum Computation by John Preskill\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://quantum-computing.ibm.com\"\u003eIBM Q Experience\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Qiskit/openqasm\"\u003ehttps://github.com/Qiskit/openqasm\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Qiskit/qiskit-tutorials\"\u003ehttps://github.com/Qiskit/qiskit-tutorials\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"ibm-q-experience\"\u003eIBM Q Experience\u003c/h1\u003e\n\u003ch3 id=\"day-1\"\u003eDay 1\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://quantum-computing.ibm.com/support/guides/getting-started-with-circuit-composer\"\u003eGetting Started with Circuit Composer\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHello Quantum World circuit transformed two qubits, from $ \\vert0\\rangle $ to $ \\frac{\\vert00\\rangle + \\vert11\\rangle}{\\sqrt{2}} $\u003c/p\u003e","title":"Journey to Quantum Computing"},{"content":"Create GKE gcloud beta container --project \u0026quot;istio-playground-239810\u0026quot; clusters create \u0026quot;istio-playground\u0026quot; \\ --zone \u0026quot;asia-east1-b\u0026quot; \\ --username \u0026quot;admin\u0026quot; \\ --cluster-version \u0026quot;1.11.8-gke.6\u0026quot; \\ --machine-type \u0026quot;n1-standard-2\u0026quot; \\ --image-type \u0026quot;COS\u0026quot; \\ --disk-type \u0026quot;pd-standard\u0026quot; \\ --disk-size \u0026quot;100\u0026quot; \\ --preemptible \\ --num-nodes \u0026quot;1\u0026quot; \\ --enable-cloud-logging \\ --enable-cloud-monitoring \\ --no-enable-ip-alias \\ --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard,Istio \\ --istio-config auth=MTLS_PERMISSIVE \\ --no-enable-autoupgrade \\ --enable-autorepair Take a Peek $ kubectl get namespaces NAME STATUS AGE default Active 2m istio-system Active 1m kube-public Active 2m kube-system Active 2m $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7f6f77cd7b-nxfbf 1/1 Running 0 3m istio-cleanup-secrets-h454m 0/1 Completed 0 3m istio-egressgateway-7c56db84cc-nlrwq 1/1 Running 0 3m istio-galley-6c747bdb4f-45jrp 1/1 Running 0 3m istio-ingressgateway-6ff68cf95d-tlkq4 1/1 Running 0 3m istio-pilot-8ff66f8c4-q9chz 2/2 Running 0 3m istio-policy-69b78b7d6-c8pld 2/2 Running 0 3m istio-sidecar-injector-558996c897-hr6q4 1/1 Running 0 3m istio-telemetry-f96459fb-5cbpg 2/2 Running 0 3m promsd-ff878d44b-hv8nh 2/2 Running 1 3m Deploy app kubectl label namespace default istio-injection=enabled Bookinfo Application\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods kubectl get services Gateway\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml kubectl get gateways kubectl get svc istio-ingressgateway -n istio-system Go to ingress public ip\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\u0026quot;http2\u0026quot;)].port}') export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\u0026quot;https\u0026quot;)].port}') curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage 404 Not Found Apply destination rules\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/destination-rule-all.yaml curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage Brief review kubectl get virtualservices kubectl get destinationrules kubectl get gateways Istio Tasks https://istio.io/docs/tasks/traffic-management/\n","permalink":"https://chechia.net/posts/2019-05-06-service-mesh-for-microservice-on-kubernetes/","summary":"基於 Kubernetes 平台上的 Istio ，實際部署，並一步一步操作Istio 的功能。","title":"Istio 三分鐘就入坑 佈署篇"},{"content":"Jenkins is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.\nJenkins X, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.\nHere we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.\nArchitecture of Jenkins X Install Jenkins with jx Create a Pipeline with jx Develope with jx client For more information about jx itself, check Jenkins-X Github Repo\nArchitecture Check this beautiful diagram.\nhttps://jenkins-x.io/architecture/diagram/ Install Create GKE cluster \u0026amp; Get Credentials gcloud init gcloud components update CLUSTER_NAME=jenkins-server #CLUSTER_NAME=jenkins-serverless gcloud container clusters create ${CLUSTER_NAME} \\ --num-nodes 1 \\ --machine-type n1-standard-4 \\ --enable-autoscaling \\ --min-nodes 1 \\ --max-nodes 2 \\ --zone asia-east1-b \\ --preemptible # After cluster initialization, get credentials to access cluster with kubectl gcloud container clusters get-credentials ${CLUSTER_NAME} # Check cluster stats. kubectl get nodes Install jx on Local Machine [Jenkins X Release](https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases)\nJX_VERSION=v2.0.2 OS_ARCH=darwin-amd64 #OS_ARCH=linux-amd64 curl -L https://github.com/jenkins-x/jx/releases/download/\u0026quot;${JX_VERSION}\u0026quot;/jx-\u0026quot;${OS_ARCH}\u0026quot;.tar.gz | tar xzv sudo mv jx /usr/local/bin jx version NAME VERSION jx 2.0.2 Kubernetes cluster v1.11.7-gke.12 kubectl v1.11.9-dispatcher helm client v2.11.0+g2e55dbe helm server v2.11.0+g2e55dbe git git version 2.20.1 Operating System Mac OS X 10.14.4 build 18E226 (Option 1) Install Serverless Jenkins Pipeline DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke' Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD Select the organization where you want to create the environment repository: chechiachang Your Kubernetes context is now set to the namespace: jx INFO[0231] To switch back to your original namespace use: jx namespace jx INFO[0231] Or to use this context/namespace in just one terminal use: jx shell INFO[0231] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0231] To import existing projects into Jenkins: jx import INFO[0231] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0231] To create a new microservice from a quickstart: jx create quickstart (Option 2) Install Static Jenkins Server DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke' Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD Select the organization where you want to create the environment repository: chechiachang INFO[0465]Your Kubernetes context is now set to the namespace: jx INFO[0465] To switch back to your original namespace use: jx namespace default INFO[0465] Or to use this context/namespace in just one terminal use: jx shell INFO[0465] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0465] To import existing projects into Jenkins: jx import INFO[0465] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0465] To create a new microservice from a quickstart: jx create quickstart Access Static Jenkins Server through Domain with username and password Domain http://jenkins.jx.11.22.33.44.nip.io/\nUninstall jx uninstall # rm -rf ~/.jx Setup CI/CD Pipeline Create Quickstart Repository kubectl get pods --namespace jx --watch # cd workspace jx create quickstart Options:\nWhich organisation do you want to use? chechiachang Enter the new repository name: serverless-jenkins-quickstart select the quickstart you wish to create [Use arrows to move, type to filter] angular-io-quickstart aspnet-app dlang-http golang-http jenkins-cwp-quickstart jenkins-quickstart node-http\nINFO[0121] Watch pipeline activity via: jx get activity -f serverless-jenkins-quickstart -w INFO[0121] Browse the pipeline log via: jx get build logs chechiachang/serverless-jenkins-quickstart/master INFO[0121] Open the Jenkins console via jx console INFO[0121] You can list the pipelines via: jx get pipelines INFO[0121] Open the Jenkins console via jx console INFO[0121] You can list the pipelines via: jx get pipelines INFO[0121] When the pipeline is complete: jx get applications Check log of the first run jx logs pipeline Add Step to Pipeline Add a setup step for pullrequest\ncd serverless-jenkins-quickstart jx create step --pipeline pullrequest \\ --lifecycle setup \\ --mode replace \\ --sh \u0026quot;echo hello world\u0026quot; Validate pipeline step for each modification\njx step validate A build-pack pod started after git push. Watch pod status with kubectl.\nkubectl get pods --namespace jx --watch Check Build Status on Prow (Serverless) http://deck.jx.130.211.245.13.nip.io/ Login with username and password\nImport Existing Repository In source code repository:\nImport jx to remote jenkins-server. This will apply a Jenkinsfile to repository by default\njx import --url git@github.com:chechiachang/serverless-jenkins-quickstart.git Update jenkins-x.yml\njx create step git commit \u0026amp; push\nTrouble Shooting Failed to get jx resources\njx get pipelines Make sure your jx (or kubectl) context is with the correct GKE and namespace\nkc config set-context gke_my-project_asia-east1-b_jenkins \\ --namespace=jx Why not use helm chart? It\u0026rsquo;s readlly depend on what we need in CI/CD automation.\nJenkins Helm Chart create Jenkins master and slave cluster on Kubernetes utilizing the Jenkins Kubernetes plugin. Jenkin Platform with jx is Jenkins Platform native to Kubernetes. It comes with powerful cloud native components like Prow automation, Nexus, Docker Registry, Tekton Pipeline, \u0026hellip;\nCheck jenkins-x examples https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs\nClient jx get urls Name URL jenkins http://jenkins.jx.11.22.33.44.nip.io jenkins-x-chartmuseum http://chartmuseum.jx.11.22.33.44.nip.io jenkins-x-docker-registry http://docker-registry.jx.11.22.33.44.nip.io jenkins-x-monocular-api http://monocular.jx.11.22.33.44.nip.io jenkins-x-monocular-ui http://monocular.jx.11.22.33.44.nip.io nexus http://nexus.jx.11.22.33.44.nip.io Get Cluster Status jx diagnose Get Applications \u0026amp; Pipelines jx get applications jx get pipelines Get CI Activities \u0026amp; build log jx get activities jx get activities --filter='jenkins-x-on-kubernetes' jx get build log INFO[0003] view the log at: http://jenkins.jx.11.22.33.44.nip.io/job/chechiachang/job/jenkins-x-on-kubernetes/job/feature-add-test/3/console ... Trigger Build \u0026amp; Check Activity jx start pipeline jx start pipeline --filter='jenkins-x-on-kubernetes/feature-add-test' jx get activities --filter='jenkins-x-on-kubernetes' Create Pull Request jx create pullrequest ","permalink":"https://chechia.net/posts/2019-04-19-jenkins-x-on-kubernetes/","summary":"\u003cp\u003e\u003ca href=\"https://jenkins.io/\"\u003eJenkins\u003c/a\u003e is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://jenkins-x.io/\"\u003eJenkins X\u003c/a\u003e, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.\u003c/p\u003e\n\u003cp\u003eHere we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.\u003c/p\u003e","title":"Jenkins X on Kubernetes"},{"content":"","permalink":"https://chechia.net/posts/2018-10-06-kubernetes-container-runtime-interface/","summary":"","title":"Kubernetes Container Runtime Interface"}]