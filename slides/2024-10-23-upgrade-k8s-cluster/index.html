<!doctype html><html lang=en><head><meta charset=utf-8><title>Kubernetes Summit: Upgrade A VM Based Cluster</title><meta name=description content="分享如何升級 VM-based Kubernetes Cluster 的版本，包含 etcd，control plane，與 node。升級前如何規劃，升級步驟該如何操作，升級後應該如何檢查。"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><link rel=stylesheet href=/reveal-js/dist/reset.css><link rel=stylesheet href=/reveal-js/dist/reveal.css><link rel=stylesheet href=/reveal-hugo/themes/robot-lung.css id=theme><link rel=stylesheet href=/highlight-js/color-brewer.min.css></head><body><div class=reveal><div class=slides><section><h3 id=upgrade-a-vm-based-cluster>Upgrade A VM Based Cluster</h3><p><a href=https://chechia.net/>Che Chia Chang</a></p><aside class=notes></aside></section><section><h3 id=關於我>關於我</h3><ul><li>Che Chia Chang</li><li>SRE @ <a href=https://www.cake.me/companies/maicoin/jobs>Maicoin</a></li><li><a href=https://mvp.microsoft.com/zh-TW/MVP/profile/e407d0b9-5c01-eb11-a815-000d3a8ccaf5>Microsoft MVP</a></li><li>投影片與講稿都在 <a href=https://chechia.net/>chechia.net</a></li><li><a href=https://chechia.net/zh-hant/talk/kubernetes-summit-get-started-with-etcd-kubernetes/>Etcd Workshop</a></li><li>鐵人賽 (Terraform / Vault 手把手入門)</li></ul></section><section><h3 id=今天的主題>今天的主題</h3><ul><li>為何要 AWS 自架 K8s</li><li>升級事前閱讀文件與規劃</li><li>需要升級的元件</li><li>實際升級的步驟</li><li>自動化 k8s 升級</li></ul></section><section><h3 id=今天的目的>今天的目的</h3><ul><li>統整散落的資訊</li><li>升級需要考慮的事情</li><li>可能哪些地方有雷，導致升級出錯</li><li>產生自己團隊的升級 SOP</li></ul></section><section><h3 id=背景什麼是-vm-based-cluster>背景：什麼是 VM-based Cluster</h3><ul><li>Managed Kubernetes Service (GKE, EKS, AKS)<ul><li>公有雲託管 control plane，透過雲提供的介面控制</li></ul></li><li>self-hosted Kubernetes<ul><li>cloud formation template ec2 node</li></ul></li></ul></section><section><p><img alt="Self-hosted K8s" loading=lazy src=https://docs.aws.amazon.com/images/whitepapers/latest/overview-deployment-options/images/image6.png></p></section><section><h3 id=什麼是-vm-based-cluster>什麼是 VM-based Cluster</h3><p>公有雲只提供 VM，自己用其它工具搭 K8s Components</p><ul><li>kubeadm, kops, kubespray / docker, containerd, cri-o / container or systemd / &mldr;</li><li>自己管理 control plane，包含 etcd, apiserver, controller-manager, scheduler</li><li>依需求選用公有雲提供的架構，ex VPC, ELB, EBS, S3, RDS, IAM, Route53, CloudWatch, CloudTrail&mldr;</li></ul></section><section><p>Self-hosted control plane</p><p><img loading=lazy src=/slides/2024-10-23-upgrade-k8s-cluster/self-hosted-k8s.png></p></section><section><h3 id=所以為何要在aws上自架-k8s>所以為何要在AWS上自架 K8s？</h3></section><section><h3 id=因為-2016-年的時候沒有-eks>因為 2016 年的時候沒有 eks</h3></section><section><h3 id=2015--年有很多-self-hosted-k8s-解決方案>2015- 年有很多 self-hosted k8s 解決方案</h3><ul><li>2014/10/15 kubernetes v0.4 <a href=https://github.com/kubernetes/kubernetes/releases/tag/v0.4>github</a></li><li>2014/11/04 <a href=https://cloud.google.com/kubernetes-engine/docs/release-notes-archive#november_4_2014>GKE release for 0.4.2</a></li><li>當時有很多解決方案再提供 self-hosted k8s，基於這些方案，產生 VM-based k8s<ul><li><a href=https://github.com/getamis/vishwakarma>github.com/getamis/vishwakarma</a></li></ul></li><li>2017/10/24 <a href=https://azure.microsoft.com/en-us/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/>AKS generally available</a></li><li>2018/06/05 <a href=https://aws.amazon.com/blogs/aws/amazon-eks-now-generally-available/>EKS generally available</a></li></ul><aside class=notes></aside></section><section><h3 id=self-hosted-control-plane>Self-hosted control plane</h3><p>self-hosted control plane 其實不難，那時有很多成熟的解決方案</p><ul><li><a href=https://chechia.net/zh-hant/talk/kubernetes-summit-get-started-with-etcd-kubernetes/>k8s summit Etcd Workshop</a></li><li>降低服務商依賴</li><li>可以跨雲，跨地端，混合雲</li><li>自訂化，換 VM disk image，CNI，CSI，Ingress Controller</li><li>成本控制</li></ul></section><section><h3 id=self-hosted-control-plane-壞處>Self-hosted control plane: 壞處</h3><ul><li>團隊多做一些事情</li><li>工程師個人可以(必須)學到很多東西</li><li>cloud managed k8s 越做越好，自己架的理由越來越少</li></ul></section><section><h3 id=self-hosted-多了要升級的東西>Self-hosted 多了要升級的東西</h3><ul><li>VM: based image / package / CVEs (不在今天的範圍)</li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/#manual-deployments>k8s doc 建議的升級順序</a></li><li>Etcd Cluster</li><li>Control Plane<ul><li>apiserver</li><li>controller manager</li><li>scheduler</li><li>cloud controller manager</li></ul></li></ul></section><section><p><img alt=https://kubernetes.io/images/docs/components-of-kubernetes.svg loading=lazy src=https://kubernetes.io/images/docs/components-of-kubernetes.svg></p></section><section><h3 id=實際來看升級的步驟>實際來看升級的步驟</h3></section><section><h3 id=實際升級的順序>實際升級的順序</h3><ul><li>研究決定升版目標<ul><li>必讀 K8s CHANGELOG / Urgent Upgrade Notes</li></ul></li><li>檢查 app 的版本依賴 (i.e 升 k8s 會壞的 app)<ul><li>K8s resource API Deprecated</li><li>App Controllers</li></ul></li><li>Etcd Cluster</li><li>Apiserver<ul><li>Controller Manager</li><li>Scheduler</li></ul></li><li>Node (kubelet, kube-proxy, CNI agent)</li></ul></section><section><h3 id=事前研究跟規劃>事前研究跟規劃</h3><ul><li>確認目前版本</li><li>目標版本<ul><li>升版的目的：(feature, EOL, &mldr;)</li><li>元件彼此相容的版本</li><li>每個小版號 Release Cycle ~ 4 months</li><li>有重疊時間，所以大概是 3 個月出一版新的</li><li>path release ~ 8 months</li></ul></li></ul></section><section><h3 id=k8s-release-cycle--4-months>K8s Release Cycle ~ 4 months</h3><p><img alt=https://kubernetes.io/images/releases/release-cycle.jpg loading=lazy src=https://kubernetes.io/images/releases/release-cycle.jpg></p></section><section><p>加上 ~ 8 months patch release</p><p><img alt=https://kubernetes.io/images/releases/release-lifecycle.jpg loading=lazy src=https://kubernetes.io/images/releases/release-lifecycle.jpg></p></section><section><h3 id=多久升級一次-k8s>多久升級一次 k8s</h3><ul><li>每季更新一次<ul><li>跟 feature，每次升 N+1 版</li><li>跟 EOL，每次升 N+1 版</li></ul></li><li>每年更，維持在 EOL 前，每次升 N+3 版</li></ul></section><section><h3 id=升級變成-routine>升級變成 routine</h3><p>更新 k8s 也不是短短幾天的工作，要有時間規劃，測試，檢查</p><ul><li>除了版本不同，每次要做的流程類似</li><li>與其是一個 task 更像是一個 routine 的工作</li><li>重點變成不是多久更新一次</li><li>而是幾然需要常做，該如何<strong>有效率(自動化)的完成升級</strong></li></ul></section><section><h3 id=必讀文件-k8s-changelog>必讀文件 K8s CHANGELOG</h3><p>Urgent Upgrade Notes (No, really, you MUST read this before you upgrade)</p><ul><li><a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md#urgent-upgrade-notes>CHANGELOG-1.31.md</a></li><li>最少要看 Urgent Upgrade Notes 跟 action required</li></ul></section><section><h3 id=不看-changelog-真的不行嗎>不看 CHANGELOG 真的不行嗎？</h3><p>可以，流程會變成</p><ul><li>k8s 升級上去</li><li>發現有東西壞掉，開始 debug</li><li>一路從 app, infra, k8s resource, 查到 k8s 上的問題</li><li>最後還是看了 CHANGELOG</li><li>你以後就會乖乖的把 CHANGELOG 看完</li></ul></section><section><h3 id=升級事前研究跟規劃>升級事前研究跟規劃</h3><ul><li>了解 k8s release lifecycle</li><li>依據團隊政策，設定更新週期</li><li>了解目前版本與目標版本的差異</li><li>上述工作轉為週期性的常態工作</li><li>加速升級的速度</li></ul></section><section><h3 id=需要升級的-k8s-components>需要升級的 K8s Components</h3><p><img alt=https://kubernetes.io/images/docs/components-of-kubernetes.svg loading=lazy src=https://kubernetes.io/images/docs/components-of-kubernetes.svg></p></section><section><h3 id=etcd-upgrade-官方文件>Etcd Upgrade 官方文件</h3><ul><li><a href=https://etcd.io/>https://etcd.io/</a></li><li>分散式 key-value store<ul><li>可以 zero downtime rolling upgrade</li><li>可以 zero downtime rolling downgrade</li></ul></li></ul></section><section><h3 id=etcd-upgrade-步驟>Etcd Upgrade 步驟</h3><p><a href=https://etcd.io/docs/v3.5/upgrades/upgrade_3_5/>https://etcd.io/docs/v3.5/upgrades/upgrade_3_5/</a></p><ul><li>etcd 在 minor version 升級時，可以 zero downtime rolling upgrade</li><li>維持 etcd cluster quorum</li><li>delete then add member (維持 quorum 先減後增）</li><li>保留 etcd data 與 endpoint ip，開回後直接上線</li><li>只需 sync delete -> add 中間的資料差</li></ul></section><section><h3 id=terraform-for-iac>terraform for IaC</h3><p>要換的 etcd VM 可以拆解成 terraform module</p><ul><li>VM -> EC2</li><li>base os -> AMI</li><li>data disk (etcd data) -> EBS</li><li>ip (etcd endpoint) -> ENI</li><li><strong>etcd binary -> S3</strong></li><li>etcd config / flags -> launch template / cloud init</li></ul></section><section><h3 id=terraform-gitflow>terraform gitflow</h3><ul><li>PR 改 binary etcd:v3.4.34 -> etcd:v3.5.4</li><li>PR review -> 自動 trigger terraform plan</li><li><strong>PR merge</strong> -> 自動 apply dev / stag</li><li>自動化執行測試</li><li>Monitoring 監測一直都在</li><li>有問題: revert PR 自動 rollback</li><li>沒問題: <strong>人工確認上述步驟，開出 prod PR</strong> -> 自動 apply prod</li><li>有問題: revert PR 自動 rollback</li></ul></section><section><h5 id=從專注完成升級變成專注在自動化流程>從專注完成升級，變成專注在自動化流程</h5><p>其他 k8s 元件的管理也是同樣方式，將人工降到最低</p></section><section><h3 id=etcd-upgrade-該注意>Etcd Upgrade 該注意</h3><p><a href=https://etcd.io/docs/v3.5/op-guide/failures/>https://etcd.io/docs/v3.5/op-guide/failures/</a></p><ul><li>Majority of members are unreachable</li><li>Network partition</li><li>遵守 SOP 先減後增</li></ul></section><section><h3 id=kube-apiserver>kube-apiserver</h3><p><img alt=https://kubernetes.io/images/docs/components-of-kubernetes.svg loading=lazy src=https://kubernetes.io/images/docs/components-of-kubernetes.svg></p></section><section><h3 id=apiserver-doc>apiserver doc</h3><p><a href=https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/#manual-deployments>https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/#manual-deployments</a></p><ul><li>先升級 kubectl</li><li>apiserver 小版號 rolling upgrade zero downtime</li><li>如果建立 k8s 時有用工具/平台，ex kubeadm, kops, kubespray，會有對應的升級指令</li><li>terraform 直接更換 apiserver 節點</li></ul></section><section><h3 id=supported-version-skew>Supported Version Skew</h3><p><a href=https://kubernetes.io/releases/version-skew-policy/>https://kubernetes.io/releases/version-skew-policy/</a></p><ul><li>kube-apiserver 彼此最多差 1 個小版號<ul><li>1.30 升 1.31 過程可以無痛</li></ul></li><li>controller manager, scheduler, cloud controller manager<ul><li>(vs apiserver) 最多差 1 個小版號</li><li>升級 apiserver 後，跟著升級同板</li></ul></li><li>kubelet, kube-proxy<ul><li>(vs apiserver) 最多差 3 個小版號</li><li>apiserver 1.30 升 1.31 時，node 1.28, 1.29, 1.30 可以無痛</li></ul></li></ul></section><section><h3 id=api-deprecated>api deprecated</h3><ul><li><a href=https://kubernetes.io/docs/reference/using-api/deprecation-guide/>Deprecated API Migration Guide</a></li><li><a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/>Deprecation policy</a></li></ul></section><section><h3 id=使用工具掃描-k8s-resource>使用工具掃描 k8s resource</h3><ul><li>如何確認 app 的 k8s resource 有沒有使用到 deprecated API</li><li><a href=https://github.com/doitintl/kube-no-trouble>https://github.com/doitintl/kube-no-trouble</a></li><li>加入到升級流程，自動化<ul><li>（沒升級的時候）週期性的掃描</li><li>app 開發的 CI 應該導入 <code>lint --kube-version</code></li><li>如果有 app 使用到 deprecated API，通知 app owner 提早更新</li><li>升級前再掃描一次</li></ul></li></ul></section><section><h3 id=kube-no-trouble>kube-no-trouble</h3><p><img loading=lazy src=/slides/2024-10-23-upgrade-k8s-cluster/kube-no-trouble.png></p></section><section><h3 id=產生新版-apiserver-vm>產生新版 apiserver VM</h3><p>要換的 apiserver VM</p><ul><li>VM -> EC2</li><li>base os -> AMI</li><li>Auto Scaling Group + Load balancer</li><li><strong>更新apiserver binary</strong> -> S3 launch template / cloud formation</li><li>apiserver config / flags -> launch template / cloud init</li></ul></section><section><h3 id=移除舊版-apiserver-graceful-shutdown>移除舊版 apiserver graceful shutdown</h3><p>load balancer -> auto scaling group -> EC2 instance</p><ul><li>透過 load balancer 的設定，確保 apiserver 有足夠的時間離線<ul><li>request 都已處理完</li><li>load balancer 關閉連入</li><li>等待 graceful period，確認沒有 request</li><li>關閉服務，關閉 EC2 instance</li></ul></li><li>監測 apiserver 的 metrics，錯誤率提高自動告警</li></ul></section><section><h3 id=scheduler-and-controller-manager>scheduler and controller manager</h3><ul><li>如果是 master VM (api-server, scheduler, controller manager, cloud controller manager)<ul><li>一起升級</li><li>一起建立新版 VM</li><li>一起移除舊版 VM</li></ul></li><li>如果是其他方式 apply，依照對應的升級指令</li></ul></section><section><h3 id=依賴-k8s-api-的-app>依賴 k8s api 的 app</h3><ul><li><a href=https://kubernetes.io/docs/concepts/architecture/controller/>https://kubernetes.io/docs/concepts/architecture/controller/</a></li><li>許多 controller 會需要依賴特定的 k8s api<ul><li>進而依賴特定版本的 k8s</li></ul></li><li>會打 k8s api 的 app<ul><li>自家寫的 app 有用 k8s <a href=https://github.com/kubernetes/client-go>client-go</a></li></ul></li></ul></section><section><h3 id=app-升級檢查>app 升級檢查</h3><ul><li>掃描 k8s resource 使用的 api version 大部分掃得出來</li><li>各 controller 的文件才會描述為何有版本依賴<ul><li>以及如何升級 app controller 到新版的 k8s api</li></ul></li><li>團隊應維護<strong>依賴 k8s api 的服務列表</strong><ul><li>或是使用工具 or script 快速列出 cluster 內部的服務</li></ul></li></ul></section><section><p><a href=https://docs.nginx.com/nginx-ingress-controller/technical-specifications/>https://docs.nginx.com/nginx-ingress-controller/technical-specifications/</a></p><p><img loading=lazy src=/slides/2024-10-23-upgrade-k8s-cluster/nginx-ingress-controller.png></p></section><section><p><a href=https://docs.nginx.com/nginx-ingress-controller/technical-specifications/>https://docs.nginx.com/nginx-ingress-controller/technical-specifications/</a></p><p><img loading=lazy src=/slides/2024-10-23-upgrade-k8s-cluster/nginx-ingress-controller-support-version.png></p></section><section><h3 id=小結k8s-control-plane-升級>小結：k8s control plane 升級</h3><ul><li>etcd</li><li>changelog</li><li>apiserver<ul><li>api deprecation</li></ul></li><li>scheduler, controller manager</li></ul></section><section><h3 id=node-升級>Node 升級</h3><ul><li>kubelet, kube-proxy 升級本身不是問題</li><li>需要搬移 worklaod</li><li>有沒有 worklaod 依賴 k8s api (ex. <a href=https://github.com/kubernetes/client-go>client-go</a><ul><li>有的話應該收錄上升級的服務列表，每次升級都需要檢查</li><li>有直接與 k8s api 互動的 app，強烈建議要寫 unit test</li></ul></li></ul></section><section><h3 id=node-升級簡單來說是建新拆舊>Node 升級簡單來說是建新拆舊</h3><p><img alt=https://kubernetes.io/images/docs/components-of-kubernetes.svg loading=lazy src=https://kubernetes.io/images/docs/components-of-kubernetes.svg></p></section><section><h3 id=node-升級需要搬移-worklaod>Node 升級需要搬移 worklaod</h3><p>開新的 node，舊的 node 上的 workload 需要轉移</p><ul><li>準備空 node</li><li>舊 node 打上 node taint，避免 worklaod 跑上去</li><li><strong>依序</strong>重啟 workload<ul><li>app 彼此可能會有依賴關係，i.e. 重啟 a 時 b 會噴錯</li><li>ex. backend + in-k8s redis</li></ul></li><li>或是依照 node drain node</li></ul></section><section><h3 id=workload-retry>Workload retry</h3><p>app 要實作 auto retry，來處理暫時斷線與達到 auto recovery</p><ul><li>重啟 workload 可能造成大量的重啟</li><li>同時大量 connect + retry 有可能會打壞被依賴的服務<ul><li>ex. backend + in-k8s redis，大量的 backend pod 同時重啟，會打壞 redis</li></ul></li><li>所以還要實作 retry delay, timeout backoff</li></ul></section><section><h3 id=如何重啟有依賴的-workload>如何重啟有依賴的 Workload</h3><p>依據團隊的目標而定</p><ul><li>ex 目標是最少整體重啟次數</li><li>先重啟被依賴的服務 ex. redis</li><li>在重啟依賴的服務 ex. backend</li><li>考驗服務的 replication 數量是否足夠，能否支撐大量重啟</li></ul><h5 id=依序重啟-workload應該變成自動化的腳本>依序重啟 workload，應該變成自動化的腳本</h5></section><section><h3 id=驗證>驗證</h3><p>Workload 轉移到新版本，測試 worklaod 是否正常運作</p><ul><li>針對 worklaod 有設定 monitoring 與 metrics</li><li>ex. cpu, memory 升級後增加太多，或是用的太少</li><li>ex. api 在重啟過程中噴 5xx，5xx 的次數異常增加，自動 alert</li><li>ex. api, websocket 錯誤比例增加</li><li>ex. non 2xx 的 access log 量增加</li></ul><h5 id=與其說是測試更像是把持續監控做個更完整>與其說是測試，更像是把持續監控做個更完整</h5></section><section><h3 id=升級後跨環境測試>升級後跨環境測試</h3><p>在測試環境導入更多測試</p><ul><li>chaos engineering 在平時就持續注入 error，根據 scenario 設定 k8s 的反應<ul><li>ex. chaos monkey, chaos mesh</li></ul></li><li>load testing 測試 app 在新版本 k8s 上的效能<ul><li>ex. <a href=https://github.com/grafana/k6>https://github.com/grafana/k6</a></li></ul></li><li>過去的 incident 的測試 script</li></ul><h5 id=不是為了升級導入這些測試而是因為有了這些測試讓升級更有信心>不是為了升級導入這些測試，而是因為有了這些測試，讓升級更有信心</h5></section><section><h5 id=k8s-升級有問題不是有問題>K8s 升級有問題，不是有問題</h5><h5 id=太晚發現才是問題>太晚發現才是問題</h5></section><section><h3 id=好的監測是升級的保護傘>好的監測是升級的保護傘</h3></section><section><p>怎麼知道 k8s 升級有沒有壞？用 kubectl 打看看</p><pre><code>while true; do
    kubectl get pods -n kube-system; 
    sleep 2;
done
</code></pre></section><section><h3 id=監測>監測</h3><p>上面的各個步驟都仰賴監測自動告警</p><ul><li>元件壞了，要有監測紀錄</li><li>重要元件壞了，要主動告警</li><li>沒壞但是有異常變動，要主動告警</li><li>出錯沒監測到，補上監測跟測試，再來一次，累積新版本的信心</li></ul></section><section><h3 id=監測參考工具>監測參考工具</h3><ul><li><a href=https://github.com/prometheus-operator/prometheus-operator>prometheus-operator</a></li><li>prometheus</li><li>grafana</li><li><a href=https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/dashboards>kubernetes-mixin</a> alert, rule, dashboards<ul><li><a href=https://samber.github.io/awesome-prometheus-alerts/>https://samber.github.io/awesome-prometheus-alerts/</a></li></ul></li></ul></section><section><h3 id=願景自動化-k8s-升級>願景：自動化 k8s 升級</h3><p>今天討論的步驟，有哪些可以進一步再自動化的</p><ul><li>看 CHANGELOG，選定目標版本</li><li>掃描服務 k8s api version</li><li>排程升級的時間</li><li>升級 etcd</li><li>升級 kube-apiserver, scheduler, controller manager</li><li>升級 node</li><li>遷移 workload</li></ul></section><section><h3 id=were-hiring>We&rsquo;re hiring!</h3><p><a href=https://www.cake.me/companies/maicoin/jobs>https://www.cake.me/companies/maicoin/jobs</a></p><p><img loading=lazy src=/slides/2024-10-23-upgrade-k8s-cluster/we-re-hiring.png></p></section><section><h3 id=參考資料>參考資料</h3><ul><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/>https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/</a></li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/>Overview: Official doc</a></li></ul></section></div></div><script type=text/javascript src=/reveal-hugo/object-assign.js></script><script src=/reveal-js/dist/reveal.js></script><script type=text/javascript src=/reveal-js/plugin/markdown/markdown.js></script><script type=text/javascript src=/reveal-js/plugin/highlight/highlight.js></script><script type=text/javascript src=/reveal-js/plugin/zoom/zoom.js></script><script type=text/javascript src=/reveal-js/plugin/notes/notes.js></script><script type=text/javascript>function camelize(e){return e&&Object.keys(e).forEach(function(t){newK=t.replace(/(_\w)/g,function(e){return e[1].toUpperCase()}),newK!=t&&(e[newK]=e[t],delete e[t])}),e}var revealHugoDefaults={center:!0,controls:!0,history:!0,progress:!0,transition:"slide"},revealHugoSiteParams={},revealHugoPageParams={custom_theme:"reveal-hugo/themes/robot-lung.css",highlight_theme:"color-brewer",margin:.2,templates:{hotpink:{background:"#FF4081",class:"hotpink"}},transition:"slide",transition_speed:"fast"},revealHugoPlugins={plugins:[RevealMarkdown,RevealHighlight,RevealZoom,RevealNotes]},options=Object.assign({},camelize(revealHugoDefaults),camelize(revealHugoSiteParams),camelize(revealHugoPageParams),camelize(revealHugoPlugins));Reveal.initialize(options)</script><script type=text/javascript src=/mermaid.min_16862243754454536095.js></script><script type=text/javascript>mermaid.initialize({startOnLoad:!1});let render=e=>{let t=e.currentSlide.querySelectorAll(".mermaid");if(!t.length)return;t.forEach(e=>{let t=e.getAttribute("data-processed");t||mermaid.init(0[0],e)})};render({currentSlide:Reveal.getCurrentSlide()}),Reveal.on("slidechanged",render),Reveal.on("ready",render)</script></body></html>