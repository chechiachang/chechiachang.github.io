<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Che-Chia Chang</title>
    <link>https://chechiachang.github.io/post/</link>
    <description>Recent content in Posts on Che-Chia Chang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>chechiachang &amp;copy; 2016</copyright>
    
	    <atom:link href="https://chechiachang.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kafka Operation Scripts</title>
      <link>https://chechiachang.github.io/post/kafka-operation-scripts/</link>
      <pubDate>Wed, 25 Sep 2019 22:50:32 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kafka-operation-scripts/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka HA on Kubernetes

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-deployment-on-kubernetes&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Deploy kafka-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-introduction&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Kafka Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-basic-usage&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;kafka 基本使用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-operation-script&amp;quot;}}&#34; target=&#34;_blank&#34;&gt;kafka operation scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;集群內部的 HA topology&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;Prometheus Metrics Exporter 很重要&lt;/li&gt;
&lt;li&gt;效能調校
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;從 Zookeeper 獲取資訊&lt;/li&gt;
&lt;li&gt;取得並處理 topic&lt;/li&gt;
&lt;li&gt;benchmark kafka&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;zookeeper&#34;&gt;zookeeper&lt;/h1&gt;

&lt;p&gt;zookeeper 是 kafka 的分散式協調系統，在 kafka 上多個節點間需要協調的內容，例如：彼此節點的ID，位置與當前狀態，或是跨節點 topic 的設定與狀態。取名叫做 zookeeper 就是在協調混亂的分散式系統，,裡面各種不同種類的服務都要協調，象個動物園管理員。&lt;a href=&#34;https://zookeeper.apache.org/doc/r3.3.3/zookeeperAdmin.html&#34; target=&#34;_blank&#34;&gt;Zookeeper 的官方文件&lt;/a&gt; 有更詳細的說明。&lt;/p&gt;

&lt;p&gt;Kafka 的節點資訊，與當前狀態，是放在 zookeeper 上，我們可以透過以下指令取得&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 首先先取得 zkCli 的 cli，這個只有連進任何一台 zookeeper 內部都有
kubectl exec -it kafka-0-zookeeper-0 --container kafka-broker bash

# 由於是在 Pod 內部，直接 localhost 詢問本地
/usr/bin/zkCli.sh -server localhost:2181

Connecting to localhost:2181
2019-09-25 15:02:36,089 [myid:] - INFO  [main:Environment@100] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2019-09-25 15:02:36,096 [myid:] - INFO  [main:Environment@100] - Client environment:host.name=kafka-0-zookeeper-0.kafka-0-zookeeper-headless.default.svc.cluster.local
2019-09-25 15:02:36,096 [myid:] - INFO  [main:Environment@100] - Client environment:java.version=1.8.0_131
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.vendor=Oracle Corporation
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.class.path=/usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.10.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper:
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.io.tmpdir=/tmp
2019-09-25 15:02:36,100 [myid:] - INFO  [main:Environment@100] - Client environment:java.compiler=&amp;lt;NA&amp;gt;
2019-09-25 15:02:36,101 [myid:] - INFO  [main:Environment@100] - Client environment:os.name=Linux
2019-09-25 15:02:36,101 [myid:] - INFO  [main:Environment@100] - Client environment:os.arch=amd64
2019-09-25 15:02:36,101 [myid:] - INFO  [main:Environment@100] - Client environment:os.version=4.14.127+
2019-09-25 15:02:36,101 [myid:] - INFO  [main:Environment@100] - Client environment:user.name=zookeeper
2019-09-25 15:02:36,102 [myid:] - INFO  [main:Environment@100] - Client environment:user.home=/home/zookeeper
2019-09-25 15:02:36,102 [myid:] - INFO  [main:Environment@100] - Client environment:user.dir=/
2019-09-25 15:02:36,105 [myid:] - INFO  [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@42110406
Welcome to ZooKeeper!
2019-09-25 15:02:36,160 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
JLine support is enabled
2019-09-25 15:02:36,374 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@876] - Socket connection established to localhost/127.0.0.1:2181, initiating session
2019-09-25 15:02:36,393 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x16d67baf1310001, negotiated timeout = 30000

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
[zk: localhost:2181(CONNECTED) 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;取得 kafka broker 資料&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# List root Nodes
$ ls /

[cluster, controller, controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]

# Brokers 的資料節點
$ ls /brokers
[ids, topics, seqid]

# List /brokers/ids 得到三個 kafka broker
$ ls /brokers/ids
[0, 1, 2]

# 列出所有 topic 名稱
ls /brokers/topics
[ticker]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ticker 是上篇範利用到的 topic&lt;/p&gt;

&lt;p&gt;簡單來說，zookeeper 存放這些狀態與 topic 的 metadata&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;儲存核心的狀態與資料，特別是 broker 萬一掛掉，也還需要維持的資料&lt;/li&gt;

&lt;li&gt;&lt;p&gt;協調工作，例如協助 broker 處理 quorum，紀錄 partition master 等&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 離開 zkCli
quit
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;kafka&#34;&gt;Kafka&lt;/h1&gt;

&lt;p&gt;這邊一樣先連線進去一台 broker，取得 kafka binary&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl exec -it kafka-0-0 --container kafka-broker bash

 ls /usr/bin/ | grep kafka
kafka-acls
kafka-broker-api-versions
kafka-configs
kafka-console-consumer
kafka-console-producer
kafka-consumer-groups
kafka-consumer-perf-test
kafka-delegation-tokens
kafka-delete-records
kafka-dump-log
kafka-log-dirs
kafka-mirror-maker
kafka-preferred-replica-election
kafka-producer-perf-test
kafka-reassign-partitions
kafka-replica-verification
kafka-run-class
kafka-server-start
kafka-server-stop
kafka-streams-application-reset
kafka-topics
kafka-verifiable-consumer
kafka-verifiable-producer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;很多工具，我們這邊只會看其中幾個&lt;/p&gt;

&lt;h3 id=&#34;topic-資訊&#34;&gt;topic 資訊&lt;/h3&gt;

&lt;p&gt;Topic 的資訊，跟 zookeeper 要&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# List topics
/usr/bin/kafka-topics --list --zookeeper kafka-0-zookeeper

ticker
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;操作-message&#34;&gt;操作 message&lt;/h3&gt;

&lt;p&gt;從 topic 取得 message&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# This will create a new console-consumer and start consuming message to stdout
/usr/bin/kafka-console-consumer \
--bootstrap-server localhost:9092 \
--topic engine_topic_soundwave_USD \
--timeout 0 \
--from-beginning
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果 ticker 那個 example pod 還在執行，這邊就會收到 ticker 的每秒 message&lt;/p&gt;

&lt;p&gt;如果沒有，也可以開啟另一個 broker 的連線&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl exec -it kafka-0-1 --container kafka-broker bash

# 使用 producer 的 console 連入，topic 把 message 塞進去
/usr/bin/kafka-console-producer \
--broker-list localhost:9092\
 --topic ticker

tick [enter]
tick [enter]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kafka-console-consumer 那個 terminal 就會收到 message&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tick
tick
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;當然也可以使用 consumer group&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Use consumer to check ticker topics
/usr/bin/kafka-console-consumer \
--bootstrap-server localhost:9092 \
--topic ticker \
--group test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有做過上面的操作產生 consumer group，就可以透過 consumer API，取得 consumer group 狀態&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Check consumer group
/usr/bin/kafka-consumer-groups \
--bootstrap-server localhost:9092 \
--group ticker \
--describe

Consumer group &#39;test&#39; has no active members.

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
ticker          0          23              23              0               -               -               -
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;topic-設定操作&#34;&gt;Topic 設定操作&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/#topicconfigs&#34; target=&#34;_blank&#34;&gt;Topic 設定文件&lt;/a&gt; 在此&lt;/p&gt;

&lt;p&gt;這邊透過 kafka-configs 從 zookeeper 取得 topic 設定，這邊的 max.message.bytes，是這個 topic 每個 message 的最大上限。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/bin/kafka-configs --zookeeper kafka-0-zookeeper:2181 --describe max.message.bytes --entity-type topics

Configs for topic &#39;__consumer_offsets&#39; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer
Configs for topic &#39;ticker&#39; are
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;__consumer__offsets&lt;/code&gt; 是系統的 topic ，紀錄目前 consumer 讀取的位置。&lt;/p&gt;

&lt;p&gt;ticker 沒有設定，就是 producer 當初產生 topic 時沒有指定，使用 default 值&lt;/p&gt;

&lt;p&gt;由於我們公司的使用情境常常會超過，所以可以檢查 producer app 那端送出的 message 大小，在比較這邊的設定。當然現在 ticker 的範例，只有一個 0-60 的數值，並不會超過。這個可以在 helm install 的時候，使用 value.yaml 傳入時更改。&lt;/p&gt;

&lt;p&gt;不喜歡這個值，可以更改，這邊增加到 16MB&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;TOPIC=ticker

/usr/bin/kafka-configs \
  --zookeeper kafka-3-zookeeper:2181 \
  --entity-type topics \
  --alter \
  --entity-name ${TOPIC} \
  --add-config max.message.bytes=16000000

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;benchmark&#34;&gt;Benchmark&lt;/h1&gt;

&lt;p&gt;使用內建工具跑 benchmark&lt;/p&gt;

&lt;p&gt;Producer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/bin/kafka-producer-perf-test \
  --num-records 100 \
  --record-size 100 \
  --topic performance-test \
  --throughput 100 \
  --producer-props bootstrap.servers=kafka:9092 max.in.flight.requests.per.connection=5 batch.size=100 compression.type=none

100 records sent, 99.108028 records/sec (0.01 MB/sec), 26.09 ms avg latency, 334.00 ms max latency, 5 ms 50th, 70 ms 95th, 334 ms 99th, 334 ms 99.9th.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Consumer&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/bin/kafka-consumer-perf-test \
  --messages 100 \
  --broker-list=kafka:9092 \
  --topic performance-test \
  --group performance-test \
  --num-fetch-threads 1
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kafka-basic-usage</title>
      <link>https://chechiachang.github.io/post/kafka-basic-usage/</link>
      <pubDate>Tue, 24 Sep 2019 21:59:49 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kafka-basic-usage/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka HA on Kubernetes

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-deployment-on-kubernetes&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Deploy kafka-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-introduction&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Kafka Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-basic-usage&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;kafka 基本使用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;kafka utility scripts, topic operation, kafka benchmark&lt;/li&gt;
&lt;li&gt;集群內部的 HA topology&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;Prometheus Metrics Exporter 很重要&lt;/li&gt;
&lt;li&gt;效能調校
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;在 Kubernetes 中連線 kafka&lt;/li&gt;
&lt;li&gt;使用 golang library 連線到 Kafka&lt;/li&gt;
&lt;li&gt;透過 kafka script 操作 kafka&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;kubernetes-中連線-kafka&#34;&gt;kubernetes 中連線 kafka&lt;/h1&gt;

&lt;p&gt;先看一看 kafka pods&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods --selector=&#39;app=kafka&#39;

NAME        READY   STATUS    RESTARTS   AGE
kafka-1-0   1/1     Running   1          26d
kafka-1-1   1/1     Running   0          26d
kafka-1-2   1/1     Running   0          26d

$ kubectl get pods -l &#39;app=zookeeper&#39;

NAME                  READY   STATUS    RESTARTS   AGE
kafka-1-zookeeper-0   1/1     Running   0          26d
kafka-1-zookeeper-1   1/1     Running   0          26d
kafka-1-zookeeper-2   1/1     Running   0          26d

$ kubectl get pods -l &#39;app=kafka-exporter&#39;

NAME                               READY   STATUS    RESTARTS   AGE
kafka-1-exporter-88786d84b-z954z   1/1     Running   5          26d
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;kubectl describe pods kafka-1-0

Name:           kafka-1-0
Namespace:      default
Priority:       0
Node:           gke-chechiachang-pool-1-e4622744-wcq0/10.140.15.212
Labels:         app=kafka
                controller-revision-hash=kafka-1-69986d7477
                release=kafka-1
                statefulset.kubernetes.io/pod-name=kafka-1-0
Annotations:    kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka-broker
Status:         Running
IP:             10.12.6.178
Controlled By:  StatefulSet/kafka-1
Containers:
  kafka-broker:
    Image:         confluentinc/cp-kafka:5.0.1
    Port:          9092/TCP
    Host Port:     0/TCP
    Command:
      sh
      -exc
      unset KAFKA_PORT &amp;amp;&amp;amp; \
      export KAFKA_BROKER_ID=${POD_NAME##*-} &amp;amp;&amp;amp; \
      export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 &amp;amp;&amp;amp; \
      exec /etc/confluent/docker/run

    Requests:
      cpu:      100m
    Liveness:   exec [sh -ec /usr/bin/jps | /bin/grep -q SupportedKafka] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:  tcp-socket :kafka delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:
      POD_IP:                                   (v1:status.podIP)
      POD_NAME:                                kafka-1-0 (v1:metadata.name)
      POD_NAMESPACE:                           default (v1:metadata.namespace)
      KAFKA_HEAP_OPTS:                         -Xmx4G -Xms1G
      KAFKA_ZOOKEEPER_CONNECT:                 kafka-1-zookeeper:2181
      KAFKA_LOG_DIRS:                          /opt/kafka/data/logs
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE:  false
      KAFKA_DEFAULT_REPLICATION_FACTOR:        3
      KAFKA_MESSAGE_MAX_BYTES:                 16000000
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR:  1
      KAFKA_JMX_PORT:                          5555
    Mounts:
      /opt/kafka/data from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2tm8c (ro)
Conditions:
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-kafka-1-0
    ReadOnly:   false
  default-token-2tm8c:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2tm8c
    Optional:    false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;講幾個重點：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;這邊跑起來的是 kafka-broker，接收 producer 與 consumer 來的 request&lt;/li&gt;
&lt;li&gt;這邊用的是 statefulsets，不是完全無狀態的 kafka broker，而把 message 記在 datadir 上，降低故障重啟時可能遺失資料的風險。&lt;/li&gt;
&lt;li&gt;啟動時，把 kubernetes 指定的 pod name 塞進環境變數，然後作為當前 broker 的 ID&lt;/li&gt;
&lt;li&gt;沒有設定 Pod antiAffinity，所以有可能會啟三個 kafka 結果三個跑在同一台 node 上，這樣 node 故障就全死，沒有HA&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;service-endpoints&#34;&gt;Service &amp;amp; Endpoints&lt;/h3&gt;

&lt;p&gt;看一下 service 與 endpoints
zookeeper 與 exporter 我們這邊先掠過不談，到專章講高可用性與服務監測時，再來討論。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get service -l &#39;app=kafka&#39;

NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kafka-1            ClusterIP   10.15.242.178   &amp;lt;none&amp;gt;        9092/TCP   26d
kafka-1-headless   ClusterIP   None            &amp;lt;none&amp;gt;        9092/TCP   26d

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;兩個 services&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一個是 cluster-ip service，有 single cluster IP 與 load-balance，DNS 會過 kube-proxy。&lt;/li&gt;
&lt;li&gt;一個是 headless service，DNS 沒有過 kube-proxy，而是由 endpoint controller 直接 address record，指向把符合 service selector 的 pod。適合做 service discovery，不會依賴於 kubernetes 的實現。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#headless-services&#34; target=&#34;_blank&#34;&gt;詳細說明在官方文件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;簡單來說，kafka broker 會做 auto service discovery，我們可以使用 headless service。&lt;/p&gt;

&lt;p&gt;客戶端(consumer &amp;amp; producer) 連入時，則使用 cluster-ip service，做 load balancing。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get endpoints -l &#39;app=kafka&#39;

NAME                            ENDPOINTS                                                          AGE
kafka-1                         10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092                  26d
kafka-1-headless                10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092                  26d
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;golang-example&#34;&gt;Golang Example&lt;/h1&gt;

&lt;p&gt;附上簡單的 Golang 客戶端，&lt;a href=&#34;https://github.com/chechiachang/kafka-on-kubernetes&#34; target=&#34;_blank&#34;&gt;完整 Github Repository 在這邊&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;context&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;strconv&amp;quot;
	&amp;quot;time&amp;quot;

	&amp;quot;github.com/segmentio/kafka-go&amp;quot; // 使用的套件
)

func main() {
	topic := &amp;quot;ticker&amp;quot; // 指定 message 要使用的 topic
	partition := 0 // 指定 partition，由於底下連線指定連線到 partition 的 leader，所以需要指定 partition
	kafkaURL := &amp;quot;kafka-0:9092&amp;quot; // 指定 kafkaURL，也可以透過 os.GetEnv() 從環境變數裡拿到。

  // producer 對指定 topic, partition 的 leader 產生連線
	producerConn, _ := kafka.DialLeader(context.Background(), &amp;quot;tcp&amp;quot;, kafkaURL, topic, partition)
  // 程式結束最後把 connection 關掉。不關會造成 broker 累積大量 connection，需要等待 broker 端 timeout 才會釋放。
	defer producerConn.Close()

	//producerConn.SetWriteDeadline(time.Now().Add(10 * time.Second))
  // 使用 go routine 跑一個 subprocess for loop，一直產生 message 到 kafka topic，這邊的範例是每秒推一個秒數。
	go func() {
		for {
			producerConn.WriteMessages(
				kafka.Message{
					Value: []byte(strconv.Itoa(time.Now().Second())),
				},
			)
			time.Sleep(1 * time.Second)
		}
	}()

	// make a new reader that consumes from topic-A, partition 0
	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers:   []string{kafkaURL},
		Topic:     topic,
		Partition: 0,
		MinBytes:  10e2, // 1KB
		MaxBytes:  10e3, // 10KB
	})
	defer r.Close()
	//r.SetOffset(42)

  // 印出 reader 收到的 message
	for {
		m, err := r.ReadMessage(context.Background())
		if err != nil {
			break
		}
		fmt.Printf(&amp;quot;%v message at offset %d: %s = %s\n&amp;quot;, time.Now(), m.Offset, string(m.Key), string(m.Value))
	}

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;這邊可以使用 Dockerfile 包成一個 container image，然後丟上 kubernetes&lt;/p&gt;

&lt;p&gt;我稍晚補一下 docker image 跟 deployment 方便大家操作好了。&lt;/p&gt;

&lt;p&gt;或是攋人測試，直接 kubectl run 一個 golang base image 讓它 sleep，然後在連進去&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl run DEPLOYMENT_NAME --image=golang:1.13.0-alpine3.10 sleep 3600

kubectl exec -it POD_NAME sh
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# 裡面沒有 Git 跟 vim 裝一下
apk add git vim

go get github.com/chechiachang/kafka-on-kubernetes

cd src/github.com/chechiachang/kafka-on-kubernetes/
vim main.go

go build .
./kafka-on-kubernetes

2019-09-24 14:20:46.872554693 +0000 UTC m=+9.154112787 message at offset 1:  = 46
2019-09-24 14:20:47.872563087 +0000 UTC m=+9.154121166 message at offset 2:  = 47
2019-09-24 14:20:48.872568848 +0000 UTC m=+9.154126926 message at offset 3:  = 48
2019-09-24 14:20:49.872574499 +0000 UTC m=+9.154132576 message at offset 4:  = 49
2019-09-24 14:20:50.872579957 +0000 UTC m=+9.154138032 message at offset 5:  = 50
2019-09-24 14:20:51.872588823 +0000 UTC m=+9.154146892 message at offset 6:  = 51
2019-09-24 14:20:52.872594672 +0000 UTC m=+9.154152748 message at offset 7:  = 52
2019-09-24 14:20:53.872599986 +0000 UTC m=+9.154158060 message at offset 8:  = 53
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;這樣就連上了，完成一個最簡單的使用範例。&lt;/p&gt;

&lt;p&gt;這個例子太過簡單，上一篇講的 consumer group, partitions, offset 什麼設定全都沒用上。實務上這些都需要好好思考，並且依據需求做調整設定。&lt;/p&gt;

&lt;h3 id=&#34;clean-up&#34;&gt;Clean up&lt;/h3&gt;

&lt;p&gt;把測試用的 deployment 幹掉&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete deployment DEPLOYMENT_NAME
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;簡述 kafka 在 kubernetes 上運行的狀況，連線方法&lt;/li&gt;
&lt;li&gt;Demo 一個小程式&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kafka-introduction</title>
      <link>https://chechiachang.github.io/post/kafka-introduction/</link>
      <pubDate>Mon, 23 Sep 2019 21:59:49 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kafka-introduction/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka HA on Kubernetes

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-deployment-on-kubernetes&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Deploy kafka-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-introduction&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Kafka Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;應用使用 kafka 的設定&lt;/li&gt;
&lt;li&gt;kafka utility scripts, topic operation, kafka benchmark&lt;/li&gt;
&lt;li&gt;集群內部的 HA topology&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;Prometheus Metrics Exporter 很重要&lt;/li&gt;
&lt;li&gt;效能調校
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;寫了部屬，本想談一下 kafka 的高可用性配置，看到大德的留言，才想到應該要先跟各位介紹一下 kafka，跟 kafka 的用途。也感謝大德路過發問，我也會順代調整內容。今天就說明何為 kafka，以及在什麼樣的狀況使用。&lt;/p&gt;

&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;簡介 kafka&lt;/li&gt;
&lt;li&gt;基本元件&lt;/li&gt;
&lt;li&gt;Kafka 的工作流程&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;簡介-kafka&#34;&gt;簡介 Kafka&lt;/h1&gt;

&lt;p&gt;Kafka 是分散式的 streaming platform，可以 subscribe &amp;amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。&lt;a href=&#34;https://kafka.apache.org/documentation/#gettingStarted&#34; target=&#34;_blank&#34;&gt;原版的說明在這邊&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;這邊有幾個東西要解釋。&lt;/p&gt;

&lt;h1 id=&#34;message-queue-system&#34;&gt;Message Queue System&lt;/h1&gt;

&lt;p&gt;當一個系統開始運作時，裡頭會有很多變數，這些變數其實就是在一定的範圍(scope）內，做訊息(message)的傳遞。例如在 app 寫了一個 function ，傳入一個變數的值給 function。&lt;/p&gt;

&lt;p&gt;在複雜的系統中，服務元件彼此也會有傳遞訊息的需求。例如我原本有一個 api-server，其中一段程式碼是效能瓶頸，我把它切出來獨立成一個 worker 的元件，讓它可以在更高效能地方執行，甚至 horizontal scaling。這種情境，辨可能歲需要把一部分的 message 從 api-server 傳到 worker，worker 把吃效能的工作做完，再把結果回傳給 api-server。這時就會需要一個穩定的 message queue system，來穩定，且高效能的傳遞這些 message。&lt;/p&gt;

&lt;p&gt;Message Queue System 實做很多，ActiveMQ, RabbitMQ, &amp;hellip; 等，一些 database 做 message queue 在某些應用場景下也十分適合，例如 Redis 是 in-memory key-value database，內部也實做 pubsub，能夠在某些環境穩定的傳送 message。&lt;/p&gt;

&lt;h1 id=&#34;request-response-vs-publish-subscribe&#34;&gt;Request-Response vs Publish-Subscribe&lt;/h1&gt;

&lt;p&gt;訊息的傳送有很多方式，例如 Http request-response 很適合 server 在無狀態(stateless) 下接受來自客戶端的訊息，每次傳送都重新建立新的 http connection，這樣做有很多好處也很多壞處。其中明顯的壞處是網路資源的浪費，以及訊息的不夠即時，指定特定收件人時發件人會造成額外負擔等。&lt;/p&gt;

&lt;p&gt;使用 Pub-sub pattern的好處，是 publisher 不需要額外處理『這個訊息要送給誰』的工作，而是讓 subscriber 來訂閱需要的訊息類別，一有新的 event 送到該訊息類別，直接透過 broker 推播給 subscriber。不僅即時，節省效能，而且訂閱的彈性很大。&lt;/p&gt;

&lt;h1 id=&#34;kafka-producer-consumer-api&#34;&gt;Kafka producer &amp;amp; Consumer API&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kafka.apache.org/23/images/kafka-apis.png&#34; alt=&#34;kafka diagram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka 作為 client 與 server 兩邊的溝通平台，提供了許多 API 葛不同角色使用。Producer 產生 message 到特定 topic 上，consumer 訂閱特定 topics，kafak 把符合條件的訊息推播給 consumer。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Producer API: 讓 app publish 一連的訊息&lt;/li&gt;
&lt;li&gt;Consumer API: 讓 app subscribe 許多特定 topic，並處理訊息串流(stream)&lt;/li&gt;
&lt;li&gt;Stream API: 讓 app 作為串流中介處理(stream processor)&lt;/li&gt;
&lt;li&gt;Connect API: 與 producer 與 consumer 可以對外部服務連結&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;topics-logs&#34;&gt;Topics &amp;amp; Logs&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://kafka.apache.org/23/images/log_anatomy.png&#34; alt=&#34;kafka-topics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Topic 是 kafka 為訊息串流提供的抽象，topic 是訊息傳送到 kafka 時賦予的類別(category)，作為 publish 與 consume 的判斷依據。&lt;/p&gt;

&lt;h1 id=&#34;partition&#34;&gt;Partition&lt;/h1&gt;

&lt;p&gt;訊息依據 topic 分類存放，並可以依據 replication factor 設定，在 kafka 中存放多個訊息分割(partition)。partition 可以想成是 message queue 的平行化 (parallel)，併發處理訊息可以大幅提昇訊息接收與發送的速度，並且多個副本也提高資料的可用性。&lt;/p&gt;

&lt;p&gt;由於訊息發送跟接收過程可能因為網路與環境而不穩定，這些相同 topic 的 partition 不一定會完全一樣。但 kafka 確保了以下幾點。&lt;/p&gt;

&lt;h1 id=&#34;guarantees&#34;&gt;Guarantees&lt;/h1&gt;

&lt;p&gt;良好配置的 kafka 有以下保證&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;訊息在系統中送出跟被收到的時間不一定，但kafak中，從相同 producer 送出的訊息，送到 topic partition 會維持送出的順序&lt;/li&gt;
&lt;li&gt;Consumer 看見的訊息是與 kafka 中的存放順序一致&lt;/li&gt;
&lt;li&gt;有 replication factor 為 N 的 topic ，可以容忍(fault-tolerance) N-1 個 kafka-server 壞掉，而不影響資料。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;當然，這邊的前提是有良好配置。錯誤的配置可能會導致訊息不穩定，效能低落，甚至遺失。&lt;/p&gt;

&lt;h1 id=&#34;producer&#34;&gt;Producer&lt;/h1&gt;

&lt;p&gt;Producer 負責把訊息推向一個 topic，並指定訊息應該放在 topic 的哪個 partition。&lt;/p&gt;

&lt;h1 id=&#34;consumer&#34;&gt;Consumer&lt;/h1&gt;

&lt;p&gt;Consumer 會自行標記，形成 consumer group，透過 consumer group 來保障訊息傳遞的次序，容錯，以及擴展的效率。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kafka.apache.org/23/images/consumer-groups.png&#34; alt=&#34;consumer group&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Consumer 透過 consumer group 共享一個 group.id。&lt;/li&gt;
&lt;li&gt;Consumer group 去所有 partitions 裡拿訊息，所有 partitions 的訊息分配到 consumer group 中的 consumer。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;app 在接收訊息時，設置正確的化，在一個 consumer group 中，可以容忍 consumer 失效，仍能確保訊息一指定的次序送達。在需要大流量時，也可調整 consumer 的數量提高負載。&lt;/p&gt;

&lt;h1 id=&#34;用例&#34;&gt;用例&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://kafka.apache.org/documentation/#uses&#34; target=&#34;_blank&#34;&gt;kafka 的使用例子&lt;/a&gt;非常的多，使用範圍非常廣泛。&lt;/p&gt;

&lt;p&gt;基本上是訊息傳遞的使用例子，kafka 大多能勝任。&lt;/p&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;p&gt;這邊只提了 kafka 的基本概念，基本元件，以及 consumer group 機制，為我們底下要談的 configuration 與 topology 鋪路。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kafka Helm Configuration</title>
      <link>https://chechiachang.github.io/post/kafka-helm-configuration/</link>
      <pubDate>Mon, 23 Sep 2019 21:55:29 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kafka-helm-configuration/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka HA on Kubernetes

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-deployment-on-kubernetes&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Deploy kafka-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;應用使用 kafka 的設定&lt;/li&gt;
&lt;li&gt;kafka utility scripts, topic operation, kafka benchmark&lt;/li&gt;
&lt;li&gt;集群內部的 HA topology&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;Prometheus Metrics Exporter 很重要&lt;/li&gt;
&lt;li&gt;效能調校
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kafka Deployment on Kubernetes</title>
      <link>https://chechiachang.github.io/post/kafka-deployment-on-kubernetes/</link>
      <pubDate>Sun, 22 Sep 2019 09:58:41 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kafka-deployment-on-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Kafka HA on Kubernetes

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;{{ ref &amp;quot;/post/kafka-deployment-on-kubernetes&amp;quot; }}&#34; target=&#34;_blank&#34;&gt;Deploy kafka-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;應用使用 kafka 的設定&lt;/li&gt;
&lt;li&gt;kafka utility scripts, topic operation, kafka benchmark&lt;/li&gt;
&lt;li&gt;集群內部的 HA topology&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;Prometheus Metrics Exporter 很重要&lt;/li&gt;
&lt;li&gt;效能調校
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;碎念&#34;&gt;碎念&lt;/h1&gt;

&lt;p&gt;30 天每天一文真的蠻逼人的，每一篇都是新寫，還要盡可能顧及文章品質，下班趕文章，各位大德寫看看就知道&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;這邊調整了當初想寫的文章，內容應該都會帶到

&lt;ul&gt;
&lt;li&gt;elk&lt;/li&gt;
&lt;li&gt;kafka-ha&lt;/li&gt;
&lt;li&gt;reids-ha&lt;/li&gt;
&lt;li&gt;prometheus&lt;/li&gt;
&lt;li&gt;kubernetes on gcp&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;但不會再一篇 10000 字了，逼死我吧&amp;hellip;&lt;/li&gt;
&lt;li&gt;寫不完的部份 30 天候會在IT邦幫忙，或是&lt;a href=&#34;https:/chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;我的 Github Page https://chechiachang.github.io/&lt;/a&gt;補完&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;簡介 kafka&lt;/li&gt;
&lt;li&gt;部屬 kafka 到 kubernetes 上&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;簡介-kafka&#34;&gt;簡介 kafka&lt;/h1&gt;

&lt;p&gt;Kafka 是分散式的 streaming platform，可以 subscribe &amp;amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。&lt;/p&gt;

&lt;p&gt;我們今天就來部屬一個 kafka。&lt;/p&gt;

&lt;h1 id=&#34;deploy&#34;&gt;Deploy&lt;/h1&gt;

&lt;p&gt;我把我的寶藏都在這了&lt;a href=&#34;https://github.com/chechiachang/kafka-on-kubernetes&#34; target=&#34;_blank&#34;&gt;https://github.com/chechiachang/kafka-on-kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下載下來的 .sh ，跑之前養成習慣貓一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat install.sh


#!/bin/bash
#
# https://github.com/helm/charts/tree/master/incubator/kafka

#HELM_NAME=kafka
HELM_NAME=kafka-1

helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator

# Stable: chart version: kafka-0.16.2	app version: 5.0.1
helm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;

&lt;p&gt;我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 kafka。所以我們先用。&lt;/p&gt;

&lt;p&gt;沒用過 helm 的大德可以參考 &lt;a href=&#34;https://helm.sh/docs/using_helm/#quickstart&#34; target=&#34;_blank&#34;&gt;Helm Quickstart&lt;/a&gt;，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm init
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;helm-chart&#34;&gt;Helm Chart&lt;/h3&gt;

&lt;p&gt;一個 helm chart 可以當成一個獨立的專案，不同的 chart 可以在 kubernetes 上協助部屬不同的項目。&lt;/p&gt;

&lt;p&gt;這邊使用了還在 incubator 的chart，雖然是 prod ready，不過使用上還是要注意。&lt;/p&gt;

&lt;p&gt;使用前先把 incubator 的 helm repo 加進來&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install&#34;&gt;Install&lt;/h3&gt;

&lt;p&gt;這邊是用 upgrade &amp;ndash;install，已安裝就 upgrade，沒安裝就 install，之後可以用這個指令升版&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;helm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;version&#34;&gt;Version&lt;/h3&gt;

&lt;p&gt;這邊使用的版本：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;chart version:    kafka-0.16.2&lt;/li&gt;
&lt;li&gt;app version:      5.0.1&lt;/li&gt;
&lt;li&gt;kafka Image:      confluentinc/cp-kafka:5.0.1&lt;/li&gt;
&lt;li&gt;zookeeper Image:  gcr.io/google_samples/k8szk:v3&lt;/li&gt;
&lt;li&gt;kafka exporter:   danielqsj/kafka-exporter:v1.2.0&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;values-staging&#34;&gt;values-staging&lt;/h3&gt;

&lt;p&gt;透過 helm chart，把啟動參數帶進去，這邊我們看幾個比較重要的，細節之後的文章在一起討論。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/chechiachang/kafka-on-kubernetes/blob/master/values-staging.yaml&#34; target=&#34;_blank&#34;&gt;https://github.com/chechiachang/kafka-on-kubernetes/blob/master/values-staging.yaml&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;replicas: 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安裝三個 kafka，topology 的東西也是敬待下篇XD&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## The kafka image repository
image: &amp;quot;confluentinc/cp-kafka&amp;quot;

## The kafka image tag

底層執行的 kafka 是 conluent kafka

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configure-resource-requests-and-limits&#34;&gt;Configure resource requests and limits&lt;/h2&gt;

&lt;h2 id=&#34;ref-http-kubernetes-io-docs-user-guide-compute-resources&#34;&gt;ref: &lt;a href=&#34;http://kubernetes.io/docs/user-guide/compute-resources/&#34; target=&#34;_blank&#34;&gt;http://kubernetes.io/docs/user-guide/compute-resources/&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 4096Mi
  # requests:
  #   cpu: 100m
  #   memory: 1024Mi
kafkaHeapOptions: &amp;ldquo;-Xmx4G -Xms1G&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
這邊可以調整在 kubernetes 上面的 limit 跟 request

* Deploy 會先去跟 node 問夠不夠，夠的話要求 node 保留這些資源給 Pod
* Runtime 超過 limit，Pod 會被 kubernetes 幹掉，不過我們是 JVM，外部 resource 爆掉前，應該會先因 heap 滿而死。一個施主自盡的感覺。
* CPU 蠻省的，吃比較多是 memory。但也要看你的使用情境

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;prometheus&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
對我們有上 promethues，基本上就是 kafka-exporter 把 kafka metrics 倒出去 prometheus，這個也是詳見下回分解。

# 跑起來了

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$ kubectl get po | grep kafka&lt;/p&gt;

&lt;p&gt;NAME                                                     READY   STATUS      RESTARTS   AGE
kafka-1-0                                                &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-1                                                &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-2                                                &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-exporter-88786d84b-z954z                         &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-zookeeper-0                                      &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-zookeeper-1                                      &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
kafka-1-zookeeper-2                                      &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;1&lt;/sub&gt;     Running     0          224d
```&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logstash on GKE</title>
      <link>https://chechiachang.github.io/post/logstash-on-gke/</link>
      <pubDate>Sat, 21 Sep 2019 15:22:23 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/logstash-on-gke/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;簡介 logstash&lt;/li&gt;
&lt;li&gt;將 logstash 部屬到 kubernetes 上&lt;/li&gt;
&lt;li&gt;設定 logstash pipeline 處理 nginx access log&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;介紹-logstash&#34;&gt;介紹 Logstash&lt;/h1&gt;

&lt;p&gt;Logstash 是開元的資料處理引擎，可以動態的將輸入的資料做大量的處裡。原先的目的是處理 log ，但目前以不限於處理 log ，各種 ELK beat 或是其他來源的不同監測數據，都能處理。&lt;/p&gt;

&lt;p&gt;Logastash 內部的功能也大多模組化，因此可以組裝不同的 plugin，來快速處理不同來源資料。&lt;/p&gt;

&lt;p&gt;基本上常見的資料來源，logstash 都能夠處理，並且有寫好的 plugin 可以直接使用，細節請見&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/introduction.html&#34; target=&#34;_blank&#34;&gt;logstash 官方文件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.elastic.co/guide/en/logstash/current/static/images/logstash.png&#34; alt=&#34;官方架構圖&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;後送資料庫與最終儲存庫&#34;&gt;後送資料庫與最終儲存庫&lt;/h1&gt;

&lt;p&gt;在開始架設 logstash 要先考慮 pipeline 處理過後送的資料庫，&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/introduction.html#_choose_your_stash&#34; target=&#34;_blank&#34;&gt;可使用的資料庫非常多&lt;/a&gt;，這邊會展示的有：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stack 標準配備送到 Elasticsearch

&lt;ul&gt;
&lt;li&gt;存放會時常查詢的熱資料，只存放一段時間前的資料&lt;/li&gt;
&lt;li&gt;太舊的資料自動 Rollout&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;最終 archieving 的資料庫，這邊使用 GCP 的 Big Query

&lt;ul&gt;
&lt;li&gt;存放查找次數少，但非常大量的歷史紀錄。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Elasticsearch 在前幾篇已經架設好，&lt;a href=&#34;https://cloud.google.com/bigquery/docs/?hl=zh-tw&#34; target=&#34;_blank&#34;&gt;GCP Big Query&lt;/a&gt; 的設定也事先開好。&lt;/p&gt;

&lt;h1 id=&#34;部屬-logstash&#34;&gt;部屬 Logstash&lt;/h1&gt;

&lt;p&gt;kubernetes resource 的 yaml 請參考 &lt;a href=&#34;https://github.com/chechiachang/elk-kubernetes/tree/master/logstash&#34; target=&#34;_blank&#34;&gt;我的 github elk-kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f config-configmap.yaml
kubectl apply -f pipelines-configmap.yam

kubectl apply -f deployment.yaml

kubectl apply -f service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;放上去的 resource&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;config-configmap:

&lt;ul&gt;
&lt;li&gt;Logstash 服務本身啟動的設定參數&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;pipelines-configmap:

&lt;ul&gt;
&lt;li&gt;Logstash 的 pipelines 設定檔案&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Lostagh Deployment

&lt;ul&gt;
&lt;li&gt;Logastash 的服務 instance&lt;/li&gt;
&lt;li&gt;可以動態 scaling，也就是會有複數 Logstash instance 做負載均衡&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Logstash service

&lt;ul&gt;
&lt;li&gt;可透過 kubernetes 內部的 kube-dns 服務&lt;/li&gt;
&lt;li&gt;集群內的 filebeat 可以直接透過 logstash.default.svc.chechiachang-cluster.local 的 dns 連線 logstash&lt;/li&gt;
&lt;li&gt;集群內的網路，直接使用 http（當然使用 https 也是可以，相關步驟請見前幾篇文章）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;簡單講一下 kubernetes service 的負載均衡，關於 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34;&gt;kubernetes service 細節這篇附上文件&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get services

NAME              TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)    AGE
logstash          ClusterIP      10.15.254.47    &amp;lt;none&amp;gt;          5044/TCP   182d

$ kubectl get endpoints

NAME              ENDPOINTS                                                          AGE
logstash          10.12.0.132:5044,10.12.10.162:5044,10.12.9.167:5044 + 12 more...   182d
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;在 Kubernetes 內部每個 Pod 都能看到 logstash, logstash.default.svc.chechiachang-cluster.local 這兩個 dns&lt;/li&gt;
&lt;li&gt;DNS 直接指向複數的 logstash endpoints， 每一個 ip 都是 kubernetes 內部配置的一個 Pod 的 IP，開啟 5044 的 logstash port&lt;/li&gt;
&lt;li&gt;Service 的 load balance 機制視 service 設定，細節可以看&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies&#34; target=&#34;_blank&#34;&gt;這邊&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;講到最白，就是 filebeat LOGSTASH URL 設定為 &lt;a href=&#34;http://logstash&#34; target=&#34;_blank&#34;&gt;http://logstash&lt;/a&gt; 就會打到其中一台 logstash&lt;/p&gt;

&lt;p&gt;更改 filebeat configmap&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl edit configmap filebeat-configmap

# Disable output to elasticsearch
output.elasticsearch:
  enabled: false

# Output to logstash
output.logstash:
  hosts: [&amp;quot;logstash:5044&amp;quot;]
  protocol: &amp;quot;http&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: 

&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;設定-logstash&#34;&gt;設定 logstash&lt;/h1&gt;

&lt;p&gt;這邊要先說，logstash 也支援 &lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.3/configuring-centralized-pipelines.html&#34; target=&#34;_blank&#34;&gt;centralized configuration&lt;/a&gt;，如果你的 logstash 不是跑在 Kubernetes 上，沒辦法配置一套 configmap 就應用到全部的 instance，記的一定要使用。&lt;/p&gt;

&lt;p&gt;Logastash 的運行設定 logstash.yml，這邊我們沒有做設定，都是預設值，有需求可以自行更改&lt;/p&gt;

&lt;p&gt;當然之後要調整 batch size 或是 queue, cache 等等效能調校，也是來這邊改，改完 configmap ，rolling update logstash 就可以。&lt;/p&gt;

&lt;p&gt;這邊主要是來講 pipeline 設定。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe configmap pipelines-configmap

apiVersion: v1
kind: ConfigMap
metadata:
  name: logstash-pipelines
  namespace: elk
  labels:
    k8s-app: logstash
data:
  # Nginx Template
  # https://www.elastic.co/guide/en/logstash/7.3/logstash-config-for-filebeat-modules.html#parsing-nginx
  nginx.conf: |
  ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Configmap 裡面只有一個 pipeline，就是 &lt;code&gt;nginx.conf&lt;/code&gt;，我們這邊就只有一條，這邊一段一段看&lt;/p&gt;

&lt;h3 id=&#34;input&#34;&gt;Input&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;input {
  beats {
    # The lisening port of logstash
    port =&amp;gt; 5044
    host =&amp;gt; &amp;quot;0.0.0.0&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;設定 Input 來源，是 beat 從 5044 進來&lt;/p&gt;

&lt;h3 id=&#34;filter&#34;&gt;Filter&lt;/h3&gt;

&lt;p&gt;接下來一大段是 filter，每個 filter 中間的 block 都是一個 plugin，logstash 支援非常多有趣的 plugin ，處理不同來源的工作，&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.3/filter-plugins.html&#34; target=&#34;_blank&#34;&gt;細節請看這篇&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filter {

  # Ignore data from other source in case filebeat input is incorrectly configured.
  if [kubernetes][container][name] == &amp;quot;nginx-ingress-controller&amp;quot; {

    # Parse message with grok
    # Use grok debugger in kibana -&amp;gt; dev_tools -&amp;gt; grok_debugger
    grok {
      match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{IPORHOST:[nginx][access][remote_ip]} - \[%{IPORHOST:[nginx][access][remote_ip_list]}\] - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \&amp;quot;%{WORD:[nginx][access][method]} %{DATA:[nginx][access][request_url]} HTTP/%{NUMBER:[nginx][access][http_version]}\&amp;quot; %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \&amp;quot;%{DATA:[nginx][access][referrer]}\&amp;quot; \&amp;quot;%{DATA:[nginx][access][agent]}\&amp;quot; %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \[%{DATA:[nginx][access][proxy_upstream_name]}\] %{DATA:[nginx][access][upstream_addr]} %{NUMBER:[nginx][access][upstream_response_length]} %{NUMBER:[nginx][access][upstream_response_time]} %{NUMBER:[nginx][access][upstream_status]} %{DATA:[nginx][access][req_id]}&amp;quot; }
    }

    # Match url parameters if has params
    grok {
      match =&amp;gt; { &amp;quot;[nginx][access][request_url]&amp;quot; =&amp;gt; &amp;quot;%{DATA:[nginx][access][url]}\?%{DATA:[nginx][access][url_params]}&amp;quot; }
    }

    # Remove and add fields
    mutate {
      remove_field =&amp;gt; &amp;quot;[nginx][access][request_url]&amp;quot;
      add_field =&amp;gt; { &amp;quot;read_timestamp&amp;quot; =&amp;gt; &amp;quot;%{@timestamp}&amp;quot; }
      # Add fileset.module:nginx to fit nginx dashboard
      add_field =&amp;gt; { &amp;quot;[fileset][module]&amp;quot; =&amp;gt; &amp;quot;nginx&amp;quot;}
      add_field =&amp;gt; { &amp;quot;[fileset][name]&amp;quot; =&amp;gt; &amp;quot;access&amp;quot;}
    }

    # Parse date string into timestamp
    date {
      match =&amp;gt; [ &amp;quot;[nginx][access][time]&amp;quot;, &amp;quot;dd/MMM/YYYY:H:m:s Z&amp;quot; ]
      remove_field =&amp;gt; &amp;quot;[nginx][access][time]&amp;quot;
    }

    # Split url_parameters with &amp;amp;
    # /api?uuid=123&amp;amp;query=456 
    # become 
    # nginx.access.url_params.uuid=123 nginx.access.url_params.query=456
    kv {
      source =&amp;gt; &amp;quot;[nginx][access][url_params]&amp;quot;
      field_split =&amp;gt; &amp;quot;&amp;amp;&amp;quot;
    }

    # Parse useragent
    useragent {
      source =&amp;gt; &amp;quot;[nginx][access][agent]&amp;quot;
      target =&amp;gt; &amp;quot;[nginx][access][user_agent]&amp;quot;
      remove_field =&amp;gt; &amp;quot;[nginx][access][agent]&amp;quot;
    }

    # Search remote_ip with GeoIP database, output geoip information for map drawing
    geoip {
      source =&amp;gt; &amp;quot;[nginx][access][remote_ip]&amp;quot;
      target =&amp;gt; &amp;quot;[nginx][access][geoip]&amp;quot;
      #fields =&amp;gt; [&amp;quot;country_name&amp;quot;,&amp;quot;city_name&amp;quot;,&amp;quot;real_region_name&amp;quot;,&amp;quot;latitude&amp;quot;,&amp;quot;longitude&amp;quot;,&amp;quot;ip&amp;quot;,&amp;quot;location&amp;quot;]
    }

    # ==============
    # Remove message to reduce data
    # ==============
    if [nginx][access][url] {
      mutate {
        # source:/var/lib/docker/containers/6e608bfc0a437c038a1dbdf2e3d28619648b58a1d1ac58635f8178fc5f871109/6e608bfc0a437c038a1dbdf2e3d28619648b58a1d1ac58635f8178fc5f871109-json.log
        remove_field =&amp;gt; &amp;quot;[source]&amp;quot;
        # Origin message
        remove_field =&amp;gt; &amp;quot;[message]&amp;quot;
        #add_field =&amp;gt; { &amp;quot;[nginx][access][message]&amp;quot; =&amp;gt; &amp;quot;[message]&amp;quot;}
        remove_field =&amp;gt; &amp;quot;[nginx][access][message]&amp;quot;
        # url_params:client_id=1d5ffd378296c154d3e32e5890d6f4eb&amp;amp;timestamp=1546849955&amp;amp;nonce=9a52e3e6283f2a9263e5301b6724e2c0d723def860c4724c9121470152a42318
        remove_field =&amp;gt; &amp;quot;[nginx][access][url_params]&amp;quot;
      }
    }

  } # nginx-ingress-controller

} # filter
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;grok&#34;&gt;Grok&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.3/plugins-filters-grok.html&#34; target=&#34;_blank&#34;&gt;Grok 本身的文件&lt;/a&gt;又是一大段，個人建議各路大德，如果要使用，請直接搜尋人家配置好的設定，不要自己寫&lt;/p&gt;

&lt;p&gt;真的要寫的話要善用工具&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kibana Grok Debugger &lt;code&gt;YOUR_KIBANA_HOST/app/kibana#/dev_tools/grokdebugger&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;或是不知名大德貢獻&lt;a href=&#34;https://grokdebug.herokuapp.com/&#34; target=&#34;_blank&#34;&gt;線上 Debugger&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;grok {
match =&amp;gt; { &amp;quot;message&amp;quot; =&amp;gt; &amp;quot;%{IPORHOST:[nginx][access][remote_ip]} - \[%{IPORHOST:[nginx][access][remote_ip_list]}\] - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \&amp;quot;%{WORD:[nginx][access][method]} %{DATA:[nginx][access][request_url]} HTTP/%{NUMBER:[nginx][access][http_version]}\&amp;quot; %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \&amp;quot;%{DATA:[nginx][access][referrer]}\&amp;quot; \&amp;quot;%{DATA:[nginx][access][agent]}\&amp;quot; %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \[%{DATA:[nginx][access][proxy_upstream_name]}\] %{DATA:[nginx][access][upstream_addr]} %{NUMBER:[nginx][access][upstream_response_length]} %{NUMBER:[nginx][access][upstream_response_time]} %{NUMBER:[nginx][access][upstream_status]} %{DATA:[nginx][access][req_id]}&amp;quot; }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其實就是 nginx 的 access log&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1.2.3.4 - [1.2.3.4] - - [21/Sep/2019:07:21:21 +0000] &amp;quot;GET /v1/core/api/list?type=queued&amp;amp;timestamp=1569050481&amp;amp;nonce=d1e80e00381e0ba6e42d4601912befcf03fbf291748e77b178230c19cd1fdbe2 HTTP/1.1&amp;quot; 200 3 &amp;quot;-&amp;quot; &amp;quot;python-requests/2.18.4&amp;quot; 425 0.031 [default-chechiachang-server-80] 10.12.10.124:8003 3 0.031 200 f43db228afe66da67b2c7417d0ad2c04
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;預設的 log 送件來，格式是 text，經過 pattern matching 後變成 json-like format，也就是可以從資料結構取得 &lt;code&gt;.nginx.access.remote_ip&lt;/code&gt; 這樣的欄位，讓原本的 access log 從 text 變成可以查找的內容。&lt;/p&gt;

&lt;p&gt;原本的 text 送進 elasticsearch 當然也可以查找，但就會在 text 裡面做全文檢索，功能很侷限，效率很差。&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;logstash 支援的 output 以及設定&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/7.3/output-plugins.html&#34; target=&#34;_blank&#34;&gt;在這邊&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output {
  elasticsearch {
    hosts =&amp;gt; [&amp;quot;https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}&amp;quot;]
    user =&amp;gt; &amp;quot;${ELASTICSEARCH_USERNAME}&amp;quot;
    password =&amp;gt; &amp;quot;${ELASTICSEARCH_PASSWORD}&amp;quot;
    index =&amp;gt; &amp;quot;%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}&amp;quot;
    manage_template =&amp;gt; false
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Elasticsearch 的配置很單純&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output {
  google_bigquery {
    project_id =&amp;gt; ${GCP_PROJECT_ID}
    dataset =&amp;gt; ${GCP_BIG_QUERY_DATASET_NAME}
    csv_schema =&amp;gt; &amp;quot;path:STRING,status:INTEGER,score:FLOAT&amp;quot;
    json_key_file =&amp;gt; ${GCP_JSON_KEY_FILE_PATH}
    error_directory =&amp;gt; &amp;quot;/tmp/bigquery-errors&amp;quot;
    date_pattern =&amp;gt; &amp;quot;%Y-%m-%dT%H:00&amp;quot;
    flush_interval_secs =&amp;gt; 30
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中的變數，我們全都用環境變數，在 deployment.yaml 配置，啟動 logstash pods 時代入&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GCP_JSON_KEY_FILE_PATH&lt;/code&gt; 這邊要配置一隻 GCP 的服務帳號金鑰，一個有 Big Query 寫入權限的 service account，把 json 使用 kubernetes secret 放到集群上，然後在 pod 上使用 volume from secret 掛載進來。
&lt;code&gt;csv_schema =&amp;gt; &amp;quot;path:STRING,status:INTEGER,score:FLOAT&amp;quot;&lt;/code&gt; 這邊要配置之後會存入 Big Query 的 csv 結構&lt;/p&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;部屬 Logstash deployment 到 kubernetes 上&lt;/li&gt;
&lt;li&gt;設定 pipeline，超多 plugin，族繁不及備載&lt;/li&gt;
&lt;li&gt;Grok 配置&lt;/li&gt;
&lt;li&gt;Big Query output 配置&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring GKE With Elk</title>
      <link>https://chechiachang.github.io/post/monitoring-gke-with-elk/</link>
      <pubDate>Thu, 19 Sep 2019 17:06:29 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/monitoring-gke-with-elk/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/running-on-kubernetes.html&#34; target=&#34;_blank&#34;&gt;官方文件&lt;/a&gt; ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。&lt;/p&gt;

&lt;p&gt;這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics)&lt;/li&gt;
&lt;li&gt;cluster: 處理 cluster 等級的 log,  event 或是 metrics&lt;/li&gt;
&lt;li&gt;pod: 針對特定 pod 直接去掛一個 sidecar&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的方法是可以混搭的，kubernetes 個個層級有&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34; target=&#34;_blank&#34;&gt;log 處理流程&lt;/a&gt;，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。&lt;/p&gt;

&lt;p&gt;簡單來說，是去對的地方找對的 log。在架構上要注意 scalability 與 resource 分配，不要影響本身提供服務的 GKE ，但又能獲得盡量即時的 log。&lt;/p&gt;

&lt;p&gt;我們這邊直接進入 kubernetes resource 的設定，底下會附上在 GKE 找 log 的過程。&lt;/p&gt;

&lt;h1 id=&#34;node-level-log-harvest&#34;&gt;Node level log harvest&lt;/h1&gt;

&lt;p&gt;為每一個 node 配置 filebeat，然後在 node 上面尋找 log，然後如我們上篇所敘述加到 input ，就可以把 log 倒出來。&lt;/p&gt;

&lt;p&gt;直覺想到就是透過 daemonsets 為每個 node 部署一個 filebeat pod，然後 mount node 的 log 資料夾，在設置 input。&lt;/p&gt;

&lt;h1 id=&#34;deploy-daemonsets&#34;&gt;Deploy daemonsets&lt;/h1&gt;

&lt;p&gt;kubernetes resource 的 yaml 請參考 &lt;a href=&#34;https://github.com/chechiachang/elk-kubernetes/tree/master/filebeat/7.3.1&#34; target=&#34;_blank&#34;&gt;我的 github elk-kubernetes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;給予足夠的 clusterrolebinding 到 elk&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/clusterrolebinding.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先更改 filebeat 的設定，如何設定 elasticsearch 與 kibana，請參考上篇。至於 input 的部份已經配置好了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim filebeat/7.3.1/daemonsets-config-configmap.yaml

kubectl apply -f filebeat/7.3.1/daemonsets-config-configmap.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部屬 filebeat daemonsets，會每一個 node 部屬一個 filebeat&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/daemonsets.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;取得 daemonsets 的狀態&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespcae elk get pods

NAME             READY   STATUS    RESTARTS   AGE
filebeat-bjfp9   1/1     Running   0          6m56s
filebeat-fzr9n   1/1     Running   0          6m56s
filebeat-vpkm7   1/1     Running   0          6m56s
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log&lt;/p&gt;

&lt;h1 id=&#34;log-havest-for-specific-pods&#34;&gt;log havest for specific pods&lt;/h1&gt;

&lt;p&gt;由於 kubernetes 上我們可以便利的調度 filebeat 的部屬方式，這邊也可以也可以使用 deployment ，配合 pod affinity，把 filebeat 放到某個想要監測的 pod，這邊的例子是 nginx-ingress-controller。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes 上有一個或多個 nginx ingress controller&lt;/li&gt;
&lt;li&gt;部屬一個或多個 filebeat 到有 nginx 的 node 上&lt;/li&gt;
&lt;li&gt;filebeat 去抓取 nginx 的 input， 並使用 filebeat 的 nginx module 做預處理

&lt;ul&gt;
&lt;li&gt;nginx module 預設路徑需要調整，這邊使用 filebeat autodiscover 來處理&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一樣 apply 前記得先檢查跟設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim filebeat/7.3.1/nginx-config-configmap.yaml

kubectl apply -f filebeat/7.3.1/nginx-config-configmap.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;部屬 filebeat deployment
由於有設定 pod affinity ，這個 filebeat 只會被放到有 nginx ingress controller 的這個節點上，並且依照 autodiscover 設定的條件去蒐集 nginx 的 log&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log&lt;/p&gt;

&lt;p&gt;另外，由於有啟動 nginx module，logstash 收到的內容已經是處理過得內容。&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;gcp-fluentd&#34;&gt;GCP fluentd&lt;/h1&gt;

&lt;p&gt;如果是使用 GKE 的朋友，可以投過開啟 stackdriver logging 的功能，把集群中服務的 log 倒到 stackdriver，基本上就是 node -&amp;gt; (daemonsets) fluentd -&amp;gt; stackdriver。&lt;/p&gt;

&lt;p&gt;這個 fluentd 是 GCP 如果有啟動 Stackdriver Logging 的話，自動幫你維護的 daemonsets，設定不可改，改了會被 overwrite 會去，所以不太方便從這邊動手腳。&lt;/p&gt;

&lt;p&gt;Btw stackdriver 最近好像改版，目前做 example 的版本已經變成 lagency （淚&lt;/p&gt;

&lt;p&gt;但我們先假設我們對這個 pod 的 log 很有興趣，然後把這邊的 log 透過 filebeat 送到 ELK 上XD&lt;/p&gt;

&lt;p&gt;因為 GKE 透過 fluentd 把 GKE 上面的 log 倒到 stackdriver，而我們是想把 log 倒到 ELK，既然這樣我們的 input 來源是相同的，而且很多處理步驟都可以在 ELK 上面互通，真的可以偷看一下 fluentd 是去哪收集 log ，怎麼處理 log pipeline，我們只要做相應設定就好。&lt;/p&gt;

&lt;p&gt;畢竟 google 都幫我們弄得妥妥的，不參考一下他的流程太可惜。&lt;/p&gt;

&lt;p&gt;偷看一下 GKE 上 fluentd 是去哪找 log ，這個是 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-gcp/fluentd-gcp-configmap.yaml&#34; target=&#34;_blank&#34;&gt;fluentd gcp configmap&lt;/a&gt;，雖然看到這邊感覺扯遠了，但因為很有趣所有我就繼續看下去，各位大德可以跳過XD&lt;/p&gt;

&lt;p&gt;configmap 中的這個 input 設定檔，其中一個 source 就是一個資料來源，相當於 filebeat 的 input。這邊這個 source 就是去 &lt;code&gt;/var/log/containers/*.log&lt;/code&gt;  收 log&lt;/p&gt;

&lt;p&gt;這邊還做了幾件事：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;打上 &lt;code&gt;reform.*&lt;/code&gt; tag，讓下個 match 可以 收進去 pipeline 處理&lt;/li&gt;

&lt;li&gt;&lt;p&gt;附帶 parse 出 time&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;containers.input.conf

&amp;lt;source&amp;gt;
@type tail
path /var/log/containers/*.log
pos_file /var/log/gcp-containers.log.pos
# Tags at this point are in the format of:
# reform.var.log.containers.&amp;lt;POD_NAME&amp;gt;_&amp;lt;NAMESPACE_NAME&amp;gt;_&amp;lt;CONTAINER_NAME&amp;gt;-&amp;lt;CONTAINER_ID&amp;gt;.log
tag reform.*
read_from_head true
&amp;lt;parse&amp;gt;
@type multi_format
&amp;lt;pattern&amp;gt;
  format json
  time_key time
  time_format %Y-%m-%dT%H:%M:%S.%NZ
&amp;lt;/pattern&amp;gt;
&amp;lt;pattern&amp;gt;
  format /^(?&amp;lt;time&amp;gt;.+) (?&amp;lt;stream&amp;gt;stdout|stderr) [^ ]* (?&amp;lt;log&amp;gt;.*)$/
  time_format %Y-%m-%dT%H:%M:%S.%N%:z
&amp;lt;/pattern&amp;gt;
&amp;lt;/parse&amp;gt;
&amp;lt;/source&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;他這邊做一些 error handling，然後用 ruby (!) parse，這邊就真的太遠，細節大家可以 google ＸＤ。不過這邊使用的 pattern matching 我們後幾篇在 logstash pipeline 上，也會有機會提到，機制是類似的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;filter reform.**&amp;gt;
  @type parser
  format /^(?&amp;lt;severity&amp;gt;\w)(?&amp;lt;time&amp;gt;\d{4} [^\s]*)\s+(?&amp;lt;pid&amp;gt;\d+)\s+(?&amp;lt;source&amp;gt;[^ \]]+)\] (?&amp;lt;log&amp;gt;.*)/
  reserve_data true
  suppress_parse_error_log true
  emit_invalid_record_to_error false
  key_name log
&amp;lt;/filter&amp;gt;

&amp;lt;match reform.**&amp;gt;
  @type record_reformer
  enable_ruby true
  &amp;lt;record&amp;gt;
    # Extract local_resource_id from tag for &#39;k8s_container&#39; monitored
    # resource. The format is:
    # &#39;k8s_container.&amp;lt;namespace_name&amp;gt;.&amp;lt;pod_name&amp;gt;.&amp;lt;container_name&amp;gt;&#39;.
    &amp;quot;logging.googleapis.com/local_resource_id&amp;quot; ${&amp;quot;k8s_container.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[1]}.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[0]}.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[2].rpartition(&#39;-&#39;)[0]}&amp;quot;}
    # Rename the field &#39;log&#39; to a more generic field &#39;message&#39;. This way the
    # fluent-plugin-google-cloud knows to flatten the field as textPayload
    # instead of jsonPayload after extracting &#39;time&#39;, &#39;severity&#39; and
    # &#39;stream&#39; from the record.
    message ${record[&#39;log&#39;]}
    # If &#39;severity&#39; is not set, assume stderr is ERROR and stdout is INFO.
    severity ${record[&#39;severity&#39;] || if record[&#39;stream&#39;] == &#39;stderr&#39; then &#39;ERROR&#39; else &#39;INFO&#39; end}
  &amp;lt;/record&amp;gt;
  tag ${if record[&#39;stream&#39;] == &#39;stderr&#39; then &#39;raw.stderr&#39; else &#39;raw.stdout&#39; end}
  remove_keys stream,log
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ssh-進去逛&#34;&gt;ssh 進去逛&lt;/h3&gt;

&lt;p&gt;想看機器上實際的 log 狀況，我們也可以直接 ssh 進去&lt;/p&gt;

&lt;p&gt;先透過 kubectl 看一下 pod&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get daemonsets --namespace kube-system

NAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                  AGE
fluentd-gcp-v3.2.0         7         7         7       7            7           beta.kubernetes.io/fluentd-ds-ready=true       196d

$ kubectl get pods --output wide --namespace kube-system

NAME                                      READY   STATUS    RESTARTS   AGE   IP          NODE                                     NOMINATED NODE   READINESS GATES
fluentd-gcp-scaler-1234567890-vfbhc       1/1     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-5gqn   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-44tl7                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-wcq0   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-5vc6l                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-tp05   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-6rqvc                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-5gqn   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-mmwk4                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-vxld   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先透過 kubectl 看一下 node&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get node

NAME                                     STATUS   ROLES    AGE   VERSION
gke-chechaichang-pool-1-123456789-3bzp   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
gke-chechaichang-pool-1-123456789-5gqn   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
gke-chechaichang-pool-1-123456789-8n8z   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
...

gcloud compute ssh gke-chechaichang-pool-1-123456789-3bzp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如使用其他雲平台的 kubernetes service，或是 bare metal 的集群，請依照各自系統的方式連進去看看。&lt;/p&gt;

&lt;h1 id=&#34;ssh-node-找-log&#34;&gt;ssh node 找 log&lt;/h1&gt;

&lt;p&gt;ssh 進去後就可以到處來探險，順便看看 GKE 跑在機器上到底做了什麼事情。&lt;/p&gt;

&lt;p&gt;如果官方有出文件，可能可以不用進來看。各位大德有發現文件請留言跟我說。我個人很喜歡自己架集群起來連就去看，面對照官方文件上寫的東西，當然大部份時候都是文件沒有帶到，有很多發現。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls /var/log

gcp-*-log.pos
kube-proxy.log
containers/
metrics/
pods/
...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;/var/log/containers 看一下，格式是 &lt;code&gt;pod_namespace_container&lt;/code&gt; 這邊是 link 到 /var/log/pods/&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ls -al /var/log/containers

lrwxrwxrwx 1 root root   105 Aug 12 07:42 fluentd-gcp-v3.2.0-st6cl_kube-system_fluentd-gcp-5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac.log -&amp;gt; /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/0.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看到 pods 就覺得是你了，裡面有 pod 資料夾，格式是 &lt;code&gt;namespace_pod_uuid&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls /var/log/pods

default_pod-1-1234567890-fxxhp_uuid
kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008
kube-system_heapster-v1.6.0-beta.1-
kube-system_kube-proxy-gke-
kube-system_l7-default-backend-
kube-system_prometheus-to-sd-
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再進去有 container log，格式是 &lt;code&gt;pod_namespace_container.log&lt;/code&gt;，也是 link&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ls -al /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/

lrwxrwxrwx 1 root root  165 Aug 12 07:42 0.log -&amp;gt; /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最終 link 到&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo su

$ ls -alh /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/
total 3.9M
drwx------  4 root root 4.0K Aug 12 07:42 .
drwx------ 92 root root  20K Sep 18 11:28 ..
-rw-r-----  1 root root 3.8M Sep 18 11:29 5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
drwx------  2 root root 4.0K Aug 12 07:42 checkpoints
-rw-------  1 root root 7.8K Aug 12 07:42 config.v2.json
-rw-r--r--  1 root root 2.3K Aug 12 07:42 hostconfig.json
drwx------  2 root root 4.0K Aug 12 07:42 mounts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;頭尾偷喵一下，確定是我們在找的東西&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;head /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
tail /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;這樣就找到我們的 log 了&lt;/p&gt;

&lt;h1 id=&#34;小節&#34;&gt;小節&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;使用 filebeat 去查找&lt;/li&gt;
&lt;li&gt;透過 kubernetes daemonsets 可以快速佈置一份 filebeat 到所有 node，且設定都是一起更新&lt;/li&gt;
&lt;li&gt;透過 kubernetes deployment 可以指定 filebeat 的位置，去跟隨想要監測的服務&lt;/li&gt;
&lt;li&gt;如果不熟 log 處理流程，可以直接看偷看大廠的服務，會有很多靈感&lt;/li&gt;
&lt;li&gt;沒事可以多跑進 Kubernetes 服務節點逛逛，有很多有趣的東西&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring GCE With ELK</title>
      <link>https://chechiachang.github.io/post/monitoring-gce-with-elk/</link>
      <pubDate>Wed, 18 Sep 2019 19:10:50 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/monitoring-gce-with-elk/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ELK 的 beats 是輕量級的系統監測收集器，beats 收集到的 data 經過 mapping 可以送到 Elasticsearch 後，進行彈性的搜尋比對。&lt;/p&gt;

&lt;p&gt;beat 有許多種類，依據收集的 data 區別：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Auditbeat: Audit data&lt;/li&gt;
&lt;li&gt;Filebeat: Log files&lt;/li&gt;
&lt;li&gt;Functionbeat: Cloud data&lt;/li&gt;
&lt;li&gt;Heartbeat: Availability&lt;/li&gt;
&lt;li&gt;Journalbeat: Systemd journals&lt;/li&gt;
&lt;li&gt;Metricbeat: Metrics&lt;/li&gt;
&lt;li&gt;Packetbeat: Network traffic&lt;/li&gt;
&lt;li&gt;Winlogbeat: Windows event logs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這邊先以 filebeat 為例，在 GCE 上收集圓端服務節點上的服務日誌與系統日誌，並在 ELK 中呈現。&lt;/p&gt;

&lt;h1 id=&#34;installation&#34;&gt;Installation&lt;/h1&gt;

&lt;p&gt;安裝及 filebeat 安全性設定的步驟，在這篇[Secure ELK Stack]() 中已經說明。這邊指附上連結，以及&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-getting-started.html&#34; target=&#34;_blank&#34;&gt;官方文件&lt;/a&gt; 提供參考。&lt;/p&gt;

&lt;h1 id=&#34;configuration&#34;&gt;Configuration&lt;/h1&gt;

&lt;p&gt;這邊談幾個使用方面的設定。&lt;/p&gt;

&lt;p&gt;首先，apt 安裝的 filebeat 預設的 /etc/filebeat/filebeat.yml 不夠完整，我們先到 github 把對應版本的完整載下來。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget https://raw.githubusercontent.com/elastic/beats/master/filebeat/filebeat.reference.yml
sudo mv filebeat.reference.yml /etc/filebeat/filebeat.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;beats-central-management&#34;&gt;Beats central management&lt;/h1&gt;

&lt;p&gt;beats 透過手動更改 config 都可以直接設定，但這邊不推薦在此設定，理由是&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;系統中通常會有大量的 filebeat，每個都要設定，數量多時根本不可能&lt;/li&gt;
&lt;li&gt;更改設定時，如果不一起更改，會造成資料格式不統一，之後清理也很麻煩&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;推薦的方式是透過 Kibana 對所有 filebeat 做集中式的的管理配置，只要初始設定連上 kibana，剩下的都透過 kibana 設定。&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/configuration-central-management.html&#34; target=&#34;_blank&#34;&gt;文件在此&lt;/a&gt;，我們有空有可以分篇談這個主題。&lt;/p&gt;

&lt;p&gt;不過這邊還是待大家過一下幾個重要的設定。畢竟要在 kibana 上配置，filebeat 的設定概念還是要有。&lt;/p&gt;

&lt;h3 id=&#34;modules&#34;&gt;modules&lt;/h3&gt;

&lt;p&gt;filebeat 有許多&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-modules.html&#34; target=&#34;_blank&#34;&gt;模組&lt;/a&gt;，裡面已經包含許多預設的 template ，可以直接使用 default 的設定去系統預設的路徑抓取檔案，並且先進一步處理，減少我們輸出到 logstash 還要再做 pipeline 預處理，非常方便。&lt;/p&gt;

&lt;p&gt;例如這個 system module 會處理系統預設的 log 路徑，只要開啟 module ，就會自動處理對應的 input。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- module: system
  syslog:
    enabled: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;剩下的就是照需求啟用 module ，並且給予對應的 input。&lt;/p&gt;

&lt;p&gt;ELK 為自己的服務設定了不少 module ，直接啟用就可以獲取這協服務元件運行的 log 與監測數值。這也是 self-monitoring 監測數據的主要來源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- module: kibana
- module: elasticsearch
- module: logstash
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;input&#34;&gt;input&lt;/h3&gt;

&lt;p&gt;filebeat 支援複數 inputs，每個 input 會啟動一個收集器，而 filebeat 收集目標是 log 檔案。基本上可以簡單理解為 filebeat 去讀取這些 log 檔案，並且在系統上紀錄讀取的進度，偵測到 log 有增加，變繼續讀取新的 log。&lt;/p&gt;

&lt;p&gt;filebeat 具體的工作機制，可以看這篇&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html&#34; target=&#34;_blank&#34;&gt;How Filebeat works?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;這篇文件也提到 filebeat 是確保至少一次(at-least-once delivery)的數據讀取，使用時要特別注意重複獲取的可能。&lt;/p&gt;

&lt;p&gt;首先把 input 加上 ubuntu 預設的 log 路徑&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/*.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;這邊注意 input 支援多種 type，參照完整設定檔案的說明配合自己的需求使用。&lt;/p&gt;

&lt;h3 id=&#34;processor&#34;&gt;Processor&lt;/h3&gt;

&lt;p&gt;在 filebeat 端先進行資料的第一層處理，可以大幅講少不必要的資料，降低檔案傳輸，以及對 elasticsearch server 的負擔。&lt;/p&gt;

&lt;h3 id=&#34;output&#34;&gt;output&lt;/h3&gt;

&lt;p&gt;output 也是 filebeat 十分重要的一環，好的 filebeat output 設定，可以大幅降低整體 ELK stack 的負擔。壞的設定也會直接塞爆 ELK stask。&lt;/p&gt;

&lt;p&gt;output.elasticsearch: 直接向後送進 elasticsearch
output.logstash: 先向後送到 logstash&lt;/p&gt;

&lt;p&gt;這邊非常推薦大家，所有的 beat 往後送進 elasticsearch 之前都先過一層 logstash，就算你的 logstash 內部完全不更改 data，沒有 pipeline mutation，還是不要省這一層。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;beat 的數量會隨應用愈來越多而線性增加，elasticsearch 很難線性 scale，或是 scale 成本很大&lt;/li&gt;
&lt;li&gt;filebeat 沒有好好調校的話，對於輸出端的網路負擔很大，不僅佔用大量連線，傳輸檔案的大小也很大。&lt;/li&gt;
&lt;li&gt;logstash 的 queue 與後送的 batch 機制比 filebeat 好使用&lt;/li&gt;
&lt;li&gt;filebeat 是收 log 的，通常 log 爆炸的時候，是應用出問題的時候，這時候需要 log 交叉比對，發現 elasticsearch 流量也爆衝，反應很應用&lt;/li&gt;
&lt;li&gt;logstash 透過一些方法，可以很輕易的 scale，由於 pipeline 本身可以分散是平行處理，scale logstash 並不會影響資料最終狀態。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;load-balance&#34;&gt;load balance&lt;/h3&gt;

&lt;p&gt;有網友留言詢問 logstash 前面的 load balance 如何處理比較好，我這邊也順便附上。不只是 logstash ，所有自身無狀態(stateless) 的服務都可以照這樣去 scale。&lt;/p&gt;

&lt;p&gt;在 kubernetes 上很好處理，使用 k8s 預設的 service 就輕易作到簡易的 load balance
* 設置複數 logstash instances
* 使用 kubernetes 內部網路 service 實現 load balancing。&lt;/p&gt;

&lt;p&gt;在 GCE 上實現的話，我說實話沒實作過，所以以下是鍵盤實現XD。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/load-balancing.html&#34; target=&#34;_blank&#34;&gt;官方文件&lt;/a&gt; 建議使用 beats 端設定多個 logstash url 來做 load balancing。&lt;/p&gt;

&lt;p&gt;但我不是很喜歡 beat 去配置多個 logstash url 的作法：beat 要感知 logstash 數量跟 url ，增加減少 logstash instance 還要更改 beats 配置，產生配置的依賴跟耦合。&lt;/p&gt;

&lt;p&gt;最好是在 logstash 前過一層 HAproxy 或是雲端服務的 Load balancer（ex. GCP https/tcp load balancer），beat 直接送進 load balance 的端點。&lt;/p&gt;

&lt;h1 id=&#34;autodiscover&#34;&gt;autodiscover&lt;/h1&gt;

&lt;p&gt;如果有使用 container ，例如 docker 或 kubernetes，由於 container 內的 log 在主機上的位置是動態路徑，這邊可以使用 autodiscover 去尋找。&lt;/p&gt;

&lt;p&gt;在 kubernetes 上面的設定，之後會另開一天討論。&lt;/p&gt;

&lt;h1 id=&#34;dashboard&#34;&gt;dashboard&lt;/h1&gt;

&lt;p&gt;kibana 預設是空的，沒有預先載入 dashboard，但我們會希望資料送進去，就有設定好的 dashboard ，圖像化把資料呈現出來。這部份需要從 beat 這邊向 kibana 寫入。&lt;/p&gt;

&lt;p&gt;在上面的部份設定好 kibana 的連線資料，沒有設定的話 beat 啟動會警告。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;setup.dashboards.enabled: true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一起中就會檢查 kibana 是否有匯入 dashboard，沒有的話就匯入。&lt;/p&gt;

&lt;p&gt;也會一併匯入 modules 的 dashboard，例如如果有啟用 nginx module 處理 nginx 的 access log，nginx module 會處理 request source ip ，並透過 geoip database, 將 ip 轉會成經緯度座標。這時如果在 kibana 上有匯入 nginx dashboard，就可以看到圖像化的全球 request 分佈圖。&lt;/p&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;取得完整 filebeat 設定檔案並設定 filebeat&lt;/li&gt;
&lt;li&gt;盡量透過 beat central management 來管理 beat 的設定檔&lt;/li&gt;
&lt;li&gt;啟用對應 module 來更優雅的處理 log&lt;/li&gt;
&lt;li&gt;後送到 elasticsearch 前的資料都必須經過精細的處理，送進去後就不好刪改了&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ELK or Not ELK</title>
      <link>https://chechiachang.github.io/post/elastic-or-not-elastic/</link>
      <pubDate>Wed, 18 Sep 2019 18:51:40 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/elastic-or-not-elastic/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;監測 Google Compute Engine 上服務的各項數據&lt;/li&gt;
&lt;li&gt;監測 Google Kubernetes Engine 的各項數據&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;有板友問到，要如何選擇要不要用 ELK，其實也這是整篇 ELK 的初衷。這邊分享一下 ELK 與其他選擇，以及選擇解決方案應該考慮的事情。&lt;/p&gt;

&lt;h1 id=&#34;其他常用的服務&#34;&gt;其他常用的服務&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34; target=&#34;_blank&#34;&gt;Prometheus&lt;/a&gt;: 開源的 time series metrics 收集系統&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/?hl=zh-tw&#34; target=&#34;_blank&#34;&gt;Stackdriver&lt;/a&gt;: GCP 的 log 與 metrics 平台&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/cloud/&#34; target=&#34;_blank&#34;&gt;Elastic Cloud&lt;/a&gt;: ELK 的 Sass&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-hosted ELK&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;或是依照需求混搭，各個服務使用的各層套件是可以相容，例如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在 GKE 上不用 beat 可以用 fluentd&lt;/li&gt;
&lt;li&gt;Prometheus -&amp;gt; Stackdriver&lt;/li&gt;
&lt;li&gt;ELK -&amp;gt; Stackdriver&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Fluentd -&amp;gt; Prometheus
&amp;hellip;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Sass vs cloud self-hosted vs on-premised&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Metrics: ELK vs Prometheus vs Stackdriver&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Logging: ELK vs Stackdriver&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;取捨原則&#34;&gt;取捨原則&lt;/h1&gt;

&lt;p&gt;各個方法都各有利弊，完全取決於需求&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;已知條件限制，例如安全性考量就是要放在私有網路防火牆內，或是預算&lt;/li&gt;
&lt;li&gt;資料讀取方式，有沒有要交叉比對收集的資料，還是單純依照時間序查詢&lt;/li&gt;
&lt;li&gt;或是資料量非常大，應用數量非常多&lt;/li&gt;
&lt;li&gt;維護的團隊，有沒有想，或有沒有能力自己養 self-host 服務&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;sass-vs-self-hosted-vs-on-premised&#34;&gt;Sass vs Self-hosted vs On-premised&lt;/h1&gt;

&lt;p&gt;Sass: 指的是直接用 Elasitc Cloud，或是直接使用公有雲的服務(ex. 在 GCP 上使用 stackdriver)&lt;/p&gt;

&lt;p&gt;Cloud Self-hosted: 在公有雲上使用 ELK&lt;/p&gt;

&lt;p&gt;On-Premised: 自己在機房搭設&lt;/p&gt;

&lt;h3 id=&#34;安全性&#34;&gt;安全性&lt;/h3&gt;

&lt;p&gt;看公司的安全政策，允許將日誌及監控數據，送到私有網路以外的地方嗎？如果在防火牆內，搞不好 port 根本就不開給你，根本不用考慮使用外部服務。&lt;/p&gt;

&lt;p&gt;要知道服務的 log 其實可以看出很多東西．如果有特別做資料分析，敏感的資料，金流相關數據，通常不會想要倒到第三方服務平台。&lt;/p&gt;

&lt;p&gt;可能有做金流的，光是安全性這點，就必須選擇自架。&lt;/p&gt;

&lt;h3 id=&#34;成本&#34;&gt;成本&lt;/h3&gt;

&lt;p&gt;金錢成本 + 維護成本&lt;/p&gt;

&lt;p&gt;金錢成本就看各個服務的計費方式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/products/elasticsearch/service/pricing&#34; target=&#34;_blank&#34;&gt;Elastic Cloud Pricing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Self-hosted ELK &amp;amp; Prometheus：機器成本&lt;/li&gt;
&lt;li&gt;公有雲服務(ex. &lt;a href=&#34;https://cloud.google.com/stackdriver/pricing?hl=zh-tw&#34; target=&#34;_blank&#34;&gt;GCP Stackdriver&lt;/a&gt;): 用量計費&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;維護成本: 工程師的月薪 * 每個月要花在維護服務的工時比例&lt;/p&gt;

&lt;p&gt;一般 Sass 代管的服務，會降低維護成本，基本上就是做到網頁點一點就可以用。&lt;/p&gt;

&lt;p&gt;如果公司有完整的維護團隊，有機房，服務的使用量也很大，當然 self-hosted 是比較省。
中小型企業以及新創，服務在公有雲上的，直接使用Sass 服務往往比較節省成本，服務直接由 Sass 維護，節省很多機器上管理跟日常維護。&lt;/p&gt;

&lt;p&gt;避免迷思，買外部服務的帳單是顯性的，報帳時看得到，而工程師維護的時間成本是隱性的。self-host 可能省下 Sass 費用，但工程因為分了時間去維護，而影響進度。這部分就看團隊如何取捨。&lt;/p&gt;

&lt;h3 id=&#34;易用性&#34;&gt;易用性&lt;/h3&gt;

&lt;p&gt;如果應用都跑在公有雲上，可以考慮使用雲平台提供的監測服務，使用便利，而且整合度高。ex  GCP 上，要啟用 Stackdriver 是非常輕鬆的事情，只是改一兩個選項，就可以開啟 / 關閉 logging 與 metrics&lt;/p&gt;

&lt;p&gt;如果是 On-premised 自家機房，也許 self-hosted 會更為適合。&lt;/p&gt;

&lt;h3 id=&#34;客製化程度&#34;&gt;客製化程度&lt;/h3&gt;

&lt;p&gt;在大多數時候，沒有需要更改到服務的核心設定，都可以不可律客製化程度，直接使用 Sass 的設定，就能滿足大部分需求。可以等有有明確需求後再考慮這一點。短期內沒有特殊需求就可以從簡使用。&lt;/p&gt;

&lt;p&gt;使用GKE 到 Stackdriver 的話，對主機本身的機器是沒有控制權的，執行的 pipeline 也不太能更改
Elastic Cloud 有提供上傳 elasticsearch config 檔案的介面，也就是可以更改 server 運行的參數設定
Self-Hosted 除了上述的設定，還可以依照需求更改 ELK / prometheus 服務，在實體機器上的 topology，cpu 記憶體的資源配置，儲存空間配置等，可以最大化機器的效能。&lt;/p&gt;

&lt;h3 id=&#34;scalability&#34;&gt;Scalability&lt;/h3&gt;

&lt;p&gt;資料流量大，儲存空間消耗多，服務負擔大，可能就會需要擴展。&lt;/p&gt;

&lt;p&gt;一個是資料量的擴展。一個是為了應付服務的負擔，對 ELK 服務元件做水平擴展。&lt;/p&gt;

&lt;p&gt;除了 elasticsearch 以爲的元件，例如 kibana，apm-server, beats 都可以透過 kubernetes 輕易的擴展，唯有 elasticsearch ，由於又牽扯上述資料量的擴展，以及分佈，還有副本管理，index 本身的 lifecycle 管理。Elasticsearch 的 scaling 設定上是蠻複雜的，也有很多工要做。index 的 shards / replicas 設定都要注意到。否則一路 scale 上去，集群大的時候彼此 sharding sync 的效能消耗是否會太重。&lt;/p&gt;

&lt;p&gt;Stackdriver 從使用者的角度，是不存在服務節點的擴展問題，節點的維護全都給 Sass 管理。資料量的擴展問題也不大，只要整理資料 pipeline，讓最後儲存的資料容易被查找。&lt;/p&gt;

&lt;h1 id=&#34;timeseries-vs-non-timeseriese&#34;&gt;Timeseries vs non-timeseriese&lt;/h1&gt;

&lt;p&gt;Prometheus &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/storage/&#34; target=&#34;_blank&#34;&gt;是自帶 time series database&lt;/a&gt;，stackdriver 也是 time series 的儲存。ELK 的 elasticsearch 是全文搜索引擎，用了 timestamp 做分析所以可以做到 time series 的資料紀錄與分析。這點在本質上是完全不同的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;光只處理 time series data，Prometheus 的 query 效能是比 elasticsearch 好很多&lt;/li&gt;
&lt;li&gt;Elasticsearch 有大量的 index 維護，需要較多系統資源處理，在沒有 query 壓力的情形下會有系統自動維護的效能消耗&lt;/li&gt;
&lt;li&gt;ELK 的資料不需要預先建模，就可以做到非常彈性的搜尋查找。Stackdriver 的話，無法用未建模的資料欄位交叉查找。

&lt;ul&gt;
&lt;li&gt;Log 收集方面&lt;/li&gt;
&lt;li&gt;Elasticsearch 中的資料欄位透過 tempalte 匯入後，都是有做 index ，所以交叉查找，例如可以從 log text 中包含特定字串的紀錄，在做 aggregate 算出其他欄位的資料分佈。會比較慢，但是是做得到的全文搜索&lt;/li&gt;
&lt;li&gt;Stackdriver 可以做基本的 filter ，例如 filter 某個欄位，但不能做太複雜的交叉比對，也不能針對 text 內容作交互查找，需要換出來另外處理。&lt;/li&gt;
&lt;li&gt;Metrics 收集方面&lt;/li&gt;
&lt;li&gt;(同上) Elasticsearch 可以用全文搜索，做到很複雜的交叉比對，例如：從 metrics 數值，計算在時間範圍的分佈情形(cpu 超過 50% 落在一天 24 小時，各個小時的次數)&lt;/li&gt;
&lt;li&gt;Stackdriver 只能做基本的 time series 查找，然後透過預先定義好的 field filter 資料，再各自圖像化。&lt;/li&gt;
&lt;li&gt;Prometheus 也是必須依照 time series 查找，語法上彈性比 stackdriver 多很多，但依樣不能搜尋沒有 index 的欄位&lt;/li&gt;
&lt;li&gt;這邊要替別提，雖然 Elasticsearch 能用全文搜索輕易地做到複雜的查詢語法，但以 metrics 來說，其實沒有太多跳脫 time series 查找的需求。能做到，但有沒有必要這樣做，可以打個問號。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;個人心得，如果驗證全新的 business model，或是還不確定的需求，可以使用 ELK 做各種複雜的查詢&lt;/p&gt;

&lt;p&gt;如果需求明確，收進來的 log 處理流程都很明確，也許不用使用 ELK。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;論系統資源 CP 值以及效能，time series 的 db 都會比 Elasticsearch 好上不少。&lt;/li&gt;
&lt;li&gt;Elasticsearch 中也不太適合一直存放大量的資料在 hot 可寫可讀狀態，繪希好很多系統資源。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;其他服務&#34;&gt;其他服務&lt;/h1&gt;

&lt;p&gt;Elastic 有出許多不同的增值服務&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Application Performance Monitoring(APM)&lt;/li&gt;
&lt;li&gt;Realtime User Monitoring(RUM)&lt;/li&gt;
&lt;li&gt;Machine Learning(ELK ML)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而 ELK 以外也都有不同的解決方案，例如&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GCP 也出了自己的 APM Sass&lt;/li&gt;
&lt;li&gt;Google Analytics(GA) 不僅能做多樣的前端使用者行為分析，還能整合 Google 收集到的使用者行為，做更多維度的分析&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;相較之下 ELK 在這塊其實沒有特別優勢。&lt;/p&gt;

&lt;h1 id=&#34;elastic-cloud&#34;&gt;Elastic Cloud&lt;/h1&gt;

&lt;p&gt;我這邊要特別說 Elastic Cloud vs ELK&lt;/p&gt;

&lt;p&gt;Elatic Cloud 的運行方式，是代為向公與恩平台(aaws, gcp,&amp;hellip;)，帶客戶向平台租用機器，然後把 ELK 服務部署到租用的機器上。用戶這邊無法直接存取機器，只能透過 ELK 介面或是 Kibana , API 進入 ELK。Elastic Cloud 會監控無誤節點的狀況，並做到一定程度的代管。&lt;/p&gt;

&lt;p&gt;這邊指的一定程度的代管，是 Elastic Cloud 只是代為部署服務，監控。有故障時並不負責排除，如果 ELK 故障，簡單的問題（ex. 記憶體資源不足）會代為重開機器，但如果是複雜的問題，還是要用戶自己處理．但是用戶又沒有主機節點的直接存取權限，所以可能會造成服務卡住無法啟動，只能透過 Elastic Cloud 的管理介面嘗試修復。&lt;/p&gt;

&lt;p&gt;使用服務除了把服務都架設完以外，還是需要定期要花時間處理 performance tuning，設定定期清理跟維護。包括 kafka, redis, mongoDB, cassandra, SQLs&amp;hellip;都是一樣，架構越複雜，效能要求越高，這部分的工都會更多。如果公司有 DBA，或是專職維護工程師，那恭喜就不用煩惱。&lt;/p&gt;

&lt;p&gt;Elasticsearch server 目前用起來，算是是數服務中，維護上會花比較多時間的服務。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;因為引擎本身設計的架構，並不是很多人都熟悉。在使用ELK同時，對ELK底層引擎的運作流程有多熟悉，會直接影響穩定性跟跑出來的效能。&lt;/li&gt;
&lt;li&gt;需要好好處理設計資料的儲存，如果使用上沒處理好，會直接讓整個ELK 掛掉。&lt;/li&gt;
&lt;li&gt;然後產品本身的維護介面，目前只是在堪用，許多重要的功能也還在開發中。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果公司有人會管 ELK，個人建議是可以 self-host&lt;/p&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;弄清楚需求，如果沒有特殊需求可以走 general solution&lt;/li&gt;
&lt;li&gt;Sass vs Self-hosted vs On-premised&lt;/li&gt;
&lt;li&gt;Time series vs non time series&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>X.509 certificate</title>
      <link>https://chechiachang.github.io/post/x.509-certificate/</link>
      <pubDate>Tue, 17 Sep 2019 10:15:36 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/x.509-certificate/</guid>
      <description>

&lt;h1 id=&#34;簡單講一下-certificate&#34;&gt;簡單講一下 certificate&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;X.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)&lt;/li&gt;
&lt;li&gt;certificate 包含公鑰及識別資訊(hostname, organization, &amp;hellip;等資訊)&lt;/li&gt;
&lt;li&gt;certificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)&lt;/li&gt;

&lt;li&gt;&lt;p&gt;使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在 elastic stack 上，如果有多個 elasticsearch server node 彼此連線，由於 node 彼此是 client 也是 server&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;使用 self-signed CA 產出來的 certificate，連入時會檢查使用的 certificate 是否由同一組 CA 簽署&lt;/li&gt;
&lt;li&gt;server 使用 certificate，確定連入 server 的 client 都帶有正確的私鑰與 public certificate，是 authenticated user&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;附帶說明，X.509 有多種檔案格式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;.pem&lt;/li&gt;
&lt;li&gt;.cer, .crt, .der&lt;/li&gt;
&lt;li&gt;.p12&lt;/li&gt;
&lt;li&gt;.p7b, .p7c&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證&lt;/p&gt;

&lt;h1 id=&#34;ca&#34;&gt;CA&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys

MAC: sha1, Iteration 100000
MAC length: 20, salt length: 20
PKCS7 Data
Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000
Certificate bag
Bag Attributes
    friendlyName: ca
    localKeyID:
subject=CN = Elastic Certificate Tool Autogenerated CA

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;issuer command name 為 Elastic autogen CA
subject command name 為 Elastic autogen CA&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/&#34; target=&#34;_blank&#34;&gt;https://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl s_client -connect google.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce&#34; target=&#34;_blank&#34;&gt;https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl verify -CAfile client-ca.cer client.cer

openssl verify -show_chain -CAfile client-ca.cer client.cer
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;certificate&#34;&gt;Certificate&lt;/h1&gt;

&lt;p&gt;用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-certificates.p12 -info -nokeys

MAC: sha1, Iteration 100000
MAC length: 20, salt length: 20
PKCS7 Data
Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000
Certificate bag
Bag Attributes
    friendlyName: elk.asia-east1-b.c.machi-x.internal
    localKeyID:
subject=CN = elk.asia-east1-b.c.machi-x.internal

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
Certificate bag
Bag Attributes
    friendlyName: ca
    2.16.840.1.113894.746875.1.1: &amp;lt;Unsupported tag 6&amp;gt;
subject=CN = Elastic Certificate Tool Autogenerated CA

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Secure Elk Stack</title>
      <link>https://chechiachang.github.io/post/secure-elk-stack/</link>
      <pubDate>Sun, 15 Sep 2019 23:00:33 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/secure-elk-stack/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;監測 Google Compute Engine 上服務的各項數據&lt;/li&gt;
&lt;li&gt;監測 Google Kubernetes Engine 的各項數據&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;p&gt;&amp;ndash;&lt;/p&gt;

&lt;p&gt;上篇&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt; 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/elasticsearch-security.html&#34; target=&#34;_blank&#34;&gt;官方的文件在這裡&lt;/a&gt;，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/security-getting-started.html&#34; target=&#34;_blank&#34;&gt;Tutorial: Getting started with security&lt;/a&gt; 開始，過程比較不會這麼血尿。&lt;/p&gt;

&lt;p&gt;總之為了啟用 authentication &amp;amp; https，這篇要做的事情：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;enable x-pack &amp;amp; activate basic license&lt;/li&gt;
&lt;li&gt;Generate self-signed ca, server certificate, client certificate&lt;/li&gt;
&lt;li&gt;Configure Elasticsearch, Kibana, &amp;amp; other components to

&lt;ul&gt;
&lt;li&gt;use server certificate when act as server&lt;/li&gt;
&lt;li&gt;use client certificate when connect to an ELK server&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;啟用-x-pack&#34;&gt;啟用 X-pack&lt;/h1&gt;

&lt;p&gt;Elasticsearch 的安全性模組由 x-pack extension 提供，在 &lt;a href=&#34;https://www.elastic.co/what-is/open-x-pack&#34; target=&#34;_blank&#34;&gt;6.3.0 之後的版本&lt;/a&gt;，安裝 elasticsearch 的過程中就預設安裝 x-pack。&lt;/p&gt;

&lt;p&gt;附上&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/get-started-enable-security.html&#34; target=&#34;_blank&#34;&gt;啟用的官方文件&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然而，由於舊版的 x-pack 是付費內容，目前的 elasticsearch 安裝完後，elasticsearch.yml 設定預設不啟用 x-pack，也就是說沒看到這篇官方文件的話，很容易就獲得沒有任何 security 功能的 ELK。&lt;/p&gt;

&lt;p&gt;雖然目前已經可以使用免費的 basic license 使用 security 功能，還是希望官方可以 default 啟用 security。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/elasticsearch/elasticsearch.yml

xpack.security.enabled: true

xpack.license.self_generated.type: basic

discovery.type: single-node
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我們這邊啟用 xpack.security，同時將 self-generated license 生出來，我們這邊只使用基本的 basic subscription。若希望啟用更多功能，可以看&lt;a href=&#34;https://www.elastic.co/cn/subscriptions&#34; target=&#34;_blank&#34;&gt;官方subcription 方案介紹&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;另外，如果不同時設定為 single-node 的話，預設會尋找其他elasticsearch node 來組成 cluster，而我們就必須要在所有 node 上啟用 security，這篇只帶大家做一個 single node cluster，簡化步驟。&lt;/p&gt;

&lt;p&gt;重啟 elasticsearch ，檢查 log，看啟動時有沒有載入 x-pack&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl restart elasticsearch

$ tail -f /var/log/elasticsearch/elasticsearch.log

[2019-09-16T07:39:49,467][INFO ][o.e.e.NodeEnvironment    ] [elk] using [1] data paths, mounts [[/mnt/disks/elk (/dev/sdb)]], net usable_space [423.6gb], net total_space [491.1gb], types [ext4]
[2019-09-16T07:39:49,474][INFO ][o.e.e.NodeEnvironment    ] [elk] heap size [3.9gb], compressed ordinary object pointers [true]
[2019-09-16T07:39:50,858][INFO ][o.e.n.Node               ] [elk] node name [elk], node ID [pC22j9D4R6uiCM7oTc1Fiw], cluster name [elasticsearch]
[2019-09-16T07:39:50,866][INFO ][o.e.n.Node               ] [elk] version[7.3.1], pid[17189], build[default/deb/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/4.15.0-1040-gcp/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/12.0.2/12.0.2+10]
[2019-09-16T07:39:50,878][INFO ][o.e.n.Node               ] [elk] JVM home [/usr/share/elasticsearch/jdk]
...
[2019-09-16T07:39:59,108][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-ccr]
[2019-09-16T07:39:59,109][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-core]
...
[2019-09-16T07:39:59,111][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-logstash]
[2019-09-16T07:39:59,113][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-voting-only-node]
[2019-09-16T07:39:59,114][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-watcher]
[2019-09-16T07:39:59,115][INFO ][o.e.p.PluginsService     ] [elk] no plugins loaded
[2019-09-16T07:40:07,964][INFO ][o.e.x.s.a.s.FileRolesStore] [elk] parsed [0] roles from file [/etc/elasticsearch/roles.yml]
[2019-09-16T07:40:10,369][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elk] [controller/17314] [Main.cc@110] controller (64 bit): Version 7.3.1 (Build 1d93901e09ef43) Copyright (c) 2019 Elasticsearch BV
[2019-09-16T07:40:11,776][DEBUG][o.e.a.ActionModule       ] [elk] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security
[2019-09-16T07:40:14,396][INFO ][o.e.d.DiscoveryModule    ] [elk] using discovery type [single-node] and seed hosts providers [settings]
[2019-09-16T07:40:16,222][INFO ][o.e.n.Node               ] [elk] initialized
[2019-09-16T07:40:16,224][INFO ][o.e.n.Node               ] [elk] starting ...
[2019-09-16T07:40:16,821][INFO ][o.e.t.TransportService   ] [elk] publish_address {10.140.0.10:9300}, bound_addresses {[::]:9300}
[2019-09-16T07:40:16,872][INFO ][o.e.c.c.Coordinator      ] [elk] cluster UUID [1CB6_Lt-TUWEmRoN9SE49w]
[2019-09-16T07:40:17,088][INFO ][o.e.c.s.MasterService    ] [elk] elected-as-master ([1] nodes joined)[{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 921, reason: master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}
[2019-09-16T07:40:17,819][INFO ][o.e.c.s.ClusterApplierService] [elk] master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}, term: 9, version: 921, reason: Publication{term=9, version=921}
[2019-09-16T07:40:17,974][INFO ][o.e.h.AbstractHttpServerTransport] [elk] publish_address {10.140.0.10:9200}, bound_addresses {[::]:9200}
[2019-09-16T07:40:17,975][INFO ][o.e.n.Node               ] [elk] started
[2019-09-16T07:40:18,455][INFO ][o.e.c.s.ClusterSettings  ] [elk] updating [xpack.monitoring.collection.enabled] from [false] to [true]
[2019-09-16T07:40:22,555][INFO ][o.e.l.LicenseService     ] [elk] license [************************************] mode [basic] - valid
[2019-09-16T07:40:22,557][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [elk] Active license is now [BASIC]; Security is enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;enable-user-authentication&#34;&gt;Enable user authentication&lt;/h1&gt;

&lt;p&gt;啟用 security 之前，我們直接連入 Kibana &lt;a href=&#34;http://10.140.0.10:5601&#34; target=&#34;_blank&#34;&gt;http://10.140.0.10:5601&lt;/a&gt; ，不用任何使用者登入，便可以完整使用 Kibana 功能（包含 admin 管理介面）。&lt;/p&gt;

&lt;p&gt;啟用 security 後，便需要使用帳號密碼登入。在這邊先用工具把使用者密碼產生出來。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# 互動式
/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive

# 自動產生
/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;密碼生出來後，就把帳號密碼收好，等等會用到。之後初次登入也是使用這些密碼。&lt;/p&gt;

&lt;h1 id=&#34;configure-passwords-on-client-side&#34;&gt;Configure passwords on client-side&lt;/h1&gt;

&lt;p&gt;由於已經啟用 authentication，其他 ELK 元件 (Kibana, logstash, filebeat, apm-server,&amp;hellip;) 連入 Elasticsearch 也都會需要各自的帳號密碼驗證。&lt;/p&gt;

&lt;p&gt;以 Kibana 為例，可以直接在 kibana.yml 中直接設定帳號密碼&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/kibana/kibana.yml

elasticsearch.hosts: [&amp;quot;http://localhost:9200&amp;quot;]
xpack.security.enabled: true

elasticsearch.username: &amp;quot;kibana&amp;quot;
elasticsearch.password: &amp;quot;***********&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;當然，這邊就是明碼的，看了不太安全。&lt;/p&gt;

&lt;p&gt;或是使用 keystore 把 built-in user 的密碼加密，存在 kibana 的 keystore 裡面，重啟 kibana 時便會載入。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/share/kibana/bin/kibana-keystore create
/usr/share/kibana/bin/kibana-keystore add elasticsearch.username
/usr/share/kibana/bin/kibana-keystore add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有啟用 Filebeat 功能，beat 元件連入 elasticsearch 一樣需要設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/share/apm-server/bin/filebeat keystore create
/usr/share/apm-server/bin/filebeat add elasticsearch.username
/usr/share/apm-server/bin/filebeat add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有啟用 application performance monitoring(APM) 功能，apm-server 元件連入 elasticsearch 一樣需要設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/share/apm-server/bin/apm-server keystore create
/usr/share/apm-server/bin/apm-server add elasticsearch.username
/usr/share/apm-server/bin/apm-server add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;encrypting-communications&#34;&gt;Encrypting Communications&lt;/h1&gt;

&lt;p&gt;上面加了 username/password authentication，但如果沒 https/tls 基本上還是裸奔。接下來要處理連線加密。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/encrypting-internode-communications.html&#34; target=&#34;_blank&#34;&gt;官方 tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一堆官方文件，我們先跳過XD&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/elasticsearch-security.html&#34; target=&#34;_blank&#34;&gt;elasticsearch security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/ssl-tls.html&#34; target=&#34;_blank&#34;&gt;elastic stack ssl tls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/configuring-tls.html#configuring-tls&#34; target=&#34;_blank&#34;&gt;elasticsearch configuring tls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/certutil.html&#34; target=&#34;_blank&#34;&gt;certutil&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;分析一下需求跟規格&#34;&gt;分析一下需求跟規格&lt;/h1&gt;

&lt;p&gt;我們需要為每一個 node 生一組 node certificate，使用 node certificate 產生 client certificates 提供給其他 client，連入時會驗證 client 是否為 authenticated user。&lt;/p&gt;

&lt;p&gt;針對目前這個 single-node ELK stack，我們可能有幾種選擇&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;簽署一個 localhost，當然這個只能在 localhost 上的客戶端元件使用，別的 node 無法用這個連入&lt;/li&gt;
&lt;li&gt;簽署一個 public DNS elk.chechiachang.com，可以在公開網路上使用，別人也可以使用這個DNS嘗試連入&lt;/li&gt;
&lt;li&gt;簽署一個私有網域的 DNS，例如在 GCP 上可以使用&lt;a href=&#34;https://cloud.google.com/compute/docs/internal-dns?hl=zh-tw&#34; target=&#34;_blank&#34;&gt;內部dns服務&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;長這樣 elk.asia-east1-b.c.chechiachang.internal&lt;/li&gt;
&lt;li&gt;[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;有需要也可以一份 server certificate 中簽署複數個 site&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我們這邊選擇使用內部 dns，elk.asia-east1-b-c-chechaichang.internal，讓這個 single-node elk 只能透過內部網路存取。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;elasticsearch: elk.asia-east1-b.c.chechaichang.internal:9200&lt;/li&gt;
&lt;li&gt;kibana: elk.asia-east1-b.c.chechaichang.internal:5601&lt;/li&gt;
&lt;li&gt;外部要連近來 kibana，我們使用 vpn 服務連進私有網路&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果想使用外部 dns，讓 elk stack 在公開網路可以使用，ex. elk.chechiachang.com，可以&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;GCP 的 load balancer掛進來，用 GCP 的 certificate manager 自動管理 certificate&lt;/li&gt;
&lt;li&gt;或是在 node 上開一個 nginx server，再把 certificate 用 certbot 生出來&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;generate-certificates&#34;&gt;Generate certificates&lt;/h1&gt;

&lt;p&gt;先把 X.509 digital certificate 的 certificate authority(CA) 生出來。我們可以設定密碼保護這個檔案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/elasticsearch/config

# CA generated with Elastic tool
/usr/share/elasticsearch/bin/elasticsearch-certutil ca \
  -out /etc/elasticsearch/config/elastic-stack-ca.p12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生出來是 PKCS#12 格式的 keystore，包含：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CA 的 public certificate&lt;/li&gt;
&lt;li&gt;CA 的基本資訊&lt;/li&gt;
&lt;li&gt;簽署其他 node certificates 使用的私鑰(private key)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;附帶說明，X.509 有多種檔案格式&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;.pem&lt;/li&gt;
&lt;li&gt;.cer, .crt, .der&lt;/li&gt;
&lt;li&gt;.p12&lt;/li&gt;
&lt;li&gt;.p7b, .p7c&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證。裡面的內容也不同。&lt;/p&gt;

&lt;p&gt;ELK 設定的過程中，由於不是所有的 ELK component 都支援使用 .p12 檔案，我們在設定過程中會互相專換，或是混用多種檔案格式。&lt;/p&gt;

&lt;h1 id=&#34;generate-certificate&#34;&gt;Generate certificate&lt;/h1&gt;

&lt;p&gt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD&lt;/p&gt;

&lt;p&gt;我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node certificate&lt;/li&gt;
&lt;li&gt;node key&lt;/li&gt;
&lt;li&gt;CA certificate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這邊只產生一組 server certificate 給 single-node cluster 的 node-1&lt;/p&gt;

&lt;p&gt;=======&lt;/p&gt;

&lt;p&gt;我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;node certificate&lt;/li&gt;
&lt;li&gt;node key&lt;/li&gt;
&lt;li&gt;CA certificate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;這邊只產生一組 server certificate 給 single-node cluster 的 node-1&lt;/p&gt;

&lt;p&gt;如果 cluster 中有多個 elasticsearch，為每個 node 產生 certificate 時都要使用同樣 CA 來簽署，讓 server 信任這組 CA。&lt;/p&gt;

&lt;p&gt;使用 elasticsearch-certutil 簡化簽署過程，從產生 CA ，到使用 CA 簽署 certificate。另外，再產生 certificate 中使用 Subject Alternative Name(SAN)，並輸入 ip 與 dns。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# certificate for site: private dns with Elastic CA
/usr/share/elasticsearch/bin/elasticsearch-certutil cert \
  --ca /etc/elasticsearch/config/elastic-stack-ca.p12 \
  --name elk.asia-east1-b.c.chechaichang.internal \
  --dns elk.asia-east1-b.c.chechaichang.internal \
  --ip 10.140.0.10 \
  -out /etc/elasticsearch/config/node-1.p12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 openssl 看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -info -nokeys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server 用這個 certificate ，啟用 ssl。&lt;/p&gt;

&lt;p&gt;client 使用這個 certificate 產生出來的 client.cer 與 client.key 與 server 連線，server 才接受客戶端是安全的。&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;25f5ab795b9e698333a36fde7ecf23a8ba9d4595
記得把所有權還給 elasticsearch 的使用者，避免 permission denied&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;# Change owner to fix read permission
chown -R elasticsearch:elasticsearch /etc/elasticsearch/config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有密碼記得也要用 keystore 把密碼加密後喂給 elasticsearch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password
/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;關於 X.509 Certifcate 之後有空我們來聊一下&lt;/p&gt;

&lt;h1 id=&#34;更新-elasticsearch-設定&#34;&gt;更新 elasticsearch 設定&lt;/h1&gt;

&lt;p&gt;Certificates 都生完了，接下來更改 elasticsearch 的參數，在 transport layer 啟用 ssl。啟用 security 後，在 transport layer 啟動 ssl 是必須的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/elasticsearch/elasticsearch.yml

xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
# use certificate. full will verify dns and ip
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12
xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;啟用 security 與 transport layer 的 ssl，然後指定 keystore路徑，讓 server 執行 client authentication
由於這筆 p12 帶有 CA certificate 作為 trusted certificate entry，所以也可以順便當作 trustore，讓 client 信任這個 CA&lt;/p&gt;

&lt;p&gt;security 這邊提供了 server side (elasticsearch) 在檢查客戶端連線時的檢查模式(vertification mode)，&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#ssl-tls-settings&#34; target=&#34;_blank&#34;&gt;文件有說明&lt;/a&gt;，可以設定&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;certificate: 檢查 certificate 加密是否有效&lt;/li&gt;
&lt;li&gt;full: 簽 node certificate 時可以指定 ip dns，啟用會檢查來源 node ip dns 是否也正確&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Optional) HTTP layer 啟動 ssl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim /etc/elasticsearch/elasticsearch.yml

xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12
xpack.security.http.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12

/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password
/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重啟 elasticsearch，看一下 log&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl restart elasticsearch
tail -f /var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然後你就發現，原來 kibana 連入 的 http 連線，不斷被 server 這端拒絕。所以以下要來設定 kibana&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;kibana&#34;&gt;Kibana&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/7.3/using-kibana-with-security.html&#34; target=&#34;_blank&#34;&gt;using kibana with security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/7.3/configuring-tls.html&#34; target=&#34;_blank&#34;&gt;kibana configuring tls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用剛剛簽的 server certificate，從裡面 parse 出 client-ca.cer，還有 client.cer 與 client.key&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kibana/config

$ openssl pkcs12 --help
Usage: pkcs12 [options]
Valid options are:
 -chain              Add certificate chain
 -nokeys             Don&#39;t output private keys
 -nocerts            Don&#39;t output certificates
 -clcerts            Only output client certificates
 -cacerts            Only output CA certificates
 -info               Print info about PKCS#12 structure
 -nodes              Don&#39;t encrypt private keys
 -in infile          Input filename

# no certs, no descript
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/kibana/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/kibana/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/kibana/config/client-ca.cer

sudo chown -R kibana:kibana /etc/kibana/config/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更改 kibana 連入 elasticsearch 的連線設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vim /etc/kigana/kibana.yml

elasticsearch.hosts: [&amp;quot;https://elk.asia-east1-b.c.chechaichang.internal:9200&amp;quot;]
xpack.security.enabled: true
elasticsearch.ssl.certificate: /etc/kibana/config/client.cer
elasticsearch.ssl.key: /etc/kibana/config/client.key
elasticsearch.ssl.certificateAuthorities: [ &amp;quot;/etc/kibana/config/client-ca.cer&amp;quot; ]
elasticsearch.ssl.verificationMode: full
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;指定 ssl.certificate, ssl.key 做連線 elasticsearch server 時的 user authentication&lt;/li&gt;
&lt;li&gt;由於我們是 self-signed CA，所以需要讓客戶端信任這個我們自簽的 CA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意這邊 elasticsearch.hosts 我們已經從 &lt;a href=&#34;http://localhost&#34; target=&#34;_blank&#34;&gt;http://localhost&lt;/a&gt; 換成 https 的內部 dns，原有的 localhost 已經無法使用（如果 elasicsearch 有 enforce https 的話）&lt;/p&gt;

&lt;p&gt;重啟 Kibana，看一下 log&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl restart kibana
journalctl -fu kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果沒有一直噴 ssl certificate error 的話，恭喜你成功了&lt;/p&gt;

&lt;p&gt;然而，除了 kibana 以外，我們還有其他的 client 需要連入 elasticsearch&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;把上述步驟在 apm-server, filebeat, 其他的 beat 上也設定&lt;/li&gt;
&lt;li&gt;如果在 k8s 上，要把 cer, key 等檔案用 volume 掛進去
&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; HEAD&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kibana server certificate&lt;/li&gt;
&lt;li&gt;filebeat client to kibana server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;=======&lt;/p&gt;

&lt;p&gt;Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kibana server certificate&lt;/li&gt;
&lt;li&gt;filebeat client to kibana server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;就是他們彼此互打，都要有 ca, key, cert&lt;/p&gt;

&lt;h3 id=&#34;但基本上的設定都一樣-下面可以不用看下去了xd&#34;&gt;但基本上的設定都一樣，下面可以不用看下去了XD&lt;/h3&gt;

&lt;p&gt;如果有用到再查文件就好，這邊直接小結&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;設定 security 前要先想號自己的需求，如何連入，安全性設定到哪邊&lt;/li&gt;
&lt;li&gt;使用 utility 自簽 CA，然後產生 server certificate&lt;/li&gt;
&lt;li&gt;使用 server certificate 再 parse 出 ca-certificate, client cers, key&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;kibana-作為-server&#34;&gt;kibana 作為 server&lt;/h1&gt;

&lt;p&gt;工作路徑可能是這樣： app(apm-client library) -&amp;gt; apm-server -&amp;gt; kibana -&amp;gt; elasticsearch&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;kibana 連入 elasticsearch時， kibana 是 client 吃 elasticsearch 的憑證&lt;/li&gt;
&lt;li&gt;apm-server 連入 kibana時，kibana 是 server，apm-server 吃 kibana 的憑證&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先更改 kibana 設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/kibana/kibana.yml

server.ssl.enabled: true
server.ssl.certificate: /etc/kibana/config/client.cer
server.ssl.key: /etc/kibana/config/client.key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重啟 kibana&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -fu kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;apm-server&#34;&gt;Apm-server&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html&#34; target=&#34;_blank&#34;&gt;https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;應用端的 apm-client (ex. apm-python-client)，連入 apm-server&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在 http 的狀況下，雖然有使用 secret-token，但還是裸奔&lt;/li&gt;
&lt;li&gt;在 https 的狀況下，要把 certificates，然後餵給應用端的client library&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更改 apm-server 的設定&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vim /etc/apm-server/apm-server.yml

host: &amp;quot;0.0.0.0:8200&amp;quot;
  secret_token: &amp;lt;設定一組夠安全的 token&amp;gt;

  rum:
    enabled: true

kibana:
  protocol: &amp;quot;https&amp;quot;
  ssl.enabled: true

output.kibana:
  enable: false # can only have 1 output
output.elasticsearch:

monitoring.elasticsearch:
  protocol: &amp;quot;https&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: &amp;quot;*******************&amp;quot;
  hosts: [&amp;quot;elk.asia-east1-b.c.checahichang.internal:9200&amp;quot;]
  ssl.enabled: true
  ssl.verification_mode: full
  ssl.certificate_authorities: [&amp;quot;/etc/apm-server/config/client-ca.cer&amp;quot;]
  ssl.certificate: &amp;quot;/etc/apm-server/config/client.cer&amp;quot;
  ssl.key: &amp;quot;/etc/apm-server/config/client.key&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重啟 apm-server&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart apm-server
journalctl -fu apm-server
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;apm-library&#34;&gt;APM library&lt;/h1&gt;

&lt;p&gt;應用端的設定就需要依據 library 的實做設定，例如 flask-apmagent-python&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ELASTIC_APM_SERVER_CERT=/etc/elk/certificates/client.cer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/apm/agent/python/current/configuration.html#config-server-cert&#34; target=&#34;_blank&#34;&gt;apm agent python config server cert&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;filebeat&#34;&gt;filebeat&lt;/h1&gt;

&lt;p&gt;記得我們在 node 上有安裝 Self-monitoring filebeat，elasticsearch 改成 ssl 這邊當然也連不盡去了，再做同樣操作&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&#34; target=&#34;_blank&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install filebeat

mkdir -p /etc/filebeat/config
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/filebeat/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/filebeat/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/filebeat/config/client-ca.cer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Restart filebeat&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart filebeat
journalctl -fu filebeat
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;如果你的應用在-kubernetes-上&#34;&gt;如果你的應用在 kubernetes 上&lt;/h1&gt;

&lt;p&gt;可以使用下面方法拿到 client.cer ，然後用 secret 塞進 k8s，在用 volume from secrets，掛給監測應用的 filebeat&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
mkdir -p /etc/beats/config
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/beats/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/beats/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/beats/config/client-ca.cer

gcloud compute scp elk:/etc/beats/config/* .
 client-ca.cer
 client.cer
 client.key

kubectl -n elk create secret generic elk-client-certificates \
  --from-file=client-ca.cer=client-ca.cer \
  --from-file=client.cer=client.cer \
  --from-file=client.key=client.key

kubectl apply -f elk/gke/filebeat/
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Self-host ELK stack - Installation</title>
      <link>https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/</link>
      <pubDate>Sun, 15 Sep 2019 11:43:03 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;

&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;

&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechiachang.github.io&#34; target=&#34;_blank&#34;&gt;https://chechiachang.github.io&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;

&lt;p&gt;&amp;ndash;&lt;/p&gt;

&lt;h1 id=&#34;簡介-elk-stack&#34;&gt;簡介 ELK stack&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/index.html&#34; target=&#34;_blank&#34;&gt;官方說明文件&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;elk-的元件&#34;&gt;ELK 的元件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Elasticsearch: 基於 Lucene 的分散式全文搜索引擎&lt;/li&gt;
&lt;li&gt;Logstash: 數據處理 pipeline&lt;/li&gt;
&lt;li&gt;Kibana: ELK stack 的管理後台與數據視覺化工具&lt;/li&gt;
&lt;li&gt;Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;elk-的工作流程&#34;&gt;ELK 的工作流程&lt;/h3&gt;

&lt;p&gt;beats -&amp;gt; (logstash) -&amp;gt; elasticsearch -&amp;gt; kibana&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起&lt;/li&gt;
&lt;li&gt;設定 beats 從指定的路徑收集 log 與 metrics&lt;/li&gt;
&lt;li&gt;設定 beats 向後輸出的遠端目標&lt;/li&gt;
&lt;li&gt;(Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch&lt;/li&gt;
&lt;li&gt;beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據&lt;/li&gt;
&lt;li&gt;透過 Kibana，將 elasticsearch 上的 log 顯示&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;官方不是有出文件嗎&#34;&gt;官方不是有出文件嗎&lt;/h1&gt;

&lt;p&gt;Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。&lt;/p&gt;

&lt;p&gt;說穿了本篇的技術含量不高，就只是一個踩雷過程。&lt;/p&gt;

&lt;p&gt;Lets get our hands dirty.&lt;/p&gt;

&lt;h1 id=&#34;warning&#34;&gt;WARNING&lt;/h1&gt;

&lt;p&gt;這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在&lt;a href=&#34;https://www.elastic.co/what-is/open-x-pack&#34; target=&#34;_blank&#34;&gt;v6.3.0 以前的版本是不包含安全性模組的&lt;/a&gt;，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。&lt;/p&gt;

&lt;p&gt;我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為
- 另起一篇對安全性設定多加說明
- 在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯&lt;/p&gt;

&lt;p&gt;雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。&lt;/p&gt;

&lt;p&gt;如果希望第一次安裝就有完整的 security 設定，請等待下篇 &lt;a href=&#34;#secure-elk-stack&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;討論需求與規格&#34;&gt;討論需求與規格&lt;/h1&gt;

&lt;p&gt;這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;elk-node-standalone 10.140.0.10
app-node-1          10.140.0.11
...                 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;本機的 ELK stack 元件，彼此透過 localhost 連線&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Elasticsearch:  localhost:9200&lt;/li&gt;
&lt;li&gt;Kibana:         localhost:5601&lt;/li&gt;
&lt;li&gt;Apm-server:     localhost:8200&lt;/li&gt;
&lt;li&gt;Self Monitoring Services&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;私有網路中的外部服務透過 10.140.0.10&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;beats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200&lt;/li&gt;
&lt;li&gt;beats 從其他 node 輸出到 Apm-server:    10.140.0.10:8200&lt;/li&gt;
&lt;li&gt;在內部網路中 透過 browser 存取 Kibana:  10.140.0.10:5601&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;standalone 的好處:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰)&lt;/li&gt;
&lt;li&gt;最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Standalone可能造成的問題:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;No High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛&lt;/li&gt;
&lt;li&gt;外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果要有 production ready 的 ELK&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;HA 開起來&lt;/li&gt;
&lt;li&gt;把服務分散到不同 node 上, 方便之後 scale out 多開幾台

&lt;ul&gt;
&lt;li&gt;elasticsearch-1, elasticsearch-2, elasticsearch-3&amp;hellip;&lt;/li&gt;
&lt;li&gt;kibana-1&lt;/li&gt;
&lt;li&gt;apm-server-1, apm-server-2, &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;主機設定&#34;&gt;主機設定&lt;/h1&gt;

&lt;p&gt;Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。&lt;/p&gt;

&lt;p&gt;GCP 上使用 Google Compote Engine 的朋友，可以照 &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/add-persistent-disk?hl=zh-tw&#34; target=&#34;_blank&#34;&gt;Google 官方操作步驟操作&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;完成後接近這樣&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ df -h
$ df --human-readable

Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       9.6G  8.9G  682M  93% /
/dev/sdb        492G   63G  429G  13% /mnt/disks/elk

$ ls /mnt/disks/elk

/mnt/disks/elk/elasticsearch
/mnt/disks/elk/apm-server
/mnt/disks/elk/kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。&lt;/p&gt;

&lt;h1 id=&#34;開防火牆&#34;&gt;開防火牆&lt;/h1&gt;

&lt;p&gt;需要開放 10.140.0.10 這台機器的幾個 port&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;elasticsearch           :9200   來源只開放私有網路其他 ip 10.140.0.0/9&lt;/li&gt;
&lt;li&gt;apm-server              :8200   (同上)&lt;/li&gt;
&lt;li&gt;kibana                  :5601   (同上)，如果想從外部透過 browser開，需要 whitelist ip&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線
- default-allow-internal: :all    :10.140.0.0/9   tcp:0-65535&lt;/p&gt;

&lt;h1 id=&#34;install-elasticsearch&#34;&gt;Install Elasticsearch&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/install-elasticsearch.html&#34; target=&#34;_blank&#34;&gt;Install Elasticsearch 官方文件 7.3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install apt-transport-https
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
add-apt-repository &amp;quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&amp;quot;
sudo apt-get update
sudo apt-get install elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安裝完後路徑長這樣&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/etc/elasticsearch
/etc/elasticsearch/elasticsearch.yml
/etc/elasticsearch/jvm.options

# Utility
/usr/share/elasticsearch/bin/

# Log
/var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs&lt;/p&gt;

&lt;h3 id=&#34;服務控制&#34;&gt;服務控制&lt;/h3&gt;

&lt;p&gt;透過 systemd 管理，我們可以用 systemctl 控制，
用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。&lt;/p&gt;

&lt;p&gt;但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /mnt/disks/elk/elasticsearch
chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;設定檔案&#34;&gt;設定檔案&lt;/h3&gt;

&lt;p&gt;ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vim /etc/elasticsearch/elasticsearch.yml

# Change Network
network.host: 0.0.0.0
# Change data path
path.data: /mnt/disks/elk/elasticsearch

vim /etc/elasticsearch/jvm-options
# Adjust heap to 4G
-Xms4g
-Xmx4g

# Enable xpack.security
discovery.seed_hosts: [&amp;quot;10.140.0.10&amp;quot;]
discovery.type: &amp;quot;single-node&amp;quot;
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.license.self_generated.type: basic
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。&lt;/p&gt;

&lt;p&gt;有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto

# Keep your passwords safe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然後把啟動 Elasticsearch&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl enable elasticsearch.service
sudo systemctl start elasticsearch.service
sudo systemctl status elasticsearch.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下 log，確定服務有在正常工作&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tail -f /var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在 node 上試打 Elasticsearch API&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:9200

{
  &amp;quot;name&amp;quot; : &amp;quot;elk&amp;quot;,
  &amp;quot;cluster_name&amp;quot; : &amp;quot;elasticsearch&amp;quot;,
  &amp;quot;cluster_uuid&amp;quot; : &amp;quot;uiMZe7VETo-H6JLFLF4SZg&amp;quot;,
  &amp;quot;version&amp;quot; : {
    &amp;quot;number&amp;quot; : &amp;quot;7.3.1&amp;quot;,
    &amp;quot;build_flavor&amp;quot; : &amp;quot;default&amp;quot;,
    &amp;quot;build_type&amp;quot; : &amp;quot;deb&amp;quot;,
    &amp;quot;build_hash&amp;quot; : &amp;quot;4749ba6&amp;quot;,
    &amp;quot;build_date&amp;quot; : &amp;quot;2019-08-19T20:19:25.651794Z&amp;quot;,
    &amp;quot;build_snapshot&amp;quot; : false,
    &amp;quot;lucene_version&amp;quot; : &amp;quot;8.1.0&amp;quot;,
    &amp;quot;minimum_wire_compatibility_version&amp;quot; : &amp;quot;6.8.0&amp;quot;,
    &amp;quot;minimum_index_compatibility_version&amp;quot; : &amp;quot;6.0.0-beta1&amp;quot;
  },
  &amp;quot;tagline&amp;quot; : &amp;quot;You Know, for Search&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;kibana&#34;&gt;Kibana&lt;/h1&gt;

&lt;p&gt;有了正常工作的 Elasticsearch，接下來要安裝 kibana，由於 apt repository 已經匯入，這邊直接&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install kibana
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一樣快速設定一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim /etc/kibana/kinana.yml

# change server.host from localhost to 0.0.0.0 to allow outside requests
server.host: &amp;quot;0.0.0.0&amp;quot;

# Add elasticsearch password
elasticsearch.username: &amp;quot;kibana&amp;quot;
elasticsearch.password:

sudo systemctl enable kibana.service
sudo systemctl start kibana.service
sudo systemctl status kibana.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;檢查 log 並試打一下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl status kibana

$ curl localhost:5601
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;透過內網 ip 也可以用 browser 存取
使用 elastic 這組帳號密碼登入，可以有管理員權限
可以檢視一下 kibana 的頁面，看一下是否系統功能都上常上線
&lt;a href=&#34;http://10.140.0.10/app/monitoring#&#34; target=&#34;_blank&#34;&gt;http://10.140.0.10/app/monitoring#&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;filebeat&#34;&gt;Filebeat&lt;/h1&gt;

&lt;p&gt;以上是 ELK 最基本架構: elasticsearch 引擎與前端視覺化管理工具 Kibana。當然現在進去 kibana 是沒有數據的，所以我們現在來安裝第一個 beat，收集第一筆數據。&lt;/p&gt;

&lt;p&gt;你可能會覺得奇怪: 我現在沒有任何需要監控的應用，去哪收集數據?&lt;/p&gt;

&lt;p&gt;ELK 提供的自我監測 (self-monitoring) 的功能，也就是在 node 上部屬 filebeat 並啟用 modules，便可以把這台 node 上的 elasticsearch 運行的狀況，包含cpu 狀況、記憶體用量、儲存空間用量、安全性告警、&amp;hellip;都做為數據，傳到 elasticsearch 中，並在 Kibana monitoring 頁面製圖顯示。&lt;/p&gt;

&lt;p&gt;這邊也剛好做為我們 ELK stack 的第一筆數據收集。&lt;/p&gt;

&lt;p&gt;WARNING: 這邊一樣要提醒， production 環境多半會使用另外一組的 elasticsearch 來監控主要的這組 elastic stack，以維持 elk stack 的穩定性，才不會自己 monitoring 自己，結果 elastic 掛了，metrics 跟錯誤訊息都看不到。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-installation.html&#34; target=&#34;_blank&#34;&gt;官方安裝文件&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install filebeat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;預設的 filebeat.yml 設定檔案不是完整的，請到官網下載完整版，但官網沒給檔案連結(慘)，只有網頁版 &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&#34; target=&#34;_blank&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我們上 github 把她載下來&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://raw.githubusercontent.com/elastic/beats/v7.3.1/filebeat/filebeat.reference.yml
$ sudo mv filebeat-reference-y
$ sudo vim /etc/filebeat/filebeat.yml

# Enable elasticsearch module and kibana module to process metrics of localhost elasticsearch &amp;amp; kibana
filebeat.modules:
- module: elasticsearch
  # Server log
  server:
    enabled: true

- module: kibana
  # All logs
  log:
    enabled: true

# The name will be added to metadata
name: filebeat-elk
fields:
  env: elk

# Add additional cloud_metadata since we&#39;re on GCP
processors:
- add_cloud_metadata: ~

# Output to elasticsearch
output.elasticsearch:
  enabled: true
  hosts: [&amp;quot;localhost:9200&amp;quot;]
  protocol: &amp;quot;http&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: 

# Configure kibana with filebeat: add template, dashboards, etc...
setup.kibana:
  host: &amp;quot;localhost:5601&amp;quot;
  protocol: &amp;quot;http&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;啟動 filebeat&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start filebeat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;看一下 log，filebeat 會開始收集 elasticsearch 的 log 與 metrics，可以在 log 上看到收集的狀況。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo journalctl -fu filebeat

Sep 15 06:28:50 elk filebeat[9143]: 2019-09-15T06:28:50.176Z        INFO        [monitoring]        log/log.go:145        Non-zero metrics in the last 30s        {&amp;quot;monitoring&amp;quot;: {&amp;quot;metrics&amp;quot;: {&amp;quot;beat&amp;quot;:{&amp;quot;cpu&amp;quot;:{&amp;quot;system&amp;quot;:{&amp;quot;ticks&amp;quot;:1670860,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:66}},&amp;quot;total&amp;quot;:{&amp;quot;ticks&amp;quot;:6964660,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:336},&amp;quot;value&amp;quot;:6964660},&amp;quot;user&amp;quot;:{&amp;quot;ticks&amp;quot;:5293800,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:270}}},&amp;quot;handles&amp;quot;:{&amp;quot;limit&amp;quot;:{&amp;quot;hard&amp;quot;:4096,&amp;quot;soft&amp;quot;:1024},&amp;quot;open&amp;quot;:11},&amp;quot;info&amp;quot;:{&amp;quot;ephemeral_id&amp;quot;:&amp;quot;62fd4bfa-1949-4356-9615-338ca6a95075&amp;quot;,&amp;quot;uptime&amp;quot;:{&amp;quot;ms&amp;quot;:786150373}},&amp;quot;memstats&amp;quot;:{&amp;quot;gc_next&amp;quot;:7681520,&amp;quot;memory_alloc&amp;quot;:4672576,&amp;quot;memory_total&amp;quot;:457564560376,&amp;quot;rss&amp;quot;:-32768},&amp;quot;runtime&amp;quot;:{&amp;quot;goroutines&amp;quot;:98}},&amp;quot;filebeat&amp;quot;:{&amp;quot;events&amp;quot;:{&amp;quot;active&amp;quot;:-29,&amp;quot;added&amp;quot;:1026,&amp;quot;done&amp;quot;:1055},&amp;quot;harvester&amp;quot;:{&amp;quot;open_files&amp;quot;:4,&amp;quot;running&amp;quot;:4}},&amp;quot;libbeat&amp;quot;:{&amp;quot;config&amp;quot;:{&amp;quot;module&amp;quot;:{&amp;quot;running&amp;quot;:0}},&amp;quot;output&amp;quot;:{&amp;quot;events&amp;quot;:{&amp;quot;acked&amp;quot;:1055,&amp;quot;active&amp;quot;:-50,&amp;quot;batches&amp;quot;:34,&amp;quot;total&amp;quot;:1005},&amp;quot;read&amp;quot;:{&amp;quot;bytes&amp;quot;:248606},&amp;quot;write&amp;quot;:{&amp;quot;bytes&amp;quot;:945393}},&amp;quot;pipeline&amp;quot;:{&amp;quot;clients&amp;quot;:9,&amp;quot;events&amp;quot;:{&amp;quot;active&amp;quot;:32,&amp;quot;published&amp;quot;:1026,&amp;quot;total&amp;quot;:1026},&amp;quot;queue&amp;quot;:{&amp;quot;acked&amp;quot;:1055}}},&amp;quot;registrar&amp;quot;:{&amp;quot;states&amp;quot;:{&amp;quot;current&amp;quot;:34,&amp;quot;update&amp;quot;:1055},&amp;quot;writes&amp;quot;:{&amp;quot;success&amp;quot;:35,&amp;quot;total&amp;quot;:35}},&amp;quot;system&amp;quot;:{&amp;quot;load&amp;quot;:{&amp;quot;1&amp;quot;:1.49,&amp;quot;15&amp;quot;:0.94,&amp;quot;5&amp;quot;:1.15,&amp;quot;norm&amp;quot;:{&amp;quot;1&amp;quot;:0.745,&amp;quot;15&amp;quot;:0.47,&amp;quot;5&amp;quot;:0.575}}}}}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果數據都有送出，就可以回到 kibana 的頁面，看一下目前這個 elasticsearch 集群，有開啟 monitoring 功能的元件們，是否都有正常工作。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://10.140.0.10/app/monitoring#&#34; target=&#34;_blank&#34;&gt;http://10.140.0.10/app/monitoring#&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;頁面長得像這樣&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;https://chechiachang.github.io/img/elk/kibana-monitoring.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; &gt;


&lt;/figure&gt;


&lt;p&gt;Standalone cluster 中的 filebeat，是還未跟 elasticsearch 配對完成的數據，會顯示在另外一個集群中，配對完後會歸到 elk cluster 中，就是我們的主要 cluster。&lt;/p&gt;

&lt;p&gt;點進去可以看各個元件的服務情形。&lt;/p&gt;

&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;簡單思考 self-host ELK stack 搭建的架構&lt;/li&gt;
&lt;li&gt;在單一 node 上安裝最簡易的 elastic stack&lt;/li&gt;
&lt;li&gt;設定元件的 output 位置&lt;/li&gt;
&lt;li&gt;設定 self-monitoring&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;恭喜各位獲得一個裸奔但是功能完整的 ELK, 我們下篇再向安全性邁進。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2020 IT邦幫忙鐵人賽</title>
      <link>https://chechiachang.github.io/post/2020-ithome-ironman-challenge/</link>
      <pubDate>Mon, 09 Sep 2019 16:56:03 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/2020-ithome-ironman-challenge/</guid>
      <description>&lt;p&gt;各位好，我是Che-Chia Chang，社群上常用的名子是 David Chang。是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 &lt;a href=&#34;https://www.meetup.com/golang-taipei-meetup/&#34; target=&#34;_blank&#34;&gt;Golang Taiwan Meetup&lt;/a&gt; 的 organizer。&lt;/p&gt;

&lt;p&gt;受到&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman/signup/team/63&#34; target=&#34;_blank&#34;&gt;友人們&lt;/a&gt;邀請（推坑）參加了&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34; target=&#34;_blank&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt;，挑戰在30天內，每天發一篇技術分享文章。一方面將工作上遇到的問題與解法分享給社群，另一方面也是給自己一點成長的壓力，把這段時間的心得沈澱下來，因此也了這系列文章。&lt;/p&gt;

&lt;p&gt;本系列文章重點有三：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;提供的解決方案，附上一步步的操作步驟。希望讓讀者可以重現完整操作步驟，直接使用，或是加以修改&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;著重 Google Cloud Platform，特別是Google Compute Engine (GCE) 與Google Kubernetes Engine (GKE) 兩大服務。這也是我最熟悉的平台，順便推廣，並分享一些雷點。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;從維運的角度除錯，分析問題，提升穩定性。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;預定的主題如下（可能會依照實際撰寫狀況微調）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ELK Stask on GCP (8)

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/&#34; target=&#34;_blank&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/secure-elk-stack/&#34; target=&#34;_blank&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gce-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/monitoring-gke-with-elk/&#34; target=&#34;_blank&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/elastic-or-not-elastic/&#34; target=&#34;_blank&#34;&gt;是否選擇 ELK 作為解決方案&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechiachang.github.io/post/logstash-on-gke/&#34; target=&#34;_blank&#34;&gt;使用 logstash pipeline 做數據前處理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GKE 維運心得 (5)

&lt;ul&gt;
&lt;li&gt;我的 Kubernetes 除錯流程&lt;/li&gt;
&lt;li&gt;Kubectl cheat sheet&lt;/li&gt;
&lt;li&gt;使用 cert-manager 維護 TLS/HTTPS&lt;/li&gt;
&lt;li&gt;使用 redhat operator-sdk 初探 CRD 與 operator&lt;/li&gt;
&lt;li&gt;我的 operator 範例分享&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;在 GKE 上部署 Kafka HA (4)

&lt;ul&gt;
&lt;li&gt;使用 helm 部署 kafka-ha&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;應用端的基本範例，效能調校&lt;/li&gt;
&lt;li&gt;在 GKE 上維運 kafka&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;在 GKE 上部署 Redis HA (4)

&lt;ul&gt;
&lt;li&gt;使用 helm 部署 redis-ha&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;應用端的基本範例，效能調校&lt;/li&gt;
&lt;li&gt;在 GKE 上維運 redis&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Prometheus / Grafana (5)

&lt;ul&gt;
&lt;li&gt;GKE 上自架 Prometheus / Grafana&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 kubernetes 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GCP 網路設定 (3)

&lt;ul&gt;
&lt;li&gt;防火牆的私有網路基本設定&lt;/li&gt;
&lt;li&gt;配合 GKE 實現負載均衡&lt;/li&gt;
&lt;li&gt;DNS 基本觀念，從 kube-dns 到 GCP DNS service&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GCP 日誌管理 (2)

&lt;ul&gt;
&lt;li&gt;基本 GCP 日誌管理與錯誤回報&lt;/li&gt;
&lt;li&gt;Stackdriver 服務的日誌管理，監測數據，告警&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;文章發表於&lt;a href=&#34;https://ithelp.ithome.com.tw/users/20120327/ironman/2444&#34; target=&#34;_blank&#34;&gt;鐵人挑戰頁面&lt;/a&gt;，同時發布與本站備份。有任何謬誤，還煩請各方大德&amp;lt;3透過底下的聯絡方式聯絡我，感激不盡。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Features&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;step-by-step guide for deployment: guarentee a running deployment on GCP&lt;/li&gt;
&lt;li&gt;basic configuration, usage, monitoring, networking on GKE&lt;/li&gt;
&lt;li&gt;debugging, stability analysis in an aspect of devop&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Topics&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;ELK stack(8)

&lt;ul&gt;
&lt;li&gt;Deploy self-hosted ELK stack on GCE instance&lt;/li&gt;
&lt;li&gt;Secure ELK stack with SSL and role-based authentication&lt;/li&gt;
&lt;li&gt;Monitoring services on Kubernetes with ELK beats&lt;/li&gt;
&lt;li&gt;Monitoring services on GCE instances&lt;/li&gt;
&lt;li&gt;Logstash pipelines and debugging walk through&lt;/li&gt;
&lt;li&gt;Elasticsearch operations: house-cleaning, tuning, pernament storage&lt;/li&gt;
&lt;li&gt;Elasticsearch maitainence, trouble shooting&lt;/li&gt;
&lt;li&gt;Get-Started with Elastic Cloud SASS&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;General operations on Kubernetes(4)

&lt;ul&gt;
&lt;li&gt;Kubernetes Debug SOP&lt;/li&gt;
&lt;li&gt;Kubectl cheat sheet&lt;/li&gt;
&lt;li&gt;Secure services with SSL by cert-manager&lt;/li&gt;
&lt;li&gt;Speed up container updating with operator&lt;/li&gt;
&lt;li&gt;My operator example&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deploy Kafka HA on Kubernetes(4)

&lt;ul&gt;
&lt;li&gt;deploy kafka-ha on Kubernertes with helm&lt;/li&gt;
&lt;li&gt;in-cluster networking configuration for high availability&lt;/li&gt;
&lt;li&gt;basic app-side usage, performance tuning&lt;/li&gt;
&lt;li&gt;Operate Kafka: update config, upgrade version, migrate data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Promethus / grafana(5)

&lt;ul&gt;
&lt;li&gt;Deploy Prometheus / Grafana stack on GCE instance&lt;/li&gt;
&lt;li&gt;Monitoring services on Kubernetes with exporters&lt;/li&gt;
&lt;li&gt;Export Kubernetes metrics to Prometheus&lt;/li&gt;
&lt;li&gt;Export Redis-ha metrics to Prometheus&lt;/li&gt;
&lt;li&gt;Export Kafka metrics to Prometheus&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GCP networking(4)

&lt;ul&gt;
&lt;li&gt;Firewall basic concept for private network with GCE instances &amp;amp; Kubernetes&lt;/li&gt;
&lt;li&gt;Load balancer for Kubernetes service &amp;amp; ingress&lt;/li&gt;
&lt;li&gt;DNS on GCP from Kube-dns to GCP DNS service&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GCP log management(3)

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;Basic usage about GCP logging &amp;amp; GCP Error Report&lt;/li&gt;
&lt;li&gt;Stackdriver, metrics, alerts&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Logging on GKE from gcp-fluentd to stackdriver&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Kuberentes Source Code</title>
      <link>https://chechiachang.github.io/post/kuberentes-source-code/</link>
      <pubDate>Fri, 21 Jun 2019 13:17:53 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kuberentes-source-code/</guid>
      <description>

&lt;h1 id=&#34;api-resources&#34;&gt;API resources&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;kubectl api-resources
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Journey to Quantum Computing</title>
      <link>https://chechiachang.github.io/post/journey-to-quantum-computing/</link>
      <pubDate>Sun, 02 Jun 2019 10:21:37 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/journey-to-quantum-computing/</guid>
      <description>

&lt;p&gt;This post is about my learning steps for quantum-computing.&lt;/p&gt;

&lt;p&gt;For a quick-start tutorial, check my workshop project throught the project link above.&lt;/p&gt;

&lt;h1 id=&#34;resources&#34;&gt;Resources&lt;/h1&gt;

&lt;p&gt;Courses&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/quantum-computing-algorithms&#34; target=&#34;_blank&#34;&gt;Coursera&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mitxpro.mit.edu/courses/course-v1:MITxPRO+QCx0+1T2019/about&#34; target=&#34;_blank&#34;&gt;on MIT x pro&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://pages.cs.wisc.edu/~dieter/Courses/2010f-CS880/lectures.html&#34; target=&#34;_blank&#34;&gt;Quantum Information Processing from UW Madison&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.theory.caltech.edu/people/preskill/ph229/&#34; target=&#34;_blank&#34;&gt;Quantum Computation by John Preskill&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://quantum-computing.ibm.com&#34; target=&#34;_blank&#34;&gt;IBM Q Experience&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Qiskit/openqasm&#34; target=&#34;_blank&#34;&gt;https://github.com/Qiskit/openqasm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Qiskit/qiskit-tutorials&#34; target=&#34;_blank&#34;&gt;https://github.com/Qiskit/qiskit-tutorials&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;ibm-q-experience&#34;&gt;IBM Q Experience&lt;/h1&gt;

&lt;h3 id=&#34;day-1&#34;&gt;Day 1&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://quantum-computing.ibm.com/support/guides/getting-started-with-circuit-composer&#34; target=&#34;_blank&#34;&gt;Getting Started with Circuit Composer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Hello Quantum World circuit transformed two qubits, from $ \vert0\rangle $ to $ \frac{\vert00\rangle + \vert11\rangle}{\sqrt{2}} $&lt;/p&gt;

&lt;p&gt;Questions&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[] Hadamard Gate&lt;/li&gt;
&lt;li&gt;[] Bell states

&lt;ul&gt;
&lt;li&gt;[] Annotations&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Istio 三分鐘就入坑 佈署篇</title>
      <link>https://chechiachang.github.io/post/service-mesh-for-microservice-on-kubernetes/</link>
      <pubDate>Mon, 06 May 2019 18:12:15 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/service-mesh-for-microservice-on-kubernetes/</guid>
      <description>

&lt;h1 id=&#34;create-gke&#34;&gt;Create GKE&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;gcloud beta container --project &amp;quot;istio-playground-239810&amp;quot; clusters create &amp;quot;istio-playground&amp;quot; \
  --zone &amp;quot;asia-east1-b&amp;quot; \
  --username &amp;quot;admin&amp;quot; \
  --cluster-version &amp;quot;1.11.8-gke.6&amp;quot; \
  --machine-type &amp;quot;n1-standard-2&amp;quot; \
  --image-type &amp;quot;COS&amp;quot; \
  --disk-type &amp;quot;pd-standard&amp;quot; \
  --disk-size &amp;quot;100&amp;quot; \
  --preemptible \
  --num-nodes &amp;quot;1&amp;quot; \
  --enable-cloud-logging \
  --enable-cloud-monitoring \
  --no-enable-ip-alias \
  --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard,Istio \
  --istio-config auth=MTLS_PERMISSIVE \
  --no-enable-autoupgrade \
  --enable-autorepair
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;take-a-peek&#34;&gt;Take a Peek&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get namespaces

NAME           STATUS    AGE
default        Active    2m
istio-system   Active    1m
kube-public    Active    2m
kube-system    Active    2m

$ kubectl get po -n istio-system
NAME                                      READY     STATUS      RESTARTS   AGE
istio-citadel-7f6f77cd7b-nxfbf            1/1       Running     0          3m
istio-cleanup-secrets-h454m               0/1       Completed   0          3m
istio-egressgateway-7c56db84cc-nlrwq      1/1       Running     0          3m
istio-galley-6c747bdb4f-45jrp             1/1       Running     0          3m
istio-ingressgateway-6ff68cf95d-tlkq4     1/1       Running     0          3m
istio-pilot-8ff66f8c4-q9chz               2/2       Running     0          3m
istio-policy-69b78b7d6-c8pld              2/2       Running     0          3m
istio-sidecar-injector-558996c897-hr6q4   1/1       Running     0          3m
istio-telemetry-f96459fb-5cbpg            2/2       Running     0          3m
promsd-ff878d44b-hv8nh                    2/2       Running     1          3m
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;deploy-app&#34;&gt;Deploy app&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;kubectl label namespace default istio-injection=enabled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Bookinfo Application&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml

kubectl get pods
kubectl get services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Gateway&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml

kubectl get gateways

kubectl get svc istio-ingressgateway -n istio-system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go to ingress public ip&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.status.loadBalancer.ingress[0].ip}&#39;)
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.spec.ports[?(@.name==&amp;quot;http2&amp;quot;)].port}&#39;)
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.spec.ports[?(@.name==&amp;quot;https&amp;quot;)].port}&#39;)

curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage

404 Not Found
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apply destination rules&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/destination-rule-all.yaml

curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;brief-review&#34;&gt;Brief review&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl get virtualservices
kubectl get destinationrules
kubectl get gateways
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;istio-tasks&#34;&gt;Istio Tasks&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://istio.io/docs/tasks/traffic-management/&#34; target=&#34;_blank&#34;&gt;https://istio.io/docs/tasks/traffic-management/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jenkins X on Kubernetes</title>
      <link>https://chechiachang.github.io/post/jenkins-x-on-kubernetes/</link>
      <pubDate>Fri, 19 Apr 2019 12:15:41 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/jenkins-x-on-kubernetes/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://jenkins.io/&#34; target=&#34;_blank&#34;&gt;Jenkins&lt;/a&gt; is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://jenkins-x.io/&#34; target=&#34;_blank&#34;&gt;Jenkins X&lt;/a&gt;, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.&lt;/p&gt;

&lt;p&gt;Here we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture&#34;&gt;Architecture of Jenkins X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install&#34;&gt;Install Jenkins with jx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pipeline&#34;&gt;Create a Pipeline with jx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#client&#34;&gt;Develope with jx client&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about jx itself, check &lt;a href=&#34;https://github.com/jenkins-x/jx&#34; target=&#34;_blank&#34;&gt;Jenkins-X Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;architecture&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;Check this beautiful diagram.&lt;/p&gt;




  




&lt;figure&gt;

&lt;img src=&#34;https://chechiachang.github.io/img/jenkins/architecture-serverless.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;a href=&#34;https://jenkins-x.io/architecture/diagram/&#34; target=&#34;_blank&#34;&gt;https://jenkins-x.io/architecture/diagram/&lt;/a&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;


&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;install&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;install&#34;&gt;Install&lt;/h1&gt;

&lt;h3 id=&#34;create-gke-cluster-get-credentials&#34;&gt;Create GKE cluster &amp;amp; Get Credentials&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;gcloud init
gcloud components update
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;CLUSTER_NAME=jenkins-server
#CLUSTER_NAME=jenkins-serverless

gcloud container clusters create ${CLUSTER_NAME} \
  --num-nodes 1 \
  --machine-type n1-standard-4 \
  --enable-autoscaling \
  --min-nodes 1 \
  --max-nodes 2 \
  --zone asia-east1-b \
  --preemptible

# After cluster initialization, get credentials to access cluster with kubectl
gcloud container clusters get-credentials ${CLUSTER_NAME}

# Check cluster stats.
kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-jx-on-local-machine&#34;&gt;Install jx on Local Machine&lt;/h3&gt;

&lt;p&gt;[Jenkins X Release](&lt;a href=&#34;https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases&#34; target=&#34;_blank&#34;&gt;https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases&lt;/a&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;JX_VERSION=v2.0.2
OS_ARCH=darwin-amd64
#OS_ARCH=linux-amd64
curl -L https://github.com/jenkins-x/jx/releases/download/&amp;quot;${JX_VERSION}&amp;quot;/jx-&amp;quot;${OS_ARCH}&amp;quot;.tar.gz | tar xzv
sudo mv jx /usr/local/bin
jx version

NAME               VERSION
jx                 2.0.2
Kubernetes cluster v1.11.7-gke.12
kubectl            v1.11.9-dispatcher
helm client        v2.11.0+g2e55dbe
helm server        v2.11.0+g2e55dbe
git                git version 2.20.1
Operating System   Mac OS X 10.14.4 build 18E226
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;option-1-install-serverless-jenkins-pipeline&#34;&gt;(Option 1) Install Serverless Jenkins Pipeline&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;DEFAULT_PASSWORD=mySecretPassWord123
jx install \
  --default-admin-password=${DEFAULT_PASSWORD} \
  --provider=&#39;gke&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Options:&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;Enter Github user name&lt;/li&gt;
&lt;li&gt;Enter Github personal api token for CI/CD&lt;/li&gt;
&lt;li&gt;Enable Github as Git pipeline server&lt;/li&gt;
&lt;li&gt;Select Jenkins installation type:

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Serverless Jenkins X Pipelines with Tekon&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Static Master Jenkins&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pick default workload build pack

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Library Workloads: CI+Release but no CD&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the organization where you want to create the environment repository:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;chechiachang&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Your Kubernetes context is now set to the namespace: jx
INFO[0231] To switch back to your original namespace use: jx namespace jx
INFO[0231] Or to use this context/namespace in just one terminal use: jx shell
INFO[0231] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/
INFO[0231] To import existing projects into Jenkins:       jx import
INFO[0231] To create a new Spring Boot microservice:       jx create spring -d web -d actuator
INFO[0231] To create a new microservice from a quickstart: jx create quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;option-2-install-static-jenkins-server&#34;&gt;(Option 2) Install Static Jenkins Server&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;DEFAULT_PASSWORD=mySecretPassWord123

jx install \
  --default-admin-password=${DEFAULT_PASSWORD} \
  --provider=&#39;gke&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Options:&lt;/p&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;Enter Github user name&lt;/li&gt;
&lt;li&gt;Enter Github personal api token for CI/CD&lt;/li&gt;
&lt;li&gt;Enable Github as Git pipeline server&lt;/li&gt;
&lt;li&gt;Select Jenkins installation type:

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Serverless Jenkins X Pipelines with Tekon&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Static Master Jenkins&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pick default workload build pack

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion&lt;/label&gt;&lt;/li&gt;
&lt;li&gt;&lt;label&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; Library Workloads: CI+Release but no CD&lt;/label&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Select the organization where you want to create the environment repository:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;chechiachang&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INFO[0465]Your Kubernetes context is now set to the namespace: jx
INFO[0465] To switch back to your original namespace use: jx namespace default
INFO[0465] Or to use this context/namespace in just one terminal use: jx shell
INFO[0465] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/
INFO[0465] To import existing projects into Jenkins:       jx import
INFO[0465] To create a new Spring Boot microservice:       jx create spring -d web -d actuator
INFO[0465] To create a new microservice from a quickstart: jx create quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Access Static Jenkins Server through Domain with username and password
Domain &lt;a href=&#34;http://jenkins.jx.11.22.33.44.nip.io/&#34; target=&#34;_blank&#34;&gt;http://jenkins.jx.11.22.33.44.nip.io/&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;uninstall&#34;&gt;Uninstall&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx uninstall
# rm -rf ~/.jx
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;pipeline&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;setup-ci-cd-pipeline&#34;&gt;Setup CI/CD Pipeline&lt;/h1&gt;

&lt;h3 id=&#34;create-quickstart-repository&#34;&gt;Create Quickstart Repository&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods --namespace jx --watch
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# cd workspace
jx create quickstart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Options:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which organisation do you want to use? chechiachang&lt;/li&gt;
&lt;li&gt;Enter the new repository name:  serverless-jenkins-quickstart&lt;/li&gt;

&lt;li&gt;&lt;p&gt;select the quickstart you wish to create  [Use arrows to move, type to filter]
angular-io-quickstart
aspnet-app
dlang-http
&amp;gt; golang-http
jenkins-cwp-quickstart
jenkins-quickstart
node-http&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INFO[0121] Watch pipeline activity via:    jx get activity -f serverless-jenkins-quickstart -w
INFO[0121] Browse the pipeline log via:    jx get build logs chechiachang/serverless-jenkins-quickstart/master
INFO[0121] Open the Jenkins console via    jx console
INFO[0121] You can list the pipelines via: jx get pipelines
INFO[0121] Open the Jenkins console via    jx console
INFO[0121] You can list the pipelines via: jx get pipelines
INFO[0121] When the pipeline is complete:  jx get applications
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;check-log-of-the-first-run&#34;&gt;Check log of the first run&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx logs pipeline
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;add-step-to-pipeline&#34;&gt;Add Step to Pipeline&lt;/h3&gt;

&lt;p&gt;Add a setup step for pullrequest&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd serverless-jenkins-quickstart
jx create step --pipeline pullrequest \
  --lifecycle setup \
  --mode replace \
  --sh &amp;quot;echo hello world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validate pipeline step for each modification&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jx step validate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A build-pack pod started after git push. Watch pod status with kubectl.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get pods --namespace jx --watch
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;check-build-status-on-prow-serverless&#34;&gt;Check Build Status on Prow (Serverless)&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://deck.jx.130.211.245.13.nip.io/&#34; target=&#34;_blank&#34;&gt;http://deck.jx.130.211.245.13.nip.io/&lt;/a&gt;
Login with username and password&lt;/p&gt;

&lt;h3 id=&#34;import-existing-repository&#34;&gt;Import Existing Repository&lt;/h3&gt;

&lt;p&gt;In source code repository:&lt;/p&gt;

&lt;p&gt;Import jx to remote jenkins-server. This will apply a Jenkinsfile to repository by default&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jx import --url git@github.com:chechiachang/serverless-jenkins-quickstart.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Update jenkins-x.yml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jx create step
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;git commit &amp;amp; push&lt;/p&gt;

&lt;h3 id=&#34;trouble-shooting&#34;&gt;Trouble Shooting&lt;/h3&gt;

&lt;p&gt;Failed to get jx resources&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jx get pipelines
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure your jx (or kubectl) context is with the correct GKE and namespace&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kc config set-context gke_my-project_asia-east1-b_jenkins \
  --namespace=jx
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;why-not-use-helm-chart&#34;&gt;Why not use helm chart?&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s readlly depend on what we need in CI/CD automation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/jenkins&#34; target=&#34;_blank&#34;&gt;Jenkins Helm Chart&lt;/a&gt; create Jenkins master and slave cluster on Kubernetes utilizing the Jenkins Kubernetes plugin.
Jenkin Platform with jx is Jenkins Platform native to Kubernetes. It comes with powerful cloud native components like Prow automation, Nexus, Docker Registry, Tekton Pipeline, &amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;check-jenkins-x-examples&#34;&gt;Check jenkins-x examples&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs&#34; target=&#34;_blank&#34;&gt;https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&#34;client&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;client&#34;&gt;Client&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;jx get urls
Name                      URL
jenkins                   http://jenkins.jx.11.22.33.44.nip.io
jenkins-x-chartmuseum     http://chartmuseum.jx.11.22.33.44.nip.io
jenkins-x-docker-registry http://docker-registry.jx.11.22.33.44.nip.io
jenkins-x-monocular-api   http://monocular.jx.11.22.33.44.nip.io
jenkins-x-monocular-ui    http://monocular.jx.11.22.33.44.nip.io
nexus                     http://nexus.jx.11.22.33.44.nip.io
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-cluster-status&#34;&gt;Get Cluster Status&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx diagnose
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-applications-pipelines&#34;&gt;Get Applications &amp;amp; Pipelines&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx get applications
jx get pipelines
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-ci-activities-build-log&#34;&gt;Get CI Activities &amp;amp; build log&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx get activities
jx get activities --filter=&#39;jenkins-x-on-kubernetes&#39;

jx get build log

INFO[0003] view the log at: http://jenkins.jx.11.22.33.44.nip.io/job/chechiachang/job/jenkins-x-on-kubernetes/job/feature-add-test/3/console
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;trigger-build-check-activity&#34;&gt;Trigger Build &amp;amp; Check Activity&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx start pipeline
jx start pipeline --filter=&#39;jenkins-x-on-kubernetes/feature-add-test&#39;

jx get activities --filter=&#39;jenkins-x-on-kubernetes&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;create-pull-request&#34;&gt;Create Pull Request&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;jx create pullrequest
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Container Runtime Interface</title>
      <link>https://chechiachang.github.io/post/kubernetes-container-runtime-interface/</link>
      <pubDate>Sat, 06 Oct 2018 12:07:00 +0800</pubDate>
      
      <guid>https://chechiachang.github.io/post/kubernetes-container-runtime-interface/</guid>
      <description>&lt;p&gt;-&amp;gt; &lt;a href=&#34;https://chechiachang.github.io/slides/container-runtime-interface/&#34; target=&#34;_blank&#34;&gt;Slides here&lt;/a&gt; &amp;lt;-&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
