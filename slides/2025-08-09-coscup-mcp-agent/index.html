<!doctype html><html lang=en><head><meta charset=utf-8><title>COSCUP 2025: 初探 Model Context Protocol 與 AI Agent Protocol：快速打造多工 AI Agent</title><meta name=description content="Model Context Protocol（MCP）是一項由 Anthropic 推出的開放標準，旨在為大型語言模型（LLMs）提供一種標準化的方式，以連接和操作各種資料來源（如本地檔案、資料庫）和工具（如 GitHub、Google Maps）。MCP 的目標是簡化 AI 應用與外部資源的整合過程，類似於 USB-C 為實體設備提供通用連接介面。隨著 AI 技術的快速發展，AI 助手需要與各種資料來源和工具進行互動，以提供更豐富和個性化的服務。Model Context Protocol（MCP）作為一種開放標準，為 AI 應用提供了一種統一的方式，連接到不同的資料來源和工具。本場演講將介紹 MCP 的架構、設計原則與實作範例，並展示如何使用開源 mcp-server 快速打造一套具備上下文共享、工具調用與多模型協作能力的 Agent Server。最後將透過實機 Demo 展現 MCP 在真實 AI Workflow 中的應用潛力。"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><link rel=stylesheet href=/reveal-js/dist/reset.css><link rel=stylesheet href=/reveal-js/dist/reveal.css><link rel=stylesheet href=/reveal-hugo/themes/robot-lung.css id=theme><link rel=stylesheet href=/highlight-js/color-brewer.min.css></head><body><div class=reveal><div class=slides><section><h4 id=coscup-2025>COSCUP 2025</h4><h3 id=初探-mcp-與-ai-agent-protocol>初探 MCP 與 AI Agent Protocol</h3><h5 id=快速打造多工-ai-agent>快速打造多工 AI Agent</h5><h5 id=-che-chia-chang--chechianet>~ Che Chia Chang @ <a href=https://chechia.net>chechia.net</a>🔗~</h5></section><section data-noprocess data-shortcode-slide><h3 id=關於我>關於我</h3><ul><li>Che Chia Chang</li><li>SRE @ <a href=https://www.cake.me/companies/maicoin/jobs>Maicoin</a></li><li><a href=https://mvp.microsoft.com/zh-TW/MVP/profile/e407d0b9-5c01-eb11-a815-000d3a8ccaf5>Microsoft MVP</a></li><li>投影片與講稿都在 <a href=https://chechia.net/>chechia.net</a></li><li><a href=/slides/2024-10-24-etcd-workshop/>K8s Summit 2024: Etcd Workshop</a></li><li><a href=/slides/2025-06-05-devops-rag-internal-ai/>DevOpsDay 2025: RAG Workshop</a></li><li>鐵人賽 (Terraform / Vault 手把手入門)</li></ul></section><section><h3 id=-我們今天要聊什麼>🤖 我們今天要聊什麼</h3><ul><li>開場先 <strong>Demo</strong> 一波！🚀🌕</li><li>什麼是 Model Context Protocol（MCP）<ul><li>為何需要 MCP</li><li>MCP 的內容 (Scope & Projects)<ul><li>規格</li><li>如何使用（SDK & Development Tools）</li></ul></li></ul></li><li>用開源 mcp-server 打造多工 AI Agent<ul><li>OpenAI Agent SDK</li><li>FastMCP</li></ul></li></ul></section><section><section data-shortcode-section><h3 id=demo->Demo 🚀🌕</h3><p>🔽</p></section><section><ul><li>我寫了 main.py (~100Lines) <a href=https://github.com/chechiachang/mcp-playground/blob/main/main.py#L26-L42>L26-L42</a>🔗接上 3 個 mcp-server<ul><li><a href=https://github.com/narumiruna/yfinance-mcp>yfinance-mcp</a></li><li><a href=https://github.com/mendableai/firecrawl-mcp-server>firecrawl official mcp</a></li><li><a href=https://github.com/narumiruna/ly-mcp>lymcp</a> Open Fun LYAPI 立法院開放 api 2.0</li></ul></li><li><a href=https://github.com/jlowin/fastmcp/tree/main>FastMCP</a> framework 實做 MCP<ul><li>使用 decorator (@mcp.tool) 定義工具</li><li>FastMCP 會自動生成<a href=https://github.com/jlowin/fastmcp/blob/main/src/fastmcp/tools/tool.py#L133-L155>符合 MCP 規格的 Tool</a>🔗</li></ul></li><li>client (main.py)<ul><li>初始化時詢問 mcp-server list tools</li><li>query 時 LLM 決定要用哪個 tool</li></ul></li></ul></section><section><h3 id=source-code>Source Code</h3><ul><li><a href=https://github.com/chechiachang/mcp-playground>https://github.com/chechiachang/mcp-playground</a>🔗</li><li><a href=https://github.com/narumiruna/ly-mcp>https://github.com/narumiruna/ly-mcp</a></li></ul></section><section><h3 id=notes>Notes</h3><ul><li>100 行程式碼就可以接上 3 個資料源</li><li>Agent 內的 context 共用<ul><li>ex. yfinance-mcp 可以查詢新聞，firecrawl-mcp 去爬新聞內容，爬完新聞再去查詢立法院的相關法案</li></ul></li><li>調用 tool 時，LLM 會自動生成 request parameter</li></ul></section></section><section><h3 id=mcp-看起來是什麼>MCP 看起來是什麼？</h3><ul><li>不用寫扣就可以透過 mcp-server 接上資料源 api ❌<ul><li>這是 mcp-server 提供的 tool 功能，不是 MCP 本身❌</li><li>MCP 提供 SDK 可以開發符合標準的 mcp-server ✅</li></ul></li><li>從 FastMCP 來看，MCP 定義了 tool, prompt, context 的結構 ✅<ul><li>這些結構可以被 LLM 使用 (ex. LLM 可以 list tools, read context)</li></ul></li><li>上面的 context 可以於 OpenAI Agent SDK 中無縫使用 ✅<ul><li>其他 LLM SDK 也可以接上</li></ul></li></ul></section><section><h3 id=tool-vs-mcp>Tool vs MCP</h3><ul><li>現在 ChatGPT 也會使用 tool 如 function calling, WebSearch, File Search, &mldr;，這些都是 OpenAI tool 的實作。</li><li>MCP 不是提供 tool，而是提供一個統一的標準，讓 LLM 可以使用這些 tool 產生的結果，格式是統一的。</li><li>由於是統一的標準，各家 LLM 與資料源才會覺得「我只要做一套，全世界都可以用」所以才爆炸性的產生一堆 MCP server。</li></ul><blockquote><p>換句話說：因為有 USB 存在，各家硬體廠商才會覺得「我只要做一套 USB 接口，所有人都可以用」，所以才會有 USB 可以支援超多功能的感覺。</p></blockquote></section><section><p>爆炸性的 mcp-server <a href=https://github.com/modelcontextprotocol/servers/pulse>https://github.com/modelcontextprotocol/servers/pulse</a>🔗</p><p><img loading=lazy src=/images/2025-mcp-servers.jpeg></p></section><section><section data-shortcode-section><h3 id=mcp-是什麼>MCP 是什麼？</h3><ul><li><a href=https://modelcontextprotocol.io/docs/learn/architecture>https://modelcontextprotocol.io/docs/learn/architecture</a></li><li>Anthropic 推出的開放標準</li><li>給大型語言模型（LLM）用的</li><li>用來「統一描述上下文 + 工具 + 記憶」</li><li>有點像是 LLM 的操作系統 API
🔽</li></ul></section><section><h3 id=為何需要mcp>為何需要MCP？🤔</h3></section><section><h3 id=不用-mcp-時>不用 MCP 時</h3><p>LLM 都有自己的 context 與 tool 格式，context 要自己整理</p><ul><li>上古時代(2023) context 想怎麼寫就怎麼寫，曾流行把 context 塞進 prompt，然後再 prompt compression</li><li><a href=https://openai.github.io/openai-agents-python/context/>OpenAI Agent SDK</a> 的 context management 建議用 Wrapper 包一層，調用 function tool 時傳入</li><li><a href=https://gofastmcp.com/servers/context>FastMCP 提供更通用的 context 功能</a><ul><li>context 統一格式傳入 tool，讓 LLM 可以讀取</li><li>包含 Logging, Progress, State, Request metadata, Server metadata,&mldr;</li><li>只記得 response.text vs 記得 context 的差別🤔</li></ul></li></ul></section><section><h3 id=透過-mcp-整合資料源>透過 MCP 整合資料源</h3><pre><code>without-mcp/
├── main.py
├── openai/
│   ├── mysql.py
│   ├── ...
│   └── github.py
├── gemini/
│   ├── mysql.py
│   ├── ...
│   └── github.py
</code></pre><ul><li>如果你是 LLM 提供者，要為每一個資料源API 寫一套接法，處理 api request 與 response 到 context</li><li>如果你是資料庫開發者，要為每一個 LLM 寫一套 sdk</li><li>然後上面都沒有提供的話，使用者就要自己維護 LLM x 資料庫數量的程式碼🤮</li><li>LLM 改版就要跟著改（不然沒有新功能直接變成上古時代🦖<ul><li>資料源(ex. GitHub)的 API 不會常大改，但 LLM 是急速更新🚀</li></ul></li></ul></section></section><section><h3 id=為什麼需要-mcp>為什麼需要 MCP？</h3><ul><li>context, prompt, tool, &mldr; 都定義在規格中</li><li><a href=https://modelcontextprotocol.io/specification/2025-06-18>MCP 的內容</a></li><li>Transport：如何傳輸 MCP context</li><li>Schema<ul><li>Resources (Context + data)</li><li>什麼是 Server, Client, Tool, TextContext, AudioContext, <a href=https://modelcontextprotocol.io/specification/2025-06-18/schema#prompt>Prompt</a> 🔗, Request, Response, &mldr;</li><li>tools/call, tools/list</li></ul></li><li>Security and Trust & Safety</li></ul></section><section><h3 id=說白話一點>說白話一點</h3><ul><li><p><a href=https://openai.github.io/openai-agents-python/mcp/>OpenAI: The Model context protocol (aka MCP) is a way to provide tools and context to the LLM.</a></p></li><li><p><a href="https://ai.google.dev/gemini-api/docs/function-calling?hl=zh-tw&amp;example=meeting#mcp">Gemini: Model Context Protocol (MCP) 是一項開放標準，可將 AI 應用程式連結至外部工具和資料。MCP 提供通用協定，供模型存取內容，例如函式 (工具)、資料來源 (資源) 或預先定義的提示。</a></p></li></ul></section><section><h3 id=mcp-解決哪些痛點>MCP 解決哪些痛點？</h3><ul><li>🔥 prompt, tool, resource 太亂 → 結構化表示</li><li>🤹‍♀️ 多任務切換 → 上下文一致</li><li>🛠️ 各步驟間 interface 接得很統一，不再 hardcode</li></ul></section><section><h3 id=mcp-如何使用>MCP 如何使用？</h3><ul><li>LLM 提供商：OpenAI agent sdk 與 gemini sdk 都支援 MCP<ul><li>開源 LLM frameworks（如 LangChain, LlamaIndex）也支援 MCP</li></ul></li><li>許多資料源提供商都支援 MCP</li><li>使用者直接使用<ul><li>LLM sdk 或 LLM frameworks</li><li>official 資料源 MCP server</li><li><a href=https://modelcontextprotocol.io/docs/sdk>MCP offifial sdk</a> 寫 client 或 server</li></ul></li></ul></section><section><h3 id=無情工商超缺人>無情工商，超缺人</h3><p>#福利佳 #薪優 #成長性高 #公司賺錢</p><p><img loading=lazy src=/slides/2025-08-09-coscup-mcp-agent/we-re-hiring.png></p><blockquote><p>想當個 sre，或是想當個會 MCP 的 sre 嗎？這邊都可以實現</p></blockquote></section><section><h3 id=demo->Demo 🚀🌕</h3></section><section><section data-shortcode-section><h3 id=什麼是-ai-agent-protocol>什麼是 AI Agent Protocol？</h3><ul><li>一組設計模式</li><li>把 MCP context 拿去 dispatch 給適合的 agent</li><li>Agent 間可以互相協作，完成一整個 workflow
🔽</li></ul></section><section><h3 id=mcp-和-agent-protocol-的關係>MCP 和 Agent Protocol 的關係</h3><ul><li>MCP 是「一個 agent 的說明書」</li><li>Agent Protocol 是「多個 agent 怎麼互動」</li><li>MCP 負責上下文、Agent Protocol 負責流程</li></ul></section><section><h3 id=小結一下概念差異>小結一下概念差異</h3><table><thead><tr><th>概念</th><th>負責的事</th></tr></thead><tbody><tr><td>MCP</td><td>一個 agent 的角色 + 工具 + 任務</td></tr><tr><td>Agent Protocol</td><td>多個 agent 的協作 + 任務分派</td></tr></tbody></table></section></section><section><h3 id=小結->小結 ✨</h3><ul><li>MCP 是 AI 應用的通用「上下文 + 工具」描述法</li><li>Agent Protocol 幫你 orchestrate 多個 Agent</li><li>用 mcp-server 可以快速打通各種 LLM + 工具實作出一套原型</li></ul></section><section><h3 id=延伸閱讀--原始碼>延伸閱讀 & 原始碼</h3><ul><li>MCP：<a href=https://modelcontextprotocol.io/specification/2025-06-18>https://modelcontextprotocol.io/specification/2025-06-18</a></li><li>開源實作：<a href=https://github.com/chechiachang/mcp-playground/tree/main>https://github.com/chechiachang/mcp-playground/tree/main</a></li><li>投影片與講稿：<a href=https://chechia.net>https://chechia.net</a></li></ul></section><section><h3 id=感謝大家->感謝大家 🙌</h3><ul><li>現場 Q&amp;A 時間</li><li>喜歡這種內容歡迎來找我聊天！</li></ul></section><section><h3 id=無情工商超缺人-1>無情工商，超缺人</h3><p>#福利佳 #薪優 #成長性高 #公司賺錢</p><p><img loading=lazy src=/slides/2025-08-09-coscup-mcp-agent/we-re-hiring.png></p></section></div></div><script type=text/javascript src=/reveal-hugo/object-assign.js></script><script src=/reveal-js/dist/reveal.js></script><script type=text/javascript src=/reveal-js/plugin/markdown/markdown.js></script><script type=text/javascript src=/reveal-js/plugin/highlight/highlight.js></script><script type=text/javascript src=/reveal-js/plugin/zoom/zoom.js></script><script type=text/javascript src=/reveal-js/plugin/notes/notes.js></script><script type=text/javascript>function camelize(e){return e&&Object.keys(e).forEach(function(t){newK=t.replace(/(_\w)/g,function(e){return e[1].toUpperCase()}),newK!=t&&(e[newK]=e[t],delete e[t])}),e}var revealHugoDefaults={center:!0,controls:!0,history:!0,progress:!0,transition:"slide"},revealHugoSiteParams={},revealHugoPageParams={custom_theme:"reveal-hugo/themes/robot-lung.css",highlight_theme:"color-brewer",margin:.2,templates:{hotpink:{background:"#FF4081",class:"hotpink"}},transition:"slide",transition_speed:"fast"},revealHugoPlugins={plugins:[RevealMarkdown,RevealHighlight,RevealZoom,RevealNotes]},options=Object.assign({},camelize(revealHugoDefaults),camelize(revealHugoSiteParams),camelize(revealHugoPageParams),camelize(revealHugoPlugins));Reveal.initialize(options)</script><script type=text/javascript src=/mermaid.min_16862243754454536095.js></script><script type=text/javascript>mermaid.initialize({startOnLoad:!1});let render=e=>{let t=e.currentSlide.querySelectorAll(".mermaid");if(!t.length)return;t.forEach(e=>{let t=e.getAttribute("data-processed");t||mermaid.init(0[0],e)})};render({currentSlide:Reveal.getCurrentSlide()}),Reveal.on("slidechanged",render),Reveal.on("ready",render)</script></body></html>