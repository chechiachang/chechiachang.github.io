<!doctype html><html lang=en><head><meta charset=utf-8><title>Kubernetes Summit: Resource as Code for Kubernetes: Stop kubectl apply</title><meta name=description content="將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><link rel=stylesheet href=/reveal-js/dist/reset.css><link rel=stylesheet href=/reveal-js/dist/reveal.css><link rel=stylesheet href=/reveal-hugo/themes/robot-lung.css id=theme><link rel=stylesheet href=/highlight-js/color-brewer.min.css></head><body><div class=reveal><div class=slides><section data-noprocess data-shortcode-slide data-background-image=onepiece.jpg><aside class=notes><p>Q1: 有過使用 helm 跟 argocd 的人請舉手
Q2: k8s object 走 gitflow 管理的比例有超過 9 成的</p></aside></section><section><h3 id=resource-as-code-for-k8s-object>Resource as Code for K8s Object</h3><h3 id=如何管理-k8s-object>如何管理 k8s object</h3><p><a href=https://chechia.net/>Che Chia Chang</a></p><aside class=notes></aside></section><section><h3 id=outline>Outline</h3><p>Manage Kubernetes Objects in Gitflow</p><ul><li>kubectl</li><li>helm chart</li><li>argocd (gitflow)<ul><li>applicationset</li></ul></li><li>test</li></ul><aside class=notes><p>今天的內容前半部像是講古</p><p>首先會講 kubectl create / apply
kubectl 使用，然後官方有提醒我們使用 kubectl 管理 k8s object 時的 trade-off，這些 trade-off 我們可以使用其他的工具來彌補</p><p>導入 helm chart，來打包 k8s objects 變成一個完整地發布單位</p><p>argocd 走 gitflow 來發布</p><p>然後在 workflow 裡加入測試，確保 k8s objects 的交付品質</p></aside></section><section><h3 id=kubectl>Kubectl</h3><pre><code># Imperative commands 
kubectl create deployment nginx --image nginx

# Imperative object configuration 
kubectl create -f nginx.yaml

# Declarative object configuration
kubectl apply -R -f configs/
</code></pre><p><a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/#management-techniques>https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/#management-techniques</a></p><aside class=notes><p>kubectl 應該是所有人學 k8s 的第一個工具
作為官方的 cli 工具，kunectl 非常強大，可以控制幾乎大部分 k8s 的 api，也能對幾乎所有 k8s object 進行操作</p></aside></section><section data-noprocess data-shortcode-slide data-background-image=kubectl.jpg><aside class=notes><p>首先，如同官方文件所描述，kubectl 的使用上，也有各種不同方法。ie. kubectl 交在不同人手上，使用方式是不同的
官方文件描述</p><ul><li>Imperative commands 指令式</li><li>Imperative object configuration 指令物件</li><li>Declarative object configuration 宣告物件</li></ul><p>kubectl create deployment nginx，一行命令告訴 k8s 你要 create deployment</p><p>kubectl create -f nginx.yaml，使用者選擇要 create / apply / delete 而 nginx.yaml 裡面描述一個 nginx deployment 物件</p><p>kubectl apply -f -R nginx/，使用者描述一個或多個物件，描述物件的狀態。apply 時，由 kubectl 決定要對 object 執行，create / update / delete</p></aside></section><section><h3 id=issue>Issue</h3><pre><code># Imperative commands 
kubectl create deployment nginx --image nginx

# Imperative object configuration 
kubectl create -f nginx.yaml

# Declarative object configuration
kubectl apply -R -f configs/
</code></pre><aside class=notes><p>kubectl 這麼好用，那為何不繼續用下去？</p><ol><li>的問題很明顯，不能 diff
2.3. 會有一個基礎的 spec，關於 deployment/nginx，因此可以在</li></ol><p>重點</p><ul><li>change review / diff</li><li>source record other than live</li><li>template，一個可重複使用的樣版</li></ul><p>2.3. 的問題，你必須對 object 夠了解，才寫得出完整沒 bug 的 spec yaml</p><ol start=3><li>聽起來是最完整的，他的問題就是要如何維持 local file 與 live 連結，或是說 sync
這個有使用 argocd 的人可能就會比較有感覺</li></ol><p>大家有興趣可以去看官方描述的內容，這裡不贅述</p></aside></section><section><h3 id=issue-1>Issue</h3><ul><li>change review / diff before apply</li><li>source of live record</li><li>template / repetitive apply</li><li>sync local to live</li></ul><aside class=notes><ul><li>change review / diff before apply</li><li>source of live record</li><li>template / repetitive apply</li><li>sync local to live</li></ul></aside></section><section><h3 id=declarative-object-configuration>Declarative object configuration</h3><pre><code>nginx
├── deployment.yaml
├── ingress.yaml
└── service.yaml
redis
├── deployment.yaml
├── ingress.yaml
└── service.yaml
microservice-a b c ...
</code></pre><aside class=notes><p>為了 change review，通常會走向 3. Declarative object configuration，可能會長這樣
一個 git repository
裡面有多個 directory，描述每一組服務所需要的資源
使用 kubectl 一次 apply 整個 directory，所以 local file 基本上也反應 live object
有 local file，很自然而然就會想要放到版本控制，例如 git，這樣又可以走 gitflow
PR -> review -> merged -> apply master / release tag</p></aside></section><section data-noprocess data-shortcode-slide data-background-image=helm.jpg><aside class=notes><p>有人在 2014 年前用過 k8s 嗎？</p><p>古早時期，要用個 redis 還要自己包 service / ingress / deployment，先去 dockerhub 找 redis，然後依據 readme 自己包 deployment，自己測試看 redis 會不會動</p><p>現在應該沒有人會因為要去使用 redis 或是 mysql，自己跑去寫 k8s object 了吧</p><p>helm v2.0-alpha 2016</p><p>現在有 helm + chart</p><p>如果只是使用低三方開源的 helm chart，社群幫你維護 service / ingress / deployment</p><ul><li>提供基本的 default value，預設就跑得起來</li><li>跑得起來後，跑得好，能使用 k8s orchestration 提供完整的功能，透過 value.yaml 控制</li><li>經過測試</li><li>issue tracking</li></ul></aside></section><section><h3 id=helm-chart>Helm chart</h3><p>k8s object 的開發，打包，測試，release</p><ul><li>k8s 十分強大，享受 orchestration</li><li>k8s object 變得太複雜</li><li>標準化 template，release + upgrade</li></ul><aside class=notes><p>chart 作為一個 k8s object 的 release / artifact，有開發流程，版本控管，測試，完整的發佈</p><p>app 本身，例如 redis，當然是整個應用的核心。但要能夠在 k8s 執行，並正確地享受 k8s orchestration 的好處，k8s object 非常的重要</p><p>甚至，k8s object 複雜度已經遠遠超過過去在 vm 上跑一個 redis，兜一個 systemd unit 就可以跑起來</p><p>k8s object 需要 release / version，才能做 object 的固定版本 apply，upgrade 生版，有問題 rollback</p><p>在 k8s 上要跑得穩邊的十分閫難，透過社群來維護大部分通運的第三方服務</p></aside></section><section><h3 id=helm-chart-library>Helm Chart Library</h3><pre><code>helm repo list

NAME       URL
bitnami    https://charts.bitnami.com/bitnami
argocd     https://argoproj.github.io/argo-helm
chaos-mesh https://charts.chaos-mesh.org
</code></pre><aside class=notes><p>k8s objects 能動很簡單，但要跑得穩又能享受 orchestration 的便利，十分困難
透過社群來維護大部分通運的第三方服務，透過單一 value.yaml</p><p>社群維護的 chart 不是就一勞永逸</p><ul><li>不熱門的 chart</li><li>特殊需求時，還是需要 fork chart 回來自己維護</li></ul><p>但無論如何，都是大幅降低維運的複雜度</p></aside></section><section><h3 id=helm-生態系>helm 生態系</h3><p>ex. helmfile</p><pre><code>repositories:
- name: argocd
  url: https://argoproj.github.io/argo-helm

helmDefaults:
  kubeContext: general
  #verify: true
  wait: true
  timeout: 300

context: general

releases:
- name: argocd
  namespace: argocd
  chart: argocd/argo-cd
  version: 5.31.0
  values:
  - values/argocd.yaml
- name: redis

- name: mysql
</code></pre><p><a href=https://github.com/cdwv/awesome-helm>https://github.com/cdwv/awesome-helm</a></p></section><section><h3 id=更高層級的封裝>更高層級的封裝</h3><pre><code>api-services
├── nginx
├── mysql
└── ingress -&gt; nlb
daemon-services
├── redis
├── mysql
└── kafka
</code></pre><aside class=notes><p>底下的依賴標準化，依據穩定的 release 發布之後
可以再進行更高層級的封裝</p><p>重複性的服務，例如每個 microservice 都需要
例如我有一百個 api service group，都是 restful api，都需要 ingress / service / nginx 等等</p><p>底下的元件標準化，降低維護成本</p><ul><li>統一版本，醫病維護，升級，退版</li></ul><p>例如 daemon service，底下依賴 queue / redis / db</p></aside></section><section><h3 id=微服務>微服務</h3><p>微服務不是問題，微服務底下的 k8s object 才是問題</p><ul><li>可以快速，標準化的產生經過測試，微服務單元</li></ul></section><section><h3 id=issues>Issues</h3><ul><li>V change review / diff before apply</li><li>source of live record</li><li>V template / repetitive apply</li><li>sync local to live</li></ul><aside class=notes><p>version control / gitflow</p><p>helm template 有提供許多語法，可以有系統化的產生重複的 k8s object
ex. 可以跑 for loop / for each
這個在 IaC 或是 resource as code 的 xxx as code 都十分有利</p></aside></section><section><h3 id=argo-cd>Argo CD</h3><p><a href=https://argo-cd.readthedocs.io/en/stable/>Declarative GitOps CD for Kubernetes</a></p><p><img loading=lazy src=https://argo-cd.readthedocs.io/en/stable/assets/argocd-ui.gif></p></section><section><h3 id=why-argo-cd>Why Argo CD?</h3><p>Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.</p><aside class=notes><p>application 應該是明確宣告的，清楚描述，並且有版本控管
描述 application 本身，附帶的設定 secret / configmap
environment 也應該是宣告式的</p><p>application 的部署與 lifecycle 管理，都應該是自動化，可以稽核，而且好理解
標準化，自動化
UI 圖像描述，一目瞭然。但事實上如果有做到講 local file 推到版本控制，光是使用 editor 也可以做到一目瞭然</p></aside></section><section><h3 id=argo-cd-1>Argo CD</h3><ul><li>gitflow / git repository</li><li>argocd application sync file from repository</li><li>argocd controller 自動化的確保 sync<ul><li>un-sync 自動化處理</li><li>無法 sync 時通知</li></ul></li></ul><aside class=notes><p>gitflow</p><ul><li>k8s object 的 change 是需要 PR review 的，不是想改就 kubectl apply 下去</li><li>k8s object 的品質把關，來自高品質的 review</li><li>針對 cluster 上的變更都有管控</li></ul><p>能夠直接掌握 live object，透過 editor 檢查 git repository，或是透過 argocd UI 檢視</p><p>local file 等於 live object
在複雜的環境裡很重要，ex. k8s 內有成千上百個 helm release，可以用 editor 檢視 local file</p><p>不然要透過 k8s 去打 api</p><ul><li>api call 也是資源啊，能省則省</li><li>editor 更直覺快速</li></ul></aside></section><section><h3 id=applicationset>applicationset</h3><p><a href=https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Git/>git generator</a></p><pre><code>apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: general-argocd
  namespace: argocd
spec:
  generators:
  - git:
      repoURL: https://github.com/chechiachang/azure-argo
      revision: master
      directories:
      - path: clusters/dev/dev-general/argocd/*
  template:
    metadata:
      name: 'dev-general-argocd-{{path.basename}}'
      labels:
        environment: dev
        type: infra
        function: argocd
        cluster: dev-general
    spec:
      project: default
      syncPolicy:
        automated:
          prune: true
      source:
        repoURL: https://github.com/chechiachang/azure-argo
        targetRevision: master
        path: 'clusters/dev/dev-general/argocd/{{path.basename}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: argocd
</code></pre><aside class=notes><p>argocd 除了 sync 以外還提供的許多功能</p><p>applicationset 定義一組 set，可以透過 generator 迭代的產生大量的 application
ex. 一次 deploy 一群 redis</p></aside></section><section><h3 id=applicationset-1>applicationset</h3><pre><code>dev-general
├── default/redis
├── default/mysql
├── default/my-app
├── nginx-ingress/nginx-ingress
└── argocd/argocd
stag-general
prod-general
</code></pre></section><section><h3 id=cluster-wide-的-k8s-object>cluster-wide 的 k8s object</h3><p>cluster-wide 的 k8s object 也很適合</p><ul><li>使用 helm template helper 來管理 value.yaml label / annotation / env / &mldr;</li><li>namespace</li><li>rbac</li></ul><aside class=notes><p>cluster-wide 的 k8s object 也很適合塞進 argocd 管理</p><p>例如 cluster access control，複雜的 rbac rule，也很適合整理成為 helm chart</p><ul><li>可以輕鬆實現 user -> rule 多對多</li><li>可以快速增減 user group</li></ul></aside></section><section><h3 id=issues-1>Issues</h3><ul><li>V change review / diff before apply</li><li>V source of live record</li><li>V template / repetitive apply</li><li>V sync local to live</li></ul></section><section><h3 id=more-issues-multi-hybrid-cluster>More Issues: multi-hybrid cluster</h3><ul><li>multiple k8s<ul><li>dev / stag / prod</li></ul></li><li>hybrid k8s<ul><li>bare metal / public cloud</li></ul></li></ul><aside class=notes><p>由於 k8s 內部的 component 都已經標準化，可以很輕易地複製測試過的 component
dev 測試 PR branch，staging 跑 release candicate，production 選擇經過測試的 release</p><p>能夠確保 dev / stag / prod 的 k8s object 是完全相同的</p><p>hybrid 環境管理，某些 k8s 元件應該安裝在哪些 cluster 上</p><ul><li>aws ingress controller / csi driver / cni node daemonsets</li><li>local nginx-ingress controller</li></ul></aside></section><section><h3 id=more-issues-multi-hybrid-cluster-1>More Issues: multi-hybrid cluster</h3><ul><li><a href=https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster/>argocd cluster generator</a></li></ul><pre><code>kind: ApplicationSet
metadata:
  name: aws-cni
  namespace: kube-system
spec:
  generators:
  - clusters:
      selector:
        matchLabels:
          eks: true
          #bare-metal: true
          staging: true
</code></pre></section><section><h3 id=more-issues-multi-hybrid-cluster-2>More Issues: multi-hybrid cluster</h3><ul><li><a href=https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster/>argocd cluster generator</a></li></ul><pre><code>aws-eks-1
├── aws cni
├── aws ingress controller
└── nginx-ingress controller
bare-metal-1
├── cilium
├── bare-metal ingress
└── nginx-ingress controller
</code></pre><aside class=notes><p>多座 cluster</p></aside></section><section><h3 id=more-issues-multi-hybrid-cluster-3>More Issues: multi-hybrid cluster</h3><ul><li>applicationset</li></ul></section><section><h3 id=more-issues-test>More Issues: Test</h3><p>k8s object 需不需要測試</p><ul><li>infra-test (bare-metal / self-hosted)</li><li>release (helm chart)</li><li>live status</li><li>stress / load test / chaos engineering</li></ul><aside class=notes><p>k8s object 需不需要測試，覺得需要測試的請舉手</p><p>k8s object 需不需要測試，就跟這個投影片一樣
有時間就講一下，沒時間就跳過吧</p><p>infra-test provider 應該都測完了
要測的是 k8s object 的功能是否符合預期</p><p>當然，有測試的 helm releae 與沒測試的 helm release 也是天差地遠</p></aside></section><section><h3 id=test-ansible-playbook>Test: ansible playbook</h3><ul><li>測試 apply 後 k8s object 的 status<ul><li>ingress 是否 ready</li><li>endpoint 是否有產生</li></ul></li><li>可以先測<ul><li>networking (ingress / svc)</li><li>storage (csi / pvc)</li></ul></li></ul><pre><code>- name: Test nginx deployment
  hosts: {{ host }}
  gather_facts: no
  vars:
    deployment_name: &quot;nginx&quot;
  tasks:
  - name: Get deployment status
    shell: kubectl get deployment {{ deployment_name }} -o=jsonpath='{.status.readyReplicas}'
    register: deployment_status
    failed_when: deployment_status.rc != 0
  - name: Verify deployment is running
    assert:
      that:
        - deployment_status.stdout != 'null'
        - deployment_status.stdout != '0'
      fail_msg: 'Deployment {{ deployment_name }} is not running.'
</code></pre></section><section><h3 id=more-issues-test-1>More Issues: Test</h3><p><del>Test</del> Monitoring</p><ul><li>prometheus rule with helm chart</li><li>ServiceMonitor</li></ul><aside class=notes><p>helm release 一個 api-services
api service 有 /metrics endpoint
透過 ServiceMonitor 來持續性的紀錄 metrics
透過 PrometheusRule 來設定異常 metrics 的告警</p></aside></section><section><h3 id=more-issues-test-2>More Issues: Test</h3><ul><li>stress-test / load test<ul><li>k6</li></ul></li><li>chaos-engineering<ul><li><a href=https://chaos-mesh.org/>https://chaos-mesh.org/</a></li><li><a href=https://github.com/asobti/kube-monkey>https://github.com/asobti/kube-monkey</a></li></ul></li></ul><aside class=notes></aside></section><section><h3 id=summary>Summary</h3><ul><li>kubectl + gitflow</li><li>helm chart</li><li>argocd: applicationset / generator</li><li>test</li></ul><p>Q & A</p></section></div></div><script type=text/javascript src=/reveal-hugo/object-assign.js></script><script src=/reveal-js/dist/reveal.js></script><script type=text/javascript src=/reveal-js/plugin/markdown/markdown.js></script><script type=text/javascript src=/reveal-js/plugin/highlight/highlight.js></script><script type=text/javascript src=/reveal-js/plugin/zoom/zoom.js></script><script type=text/javascript src=/reveal-js/plugin/notes/notes.js></script><script type=text/javascript>function camelize(e){return e&&Object.keys(e).forEach(function(t){newK=t.replace(/(_\w)/g,function(e){return e[1].toUpperCase()}),newK!=t&&(e[newK]=e[t],delete e[t])}),e}var revealHugoDefaults={center:!0,controls:!0,history:!0,progress:!0,transition:"slide"},revealHugoSiteParams={},revealHugoPageParams={custom_theme:"reveal-hugo/themes/robot-lung.css",highlight_theme:"color-brewer",margin:.2,templates:{hotpink:{background:"#FF4081",class:"hotpink"}},transition:"slide",transition_speed:"fast"},revealHugoPlugins={plugins:[RevealMarkdown,RevealHighlight,RevealZoom,RevealNotes]},options=Object.assign({},camelize(revealHugoDefaults),camelize(revealHugoSiteParams),camelize(revealHugoPageParams),camelize(revealHugoPlugins));Reveal.initialize(options)</script><script type=text/javascript src=/mermaid.min_16862243754454536095.js></script><script type=text/javascript>mermaid.initialize({startOnLoad:!1});let render=e=>{let t=e.currentSlide.querySelectorAll(".mermaid");if(!t.length)return;t.forEach(e=>{let t=e.getAttribute("data-processed");t||mermaid.init(0[0],e)})};render({currentSlide:Reveal.getCurrentSlide()}),Reveal.on("slidechanged",render),Reveal.on("ready",render)</script></body></html>