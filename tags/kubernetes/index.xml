<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Che-Chia Chang</title>
    <link>https://chechia.net/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Che-Chia Chang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>chechiachang &amp;copy; 2016</copyright>
    <lastBuildDate>Sun, 14 Jun 2020 16:46:09 +0800</lastBuildDate>
    
	    <atom:link href="https://chechia.net/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>從零開始的 Infrastructure as Code: Terraform - 01</title>
      <link>https://chechia.net/post/terraform-infrastructure-as-code/</link>
      <pubDate>Sun, 14 Jun 2020 16:46:09 +0800</pubDate>
      
      <guid>https://chechia.net/post/terraform-infrastructure-as-code/</guid>
      <description>&lt;p&gt;This article is part of &lt;a href=&#34;https://chechia.net/post/terraform-infrastructure-as-code/&#34;&gt;從零開始的 Infrastructu as Code: Terraform&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/terraform-infrastructure-as-code/&#34;&gt;01 - Introduction to Infrastructure as Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[02 - Terraform 簡介與基本操作]&lt;/li&gt;
&lt;li&gt;[03 - 為公司導入 Terraform]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chechiachang/terraform-playground&#34;&gt;Get-started examples / SOP on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/terraform-infrastructure-as-code/&#34;&gt;Introducation to Terraform Iac: Speaker transcript&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check my website &lt;a href=&#34;https://chechia.net&#34;&gt;chechia.net&lt;/a&gt; for other blog. &lt;a href=&#34;https://www.facebook.com/engineer.from.scratch&#34;&gt;Follow my page to get notification&lt;/a&gt;. Like my page if you really like it :)&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;outlline&#34;&gt;Outlline&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;our story: issues, steps, &amp;amp; results&lt;/li&gt;
&lt;li&gt;basics IaC, terraform&lt;/li&gt;
&lt;li&gt;benefits&lt;/li&gt;
&lt;li&gt;risks and 坑&lt;/li&gt;
&lt;li&gt;to be or not to be&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;experience oriented&lt;/p&gt;
&lt;h1 id=&#34;our-stories&#34;&gt;Our stories&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;100+ devs, many teams&lt;/li&gt;
&lt;li&gt;25+ projects&lt;/li&gt;
&lt;li&gt;50+ GKEs&lt;/li&gt;
&lt;li&gt;80+ SQLs&lt;/li&gt;
&lt;li&gt;IAMs, redis, VPCs, load-balancers, &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;issues&#34;&gt;Issues&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Ops manually create resources through GUI by SOP.&lt;/li&gt;
&lt;li&gt;We have many isolated, separeated resources, VPCs. It&amp;rsquo;s our culture, and we (devops) want to change.&lt;/li&gt;
&lt;li&gt;Some projects have short life-cycle. Rapid resources created &amp;amp; destroy.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;our-user-story&#34;&gt;Our user story&lt;/h1&gt;
&lt;p&gt;As a devops,
I would like to introduce terraform (IaC)
so that I can&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;review all existing resources&lt;/li&gt;
&lt;li&gt;minimize error from manual operation&lt;/li&gt;
&lt;li&gt;ASAP!!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a devops,
I would like to fully enforce terraform (IaC)
so that I can&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;minimize efforts to operate infra&lt;/li&gt;
&lt;li&gt;delegate infra operations to junior team members&lt;/li&gt;
&lt;li&gt;minimize IAM privilges&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;import existing resources&lt;/li&gt;
&lt;li&gt;review existing resources code&lt;/li&gt;
&lt;li&gt;plan best practice resource templates&lt;/li&gt;
&lt;li&gt;create new resources with templates&lt;/li&gt;
&lt;li&gt;introduce git workflow, plan, commit, PR, and review&lt;/li&gt;
&lt;li&gt;add wrapper handler&lt;/li&gt;
&lt;li&gt;automation pipeline&lt;/li&gt;
&lt;li&gt;repeat 2-4&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;iac&#34;&gt;IaC&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Programatic way to operate infra&lt;/li&gt;
&lt;li&gt;declarative (functional) vs. imperative (procedural)&lt;/li&gt;
&lt;li&gt;Perfect for public cloud, cloud native, virtualized resources&lt;/li&gt;
&lt;li&gt;Benefits: cost (reduction), speed (faster execution) and risk (remove errors and security violations)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;terraform&#34;&gt;Terraform&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.terraform.io/&#34;&gt;Terraform&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Declarative (functional) IaC&lt;/li&gt;
&lt;li&gt;Invoke API delegation&lt;/li&gt;
&lt;li&gt;State management&lt;/li&gt;
&lt;li&gt;providers: azure / aws / gcp /alicloud / &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;demo&#34;&gt;Demo&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/chechiachang/terraform-playground&#34;&gt;https://github.com/chechiachang/terraform-playground&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;scope&#34;&gt;Scope&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Compute Instances&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Databases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;IAM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Networking&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load Balancer&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;expected-benefits&#34;&gt;Expected benefits&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Minimize manual operation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zero manual operation error&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standarized infra. Infra as a (stable) product.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fast, really fast to duplicate envs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Infra workflow with infra review&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easy to create identical dev, staging, prod envs&lt;/li&gt;
&lt;li&gt;Reviewed infra. Better workflow. Code needs reviews, so do infra.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fully automized infra pipeline.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;other-benefits&#34;&gt;Other Benefits&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Don&amp;rsquo;t afraid to change prod sites anymore
&lt;ul&gt;
&lt;li&gt;We made a massive infra migration in this quater!!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Better readability to GUI. Allow comment everywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;risks&#34;&gt;Risks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Incorrect usage could cause massive destruction.
&lt;ul&gt;
&lt;li&gt;如果看見 destroy 的提示，請雙手離開鍵盤。 ~ first line in our SOP&lt;/li&gt;
&lt;li&gt;If see &amp;ldquo;destroy&amp;rdquo;, cancel operation &amp;amp; call for help.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;State management&lt;/li&gt;
&lt;li&gt;A little latency between infra version and terraform provider version&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;reduce-risks&#34;&gt;Reduce Risks&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Sufficient understanding to infra &amp;amp; terraform&lt;/li&gt;
&lt;li&gt;Sufficient training to juniors&lt;/li&gt;
&lt;li&gt;Minimize IAM privilege: remove update / delete permissions&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;git-flow&#34;&gt;Git-flow&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/chechiachang/terraform-playground/blob/master/SOP.md&#34;&gt;Our SOP&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;edit tf&lt;/li&gt;
&lt;li&gt;push new branch commit&lt;/li&gt;
&lt;li&gt;PR, review &amp;amp; discussion&lt;/li&gt;
&lt;li&gt;merge &amp;amp; apply&lt;/li&gt;
&lt;li&gt;revert to previous tag if necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;utility-provide-template&#34;&gt;(Utility) Provide template&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;wrap resources for
&lt;ul&gt;
&lt;li&gt;better accesibility&lt;/li&gt;
&lt;li&gt;lower operation risks&lt;/li&gt;
&lt;li&gt;uniform naming convention&lt;/li&gt;
&lt;li&gt;best practice&lt;/li&gt;
&lt;li&gt;suggested default value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;about-introducing-new-tool&#34;&gt;About introducing new tool&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The hardest part is always people
&lt;ul&gt;
&lt;li&gt;Focus on critical issues (痛點) instead of tool itself. &amp;ldquo;We introduce tool to solve&amp;hellip;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Put result into statistics &amp;ldquo;The outage due to misconfig is reduced by&amp;hellip;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;overall-my-iac-experience-is-great&#34;&gt;Overall, my IaC experience is GREAT!&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;IaC to automation.&lt;/li&gt;
&lt;li&gt;Comment (for infra) is important. You have to write doc anyway. Why not put in IaC?&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;qa&#34;&gt;Q&amp;amp;A&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Full transcript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Presentation file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chechiachang/chechiachang.github.io-src/blob/master/content/post/terraform-infrastructure-as-code/index.md&#34;&gt;Source Code on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net&#34;&gt;chechia.net&lt;/a&gt; &amp;lt;- full contents&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.facebook.com/engineer.from.scratch&#34;&gt;Follow my page to get notification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Like it if you really like it :)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendixi-more-about-terraform&#34;&gt;Appendix.I more about terraform&lt;/h1&gt;
&lt;p&gt;terraform validate
terraform import
terraform module
terraform cloud &amp;amp; state management&lt;/p&gt;
&lt;h1 id=&#34;appendixi-understand-state-conflict&#34;&gt;Appendix.I understand State conflict&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Shared but synced&lt;/li&gt;
&lt;li&gt;watch out for state conflicts when colaborating
&lt;ul&gt;
&lt;li&gt;state diff. could cause terraform mis-plan&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution: synced state lock
&lt;ul&gt;
&lt;li&gt;Colatorative edit (git branch &amp;amp; PR), synchronized terraform plan &amp;amp; apply&lt;/li&gt;
&lt;li&gt;or better: automation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendixii-understand-resources-from-api-aspect&#34;&gt;Appendix.II understand resources from API aspect&lt;/h1&gt;
&lt;p&gt;GCP Load Balancer&lt;/p&gt;
&lt;h3 id=&#34;gcp-load-balancing&#34;&gt;GCP Load Balancing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;understand resources from API aspect&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how terraform work with GCP API&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;internal&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regional
&lt;ul&gt;
&lt;li&gt;pass-through: tcp / udp   -&amp;gt; internal TCP/UDP&lt;/li&gt;
&lt;li&gt;proxy: http / https       -&amp;gt; internal HTTP(S)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;external&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;regional
&lt;ul&gt;
&lt;li&gt;pass-through: tcp / udp   -&amp;gt; tcp/udp network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;global / effective regional
&lt;ul&gt;
&lt;li&gt;proxy
&lt;ul&gt;
&lt;li&gt;tcp                     -&amp;gt; TCP Proxy&lt;/li&gt;
&lt;li&gt;ssl                     -&amp;gt; SSL Proxy&lt;/li&gt;
&lt;li&gt;http / https            -&amp;gt; External HTTP(S)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;terraform-resource&#34;&gt;Terraform Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;forwarding_rule&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;forwarding_rule: tcp &amp;amp; http&lt;/li&gt;
&lt;li&gt;global_forwarding_rule: only http&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;backend_service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;backend_service
&lt;ul&gt;
&lt;li&gt;health_check&lt;/li&gt;
&lt;li&gt;http_health_check&lt;/li&gt;
&lt;li&gt;https_health_check&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;region_backend_service
&lt;ul&gt;
&lt;li&gt;region_health_check&lt;/li&gt;
&lt;li&gt;region_http_health_check&lt;/li&gt;
&lt;li&gt;region_https_health_check&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;some-ways-to-do-iac&#34;&gt;Some ways to do IaC&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Cloud Formation&lt;/li&gt;
&lt;li&gt;bash script with API / client&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;引言-infrastructure-as-code&#34;&gt;引言 Infrastructure as Code&lt;/h1&gt;
&lt;p&gt;從字面上解釋，IaC 就是用程式碼描述 infrastructure。那為何會出現這個概念？&lt;/p&gt;
&lt;p&gt;如果不 IaC 是什麼狀況？我們還是可以透過 GUI 或是 API 操作。隨叫隨用&lt;/p&gt;
&lt;p&gt;雲端運算風行，工程師可以很在 GUI 介面上，很輕易的部署資料中心的架構。輸入基本資訊，滑鼠點個一兩下，就可以在遠端啟用運算機器，啟用資料庫，設置虛擬網路與路由，幾分鐘就可以完成架設服務的基礎建設(infrastructure)，開始運行服務。&lt;/p&gt;
&lt;p&gt;然而隨著&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;雲平台提供更多新的（複雜的）服務
&lt;ul&gt;
&lt;li&gt;服務彼此可能是有相依性（dependency），服務需要仰賴其他服務&lt;/li&gt;
&lt;li&gt;或是動態耦合，更改服務會連動其他服務，一髮動全身&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;需要縝密的存取控管（access control）
&lt;ul&gt;
&lt;li&gt;防火牆，路由規則&lt;/li&gt;
&lt;li&gt;雲平台上，團隊成員的存取權限&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;專案的規模與複雜度增加
&lt;ul&gt;
&lt;li&gt;多環境的部署&lt;/li&gt;
&lt;li&gt;多個備援副本設定&lt;/li&gt;
&lt;li&gt;大量機器形成的集群&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;iac-的實際需求&#34;&gt;IaC 的實際需求&lt;/h1&gt;
&lt;p&gt;以下這些對話是不是很耳熟？&lt;/p&gt;
&lt;h1 id=&#34;iac-的實際需求-1&#34;&gt;IaC 的實際需求&lt;/h1&gt;
&lt;p&gt;沒有需求，就不需要找尋新的解決方案。&lt;/p&gt;
&lt;p&gt;有看上面目錄的朋友，應該知道這系列文章的後面，我會實際分享於公司內部導入 Terraform 與 IaC 方法的過程。&lt;/p&gt;
&lt;p&gt;各位讀者會找到這篇文，大概都是因為實際搜尋了 Terraform 或是 IaC 的關鍵字才找到這篇。&lt;/p&gt;
&lt;p&gt;如果沒有需求，自己因為覺得有趣而拉下來研究，&lt;/p&gt;
&lt;p&gt;如果沒有明確需求，就貿然導入
無謂增加亂度&lt;/p&gt;
&lt;h1 id=&#34;iac-的實現工具&#34;&gt;IaC 的實現工具&lt;/h1&gt;
&lt;h1 id=&#34;為何選擇-terraform&#34;&gt;為何選擇 Terraform&lt;/h1&gt;
&lt;h1 id=&#34;建議&#34;&gt;建議&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;如果不熟，從 import 現有最好的資源開始。把 70 分保住，再向 80 90 邁進。&lt;/li&gt;
&lt;li&gt;善用 module 封裝，只露出會用到的參數。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Ithome Cloud Summit 2020: Terraform Infrastructure as Code</title>
      <link>https://chechia.net/talk/ithome2020-cloudsummit-terraform-iac/</link>
      <pubDate>Tue, 11 Feb 2020 17:26:22 +0800</pubDate>
      
      <guid>https://chechia.net/talk/ithome2020-cloudsummit-terraform-iac/</guid>
      <description>&lt;p&gt;Infrastructure as code 的概念已經推廣許久，不是什麼新概念，然而公司卻尚未導入。op 需要新環境都是透過 GUI 去開環境。&lt;/p&gt;
&lt;p&gt;環境越開越多，漸漸浮現出幾個痛點：&lt;/p&gt;
&lt;p&gt;「這個環境怎麼少一個設定」-&amp;gt; 環境沒標準化&lt;/p&gt;
&lt;p&gt;「是誰改了這個設定」-&amp;gt; 環境的變更沒有 change log，無法 blame&lt;/p&gt;
&lt;p&gt;「你環境開錯了吧」 -&amp;gt; 環境交付沒自動化測試&lt;/p&gt;
&lt;p&gt;其他如：只看單一環境不在乎全局設定，環境數量多不易管理或更新維護，人工操作錯誤率高&amp;hellip;等問題層出不窮。&lt;/p&gt;
&lt;p&gt;本次分享以一步一步實際導入 Terraform 的經驗，描述如何針對需求對症下藥，帶領公司導入 infrastructure as code，內容包含實作步驟，心得感想，並紀錄沿路踩過的雷，希望能提供其他團隊導入經驗。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Native Taiwan #21: Deploy Kafka on Kubernetes</title>
      <link>https://chechia.net/talk/kubernetes-kafka-deployment/</link>
      <pubDate>Thu, 17 Oct 2019 20:50:50 +0800</pubDate>
      
      <guid>https://chechia.net/talk/kubernetes-kafka-deployment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cert Manager Deployment on Kubernetes</title>
      <link>https://chechia.net/post/cert-manager-deployment/</link>
      <pubDate>Thu, 10 Oct 2019 16:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/cert-manager-deployment/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;p&gt;這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress Controller&lt;/li&gt;
&lt;li&gt;Cert-manager&lt;/li&gt;
&lt;li&gt;Kubernetes CRD &amp;amp; Operator-sdk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Cert-manager Introduction&lt;/li&gt;
&lt;li&gt;Deploy cert-manager&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;簡介-cert-manager&#34;&gt;簡介 cert-manager&lt;/h1&gt;
&lt;p&gt;TLS certificate 管理很重要，但在 kubernetes 上管理 TLS certificates 很麻煩。&lt;/p&gt;
&lt;p&gt;以往我們使用 &lt;a href=&#34;https://letsencrypt.org/zh-tw/&#34;&gt;Let&amp;rsquo;s Encrypt&lt;/a&gt; 提供的免費自動化憑證頒發，搭配 &lt;a href=&#34;https://github.com/jetstack/kube-lego&#34;&gt;kube-lego&lt;/a&gt; 來自動處理 certificate issuing，然而隨著 kube-lego 已不再更新後，官方建議改使用 &lt;a href=&#34;https://github.com/jetstack/cert-manager/&#34;&gt;Cert-manager&lt;/a&gt; 來進行 kubernetes 上的憑證自動化管理。&lt;/p&gt;
&lt;p&gt;cert-manager 是 kubernetes 原生的憑證管理 controller。是的他的核心也是一個 controller，透過 kubernetes object 定義 desired state，監控集群上的實際狀態，然後根據 resource object 產生憑證。cert-manager 做幾件事情&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 kubernetes 上 使用 CRD (Customized Resource Definition) 來定義 certificate issuing 的 desired state&lt;/li&gt;
&lt;li&gt;向 let&amp;rsquo;s encrypt 取得公開的憑證&lt;/li&gt;
&lt;li&gt;在 kubernetes 上自動檢查憑證的有效期限，並自動在有效時限內 renew certificate。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;安裝&#34;&gt;安裝&lt;/h1&gt;
&lt;p&gt;官方文件有提供 &lt;a href=&#34;https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html&#34;&gt;詳細步驟&lt;/a&gt; 可以直接使用 release 的 yaml 部屬，也可以透過 helm。&lt;/p&gt;
&lt;h3 id=&#34;使用-yaml-部屬&#34;&gt;使用 yaml 部屬&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# Create a namespace to run cert-manager in
kubectl create namespace cert-manager

# Disable resource validation on the cert-manager namespace
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;開一個獨立的 namespace 來管理 cert-manager resources&lt;/p&gt;
&lt;p&gt;取消 namespcae 中的 kubernetes validating webhook。由於 cert-manager 本身就會使用 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&#34;&gt;ValidatingWebhookConfiguration&lt;/a&gt; 來為 cert-manager 定義的 Issuer, Certificate resource 做 validating。然而這會造成 cert-manager 與 webhook 的循環依賴 (circling dependency)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Install the CustomResourceDefinitions and cert-manager itself
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# Install the CustomResourceDefinitions and cert-manager itself
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這個 yaml 裡面還有幾個元件&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cluster Role-bindings&lt;/li&gt;
&lt;li&gt;CustomResourceDefinition
&lt;ul&gt;
&lt;li&gt;certificaterequests.certmanager.k8s.io&lt;/li&gt;
&lt;li&gt;certificates.certmanager.k8s.io&lt;/li&gt;
&lt;li&gt;challenges.certmanager.k8s.io&lt;/li&gt;
&lt;li&gt;clusterissuers.certmanager.k8s.io&lt;/li&gt;
&lt;li&gt;issuers.certmanager.k8s.io&lt;/li&gt;
&lt;li&gt;orders.certmanager.k8s.io&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些元件的細節，留待運作原理分析時再詳解。&lt;/p&gt;
&lt;h3 id=&#34;helm-deployment&#34;&gt;helm deployment&lt;/h3&gt;
&lt;p&gt;這邊也附上使用 helm 安裝的步驟&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

# Install the CustomResourceDefinition resources separately
kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml

# Create the namespace for cert-manager
kubectl create namespace cert-manager

# Label the cert-manager namespace to disable resource validation
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true

# Add the Jetstack Helm repository
helm repo add jetstack https://charts.jetstack.io

# Update your local Helm chart repository cache
helm repo update

# Install the cert-manager Helm chart
helm install \
  --name cert-manager \
  --namespace cert-manager \
  --version v0.10.1 \
  jetstack/cert-managerNAMESPACE=cert-manager
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部屬完檢查一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --namespace cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-5c6866597-zw7kh               1/1     Running   0          2m
cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m
cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這邊部屬完，會獲得完整的 cert-manager 與 cert-manager CRD，但 certificate 的 desired state object 還沒部屬。也就是關於我們要如何 issue certificate 的相關描述，都還沒有 deploy， cert-manager 自然不會工作。關於 issuing resources configuration，我們下次再聊。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Nginx Ingress Controller</title>
      <link>https://chechia.net/post/kubernetes-nginx-ingress-controller/</link>
      <pubDate>Tue, 08 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/kubernetes-nginx-ingress-controller/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;p&gt;這邊該了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress Controller&lt;/li&gt;
&lt;li&gt;Cert-manager&lt;/li&gt;
&lt;li&gt;Jenkin-x on Kubernetes&lt;/li&gt;
&lt;li&gt;Kubernetes CRD &amp;amp; Operator-sdk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;nginx-ingress-controller&#34;&gt;Nginx Ingress Controller&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;簡介 nginx &amp;amp; Ingress Controller&lt;/li&gt;
&lt;li&gt;部屬並設定 nginx ingress controller&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;nginx-introduction&#34;&gt;Nginx Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://nginx.org/en/docs/&#34;&gt;Nginx&lt;/a&gt; 是一款高效能、耐用、且功能強大的 load balancer 以及 web server，也是市占率最高的 web server 之一。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高效能的 web server，遠勝傳統 apache server 的資源與效能&lt;/li&gt;
&lt;li&gt;大量的模組與擴充功能&lt;/li&gt;
&lt;li&gt;有充足的安全性功能與設定&lt;/li&gt;
&lt;li&gt;輕量&lt;/li&gt;
&lt;li&gt;容易水平擴展&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ingress--ingress-controller&#34;&gt;Ingress &amp;amp; Ingress Controller&lt;/h1&gt;
&lt;p&gt;這邊簡單講一下 &lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress/&#34;&gt;kubernetes ingress&lt;/a&gt;。當我們在使用 kubernetes 時需要將外部流量 route 到集群內部，這邊使用 Ingress 這個 api resource，來定義外部到內部的設定，例如:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;service 連接&lt;/li&gt;
&lt;li&gt;load balance 設定&lt;/li&gt;
&lt;li&gt;SSL/TLS 終端&lt;/li&gt;
&lt;li&gt;虛擬主機設定&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一個簡單的 ingress 大概長這樣&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: test
          servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;除了一般的 k8s 資源，nginx 主要的設定會落在 spec，以及依賴底下實作不同，額外設定的 annotation。&lt;/p&gt;
&lt;p&gt;這邊可以看到 spec.rule 定義了外部 http 流量，引導到 backend service 的路徑。&lt;/p&gt;
&lt;p&gt;annotations 下已經標註的 nginx.ingress 的 annotation，來快速增加額外的設定。&lt;/p&gt;
&lt;h1 id=&#34;ingress--ingress-controller-1&#34;&gt;Ingress &amp;amp; Ingress Controller&lt;/h1&gt;
&lt;p&gt;雖然已經指定 nginx 的 annotation，但這邊要注意，ingress resource 本身是不指定底層的實現 (ingress controller)，也就是說，底下是 nginx 也好，traefik 也行，只要能夠實現 ingress 裏頭設定的 routing rules 就可以。&lt;/p&gt;
&lt;p&gt;只設定好 ingress，集群上是不會有任何作用的，還需要在集群上安裝 ingress controller 的實作，實作安裝完了以後，會依據 ingress 的設定，在 controller 裏頭實現，不管是 routing、ssl/tls termination、load balancing 等等功能。如同許多 Kubernetes resource 的設計理念一樣，這邊也很優雅的用 ingress 與 ingress controller，拆分的需求設定與實作實現兩邊的職責。&lt;/p&gt;
&lt;p&gt;例如以 nginx ingress controller，安裝完後會依據 ingress 的設定，在 nginx pod 裡設定對應的 routing rules，如果有 ssl/tls 設定，也一併載入。&lt;/p&gt;
&lt;p&gt;Kubernetes 官方文件提供了&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers&#34;&gt;許多不同的 controller&lt;/a&gt; 可以依照需求選擇。&lt;/p&gt;
&lt;p&gt;但如果不知道如何選擇，個人會推薦使用 nginx ingress controller，穩定、功能強大、設定又不至於太過複雜，基本的設定就能很好的支撐服務，不熟悉的大德們比較不容易被雷到。&lt;/p&gt;
&lt;p&gt;底下我們就要來開始使用 nginx ingress controller。&lt;/p&gt;
&lt;h1 id=&#34;deployment&#34;&gt;Deployment&lt;/h1&gt;
&lt;p&gt;我們這邊使用的 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;ingress-nginx&lt;/a&gt; 是 kubernetes org 內維護的專案，專案內容主要是再 k8s 上執行 nginx，抽象與實作的整合，並透過 configmap 來設定 nginx。針對 nginx ingress kubernetes 官方有提供&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/&#34;&gt;非常詳細的說明文件&lt;/a&gt; ，剛接觸 nginx 的大德可以透過這份文件，快速的操作 nginx 的設定，而不用直接寫 nginx.conf 的設定檔案。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;repo 版本是 nginx-0.26.1&lt;/li&gt;
&lt;li&gt;Image 版本是 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;
&lt;p&gt;我們這邊用 helm 部屬，&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress&#34;&gt;Nginx Ingress Controller Stable Chart&lt;/a&gt;，讓各位大德用最簡單的步驟，獲得一個功能完整的 nginx ingress controller。&lt;/p&gt;
&lt;p&gt;與前面幾個 helm chart 一樣，我們可以先取得 default values.yaml 設定檔，再進行更改。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget https://raw.githubusercontent.com/helm/charts/master/stable/nginx-ingress/values.yaml
$ vim values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安裝時也可以使用 &amp;ndash;set 來變更&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration&#34;&gt;安裝 chart 時的 parameters&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress \
	--set controller.metrics.enabled=true \
	-f values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安裝完後，resource 很快就起來。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get all --selector app=nginx-ingress
NAME                                                 READY   STATUS    RESTARTS   AGE
pod/nginx-ingress-controller-7bbcbdcf7f-tx69n        1/1     Running   0          216d
pod/nginx-ingress-default-backend-544cfb69fc-rnn6h   1/1     Running   0          216d

NAME                                    TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                      AGE
service/nginx-ingress-controller        LoadBalancer   10.15.246.22   34.35.36.37    80:30782/TCP,443:31933/TCP   216d
service/nginx-ingress-default-backend   ClusterIP      10.15.243.19   &amp;lt;none&amp;gt;         80/TCP                       216d

NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-ingress-controller        1/1     1            1           216d
deployment.apps/nginx-ingress-default-backend   1/1     1            1           216d

NAME                                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-ingress-controller-7bbcbdcf7f        1         1         1       216d
replicaset.apps/nginx-ingress-default-backend-544cfb69fc   1         1         1       216d

kubectl get configmap -l app=nginx-ingress
NAME                       DATA   AGE
nginx-ingress-controller   2      216d

kubectl get ingress
NAME            HOSTS                  ADDRESS       PORTS     AGE
ingress-nginx   api.chechiachang.com   34.35.36.37   80, 443   216d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;兩個 Pods&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx ingress controller 是主要的 nginx pod，裡面跑的是 nginx&lt;/li&gt;
&lt;li&gt;Nginx default backend 跑的是 default backend，nginx 看不懂了 route request 都往這邊送。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Service&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;nginx-ingress-contrller 是我們在 GCP 上，在集群外部的 GCP 上的對外接口。如果在不同平台上，依據預設 service load balancer 有不同實作。&lt;/li&gt;
&lt;li&gt;在 gcp 上，會需要時間來啟動 load balancer，等 load balancer 啟動完成，service 這邊就可以取得外部的 ip，接受 load balancer 來的流量&lt;/li&gt;
&lt;li&gt;另外一個 service 就是 default backend 的 service&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;踩雷&#34;&gt;踩雷&lt;/h1&gt;
&lt;p&gt;第一個雷點是 helm chart install 帶入的 &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/nginx-ingress#configuration&#34;&gt;parameters&lt;/a&gt;，有些 parameter 是直接影響 deployment 的設定，如果沒注意到，安裝完後沒辦法透過 hot reload 來處理，只能幹掉重來。建議把這份表格都看過一次，再依照環境與需求補上。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ helm install stable/nginx-ingress \
	--set controller.metrics.enabled=true \
	--set controller.service.externalTrafficPolicy=Local \
	-f values.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這邊開了 prometheus metrics exporter，以及 source IP preservation。&lt;/p&gt;
&lt;h1 id=&#34;nginx-config&#34;&gt;Nginx Config&lt;/h1&gt;
&lt;p&gt;再安裝完後，外部的 load balancer 啟用後，就可以透過 GCP 的 external ip 連入 nginx，nginx 依照設定的 rule 向後端服務做集群內的 load balancing 與 routing。&lt;/p&gt;
&lt;p&gt;如果在使用過程中，有需要執行更改設定，或是 hot reload config，在 kubernetes 上要如何做呢? 我們下回分解。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Nginx Ingress Controller Config</title>
      <link>https://chechia.net/post/kubernetes-nginx-ingress-config/</link>
      <pubDate>Tue, 08 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/kubernetes-nginx-ingress-config/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;p&gt;這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress Controller&lt;/li&gt;
&lt;li&gt;Cert-manager&lt;/li&gt;
&lt;li&gt;Jenkin-x on Kubernetes&lt;/li&gt;
&lt;li&gt;Kubernetes CRD &amp;amp; Operator-sdk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Nginx Ingress Controller 運作原理&lt;/li&gt;
&lt;li&gt;設定 Nginx Ingress Controller&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;運作原理&#34;&gt;運作原理&lt;/h1&gt;
&lt;p&gt;昨天講完 nginx ingress controller 部屬，今天來談談 controller 是如何運作的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nginx 使用 config file (nginx.conf) 做全域設定，為了讓 nginx 能隨 config file 更新，controller 要偵測 config file 變更，並且 reload nginx&lt;/li&gt;
&lt;li&gt;針對 upstream (後端 app 的 endpoint) 變更，使用 lua-nginx-module 來更新。因為 kubernetes 上，service 後的服務常常會動態的變更，scaling，但 endpint ip list 又需要更新到 nginx，所以使用 lua 額外處理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 kubernetes 上要如何做到上述兩件事呢?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一般 controller 都使用同步 loop 來檢查 current state 是否與 desired state&lt;/li&gt;
&lt;li&gt;desired state 使用 k8s object 描述，例如 ingress, services, configmap 等等 object&lt;/li&gt;
&lt;li&gt;Nginx ingress controller 這邊使用的是 client-go 中的 Kubernetes Informer 的 &lt;a href=&#34;https://godoc.org/k8s.io/client-go/informers#SharedInformerFactory&#34;&gt;SharedInformer&lt;/a&gt;，可以根據 object 的更新執行 callback&lt;/li&gt;
&lt;li&gt;由於無法檢查每一次的 object 更動，是否對 config 產生影響，這邊直接每次更動都產生全新的 model&lt;/li&gt;
&lt;li&gt;如果新產生的 model 與現有相同，就跳過 reload&lt;/li&gt;
&lt;li&gt;如果 model 只影響 endpoint，使用 nginx 內部的 lua handler 產生新的 endpoint list，來避免因為 upstream 服務變更造成的頻繁 reload&lt;/li&gt;
&lt;li&gt;如果新 Model 影響不只 endpoint，則取代現有 model，然後觸發 reload&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具體會觸發 reload 的事件，&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/how-it-works/#when-a-reload-is-required&#34;&gt;請見官方文件&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;除了監測 objects，build model，觸發 reload，之前 controller 還會將 ingress 送到 &lt;a href=&#34;https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook&#34;&gt;kubernetes validating admission webhook server&lt;/a&gt; 做驗證，避免描述 desired state 的 ingress 有 syntax error，導致整個 controller 爆炸。&lt;/p&gt;
&lt;h1 id=&#34;configuration&#34;&gt;Configuration&lt;/h1&gt;
&lt;p&gt;要透過 controller 更改 nginx 設定，有以下三種方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;更改 configmap，對全域的 controller 設定&lt;/li&gt;
&lt;li&gt;更改 ingress 上的 annotation，這些 annotation 針對獨立 ingress 生效&lt;/li&gt;
&lt;li&gt;有更深入的客製化，是上述兩者達不到或尚未實作，可以使用 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/custom-template/&#34;&gt;Custom Template&lt;/a&gt; 來做到，把 nginx.tmpl mount 進 controller&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;configmap&#34;&gt;Configmap&lt;/h1&gt;
&lt;p&gt;由於把全域設定放到 configmap 上，nginx ingress controller 非常好調度與擴展，&lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/&#34;&gt;controller 官方說明文件&lt;/a&gt; 除了列出目前已經支援的設定外，也直接附上 nginx 官方的文件說明連結，讓使用者查詢時方便比對。&lt;/p&gt;
&lt;p&gt;當需要更改需求，可以 google nginx 的關鍵字，找到 nginx 上設定的功能選項後，來 controller 的文件，找看看目前是否已經支援。有時候有需要對照 nginx 官方文件，來正確設定 controller。&lt;/p&gt;
&lt;h1 id=&#34;annotation&#34;&gt;Annotation&lt;/h1&gt;
&lt;p&gt;有很多 Nginx 的設定是根據 ingress 不同而有調整，例如針對這個 ingress 做白名單，設定 session，設定 ssl 等等，這些針對特定 ingress 所做的設定，可以直接寫在 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/&#34;&gt;ingress annotation&lt;/a&gt; 裡面。&lt;/p&gt;
&lt;p&gt;例如下面這個 Ingress&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: &#39;true&#39;
    certmanager.k8s.io/cluster-issuer: letsencrypt-prod
    kubernetes.io/ingress.allow-http: &amp;quot;true&amp;quot;
    ingress.kubernetes.io/force-ssl-redirect: &amp;quot;true&amp;quot;
    nginx.ingress.kubernetes.io/whitelist-source-range: &amp;quot;34.35.36.37&amp;quot;
    nginx.ingress.kubernetes.io/proxy-body-size: &amp;quot;20m&amp;quot;
    ingress.kubernetes.io/proxy-body-size: &amp;quot;20m&amp;quot;
    # https://github.com/Shopify/ingress/blob/master/docs/user-guide/nginx-configuration/annotations.md#custom-nginx-upstream-hashing
    nginx.ingress.kubernetes.io/load-balance: &amp;quot;ip_hash&amp;quot;
    # https://kubernetes.github.io/ingress-nginx/examples/affinity/cookie/
    nginx.org/server-snippets: gzip on;
    nginx.ingress.kubernetes.io/affinity: &amp;quot;cookie&amp;quot;
    nginx.ingress.kubernetes.io/session-cookie-name: &amp;quot;route&amp;quot;
    nginx.ingress.kubernetes.io/session-cookie-hash: &amp;quot;sha1&amp;quot;
    nginx.ingress.kubernetes.io/session-cookie-expires: &amp;quot;3600&amp;quot;
    nginx.ingress.kubernetes.io/session-cookie-max-age: &amp;quot;3600&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;nginx.ingress.kubernetes.io
&lt;ul&gt;
&lt;li&gt;whitelist-source-range: 只允許白名單 ip&lt;/li&gt;
&lt;li&gt;load-balance: &amp;ldquo;ip_hash&amp;rdquo;: 更改預設 round_robin 的 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#load-balance&#34;&gt;load balance&lt;/a&gt;，為了做 session cookie&lt;/li&gt;
&lt;li&gt;affinity: &amp;ldquo;cookie&amp;rdquo;: 設定 upstream 的 &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#session-affinity&#34;&gt;session affinity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;session-cookie-name: &amp;ldquo;route&amp;rdquo;&lt;/li&gt;
&lt;li&gt;session-cookie-hash: &amp;ldquo;sha1&amp;rdquo;&lt;/li&gt;
&lt;li&gt;session-cookie-expires: &amp;ldquo;3600&amp;rdquo;&lt;/li&gt;
&lt;li&gt;session-cookie-max-age: &amp;ldquo;3600&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果後端 server 有 session 需求，希望相同 source ip 來的 request 能持續到相同的 endpoint。才做了以上設定。&lt;/p&gt;
&lt;h1 id=&#34;helm-configuration&#34;&gt;helm configuration&lt;/h1&gt;
&lt;p&gt;helm 的 configuration 也是重要的設定，這裡在安裝時決定了 nginx ingress controller 的 topology、replicas、resource、k8s runtime 設定如 healthz &amp;amp; readiness、其實都會影響 nginx 具體的設定。這部分就會有很多考量。有機會我們再來分享。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prometheus &amp; Kubernetes State Metrics Exporter</title>
      <link>https://chechia.net/post/prometheus-kube-state-metrics-exporter/</link>
      <pubDate>Mon, 07 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/prometheus-kube-state-metrics-exporter/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus / Grafana (5)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deployment-on-kubernetes/&#34;&gt;GKE 上自架 Prometheus / Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GKE 上自架 Grafana 與設定&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;Node Exporter 與 kube metrics exporter&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;li&gt;自幹 exporter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;如果要透過 prometheus 來監控集群的運行狀況，有兩個 exporter 是必裝的，一個是把 node 狀態 export 出來的 node exporter，一個是把 kubernetes 集群狀態 export 出來的 kube state metrics exporter。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node Exporter 簡介&lt;/li&gt;
&lt;li&gt;kube metrics exporter 安裝與設定&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;node-exporter&#34;&gt;Node Exporter&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/prometheus/node_exporter&#34;&gt;Node Exporter&lt;/a&gt; 是 prometheus 官方維護的一個子項目，主要在把類 unix 硬體 kernel 的 metrics 送出來。官方也支援 windows node 與 nvidia gpu metrics，可以說是功能強大。&lt;/p&gt;
&lt;p&gt;為了能夠監測 kubernetes node 的基礎設施狀態，通常都會使用 node exporter。&lt;/p&gt;
&lt;p&gt;node exporter 安裝，我們在安裝 prometheus helm chart 時就一並安裝了。這邊看一下設定與運行。&lt;/p&gt;
&lt;h1 id=&#34;collectors&#34;&gt;Collectors&lt;/h1&gt;
&lt;p&gt;Node exporter 把不同位置收集到的不同類型的 metrics ，做成各自獨立的 colletor，使用者可以根據求需求來啟用或是不啟用 collector，&lt;a href=&#34;https://github.com/prometheus/node_exporter#enabled-by-default&#34;&gt;完整的 collector 目錄&lt;/a&gt; 在這邊。&lt;/p&gt;
&lt;p&gt;如果有看我們第一部份的 ELK part，應該會覺得這裡的設定，跟 metricbeat 非常像，基本上這兩者做的事情是大同小異的，收集 metrics 來源都是同樣的類 unix 系統，只是往後送的目標不一樣 (雖然現在兩者都可以兼容混搭了)。如果有接觸過其他平台的 metrics collector，也會發現其實大家做的都差不多。&lt;/p&gt;
&lt;h1 id=&#34;textfile-collector&#34;&gt;Textfile Collector&lt;/h1&gt;
&lt;p&gt;Prometheus 除了有 scrape 機制，讓 prometheus 去 exporter 撈資料外，還有另外一個機制，叫做 &lt;a href=&#34;https://github.com/prometheus/pushgateway&#34;&gt;Pushgateway&lt;/a&gt;，這個我們在部屬 prometheus 時也部屬了一個。這邊簡單說明一下。&lt;/p&gt;
&lt;p&gt;經常性執行的服務(redis, kafka,&amp;hellip;)會一直運行，prometheus 透過這些服務的 metrics 取得 runtime metrics，作為監控資料。可是有一些 job 是暫時性的任務，例如果一個 batch job，這些服務不會有一直運行的 runtime metrics，也不會有 exporter。但這時又希望監控這些 job 的狀態，就可以使用 Pushgateway。&lt;/p&gt;
&lt;p&gt;Pushgateway 的作用機制，就是指定收集的目標資料夾，需要監測的 batch job，只要把希望監測的資料，寫到該資料夾。Pushgateway 會依據寫入的資料，轉成 time series metrics，並且 export 出來。&lt;/p&gt;
&lt;p&gt;這種去 tail 指定目錄檔案，然後把 metrics 後送的機制，是否跟 filebeat 有一點類似? 只是 filebeat 一般取得資料後，會主動推送到 ELK 上，prometheus pushgateway 會暴露出 metrics 後，讓 prometheus server 來 scrape。&lt;/p&gt;
&lt;p&gt;Pushgateway 也會在收集資料時打上需要的 label，方面後段處理資料。&lt;/p&gt;
&lt;h1 id=&#34;kubernetes-state-metrics-exporter&#34;&gt;Kubernetes State Metrics (Exporter)&lt;/h1&gt;
&lt;p&gt;Node Exporter 將 kubernetes 集群底下的 Node 的硬體狀態，例如 cpu, memory, storage,&amp;hellip; expose 出來，然而我們在維運 kubernetes 還需要從 api server 獲得集群內部的資料，例如說 pod state, container state, endpoints, service, &amp;hellip;等，這邊可以使用 kube-state-metrics 來處理。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics&#34;&gt;kube-state-metrics&lt;/a&gt; 是 kubernetes 官方維護的專案，做的事情就是向 api server 詢問 kubernetes 的 state，例如 pod state, deployment state，然後跟 prometheus exporter 一，開放一個 http endpoint，讓需要的服務來 scrape metrics。&lt;/p&gt;
&lt;p&gt;工作雲裡也很單純，kubernetes api server 可以查詢 pod 當下的狀態，kube-state-metrics 則會把當下的狀態依照時間序，做成 time series 的 metrics，例如這個 pod 什麼時候是活著，什麼時候因為故障而 error。&lt;/p&gt;
&lt;p&gt;kube-state-metrics 預設的輸出格式是 plaintext，直接符合 Prometheus client endpoint 的格式&lt;/p&gt;
&lt;h1 id=&#34;deployment&#34;&gt;Deployment&lt;/h1&gt;
&lt;p&gt;如果依照第一篇安裝 prometheus helm 的步驟，現在應該已經安裝完 kube-state-metrics 了。如果沒有安裝，也可以依照官方說明的基本範例安裝。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone git@github.com:kubernetes/kube-state-metrics.git

cd kube-state-metrics

kubectl apply -f examples/standard/*.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安裝完可以看到&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get pods --selector &#39;app=prometheus,component=kube-state-metrics&#39;

NAME                                             READY   STATUS    RESTARTS   AGE
prometheus-kube-state-metrics-85f6d75f8b-7vlkp   1/1     Running   0          201d

$ kubectl get svc --selector &#39;app=prometheus,component=kube-state-metrics&#39;

NAME                            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
prometheus-kube-state-metrics   ClusterIP   None         &amp;lt;none&amp;gt;        80/TCP    201d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我們可以透過 service 打到 pod 的 /metrics 來取得 metrics。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it busybox sh

curl prometheus-kube-state-metrics:8080
&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Kube Metrics Server&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;Kube Metrics&amp;lt;/h1&amp;gt;
    &amp;lt;ul&amp;gt;
    	&amp;lt;li&amp;gt;&amp;lt;a href=&#39;https://chechia.net/metrics&#39;&amp;gt;metrics&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
    	&amp;lt;li&amp;gt;&amp;lt;a href=&#39;https://chechia.net/healthz&#39;&amp;gt;healthz&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;
    &amp;lt;/ul&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

curl prometheus-kube-state-metrics:8081

&amp;lt;html&amp;gt;
    &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;Kube-State-Metrics Metrics Server&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
    &amp;lt;body&amp;gt;
    &amp;lt;h1&amp;gt;Kube-State-Metrics Metrics&amp;lt;/h1&amp;gt;
    &amp;lt;ul&amp;gt;
    	&amp;lt;li&amp;gt;
			&amp;lt;a href=&#39;https://chechia.net/metrics&#39;&amp;gt;metrics&amp;lt;/a&amp;gt;
		&amp;lt;/li&amp;gt;
    &amp;lt;/ul&amp;gt;
    &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這邊有兩套 metrics，一個是 kube-state-metrics 自己自我監測的 metrics，在 8081，另外一個才是 kube metrics，在 8080，兩個都要收，記得不要收錯了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl prometheus-kube-state-metrics:8080/metrics

打下去就可以看到超多 metrics 。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics/tree/master/docs&#34;&gt;Metrics 的清單與說明文件&lt;/a&gt;，有用到的 metrics 使用前都可以來查一下定義解釋。&lt;/p&gt;
&lt;p&gt;理論上不用每個 metrics 都 expose 出來，有需要可以把不會用到的 metrics 關一關，可以節省 kube-state-metrics 的 cpu 消耗。&lt;/p&gt;
&lt;h1 id=&#34;resource-recommendation&#34;&gt;Resource Recommendation&lt;/h1&gt;
&lt;p&gt;kube-state-metrics 很貼心的還附上&lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics#scaling-kube-state-metrics&#34;&gt;建議的資源分配&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;As a general rule, you should allocate

200MiB memory
0.1 cores
For clusters of more than 100 nodes, allocate at least

2MiB memory per node
0.001 cores per node
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;scaling&#34;&gt;Scaling&lt;/h1&gt;
&lt;p&gt;kube-state-metrics 還有提供 horizontal scaling 的解決方案，如果你的集群很大，node 數量已經讓 kube-state-metrics 無法負荷，也可以使用 sharding 的機制，把 metrics 的工作散布到多個 kube-state-metrics，再讓 prometheus 去收集統整。這部分我覺得很有趣，但還沒實作過，我把&lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics#horizontal-scaling-sharding&#34;&gt;文件&lt;/a&gt; 放在這邊，有緣大德有時做過請來討論分享。&lt;/p&gt;
&lt;h1 id=&#34;dashboard&#34;&gt;Dashboard&lt;/h1&gt;
&lt;p&gt;metrics 抓出來，當然要開一下 dashboard，這邊使用的是這個&lt;a href=&#34;https://grafana.com/grafana/dashboards/7249&#34;&gt;kubernetes cluster&lt;/a&gt;，支援&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node exporter&lt;/li&gt;
&lt;li&gt;kube state metrics&lt;/li&gt;
&lt;li&gt;nginx ingress controller&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;三個願望一次滿足~&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;跑 kubernetes 務必使用這兩個 exporter&lt;/li&gt;
&lt;li&gt;kube-state-metrics 整理得很舒服，有時間可以多看看這個專案&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Prometheus Exporter Library &amp; Redis Exporter</title>
      <link>https://chechia.net/post/prometheus-exporter-library-redis-exporter/</link>
      <pubDate>Sun, 06 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/prometheus-exporter-library-redis-exporter/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus / Grafana (5)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deployment-on-kubernetes/&#34;&gt;GKE 上自架 Prometheus / Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GKE 上自架 Grafana 與設定&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;自幹 exporter&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 kubernetes 的監測數據&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Exporter 工作原理簡介&lt;/li&gt;
&lt;li&gt;Prometheus exporter library&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;exporter-workflow&#34;&gt;Exporter workflow&lt;/h1&gt;
&lt;p&gt;上次講到 exporter 可以從服務端把運行資料抽出來，並開成 http endpoint，讓 prometheus 來 scrape metrics。那 exporter 本身是如何取得服務內部的 metrics 呢? 我們今天就稍微看一下。&lt;/p&gt;
&lt;h1 id=&#34;redis-exporter&#34;&gt;Redis Exporter&lt;/h1&gt;
&lt;p&gt;我們今天以 &lt;a href=&#34;https://github.com/oliver006/redis_exporter&#34;&gt;Redis Exporter&lt;/a&gt; 為例，研究一下外部的 exporter 是如何取得 redis 內部的 metrcs。&lt;/p&gt;
&lt;p&gt;Redis exporter 是用 golang 寫的一個小程式，總共算算才 1000 行，而且很多都是對 redis 內部 metrics 的清單，以及轉化成 prometheus metrics 的 tool functions，主要的邏輯非常簡單。我們簡單看一下源碼。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/oliver006/redis_exporter/blob/master/exporter.go#L386&#34;&gt;Collect&lt;/a&gt; 是主要的收集邏輯，就是執行 scrapeRedisHost(ch) ，然後把收集到的資訊，使用 &lt;a href=&#34;https://github.com/prometheus/client_golang&#34;&gt;Prometheus Go Client Library&lt;/a&gt; 的工具將資料註冊成 prometheus metrics&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;func (e *Exporter) Collect(ch chan&amp;lt;- prometheus.Metric) {
	e.Lock()
	defer e.Unlock()
	e.totalScrapes.Inc()

	if e.redisAddr != &amp;quot;&amp;quot; {
		start := time.Now().UnixNano()
		var up float64 = 1

    // 從 host scrape 資料，然後塞進 channel streaming 出來。
		if err := e.scrapeRedisHost(ch); err != nil {
			up = 0
			e.registerConstMetricGauge(ch, &amp;quot;exporter_last_scrape_error&amp;quot;, 1.0, fmt.Sprintf(&amp;quot;%s&amp;quot;, err))
		} else {
			e.registerConstMetricGauge(ch, &amp;quot;exporter_last_scrape_error&amp;quot;, 0, &amp;quot;&amp;quot;)
		}

		e.registerConstMetricGauge(ch, &amp;quot;up&amp;quot;, up)
		e.registerConstMetricGauge(ch, &amp;quot;exporter_last_scrape_duration_seconds&amp;quot;, float64(time.Now().UnixNano()-start)/1000000000)
	}

	ch &amp;lt;- e.totalScrapes
	ch &amp;lt;- e.scrapeDuration
	ch &amp;lt;- e.targetScrapeRequestErrors
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;scrapeRedisHost 內部的主要邏輯，又集中在&lt;a href=&#34;https://github.com/oliver006/redis_exporter/blob/master/exporter.go#L1144&#34;&gt;執行 Info&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  // 執行 info 
	infoAll, err := redis.String(doRedisCmd(c, &amp;quot;INFO&amp;quot;, &amp;quot;ALL&amp;quot;))
	if err != nil {
		infoAll, err = redis.String(doRedisCmd(c, &amp;quot;INFO&amp;quot;))
		if err != nil {
			log.Errorf(&amp;quot;Redis INFO err: %s&amp;quot;, err)
			return err
		}
	}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也就是說當我們在 redis-cli 連入 redis 時，可以執行 Info command，取得 redis 內部的資訊，包含節點設店與狀態，集群設定，資料的統計數據等等。然後 exporter 這邊維護持續去向 redis 更新 info ，並且把 info data 轉化成 time series 的 metrcs，再透過 &lt;a href=&#34;https://github.com/prometheus/client_golang/tree/master/prometheus/promhttp&#34;&gt;Prometheus Client promhttp&lt;/a&gt; 提供的 http endpoint library，變成 http endpoint。&lt;/p&gt;
&lt;p&gt;首先看一下 &lt;a href=&#34;https://redis.io/commands/info&#34;&gt;redis info command 的文件&lt;/a&gt;，這邊有說明 info 的 option ，以及 option 各自提供的資料，包括 server 狀態，賀戶端連線狀況，系統資源，複本狀態等等。我們也可以自己透過 info 取得資料。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get po | grep redis

redis-2-redis-ha-server-0                                3/3     Running     0          11d
redis-2-redis-ha-server-1                                3/3     Running     0          11d
redis-2-redis-ha-server-2                                3/3     Running     0          11d

$ kubectl exec -it redis-2-redis-ha-server-0  sh
$ redis-cli -h haproxy-service  -a REDIS_PASSWORD
$ haproxy-service:6379&amp;gt;

$ haproxy-service:6379&amp;gt; info server
# Server
redis_version:5.0.5
redis_git_sha1:00000000
redis_git_dirty:0
redis_build_id:4d072dc1c62d5672
redis_mode:standalone
os:Linux 4.14.127+ x86_64
arch_bits:64
multiplexing_api:epoll
atomicvar_api:atomic-builtin
gcc_version:8.3.0
process_id:1
run_id:63a97460b7c3745577931dad406df9609c4e2464
tcp_port:6379
uptime_in_seconds:976082
uptime_in_days:11
...

$ haproxy-service:6379&amp;gt; info clients
# Clients
connected_clients:100
client_recent_max_input_buffer:2
client_recent_max_output_buffer:0
blocked_clients:1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Redis exporter 收集這些數據，透過 prometheus client library 把資料轉成 time series prometheus metrics。然後透過 library 放在 http enpoint 上。&lt;/p&gt;
&lt;p&gt;配合上次說過的 redis overview dashboard，可以直接在 Grafana 上使用&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/1222339/19412031/897549c6-92da-11e6-84a0-b091f9deb81d.png&#34; alt=&#34;Redis Overvies library&#34;&gt;&lt;/p&gt;
&lt;p&gt;這邊 dashboard 顯示幾個重要的 metrics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uptime&lt;/li&gt;
&lt;li&gt;Memory Usage，要設定用量太高自動報警&lt;/li&gt;
&lt;li&gt;Command 的執行狀況，回應時間&lt;/li&gt;
&lt;li&gt;訊息的流量，以及超出 time-to-live 的資料清除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;都是需要好好加上 alert 的核心 metrics&lt;/p&gt;
&lt;h1 id=&#34;貢獻-exporter&#34;&gt;貢獻 exporter&lt;/h1&gt;
&lt;p&gt;其他服務的 exporter 工作原理也相似，如果服務本身有內部的 metrics，可以透過 client command 或是 API 取得，exporter 的工作就只是轉成 time series data。&lt;/p&gt;
&lt;p&gt;如果有比較特殊的 metrics 沒有匯出，例如說自家的 metrics ，但又希望能放到 prometheus 上監測，例如每秒收到多少 request count，回應速度，錯誤訊息的統計&amp;hellip;&amp;hellip;等，這點也可以使用 client library 自幹 exporter 然後 expose http endpoint，這樣在 prometheus 上也可以看到自家產品的 metrics，非常好用。有機會我們來聊自幹 exporter。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prometheus Deployment on Kubernetes</title>
      <link>https://chechia.net/post/prometheus-deployment-on-kubernetes/</link>
      <pubDate>Fri, 04 Oct 2019 16:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/prometheus-deployment-on-kubernetes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus / Grafana (5)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deployment-on-kubernetes/&#34;&gt;GKE 上自架 Prometheus / Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deploy-grafana/&#34;&gt;GKE 上自架 Grafana 與設定&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 kubernetes 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus Introduction&lt;/li&gt;
&lt;li&gt;Deploy Prometheus&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;prometheus-introduction&#34;&gt;Prometheus Introduction&lt;/h1&gt;
&lt;p&gt;生產環境與非生產環境，其中的一指標就是有沒有足夠完整的服務監測系統，這句話可以看出服務監測對於產品化是多麼重要。而監控資料 (metrics) 的收集與可視化工具其實非常多，例如上周介紹的 ELK Stack，這次我們要來介紹另外一個很多人使用的 prometheus。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://prometheus.io/&#34;&gt;Promethues 在官網上提到&lt;/a&gt; 是一個 Monitoring system and time series database&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以收集高維度的資料&lt;/li&gt;
&lt;li&gt;使用自己的 PromQL 做有效且精簡的資料查詢&lt;/li&gt;
&lt;li&gt;內建資料瀏覽器，並且與 Grafana 高度整合&lt;/li&gt;
&lt;li&gt;支援 sharding 與 federation，來達到水平擴展&lt;/li&gt;
&lt;li&gt;有許多隨插即用的整合 exporter，例如 redis-exporter, kafka-exporter，kubernetes-exporter ，都可以直接取得資料&lt;/li&gt;
&lt;li&gt;支援 alert，使用 PromQL 以及多功能的告警，可以設定精準的告警條件&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;與-elk-做比較&#34;&gt;與 ELK 做比較&lt;/h1&gt;
&lt;p&gt;基本上 Prometheus 跟 ELK 比，其實是很奇怪的一件事，但這也是最常被問的一個問題。兩者在本質上是完全不同的系統。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus 是 based on time series database 的資料收集系統&lt;/li&gt;
&lt;li&gt;ELK 是基於全文搜索引擎的資料查詢系統&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;是的，他們都能做 metrics 收集，在有限的尺度下，能達到一樣的效果。但這樣說的意思就等於是在說 mesos DC/OS 與 kubenetes 都能跑 container cluster 一樣，底下是完全不一樣的東西。&lt;/p&gt;
&lt;p&gt;兩者的差異使用上差非常多&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;metrics 結構: ELK 借助全文搜索引擎，基本上送什麼資料近來都可以查找。Prometheus metrics 拉進來是 time series 的 key-value pairs。&lt;/li&gt;
&lt;li&gt;維護同樣的 metrics，prometheus 的使用的儲存空間遠小於 elasticsearch&lt;/li&gt;
&lt;li&gt;prometheus 針對 time based 的搜尋做了很多優化，效能很高&lt;/li&gt;
&lt;li&gt;Prometheus 對於記憶體與 cpu 的消耗也少很多&lt;/li&gt;
&lt;li&gt;Elasticsearch 資源上很貴，是因為在處理大量 text log 的時候，他能夠用後段的 pipeline 處理內容，再進行交叉比對，可以從 text 裡面提取很多未事先定義的資料&lt;/li&gt;
&lt;li&gt;Elasticsearch 的維護工作也比較複雜困難&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果要收集服務運行資料，可以直接選 prometheus。如果有收集 log 進行交叉比對，可以考慮 elk。&lt;/p&gt;
&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;
&lt;p&gt;我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 prometheus。所以我們先用。&lt;/p&gt;
&lt;p&gt;沒用過 helm 的大德可以參考 &lt;a href=&#34;https://helm.sh/docs/using_helm/#quickstart&#34;&gt;Helm Quickstart&lt;/a&gt;，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好&lt;/p&gt;
&lt;h1 id=&#34;deploy-prometheus&#34;&gt;Deploy Prometheus&lt;/h1&gt;
&lt;p&gt;我把我的寶藏都放在這了&lt;a href=&#34;https://github.com/chechiachang/prometheus-kubernetes&#34;&gt;https://github.com/chechiachang/prometheus-kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下載下來的 .sh ，跑之前養成習慣貓一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat install.sh

#!/bin/bash
HELM_NAME=prometheus-1

helm upgrade --install ${HELM_NAME} stable/prometheus \
  --namespace default \
  --values values-staging.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/prometheus&#34;&gt;Prometheus Stable Chart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;values.yaml 很長，但其實各個元件設定是重複的,設定好各自的 image,
replicas, service, topology 等等&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alertmanager:
  enabled: true

kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true

server:
  enabled: true

pushgateway:
  enabled: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;底下有更多 runtime 的設定檔&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定義好 global 的 scrape 間距，越短 metrics 維度就越精準&lt;/li&gt;
&lt;li&gt;PersistenVolume 強謝建議開起來，維持歷史的資料
&lt;ul&gt;
&lt;li&gt;加上 storage usage 的 self monitoring（之後會講) 才不會滿出來 server 掛掉&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;server 的 scrapeConfigs 是 server 去收集的 job 設定。稍後再來細講。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;server:
  global:
    ## How frequently to scrape targets by default
    ##
    scrape_interval: 10s
    ## How long until a scrape request times out
    ##
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    ##
    evaluation_interval: 10s
  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: &amp;quot;&amp;quot;

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 80Gi

alertmanagerFiles:

serverFiles:

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部屬完看一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --selector=&#39;app=prometheus&#39;

NAME                                             READY   STATUS    RESTARTS   AGE
prometheus-alertmanager-694d6694c6-dvkwd         2/2     Running   0          8d
prometheus-kube-state-metrics-85f6d75f8b-7vlkp   1/1     Running   0          8d
prometheus-node-exporter-2mpjc                   1/1     Running   0          8d
prometheus-node-exporter-kg7fj                   1/1     Running   0          51d
prometheus-node-exporter-snnn5                   1/1     Running   0          8d
prometheus-pushgateway-5cdfb4979c-dnmjn          1/1     Running   0          8d
prometheus-server-59b8b8ccb4-bplkx               2/2     Running   0          8d

kubectl get services --selector=&#39;app=prometheus&#39;

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
prometheus-alertmanager         ClusterIP   10.15.241.66   &amp;lt;none&amp;gt;        80/TCP     197d
prometheus-kube-state-metrics   ClusterIP   None           &amp;lt;none&amp;gt;        80/TCP     197d
prometheus-node-exporter        ClusterIP   None           &amp;lt;none&amp;gt;        9100/TCP   197d
prometheus-pushgateway          ClusterIP   10.15.254.0    &amp;lt;none&amp;gt;        9091/TCP   197d
prometheus-server               ClusterIP   10.15.245.10   &amp;lt;none&amp;gt;        80/TCP     197d

kubectl get endpoints --selector=&#39;app=prometheus&#39;

NAME                            ENDPOINTS                                             AGE
prometheus-alertmanager         10.12.6.220:9093                                      197d
prometheus-kube-state-metrics   10.12.6.222:8080                                      197d
prometheus-node-exporter        10.140.0.30:9100,10.140.0.9:9100,10.140.15.212:9100   197d
prometheus-pushgateway          10.12.6.211:9091                                      197d
prometheus-server               10.12.3.14:9090                                       197d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;簡單說明一下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prometheus-server 是主要的 api-server 以及 time series database&lt;/li&gt;
&lt;li&gt;alertmanager 負責告警工作&lt;/li&gt;
&lt;li&gt;pushgateway 提供 client 端主動推送 metrics 給 server 的 endpoint&lt;/li&gt;
&lt;li&gt;kube-state-metrics 是開來收集 cluster wide 的 metrics, 像是 pods running counts, deployment ready count, total pods number 等等 metrics&lt;/li&gt;
&lt;li&gt;node-exporter 是 daemonsets, 把每一個 node 的 metrics, 像是 memory, cpu, disk&amp;hellip;等資料,收集出來&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主要服務存取就是透過 prometheus-server&lt;/p&gt;
&lt;h1 id=&#34;access-prometheus-server&#34;&gt;Access Prometheus server&lt;/h1&gt;
&lt;p&gt;除了直接 exec -it 進去 prometheus-server 以外，由於 prometheus 本身有提供 web portal, 所以我們這邊透過 port forwarding 打到本機上&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROMETHEUS_POD_NAME=$(kc get po -n default --selector=&#39;app=prometheus,component=server&#39; -o=jsonpath=&#39;{.items[0].metadata.name}&#39;)

kubectl --namespace default port-forward ${PROMETHEUS_POD_NAME} 9090
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;透過 browser 就可以連入操作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:9090
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;也可以透過 &lt;a href=&#34;https://prometheus.io/docs/prometheus/latest/querying/api/&#34;&gt;HTTP API&lt;/a&gt; 用程式接入控制&lt;/p&gt;
&lt;h1 id=&#34;prometheus-web&#34;&gt;Prometheus Web&lt;/h1&gt;
&lt;p&gt;Prometheus 本慎提供的 UI 其實功能就很強大&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以查到 (已經匯入存在) 的 metrics&lt;/li&gt;
&lt;li&gt;可以在上面執行 PromQL 查詢語法&lt;/li&gt;
&lt;li&gt;查詢運行的 status&lt;/li&gt;
&lt;li&gt;查詢目前所有收集的 targets 的狀態,有收集器掛了也可以在這邊看到&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;輕鬆自架 prometheus&lt;/li&gt;
&lt;li&gt;Prometheus 頁面有精簡，但是功能完整的 graph 製圖&lt;/li&gt;
&lt;li&gt;但大家通常會使用 Grafana 搭配使用, 用過都說讚, 我們明天繼續&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Prometheus Deploy Grafana</title>
      <link>https://chechia.net/post/prometheus-deploy-grafana/</link>
      <pubDate>Fri, 04 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/prometheus-deploy-grafana/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus / Grafana (5)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deployment-on-kubernetes/&#34;&gt;GKE 上自架 Prometheus / Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GKE 上自架 Grafana 與設定&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 kubernetes 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Grafana Introduction&lt;/li&gt;
&lt;li&gt;Deploy Grafana&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;grafana-introduction&#34;&gt;Grafana Introduction&lt;/h1&gt;
&lt;p&gt;上偏我們簡單介紹了 Prometheus，prometheus 的 Web Portol 已經附上簡單的 Query 與 Graph 工具，但一般我們在使用時，還是會搭配 Grafana 來使用。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://grafana.com/grafana/&#34;&gt;Grafana 在官網上提到&lt;/a&gt; 是一個 Analytics system，可以協助了解運行資料，建立完整的 dashboard。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支援許多圖表，直線圖，長條圖，區域分析，基本上需要的都有&lt;/li&gt;
&lt;li&gt;在圖表上定義 alter，並且主動告警，整合其他通訊軟體&lt;/li&gt;
&lt;li&gt;對後端 data source 的整合，可以同時使用 ELK, prometheus, influxdb 等 30 多種的資料來源&lt;/li&gt;
&lt;li&gt;有許多公開的 plugin 與 dashboard 可以匯入使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;總之功能強大，至於用起來的感覺，個人是非常推薦。如果有大得想要試玩看看，可以直接到 &lt;a href=&#34;https://play.grafana.org/d/000000029/prometheus-demo-dashboard?orgId=1&amp;amp;refresh=5m&#34;&gt;Grafana Live Demo&lt;/a&gt; 上面試玩&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一般使用都會圍繞 dashboard 為核心，透過單一畫面，一覽目前使用者需要讀取的資料&lt;/li&gt;
&lt;li&gt;左上角的下拉選單，可以選擇不同的 dashboards&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;與-kibana-做比較&#34;&gt;與 Kibana 做比較&lt;/h1&gt;
&lt;p&gt;雖然大部分使用上，我們都會使用 ELK 一套，而 Prometheus + Grafana 另一套。但其實兩邊的 data source 都可以互接。例如 grafana 可以吃 elasticsearch 的 data source，而 kibana 有 prometheus module。&lt;/p&gt;
&lt;p&gt;我們這邊基於兩款前端分析工具，稍微做個比較，底層的 data source 差異這邊先不提。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;都是開源: 兩者的開源社群都非常強大&lt;/li&gt;
&lt;li&gt;兩者內建的 dashboard 都非常完整，而且不斷推出新功能&lt;/li&gt;
&lt;li&gt;Log vs Metrics:
&lt;ul&gt;
&lt;li&gt;Kibana 的 metrics 也是像 log 一樣的 key value pairs，能夠 explore 未定義的 log&lt;/li&gt;
&lt;li&gt;Grafana 的 UI 專注於呈現 time series 的 metrics，並沒有提供 data 的欄位搜尋，而是使用語法 Query 來取得數據&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data source:
&lt;ul&gt;
&lt;li&gt;Grafana 可以收集各種不同的後端資料來源&lt;/li&gt;
&lt;li&gt;ELK 主要核心還是 ELK stack，用其他 Module 輔助其他資料源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;deploy-grafana&#34;&gt;Deploy Grafana&lt;/h1&gt;
&lt;p&gt;我把我的寶藏都放在這了&lt;a href=&#34;https://github.com/chechiachang/prometheus-kubernetes&#34;&gt;https://github.com/chechiachang/prometheus-kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下載下來的 .sh ，跑之前養成習慣貓一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd grafana

cat install.sh

#!/bin/bash
HELM_NAME=grafana-1

helm upgrade --install grafana stable/grafana \
  --namespace default \
  --values values-staging.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;helm&#34;&gt;Helm&lt;/h3&gt;
&lt;p&gt;我們這邊用 helm 部屬，&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/grafana&#34;&gt;Grafana Stable Chart&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;configuration&#34;&gt;Configuration&lt;/h3&gt;
&lt;p&gt;簡單看一下設定檔&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim values-staging.yaml

replicas: 1

deploymentStrategy: RollingUpdate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Grafana 是支援 &lt;a href=&#34;https://grafana.com/docs/tutorials/ha_setup/&#34;&gt;Grafana HA&lt;/a&gt; ，其實也非常簡單，就是把 grafana 本身的 dashboard database 從每個 grafana 一台 SQLite，變成外部統一的 MySQL，統一讀取後端資料，前端就可水平擴展。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;readinessProbe:
  httpGet:
    path: /api/health
    port: 3000

livenessProbe:
  httpGet:
    path: /api/health
    port: 3000
  initialDelaySeconds: 60
  timeoutSeconds: 30
  failureThreshold: 10

image:
  repository: grafana/grafana
  tag: 6.0.0
  pullPolicy: IfNotPresent

  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistrKeySecretName
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一些 Pod 的基本配置， health check 使用內建的 api，有需要也可以直接打 api&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;securityContext:
  runAsUser: 472
  fsGroup: 472


extraConfigmapMounts: []
  # - name: certs-configmap
  #   mountPath: /etc/grafana/ssl/
  #   configMap: certs-configmap
  #   readOnly: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有要開外部 ingress，需要 ssl 的話可以從這邊掛進去&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
## ref: http://kubernetes.io/docs/user-guide/services/
##
service:
  type: LoadBalancer
  port: 80
  targetPort: 3000
    # targetPort: 4181 To be used with a proxy extraContainer
  annotations: {}
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: &amp;quot;true&amp;quot;
  labels: {}
  path: /
  hosts:
    - chart-example.local
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這邊可以開 service load balancer, 以及 ingress，看實際使用的需求&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;persistence:
  enabled: true
  initChownData: true
  # storageClassName: default
  accessModes:
    - ReadWriteOnce
  size: 10Gi
  # annotations: {}
  # subPath: &amp;quot;&amp;quot;
  # existingClaim:
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Persistent Volume 作為本地儲存建議都開起來，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Administrator credentials when not using an existing secret (see below)
adminUser: admin
# adminPassword: strongpassword

# Use an existing secret for the admin user.
admin:
  existingSecret: &amp;quot;&amp;quot;
  userKey: admin-user
  passwordKey: admin-password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;帳號密碼建議使用 secret 掛進去&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;datasources: {}
#  datasources.yaml:
#    apiVersion: 1
#    datasources:
#    - name: Prometheus
#      type: prometheus
#      url: http://prometheus-prometheus-server
#      access: proxy
#      isDefault: true

## Configure grafana dashboard providers
## ref: http://docs.grafana.org/administration/provisioning/#dashboards
##
## `path` must be /var/lib/grafana/dashboards/&amp;lt;provider_name&amp;gt;
##
dashboardProviders: {}
#  dashboardproviders.yaml:
#    apiVersion: 1
#    providers:
#    - name: &#39;default&#39;
#      orgId: 1
#      folder: &#39;&#39;
#      type: file
#      disableDeletion: false
#      editable: true
#      options:
#        path: /var/lib/grafana/dashboards/default

## Configure grafana dashboard to import
## NOTE: To use dashboards you must also enable/configure dashboardProviders
## ref: https://grafana.com/dashboards
##
## dashboards per provider, use provider name as key.
##
dashboards: {}
  # default:
  #   some-dashboard:
  #     json: |
  #       $RAW_JSON
  #   custom-dashboard:
  #     file: dashboards/custom-dashboard.json
  #   prometheus-stats:
  #     gnetId: 2
  #     revision: 2
  #     datasource: Prometheus
  #   local-dashboard:
  #     url: https://example.com/repository/test.json
  #   local-dashboard-base64:
  #     url: https://example.com/repository/test-b64.json
  #     b64content: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Data source, Dashboard 想要直接載入，可以在這邊設定，或是 grafana 起來後，透過 Web UI 進去新增也可以&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Grafana&#39;s primary configuration
## NOTE: values in map will be converted to ini format
## ref: http://docs.grafana.org/installation/configuration/
##
grafana.ini:
  paths:
    data: /var/lib/grafana/data
    logs: /var/log/grafana
    plugins: /var/lib/grafana/plugins
    provisioning: /etc/grafana/provisioning
  analytics:
    check_for_updates: true
  log:
    mode: console
  grafana_net:
    url: https://grafana.net
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然後是 grafana.ini 核心 runtime 設定，更多設定可以參考&lt;a href=&#34;http://docs.grafana.org/installation/configuration/&#34;&gt;官方文件&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;deployment&#34;&gt;Deployment&lt;/h1&gt;
&lt;p&gt;部屬完看一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get po --selector=&#39;app=grafana&#39;


&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;access&#34;&gt;Access&lt;/h1&gt;
&lt;p&gt;如果沒有透過 service load balancer 打出來，一樣可以使用 kubectl 做 port forwarding，權限就是 context 的權限，沒有 cluster context 的使用者就會進步來&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;GRAFANA_POD_NAME=$(kc get po -n default --selector=&#39;app=grafana&#39; -o=jsonpath=&#39;{.items[0].metadata.name}&#39;)
kubectl --namespace default port-forward ${GRAFANA_POD_NAME} 3000

http://localhost:3000
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;由於我們透過 service load balancer，gcp 會在外部幫忙架一個 load balancer，
可以直接透過 load balancer ip 存取，如果想設定 dns，指向這個 ip 後記得去調整 grafana 的 server hostname。&lt;/p&gt;
&lt;p&gt;使用 secret 的密碼登入，username: grafana，這個是系統管理員&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get secret --namespace default grafana -o jsonpath=&amp;quot;{.data.admin-password}&amp;quot; | base64 --decode ; echo
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;configuration-1&#34;&gt;Configuration&lt;/h1&gt;
&lt;p&gt;近來畫面後先到左邊的&lt;a href=&#34;https://play.grafana.org/plugins&#34;&gt;Configuration&lt;/a&gt; 調整&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;產生新的 user org 與 user，把 admin 權限控制在需要的人手上&lt;/li&gt;
&lt;li&gt;把 prometheus data source 加進來，就可以直接看到 prometheus 裡面的資料。&lt;/li&gt;
&lt;li&gt;切換到非管理員的 user 繼續操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;import-dashboard&#34;&gt;Import Dashboard&lt;/h3&gt;
&lt;p&gt;Grafana 網站上已經有&lt;a href=&#34;https://grafana.com/grafana/dashboards&#34;&gt;超多設置好的 Dashboard&lt;/a&gt; 可以直接 import，大部分的服務都已經有別人幫我們把視覺畫圖表拉好，使用社群主流的 exporter 的話，參數直接接好。我們匯入後再進行簡單的客製化調整即可。&lt;/p&gt;
&lt;p&gt;我們鐵人賽有用到的服務，都已經有 dashboard&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubernetes Cluster: 6417
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/dashboards/6417&#34;&gt;https://grafana.com/dashboards/6417&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kafka Exporter Overview: 7589
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/dashboards/7589&#34;&gt;https://grafana.com/dashboards/7589&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Prometheus Redis: 763
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/dashboards/763&#34;&gt;https://grafana.com/dashboards/763&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kubernetes Deployment Statefulset Daemonset metrics: 8588
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/dashboards/8588&#34;&gt;https://grafana.com/dashboards/8588&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Haproxy Metrics Servers: 367
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/dashboards/367&#34;&gt;https://grafana.com/dashboards/367&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Go to grafana lab to find more dashboards&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;export-dashboard&#34;&gt;Export Dashboard&lt;/h3&gt;
&lt;p&gt;dashboard 會依照登入使用者的需求做調整，每個腳色需要看到的圖表都不同，基本上讓各個腳色都能一眼看到所需的表格即可&lt;/p&gt;
&lt;p&gt;自己的調整過的 dashboard 也可以匯出分享&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;p&gt;到這邊就可以正常使用 grafana了，資料來源的 exporter 我們會搭配前幾周分享過的服務，一起來講&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prometheus Deploy Grafana</title>
      <link>https://chechia.net/post/prometheus-scrape/</link>
      <pubDate>Fri, 04 Oct 2019 08:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/prometheus-scrape/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus / Grafana (5)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/prometheus-deployment-on-kubernetes/&#34;&gt;GKE 上自架 Prometheus / Grafana&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GKE 上自架 Grafana 與設定&lt;/li&gt;
&lt;li&gt;使用 exporter 監測 GKE 上的各項服務&lt;/li&gt;
&lt;li&gt;輸出 kubernetes 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 redis-ha 的監測數據&lt;/li&gt;
&lt;li&gt;輸出 kafka 的監測數據&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Prometheus scrape&lt;/li&gt;
&lt;li&gt;scrape_configs&lt;/li&gt;
&lt;li&gt;Node exporter&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;scrape&#34;&gt;Scrape&lt;/h1&gt;
&lt;p&gt;Prometheus 收集 metrics 的方式，是從被監測的目標的 http endpoints 收集 (scrape) metrics，目標服務有提供 export metrics 的 endpoint 的話，稱作 exporter。例如 kafka-exporter 就會收集 kafka 運行的 metrics，變成 http endpoint instance，prometheus 從 instance 上面收集資料。&lt;/p&gt;
&lt;p&gt;Promethesu 自己也是也提供 metrics endpoint，並且自己透過 scrape 自己的 metrics endpoint 來取得 self-monitoring 的 metrics。把自己當作外部服務監測。下面的設定就是直接透過 http://localhost:9090/metrics 取得。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
    monitor: &#39;codelab-monitor&#39;

# A scrape configuration containing exactly one endpoint to scrape:
# Here it&#39;s Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=&amp;lt;job_name&amp;gt;` to any timeseries scraped from this config.
  - job_name: &#39;prometheus&#39;

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: [&#39;localhost:9090&#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;透過 Grafana -&amp;gt; explore 就可以看到 Prometheus 的 metrics&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;static/img/prometheus-self-metrics.jpg&#34; alt=&#34;Prometheus Self Metrics&#34;&gt;&lt;/p&gt;
&lt;p&gt;而使用 metrics 時最好先查到說明文件，確定 metrics 的定義與計算方法，才可以有效的製圖。關於 &lt;a href=&#34;https://wiki.lnd.bz/display/LFTC/Prometheus&#34;&gt;Prometheus Exporter 的 metrics 說明&lt;/a&gt; 可以到這裡來找。&lt;/p&gt;
&lt;h1 id=&#34;dashboard&#34;&gt;Dashboard&lt;/h1&gt;
&lt;p&gt;收集到 metrics 之後就可以在 prometheus 中 query，但一般使用不會一直跑進來下 query，而是會直接搭配 dashboard 製圖呈現，讓資料一覽無遺。&lt;/p&gt;
&lt;p&gt;例如 prometheus 自身的 metrics 也已經有搭配好的 &lt;a href=&#34;https://grafana.com/grafana/dashboards/3662&#34;&gt;Prometheus overview dashboard&lt;/a&gt; 可以使用。&lt;/p&gt;
&lt;p&gt;使用方法非常簡單，直接透過 Grafana import dashboard，裡面就把重要的 prometheus metrics 都放在 dashboard 上了。不能更方便了。&lt;/p&gt;
&lt;h1 id=&#34;exporters&#34;&gt;Exporters&lt;/h1&gt;
&lt;p&gt;Prometheus 支援超級多 exporter，包含 prometheus 自身直接維護的 exporter，還有非常多外部服務友也開源的 exporter 可以使用，&lt;a href=&#34;https://prometheus.io/docs/instrumenting/exporters/#exporters-and-integrations&#34;&gt;清單可以到這裡看&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;有希望自己公司的服務，也使用 prometheus&lt;/p&gt;
&lt;h1 id=&#34;node-exporter&#34;&gt;Node Exporter&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/prometheus/node_exporter&#34;&gt;prometheus/node_exporter&lt;/a&gt; 是 Prometheus 直接維護的 project，主要用途就是將 node / vm 的運行 metrics export 出來。有點類似 ELK 的 metricbeat。&lt;/p&gt;
&lt;p&gt;我們這邊是在 kubernetes 上執行，所以直接做成 daemonsets 在 k8s 上跑，部屬方面在 deploy prometheus-server 的 helm chart 中，就已經附帶整合，部屬到每一台 node 上。&lt;/p&gt;
&lt;p&gt;如果是在 kubernetes 外的環境，例如說 on premise server，或是 gcp instance，希望自己部屬 node exporter 的話，可以參考&lt;a href=&#34;https://prometheus.io/docs/guides/node-exporter/&#34;&gt;這篇教學文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我們這邊可以看一下 config，以及 job 定義。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim values-staging.yaml

  # Enable nodeExporter
  nodeExporter:
    create: true

  prometheus.yml:
    rule_files:
      - /etc/config/rules
      - /etc/config/alerts

    scrape_configs:

    # Add kubernetes node job
    - job_name: &#39;kubernetes-nodes&#39;

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS &amp;amp; bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery &amp;amp; scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # &amp;lt;kubernetes_sd_config&amp;gt;.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;kubernetes_sd_config: 可以透過 kubernetes API 來取得 scrape target，以這邊的設定，是使用 node role 去集群取得 node，並且每一台 node 都當成一個 target，這樣就不用把所有 node 都手動加到 job 的 instance list 裡面。&lt;/p&gt;
&lt;p&gt;從 node role 取得的 instance 會使用 ip 標註或是 hostname 標註。node role 有提供 node 範圍的 meta labels，例如 __meta_kubernetes_node_name, _&lt;em&gt;meta_kubernetes_node_address&lt;/em&gt; 等等，方便查找整理資料。&lt;/p&gt;
&lt;p&gt;relabel_configs: 針對資料做額外標記，方便之後在 grafana 上面依據需求 query。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring GKE With Elk</title>
      <link>https://chechia.net/post/monitoring-gke-with-elk/</link>
      <pubDate>Thu, 19 Sep 2019 17:06:29 +0800</pubDate>
      
      <guid>https://chechia.net/post/monitoring-gke-with-elk/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/self-host-elk-stack-on-gcp/&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/secure-elk-stack/&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/monitoring-gce-with-elk/&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/monitoring-gke-with-elk/&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/running-on-kubernetes.html&#34;&gt;官方文件&lt;/a&gt; ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。&lt;/p&gt;
&lt;p&gt;這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics)&lt;/li&gt;
&lt;li&gt;cluster: 處理 cluster 等級的 log,  event 或是 metrics&lt;/li&gt;
&lt;li&gt;pod: 針對特定 pod 直接去掛一個 sidecar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的方法是可以混搭的，kubernetes 個個層級有&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/&#34;&gt;log 處理流程&lt;/a&gt;，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。&lt;/p&gt;
&lt;p&gt;簡單來說，是去對的地方找對的 log。在架構上要注意 scalability 與 resource 分配，不要影響本身提供服務的 GKE ，但又能獲得盡量即時的 log。&lt;/p&gt;
&lt;p&gt;我們這邊直接進入 kubernetes resource 的設定，底下會附上在 GKE 找 log 的過程。&lt;/p&gt;
&lt;h1 id=&#34;node-level-log-harvest&#34;&gt;Node level log harvest&lt;/h1&gt;
&lt;p&gt;為每一個 node 配置 filebeat，然後在 node 上面尋找 log，然後如我們上篇所敘述加到 input ，就可以把 log 倒出來。&lt;/p&gt;
&lt;p&gt;直覺想到就是透過 daemonsets 為每個 node 部署一個 filebeat pod，然後 mount node 的 log 資料夾，在設置 input。&lt;/p&gt;
&lt;h1 id=&#34;deploy-daemonsets&#34;&gt;Deploy daemonsets&lt;/h1&gt;
&lt;p&gt;kubernetes resource 的 yaml 請參考 &lt;a href=&#34;https://github.com/chechiachang/elk-kubernetes/tree/master/filebeat/7.3.1&#34;&gt;我的 github elk-kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;給予足夠的 clusterrolebinding 到 elk&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/clusterrolebinding.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;先更改 filebeat 的設定，如何設定 elasticsearch 與 kibana，請參考上篇。至於 input 的部份已經配置好了。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim filebeat/7.3.1/daemonsets-config-configmap.yaml

kubectl apply -f filebeat/7.3.1/daemonsets-config-configmap.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部屬 filebeat daemonsets，會每一個 node 部屬一個 filebeat&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/daemonsets.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;取得 daemonsets 的狀態&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl --namespcae elk get pods

NAME             READY   STATUS    RESTARTS   AGE
filebeat-bjfp9   1/1     Running   0          6m56s
filebeat-fzr9n   1/1     Running   0          6m56s
filebeat-vpkm7   1/1     Running   0          6m56s
...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log&lt;/p&gt;
&lt;h1 id=&#34;log-havest-for-specific-pods&#34;&gt;log havest for specific pods&lt;/h1&gt;
&lt;p&gt;由於 kubernetes 上我們可以便利的調度 filebeat 的部屬方式，這邊也可以也可以使用 deployment ，配合 pod affinity，把 filebeat 放到某個想要監測的 pod，這邊的例子是 nginx-ingress-controller。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 上有一個或多個 nginx ingress controller&lt;/li&gt;
&lt;li&gt;部屬一個或多個 filebeat 到有 nginx 的 node 上&lt;/li&gt;
&lt;li&gt;filebeat 去抓取 nginx 的 input， 並使用 filebeat 的 nginx module 做預處理
&lt;ul&gt;
&lt;li&gt;nginx module 預設路徑需要調整，這邊使用 filebeat autodiscover 來處理&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一樣 apply 前記得先檢查跟設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim filebeat/7.3.1/nginx-config-configmap.yaml

kubectl apply -f filebeat/7.3.1/nginx-config-configmap.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;部屬 filebeat deployment
由於有設定 pod affinity ，這個 filebeat 只會被放到有 nginx ingress controller 的這個節點上，並且依照 autodiscover 設定的條件去蒐集 nginx 的 log&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f filebeat/7.3.1/nginx-deployment.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log&lt;/p&gt;
&lt;p&gt;另外，由於有啟動 nginx module，logstash 收到的內容已經是處理過得內容。&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;gcp-fluentd&#34;&gt;GCP fluentd&lt;/h1&gt;
&lt;p&gt;如果是使用 GKE 的朋友，可以投過開啟 stackdriver logging 的功能，把集群中服務的 log 倒到 stackdriver，基本上就是 node -&amp;gt; (daemonsets) fluentd -&amp;gt; stackdriver。&lt;/p&gt;
&lt;p&gt;這個 fluentd 是 GCP 如果有啟動 Stackdriver Logging 的話，自動幫你維護的 daemonsets，設定不可改，改了會被 overwrite 會去，所以不太方便從這邊動手腳。&lt;/p&gt;
&lt;p&gt;Btw stackdriver 最近好像改版，目前做 example 的版本已經變成 lagency （淚&lt;/p&gt;
&lt;p&gt;但我們先假設我們對這個 pod 的 log 很有興趣，然後把這邊的 log 透過 filebeat 送到 ELK 上XD&lt;/p&gt;
&lt;p&gt;因為 GKE 透過 fluentd 把 GKE 上面的 log 倒到 stackdriver，而我們是想把 log 倒到 ELK，既然這樣我們的 input 來源是相同的，而且很多處理步驟都可以在 ELK 上面互通，真的可以偷看一下 fluentd 是去哪收集 log ，怎麼處理 log pipeline，我們只要做相應設定就好。&lt;/p&gt;
&lt;p&gt;畢竟 google 都幫我們弄得妥妥的，不參考一下他的流程太可惜。&lt;/p&gt;
&lt;p&gt;偷看一下 GKE 上 fluentd 是去哪找 log ，這個是 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-gcp/fluentd-gcp-configmap.yaml&#34;&gt;fluentd gcp configmap&lt;/a&gt;，雖然看到這邊感覺扯遠了，但因為很有趣所有我就繼續看下去，各位大德可以跳過XD&lt;/p&gt;
&lt;p&gt;configmap 中的這個 input 設定檔，其中一個 source 就是一個資料來源，相當於 filebeat 的 input。這邊這個 source 就是去 &lt;code&gt;/var/log/containers/*.log&lt;/code&gt;  收 log&lt;/p&gt;
&lt;p&gt;這邊還做了幾件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;打上 &lt;code&gt;reform.*&lt;/code&gt; tag，讓下個 match 可以 收進去 pipeline 處理&lt;/li&gt;
&lt;li&gt;附帶 parse 出 time&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;containers.input.conf

&amp;lt;source&amp;gt;
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/gcp-containers.log.pos
  # Tags at this point are in the format of:
  # reform.var.log.containers.&amp;lt;POD_NAME&amp;gt;_&amp;lt;NAMESPACE_NAME&amp;gt;_&amp;lt;CONTAINER_NAME&amp;gt;-&amp;lt;CONTAINER_ID&amp;gt;.log
  tag reform.*
  read_from_head true
  &amp;lt;parse&amp;gt;
    @type multi_format
    &amp;lt;pattern&amp;gt;
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    &amp;lt;/pattern&amp;gt;
    &amp;lt;pattern&amp;gt;
      format /^(?&amp;lt;time&amp;gt;.+) (?&amp;lt;stream&amp;gt;stdout|stderr) [^ ]* (?&amp;lt;log&amp;gt;.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    &amp;lt;/pattern&amp;gt;
  &amp;lt;/parse&amp;gt;
&amp;lt;/source&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;他這邊做一些 error handling，然後用 ruby (!) parse，這邊就真的太遠，細節大家可以 google ＸＤ。不過這邊使用的 pattern matching 我們後幾篇在 logstash pipeline 上，也會有機會提到，機制是類似的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;filter reform.**&amp;gt;
  @type parser
  format /^(?&amp;lt;severity&amp;gt;\w)(?&amp;lt;time&amp;gt;\d{4} [^\s]*)\s+(?&amp;lt;pid&amp;gt;\d+)\s+(?&amp;lt;source&amp;gt;[^ \]]+)\] (?&amp;lt;log&amp;gt;.*)/
  reserve_data true
  suppress_parse_error_log true
  emit_invalid_record_to_error false
  key_name log
&amp;lt;/filter&amp;gt;

&amp;lt;match reform.**&amp;gt;
  @type record_reformer
  enable_ruby true
  &amp;lt;record&amp;gt;
    # Extract local_resource_id from tag for &#39;k8s_container&#39; monitored
    # resource. The format is:
    # &#39;k8s_container.&amp;lt;namespace_name&amp;gt;.&amp;lt;pod_name&amp;gt;.&amp;lt;container_name&amp;gt;&#39;.
    &amp;quot;logging.googleapis.com/local_resource_id&amp;quot; ${&amp;quot;k8s_container.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[1]}.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[0]}.#{tag_suffix[4].rpartition(&#39;.&#39;)[0].split(&#39;_&#39;)[2].rpartition(&#39;-&#39;)[0]}&amp;quot;}
    # Rename the field &#39;log&#39; to a more generic field &#39;message&#39;. This way the
    # fluent-plugin-google-cloud knows to flatten the field as textPayload
    # instead of jsonPayload after extracting &#39;time&#39;, &#39;severity&#39; and
    # &#39;stream&#39; from the record.
    message ${record[&#39;log&#39;]}
    # If &#39;severity&#39; is not set, assume stderr is ERROR and stdout is INFO.
    severity ${record[&#39;severity&#39;] || if record[&#39;stream&#39;] == &#39;stderr&#39; then &#39;ERROR&#39; else &#39;INFO&#39; end}
  &amp;lt;/record&amp;gt;
  tag ${if record[&#39;stream&#39;] == &#39;stderr&#39; then &#39;raw.stderr&#39; else &#39;raw.stdout&#39; end}
  remove_keys stream,log
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;ssh-進去逛&#34;&gt;ssh 進去逛&lt;/h3&gt;
&lt;p&gt;想看機器上實際的 log 狀況，我們也可以直接 ssh 進去&lt;/p&gt;
&lt;p&gt;先透過 kubectl 看一下 pod&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get daemonsets --namespace kube-system

NAME                       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                  AGE
fluentd-gcp-v3.2.0         7         7         7       7            7           beta.kubernetes.io/fluentd-ds-ready=true       196d

$ kubectl get pods --output wide --namespace kube-system

NAME                                      READY   STATUS    RESTARTS   AGE   IP          NODE                                     NOMINATED NODE   READINESS GATES
fluentd-gcp-scaler-1234567890-vfbhc       1/1     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-5gqn   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-44tl7                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-wcq0   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-5vc6l                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-tp05   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-6rqvc                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-5gqn   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
fluentd-gcp-v3.2.0-mmwk4                  2/2     Running   0          37d   10.140.0.   gke-chechiachang-pool-1-123456789-vxld   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;先透過 kubectl 看一下 node&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get node

NAME                                     STATUS   ROLES    AGE   VERSION
gke-chechaichang-pool-1-123456789-3bzp   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
gke-chechaichang-pool-1-123456789-5gqn   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
gke-chechaichang-pool-1-123456789-8n8z   Ready    &amp;lt;none&amp;gt;   37d   v1.13.7-gke.8
...

gcloud compute ssh gke-chechaichang-pool-1-123456789-3bzp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如使用其他雲平台的 kubernetes service，或是 bare metal 的集群，請依照各自系統的方式連進去看看。&lt;/p&gt;
&lt;h1 id=&#34;ssh-node-找-log&#34;&gt;ssh node 找 log&lt;/h1&gt;
&lt;p&gt;ssh 進去後就可以到處來探險，順便看看 GKE 跑在機器上到底做了什麼事情。&lt;/p&gt;
&lt;p&gt;如果官方有出文件，可能可以不用進來看。各位大德有發現文件請留言跟我說。我個人很喜歡自己架集群起來連就去看，面對照官方文件上寫的東西，當然大部份時候都是文件沒有帶到，有很多發現。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ls /var/log

gcp-*-log.pos
kube-proxy.log
containers/
metrics/
pods/
...

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;/var/log/containers 看一下，格式是 &lt;code&gt;pod_namespace_container&lt;/code&gt; 這邊是 link 到 /var/log/pods/&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ ls -al /var/log/containers

lrwxrwxrwx 1 root root   105 Aug 12 07:42 fluentd-gcp-v3.2.0-st6cl_kube-system_fluentd-gcp-5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac.log -&amp;gt; /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/0.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看到 pods 就覺得是你了，裡面有 pod 資料夾，格式是 &lt;code&gt;namespace_pod_uuid&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls /var/log/pods

default_pod-1-1234567890-fxxhp_uuid
kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008
kube-system_heapster-v1.6.0-beta.1-
kube-system_kube-proxy-gke-
kube-system_l7-default-backend-
kube-system_prometheus-to-sd-
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;再進去有 container log，格式是 &lt;code&gt;pod_namespace_container.log&lt;/code&gt;，也是 link&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls -al /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/

lrwxrwxrwx 1 root root  165 Aug 12 07:42 0.log -&amp;gt; /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;最終 link 到&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo su

$ ls -alh /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/
total 3.9M
drwx------  4 root root 4.0K Aug 12 07:42 .
drwx------ 92 root root  20K Sep 18 11:28 ..
-rw-r-----  1 root root 3.8M Sep 18 11:29 5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
drwx------  2 root root 4.0K Aug 12 07:42 checkpoints
-rw-------  1 root root 7.8K Aug 12 07:42 config.v2.json
-rw-r--r--  1 root root 2.3K Aug 12 07:42 hostconfig.json
drwx------  2 root root 4.0K Aug 12 07:42 mounts
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;頭尾偷喵一下，確定是我們在找的東西&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;head /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
tail /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這樣就找到我們的 log 了&lt;/p&gt;
&lt;h1 id=&#34;小節&#34;&gt;小節&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;使用 filebeat 去查找&lt;/li&gt;
&lt;li&gt;透過 kubernetes daemonsets 可以快速佈置一份 filebeat 到所有 node，且設定都是一起更新&lt;/li&gt;
&lt;li&gt;透過 kubernetes deployment 可以指定 filebeat 的位置，去跟隨想要監測的服務&lt;/li&gt;
&lt;li&gt;如果不熟 log 處理流程，可以直接看偷看大廠的服務，會有很多靈感&lt;/li&gt;
&lt;li&gt;沒事可以多跑進 Kubernetes 服務節點逛逛，有很多有趣的東西&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>X.509 certificate</title>
      <link>https://chechia.net/post/x.509-certificate/</link>
      <pubDate>Tue, 17 Sep 2019 10:15:36 +0800</pubDate>
      
      <guid>https://chechia.net/post/x.509-certificate/</guid>
      <description>&lt;h1 id=&#34;簡單講一下-certificate&#34;&gt;簡單講一下 certificate&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;X.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;certificate 包含公鑰及識別資訊(hostname, organization, &amp;hellip;等資訊)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;certificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 elastic stack 上，如果有多個 elasticsearch server node 彼此連線，由於 node 彼此是 client 也是 server&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用 self-signed CA 產出來的 certificate，連入時會檢查使用的 certificate 是否由同一組 CA 簽署&lt;/li&gt;
&lt;li&gt;server 使用 certificate，確定連入 server 的 client 都帶有正確的私鑰與 public certificate，是 authenticated user&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;附帶說明，X.509 有多種檔案格式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.pem&lt;/li&gt;
&lt;li&gt;.cer, .crt, .der&lt;/li&gt;
&lt;li&gt;.p12&lt;/li&gt;
&lt;li&gt;.p7b, .p7c&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證&lt;/p&gt;
&lt;h1 id=&#34;ca&#34;&gt;CA&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys

MAC: sha1, Iteration 100000
MAC length: 20, salt length: 20
PKCS7 Data
Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000
Certificate bag
Bag Attributes
    friendlyName: ca
    localKeyID:
subject=CN = Elastic Certificate Tool Autogenerated CA

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;issuer command name 為 Elastic autogen CA
subject command name 為 Elastic autogen CA&lt;/p&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/&#34;&gt;https://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl s_client -connect google.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&#34;https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce&#34;&gt;https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;openssl verify -CAfile client-ca.cer client.cer

openssl verify -show_chain -CAfile client-ca.cer client.cer
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;certificate&#34;&gt;Certificate&lt;/h1&gt;
&lt;p&gt;用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-certificates.p12 -info -nokeys

MAC: sha1, Iteration 100000
MAC length: 20, salt length: 20
PKCS7 Data
Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000
PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000
Certificate bag
Bag Attributes
    friendlyName: elk.asia-east1-b.c.machi-x.internal
    localKeyID:
subject=CN = elk.asia-east1-b.c.machi-x.internal

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----
Certificate bag
Bag Attributes
    friendlyName: ca
    2.16.840.1.113894.746875.1.1: &amp;lt;Unsupported tag 6&amp;gt;
subject=CN = Elastic Certificate Tool Autogenerated CA

issuer=CN = Elastic Certificate Tool Autogenerated CA

-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Secure Elk Stack</title>
      <link>https://chechia.net/post/secure-elk-stack/</link>
      <pubDate>Sun, 15 Sep 2019 23:00:33 +0800</pubDate>
      
      <guid>https://chechia.net/post/secure-elk-stack/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/self-host-elk-stack-on-gcp/&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/secure-elk-stack/&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;監測 Google Compute Engine 上服務的各項數據&lt;/li&gt;
&lt;li&gt;監測 Google Kubernetes Engine 的各項數據&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;p&gt;上篇&lt;a href=&#34;https://chechia.net/post/self-host-elk-stack-on-gcp/&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt; 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/elasticsearch-security.html&#34;&gt;官方的文件在這裡&lt;/a&gt;，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/security-getting-started.html&#34;&gt;Tutorial: Getting started with security&lt;/a&gt; 開始，過程比較不會這麼血尿。&lt;/p&gt;
&lt;p&gt;總之為了啟用 authentication &amp;amp; https，這篇要做的事情：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enable x-pack &amp;amp; activate basic license&lt;/li&gt;
&lt;li&gt;Generate self-signed ca, server certificate, client certificate&lt;/li&gt;
&lt;li&gt;Configure Elasticsearch, Kibana, &amp;amp; other components to
&lt;ul&gt;
&lt;li&gt;use server certificate when act as server&lt;/li&gt;
&lt;li&gt;use client certificate when connect to an ELK server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;啟用-x-pack&#34;&gt;啟用 X-pack&lt;/h1&gt;
&lt;p&gt;Elasticsearch 的安全性模組由 x-pack extension 提供，在 &lt;a href=&#34;https://www.elastic.co/what-is/open-x-pack&#34;&gt;6.3.0 之後的版本&lt;/a&gt;，安裝 elasticsearch 的過程中就預設安裝 x-pack。&lt;/p&gt;
&lt;p&gt;附上&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/get-started-enable-security.html&#34;&gt;啟用的官方文件&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;然而，由於舊版的 x-pack 是付費內容，目前的 elasticsearch 安裝完後，elasticsearch.yml 設定預設不啟用 x-pack，也就是說沒看到這篇官方文件的話，很容易就獲得沒有任何 security 功能的 ELK。&lt;/p&gt;
&lt;p&gt;雖然目前已經可以使用免費的 basic license 使用 security 功能，還是希望官方可以 default 啟用 security。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/elasticsearch/elasticsearch.yml

xpack.security.enabled: true

xpack.license.self_generated.type: basic

discovery.type: single-node
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我們這邊啟用 xpack.security，同時將 self-generated license 生出來，我們這邊只使用基本的 basic subscription。若希望啟用更多功能，可以看&lt;a href=&#34;https://www.elastic.co/cn/subscriptions&#34;&gt;官方subcription 方案介紹&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;另外，如果不同時設定為 single-node 的話，預設會尋找其他elasticsearch node 來組成 cluster，而我們就必須要在所有 node 上啟用 security，這篇只帶大家做一個 single node cluster，簡化步驟。&lt;/p&gt;
&lt;p&gt;重啟 elasticsearch ，檢查 log，看啟動時有沒有載入 x-pack&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart elasticsearch

$ tail -f /var/log/elasticsearch/elasticsearch.log

[2019-09-16T07:39:49,467][INFO ][o.e.e.NodeEnvironment    ] [elk] using [1] data paths, mounts [[/mnt/disks/elk (/dev/sdb)]], net usable_space [423.6gb], net total_space [491.1gb], types [ext4]
[2019-09-16T07:39:49,474][INFO ][o.e.e.NodeEnvironment    ] [elk] heap size [3.9gb], compressed ordinary object pointers [true]
[2019-09-16T07:39:50,858][INFO ][o.e.n.Node               ] [elk] node name [elk], node ID [pC22j9D4R6uiCM7oTc1Fiw], cluster name [elasticsearch]
[2019-09-16T07:39:50,866][INFO ][o.e.n.Node               ] [elk] version[7.3.1], pid[17189], build[default/deb/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/4.15.0-1040-gcp/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/12.0.2/12.0.2+10]
[2019-09-16T07:39:50,878][INFO ][o.e.n.Node               ] [elk] JVM home [/usr/share/elasticsearch/jdk]
...
[2019-09-16T07:39:59,108][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-ccr]
[2019-09-16T07:39:59,109][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-core]
...
[2019-09-16T07:39:59,111][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-logstash]
[2019-09-16T07:39:59,113][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-voting-only-node]
[2019-09-16T07:39:59,114][INFO ][o.e.p.PluginsService     ] [elk] loaded module [x-pack-watcher]
[2019-09-16T07:39:59,115][INFO ][o.e.p.PluginsService     ] [elk] no plugins loaded
[2019-09-16T07:40:07,964][INFO ][o.e.x.s.a.s.FileRolesStore] [elk] parsed [0] roles from file [/etc/elasticsearch/roles.yml]
[2019-09-16T07:40:10,369][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elk] [controller/17314] [Main.cc@110] controller (64 bit): Version 7.3.1 (Build 1d93901e09ef43) Copyright (c) 2019 Elasticsearch BV
[2019-09-16T07:40:11,776][DEBUG][o.e.a.ActionModule       ] [elk] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security
[2019-09-16T07:40:14,396][INFO ][o.e.d.DiscoveryModule    ] [elk] using discovery type [single-node] and seed hosts providers [settings]
[2019-09-16T07:40:16,222][INFO ][o.e.n.Node               ] [elk] initialized
[2019-09-16T07:40:16,224][INFO ][o.e.n.Node               ] [elk] starting ...
[2019-09-16T07:40:16,821][INFO ][o.e.t.TransportService   ] [elk] publish_address {10.140.0.10:9300}, bound_addresses {[::]:9300}
[2019-09-16T07:40:16,872][INFO ][o.e.c.c.Coordinator      ] [elk] cluster UUID [1CB6_Lt-TUWEmRoN9SE49w]
[2019-09-16T07:40:17,088][INFO ][o.e.c.s.MasterService    ] [elk] elected-as-master ([1] nodes joined)[{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 921, reason: master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}
[2019-09-16T07:40:17,819][INFO ][o.e.c.s.ClusterApplierService] [elk] master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}, term: 9, version: 921, reason: Publication{term=9, version=921}
[2019-09-16T07:40:17,974][INFO ][o.e.h.AbstractHttpServerTransport] [elk] publish_address {10.140.0.10:9200}, bound_addresses {[::]:9200}
[2019-09-16T07:40:17,975][INFO ][o.e.n.Node               ] [elk] started
[2019-09-16T07:40:18,455][INFO ][o.e.c.s.ClusterSettings  ] [elk] updating [xpack.monitoring.collection.enabled] from [false] to [true]
[2019-09-16T07:40:22,555][INFO ][o.e.l.LicenseService     ] [elk] license [************************************] mode [basic] - valid
[2019-09-16T07:40:22,557][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [elk] Active license is now [BASIC]; Security is enabled
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;enable-user-authentication&#34;&gt;Enable user authentication&lt;/h1&gt;
&lt;p&gt;啟用 security 之前，我們直接連入 Kibana http://10.140.0.10:5601 ，不用任何使用者登入，便可以完整使用 Kibana 功能（包含 admin 管理介面）。&lt;/p&gt;
&lt;p&gt;啟用 security 後，便需要使用帳號密碼登入。在這邊先用工具把使用者密碼產生出來。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# 互動式
/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive

# 自動產生
/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;密碼生出來後，就把帳號密碼收好，等等會用到。之後初次登入也是使用這些密碼。&lt;/p&gt;
&lt;h1 id=&#34;configure-passwords-on-client-side&#34;&gt;Configure passwords on client-side&lt;/h1&gt;
&lt;p&gt;由於已經啟用 authentication，其他 ELK 元件 (Kibana, logstash, filebeat, apm-server,&amp;hellip;) 連入 Elasticsearch 也都會需要各自的帳號密碼驗證。&lt;/p&gt;
&lt;p&gt;以 Kibana 為例，可以直接在 kibana.yml 中直接設定帳號密碼&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/kibana/kibana.yml

elasticsearch.hosts: [&amp;quot;http://localhost:9200&amp;quot;]
xpack.security.enabled: true

elasticsearch.username: &amp;quot;kibana&amp;quot;
elasticsearch.password: &amp;quot;***********&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;當然，這邊就是明碼的，看了不太安全。&lt;/p&gt;
&lt;p&gt;或是使用 keystore 把 built-in user 的密碼加密，存在 kibana 的 keystore 裡面，重啟 kibana 時便會載入。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/share/kibana/bin/kibana-keystore create
/usr/share/kibana/bin/kibana-keystore add elasticsearch.username
/usr/share/kibana/bin/kibana-keystore add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果有啟用 Filebeat 功能，beat 元件連入 elasticsearch 一樣需要設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/share/apm-server/bin/filebeat keystore create
/usr/share/apm-server/bin/filebeat add elasticsearch.username
/usr/share/apm-server/bin/filebeat add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果有啟用 application performance monitoring(APM) 功能，apm-server 元件連入 elasticsearch 一樣需要設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/share/apm-server/bin/apm-server keystore create
/usr/share/apm-server/bin/apm-server add elasticsearch.username
/usr/share/apm-server/bin/apm-server add elasticsearch.password
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;encrypting-communications&#34;&gt;Encrypting Communications&lt;/h1&gt;
&lt;p&gt;上面加了 username/password authentication，但如果沒 https/tls 基本上還是裸奔。接下來要處理連線加密。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/encrypting-internode-communications.html&#34;&gt;官方 tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一堆官方文件，我們先跳過XD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/elasticsearch-security.html&#34;&gt;elasticsearch security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-overview/7.3/ssl-tls.html&#34;&gt;elastic stack ssl tls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/configuring-tls.html#configuring-tls&#34;&gt;elasticsearch configuring tls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/certutil.html&#34;&gt;certutil&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;分析一下需求跟規格&#34;&gt;分析一下需求跟規格&lt;/h1&gt;
&lt;p&gt;我們需要為每一個 node 生一組 node certificate，使用 node certificate 產生 client certificates 提供給其他 client，連入時會驗證 client 是否為 authenticated user。&lt;/p&gt;
&lt;p&gt;針對目前這個 single-node ELK stack，我們可能有幾種選擇&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;簽署一個 localhost，當然這個只能在 localhost 上的客戶端元件使用，別的 node 無法用這個連入&lt;/li&gt;
&lt;li&gt;簽署一個 public DNS elk.chechiachang.com，可以在公開網路上使用，別人也可以使用這個DNS嘗試連入&lt;/li&gt;
&lt;li&gt;簽署一個私有網域的 DNS，例如在 GCP 上可以使用&lt;a href=&#34;https://cloud.google.com/compute/docs/internal-dns?hl=zh-tw&#34;&gt;內部dns服務&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;長這樣 elk.asia-east1-b.c.chechiachang.internal&lt;/li&gt;
&lt;li&gt;[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;有需要也可以一份 server certificate 中簽署複數個 site&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我們這邊選擇使用內部 dns，elk.asia-east1-b-c-chechaichang.internal，讓這個 single-node elk 只能透過內部網路存取。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch: elk.asia-east1-b.c.chechaichang.internal:9200&lt;/li&gt;
&lt;li&gt;kibana: elk.asia-east1-b.c.chechaichang.internal:5601&lt;/li&gt;
&lt;li&gt;外部要連近來 kibana，我們使用 vpn 服務連進私有網路&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果想使用外部 dns，讓 elk stack 在公開網路可以使用，ex. elk.chechiachang.com，可以&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP 的 load balancer掛進來，用 GCP 的 certificate manager 自動管理 certificate&lt;/li&gt;
&lt;li&gt;或是在 node 上開一個 nginx server，再把 certificate 用 certbot 生出來&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;generate-certificates&#34;&gt;Generate certificates&lt;/h1&gt;
&lt;p&gt;先把 X.509 digital certificate 的 certificate authority(CA) 生出來。我們可以設定密碼保護這個檔案&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/elasticsearch/config

# CA generated with Elastic tool
/usr/share/elasticsearch/bin/elasticsearch-certutil ca \
  -out /etc/elasticsearch/config/elastic-stack-ca.p12
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;生出來是 PKCS#12 格式的 keystore，包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CA 的 public certificate&lt;/li&gt;
&lt;li&gt;CA 的基本資訊&lt;/li&gt;
&lt;li&gt;簽署其他 node certificates 使用的私鑰(private key)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;附帶說明，X.509 有多種檔案格式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;.pem&lt;/li&gt;
&lt;li&gt;.cer, .crt, .der&lt;/li&gt;
&lt;li&gt;.p12&lt;/li&gt;
&lt;li&gt;.p7b, .p7c&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證。裡面的內容也不同。&lt;/p&gt;
&lt;p&gt;ELK 設定的過程中，由於不是所有的 ELK component 都支援使用 .p12 檔案，我們在設定過程中會互相專換，或是混用多種檔案格式。&lt;/p&gt;
&lt;h1 id=&#34;generate-certificate&#34;&gt;Generate certificate&lt;/h1&gt;
&lt;p&gt;&amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD&lt;/p&gt;
&lt;p&gt;我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node certificate&lt;/li&gt;
&lt;li&gt;node key&lt;/li&gt;
&lt;li&gt;CA certificate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這邊只產生一組 server certificate 給 single-node cluster 的 node-1&lt;/p&gt;
&lt;p&gt;=======&lt;/p&gt;
&lt;p&gt;我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;node certificate&lt;/li&gt;
&lt;li&gt;node key&lt;/li&gt;
&lt;li&gt;CA certificate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這邊只產生一組 server certificate 給 single-node cluster 的 node-1&lt;/p&gt;
&lt;p&gt;如果 cluster 中有多個 elasticsearch，為每個 node 產生 certificate 時都要使用同樣 CA 來簽署，讓 server 信任這組 CA。&lt;/p&gt;
&lt;p&gt;使用 elasticsearch-certutil 簡化簽署過程，從產生 CA ，到使用 CA 簽署 certificate。另外，再產生 certificate 中使用 Subject Alternative Name(SAN)，並輸入 ip 與 dns。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# certificate for site: private dns with Elastic CA
/usr/share/elasticsearch/bin/elasticsearch-certutil cert \
  --ca /etc/elasticsearch/config/elastic-stack-ca.p12 \
  --name elk.asia-east1-b.c.chechaichang.internal \
  --dns elk.asia-east1-b.c.chechaichang.internal \
  --ip 10.140.0.10 \
  -out /etc/elasticsearch/config/node-1.p12
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;用 openssl 看一下內容，如果有密碼這邊要用密碼解鎖&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -info -nokeys
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;server 用這個 certificate ，啟用 ssl。&lt;/p&gt;
&lt;p&gt;client 使用這個 certificate 產生出來的 client.cer 與 client.key 與 server 連線，server 才接受客戶端是安全的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;25f5ab795b9e698333a36fde7ecf23a8ba9d4595
記得把所有權還給 elasticsearch 的使用者，避免 permission denied&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;# Change owner to fix read permission
chown -R elasticsearch:elasticsearch /etc/elasticsearch/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有密碼記得也要用 keystore 把密碼加密後喂給 elasticsearch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password
/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;關於 X.509 Certifcate 之後有空我們來聊一下&lt;/p&gt;
&lt;h1 id=&#34;更新-elasticsearch-設定&#34;&gt;更新 elasticsearch 設定&lt;/h1&gt;
&lt;p&gt;Certificates 都生完了，接下來更改 elasticsearch 的參數，在 transport layer 啟用 ssl。啟用 security 後，在 transport layer 啟動 ssl 是必須的。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/elasticsearch/elasticsearch.yml

xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
# use certificate. full will verify dns and ip
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12
xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;啟用 security 與 transport layer 的 ssl，然後指定 keystore路徑，讓 server 執行 client authentication
由於這筆 p12 帶有 CA certificate 作為 trusted certificate entry，所以也可以順便當作 trustore，讓 client 信任這個 CA&lt;/p&gt;
&lt;p&gt;security 這邊提供了 server side (elasticsearch) 在檢查客戶端連線時的檢查模式(vertification mode)，&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#ssl-tls-settings&#34;&gt;文件有說明&lt;/a&gt;，可以設定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;certificate: 檢查 certificate 加密是否有效&lt;/li&gt;
&lt;li&gt;full: 簽 node certificate 時可以指定 ip dns，啟用會檢查來源 node ip dns 是否也正確&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Optional) HTTP layer 啟動 ssl&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim /etc/elasticsearch/elasticsearch.yml

xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12
xpack.security.http.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12

/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password
/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重啟 elasticsearch，看一下 log&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart elasticsearch
tail -f /var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然後你就發現，原來 kibana 連入 的 http 連線，不斷被 server 這端拒絕。所以以下要來設定 kibana&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;kibana&#34;&gt;Kibana&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/7.3/using-kibana-with-security.html&#34;&gt;using kibana with security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/kibana/7.3/configuring-tls.html&#34;&gt;kibana configuring tls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用剛剛簽的 server certificate，從裡面 parse 出 client-ca.cer，還有 client.cer 與 client.key&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kibana/config

$ openssl pkcs12 --help
Usage: pkcs12 [options]
Valid options are:
 -chain              Add certificate chain
 -nokeys             Don&#39;t output private keys
 -nocerts            Don&#39;t output certificates
 -clcerts            Only output client certificates
 -cacerts            Only output CA certificates
 -info               Print info about PKCS#12 structure
 -nodes              Don&#39;t encrypt private keys
 -in infile          Input filename

# no certs, no descript
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/kibana/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/kibana/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/kibana/config/client-ca.cer

sudo chown -R kibana:kibana /etc/kibana/config/
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;更改 kibana 連入 elasticsearch 的連線設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vim /etc/kigana/kibana.yml

elasticsearch.hosts: [&amp;quot;https://elk.asia-east1-b.c.chechaichang.internal:9200&amp;quot;]
xpack.security.enabled: true
elasticsearch.ssl.certificate: /etc/kibana/config/client.cer
elasticsearch.ssl.key: /etc/kibana/config/client.key
elasticsearch.ssl.certificateAuthorities: [ &amp;quot;/etc/kibana/config/client-ca.cer&amp;quot; ]
elasticsearch.ssl.verificationMode: full
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;指定 ssl.certificate, ssl.key 做連線 elasticsearch server 時的 user authentication&lt;/li&gt;
&lt;li&gt;由於我們是 self-signed CA，所以需要讓客戶端信任這個我們自簽的 CA&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意這邊 elasticsearch.hosts 我們已經從 http://localhost 換成 https 的內部 dns，原有的 localhost 已經無法使用（如果 elasicsearch 有 enforce https 的話）&lt;/p&gt;
&lt;p&gt;重啟 Kibana，看一下 log&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl restart kibana
journalctl -fu kibana
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果沒有一直噴 ssl certificate error 的話，恭喜你成功了&lt;/p&gt;
&lt;p&gt;然而，除了 kibana 以外，我們還有其他的 client 需要連入 elasticsearch&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把上述步驟在 apm-server, filebeat, 其他的 beat 上也設定&lt;/li&gt;
&lt;li&gt;如果在 k8s 上，要把 cer, key 等檔案用 volume 掛進去
&amp;laquo;&amp;laquo;&amp;laquo;&amp;lt; HEAD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kibana server certificate&lt;/li&gt;
&lt;li&gt;filebeat client to kibana server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;=======&lt;/p&gt;
&lt;p&gt;Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kibana server certificate&lt;/li&gt;
&lt;li&gt;filebeat client to kibana server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;就是他們彼此互打，都要有 ca, key, cert&lt;/p&gt;
&lt;h3 id=&#34;但基本上的設定都一樣下面可以不用看下去了xd&#34;&gt;但基本上的設定都一樣，下面可以不用看下去了XD&lt;/h3&gt;
&lt;p&gt;如果有用到再查文件就好，這邊直接小結&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;設定 security 前要先想號自己的需求，如何連入，安全性設定到哪邊&lt;/li&gt;
&lt;li&gt;使用 utility 自簽 CA，然後產生 server certificate&lt;/li&gt;
&lt;li&gt;使用 server certificate 再 parse 出 ca-certificate, client cers, key&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;kibana-作為-server&#34;&gt;kibana 作為 server&lt;/h1&gt;
&lt;p&gt;工作路徑可能是這樣： app(apm-client library) -&amp;gt; apm-server -&amp;gt; kibana -&amp;gt; elasticsearch&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kibana 連入 elasticsearch時， kibana 是 client 吃 elasticsearch 的憑證&lt;/li&gt;
&lt;li&gt;apm-server 連入 kibana時，kibana 是 server，apm-server 吃 kibana 的憑證&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;首先更改 kibana 設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo vim /etc/kibana/kibana.yml

server.ssl.enabled: true
server.ssl.certificate: /etc/kibana/config/client.cer
server.ssl.key: /etc/kibana/config/client.key
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重啟 kibana&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;journalctl -fu kibana
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;apm-server&#34;&gt;Apm-server&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html&#34;&gt;https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;應用端的 apm-client (ex. apm-python-client)，連入 apm-server&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 http 的狀況下，雖然有使用 secret-token，但還是裸奔&lt;/li&gt;
&lt;li&gt;在 https 的狀況下，要把 certificates，然後餵給應用端的client library&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更改 apm-server 的設定&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vim /etc/apm-server/apm-server.yml

host: &amp;quot;0.0.0.0:8200&amp;quot;
  secret_token: &amp;lt;設定一組夠安全的 token&amp;gt;

  rum:
    enabled: true

kibana:
  protocol: &amp;quot;https&amp;quot;
  ssl.enabled: true

output.kibana:
  enable: false # can only have 1 output
output.elasticsearch:

monitoring.elasticsearch:
  protocol: &amp;quot;https&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: &amp;quot;*******************&amp;quot;
  hosts: [&amp;quot;elk.asia-east1-b.c.checahichang.internal:9200&amp;quot;]
  ssl.enabled: true
  ssl.verification_mode: full
  ssl.certificate_authorities: [&amp;quot;/etc/apm-server/config/client-ca.cer&amp;quot;]
  ssl.certificate: &amp;quot;/etc/apm-server/config/client.cer&amp;quot;
  ssl.key: &amp;quot;/etc/apm-server/config/client.key&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;重啟 apm-server&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl restart apm-server
journalctl -fu apm-server
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;apm-library&#34;&gt;APM library&lt;/h1&gt;
&lt;p&gt;應用端的設定就需要依據 library 的實做設定，例如 flask-apmagent-python&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ELASTIC_APM_SERVER_CERT=/etc/elk/certificates/client.cer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/apm/agent/python/current/configuration.html#config-server-cert&#34;&gt;apm agent python config server cert&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;filebeat&#34;&gt;filebeat&lt;/h1&gt;
&lt;p&gt;記得我們在 node 上有安裝 Self-monitoring filebeat，elasticsearch 改成 ssl 這邊當然也連不盡去了，再做同樣操作&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install filebeat

mkdir -p /etc/filebeat/config
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/filebeat/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/filebeat/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/filebeat/config/client-ca.cer
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Restart filebeat&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl restart filebeat
journalctl -fu filebeat
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;h1 id=&#34;如果你的應用在-kubernetes-上&#34;&gt;如果你的應用在 kubernetes 上&lt;/h1&gt;
&lt;p&gt;可以使用下面方法拿到 client.cer ，然後用 secret 塞進 k8s，在用 volume from secrets，掛給監測應用的 filebeat&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
mkdir -p /etc/beats/config
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes &amp;gt; /etc/beats/config/client.key
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys &amp;gt; /etc/beats/config/client.cer
openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain &amp;gt; /etc/beats/config/client-ca.cer

gcloud compute scp elk:/etc/beats/config/* .
 client-ca.cer
 client.cer
 client.key

kubectl -n elk create secret generic elk-client-certificates \
  --from-file=client-ca.cer=client-ca.cer \
  --from-file=client.cer=client.cer \
  --from-file=client.key=client.key

kubectl apply -f elk/gke/filebeat/
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Self-host ELK stack - Installation</title>
      <link>https://chechia.net/post/self-host-elk-stack-on-gcp/</link>
      <pubDate>Sun, 15 Sep 2019 11:43:03 +0800</pubDate>
      
      <guid>https://chechia.net/post/self-host-elk-stack-on-gcp/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/self-host-elk-stack-on-gcp/&#34;&gt;Self-host ELK stack on GCP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/secure-elk-stack/&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/monitoring-gce-with-elk/&#34;&gt;監測 Google Compute Engine 上服務的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/monitoring-gke-with-elk/&#34;&gt;監測 Google Kubernetes Engine 的各項數據&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用 logstash pipeline 做數據前處理&lt;/li&gt;
&lt;li&gt;Elasticsearch 日常維護：數據清理，效能調校，永久儲存&lt;/li&gt;
&lt;li&gt;Debug ELK stack on GCP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作為範例的 ELK 的版本是當前的 stable release 7.3.1。&lt;/p&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&amp;ndash;&lt;/p&gt;
&lt;h1 id=&#34;簡介-elk-stack&#34;&gt;簡介 ELK stack&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/index.html&#34;&gt;官方說明文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;elk-的元件&#34;&gt;ELK 的元件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch: 基於 Lucene 的分散式全文搜索引擎&lt;/li&gt;
&lt;li&gt;Logstash: 數據處理 pipeline&lt;/li&gt;
&lt;li&gt;Kibana: ELK stack 的管理後台與數據視覺化工具&lt;/li&gt;
&lt;li&gt;Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;elk-的工作流程&#34;&gt;ELK 的工作流程&lt;/h3&gt;
&lt;p&gt;beats -&amp;gt; (logstash) -&amp;gt; elasticsearch -&amp;gt; kibana&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起&lt;/li&gt;
&lt;li&gt;設定 beats 從指定的路徑收集 log 與 metrics&lt;/li&gt;
&lt;li&gt;設定 beats 向後輸出的遠端目標&lt;/li&gt;
&lt;li&gt;(Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch&lt;/li&gt;
&lt;li&gt;beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據&lt;/li&gt;
&lt;li&gt;透過 Kibana，將 elasticsearch 上的 log 顯示&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;官方不是有出文件嗎&#34;&gt;官方不是有出文件嗎&lt;/h1&gt;
&lt;p&gt;Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。&lt;/p&gt;
&lt;p&gt;說穿了本篇的技術含量不高，就只是一個踩雷過程。&lt;/p&gt;
&lt;p&gt;Lets get our hands dirty.&lt;/p&gt;
&lt;h1 id=&#34;warning&#34;&gt;WARNING&lt;/h1&gt;
&lt;p&gt;這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在&lt;a href=&#34;https://www.elastic.co/what-is/open-x-pack&#34;&gt;v6.3.0 以前的版本是不包含安全性模組的&lt;/a&gt;，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。&lt;/p&gt;
&lt;p&gt;我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另起一篇對安全性設定多加說明&lt;/li&gt;
&lt;li&gt;在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。&lt;/p&gt;
&lt;p&gt;如果希望第一次安裝就有完整的 security 設定，請等待下篇 &lt;a href=&#34;#secure-elk-stack&#34;&gt;Secure ELK Stask&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;討論需求與規格&#34;&gt;討論需求與規格&lt;/h1&gt;
&lt;p&gt;這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;elk-node-standalone 10.140.0.10
app-node-1          10.140.0.11
...                 ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本機的 ELK stack 元件，彼此透過 localhost 連線&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Elasticsearch:  localhost:9200&lt;/li&gt;
&lt;li&gt;Kibana:         localhost:5601&lt;/li&gt;
&lt;li&gt;Apm-server:     localhost:8200&lt;/li&gt;
&lt;li&gt;Self Monitoring Services&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;私有網路中的外部服務透過 10.140.0.10&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;beats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200&lt;/li&gt;
&lt;li&gt;beats 從其他 node 輸出到 Apm-server:    10.140.0.10:8200&lt;/li&gt;
&lt;li&gt;在內部網路中 透過 browser 存取 Kibana:  10.140.0.10:5601&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;standalone 的好處:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰)&lt;/li&gt;
&lt;li&gt;最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Standalone可能造成的問題:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛&lt;/li&gt;
&lt;li&gt;外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果要有 production ready 的 ELK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HA 開起來&lt;/li&gt;
&lt;li&gt;把服務分散到不同 node 上, 方便之後 scale out 多開幾台
&lt;ul&gt;
&lt;li&gt;elasticsearch-1, elasticsearch-2, elasticsearch-3&amp;hellip;&lt;/li&gt;
&lt;li&gt;kibana-1&lt;/li&gt;
&lt;li&gt;apm-server-1, apm-server-2, &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;主機設定&#34;&gt;主機設定&lt;/h1&gt;
&lt;p&gt;Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。&lt;/p&gt;
&lt;p&gt;GCP 上使用 Google Compote Engine 的朋友，可以照 &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/add-persistent-disk?hl=zh-tw&#34;&gt;Google 官方操作步驟操作&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;完成後接近這樣&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ df -h
$ df --human-readable

Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       9.6G  8.9G  682M  93% /
/dev/sdb        492G   63G  429G  13% /mnt/disks/elk

$ ls /mnt/disks/elk

/mnt/disks/elk/elasticsearch
/mnt/disks/elk/apm-server
/mnt/disks/elk/kibana
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。&lt;/p&gt;
&lt;h1 id=&#34;開防火牆&#34;&gt;開防火牆&lt;/h1&gt;
&lt;p&gt;需要開放 10.140.0.10 這台機器的幾個 port&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch           :9200   來源只開放私有網路其他 ip 10.140.0.0/9&lt;/li&gt;
&lt;li&gt;apm-server              :8200   (同上)&lt;/li&gt;
&lt;li&gt;kibana                  :5601   (同上)，如果想從外部透過 browser開，需要 whitelist ip&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;default-allow-internal: :all    :10.140.0.0/9   tcp:0-65535&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;install-elasticsearch&#34;&gt;Install Elasticsearch&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/7.3/install-elasticsearch.html&#34;&gt;Install Elasticsearch 官方文件 7.3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install apt-transport-https
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
add-apt-repository &amp;quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&amp;quot;
sudo apt-get update
sudo apt-get install elasticsearch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;安裝完後路徑長這樣&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/etc/elasticsearch
/etc/elasticsearch/elasticsearch.yml
/etc/elasticsearch/jvm.options

# Utility
/usr/share/elasticsearch/bin/

# Log
/var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs&lt;/p&gt;
&lt;h3 id=&#34;服務控制&#34;&gt;服務控制&lt;/h3&gt;
&lt;p&gt;透過 systemd 管理，我們可以用 systemctl 控制，
用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。&lt;/p&gt;
&lt;p&gt;但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p /mnt/disks/elk/elasticsearch
chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;設定檔案&#34;&gt;設定檔案&lt;/h3&gt;
&lt;p&gt;ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vim /etc/elasticsearch/elasticsearch.yml

# Change Network
network.host: 0.0.0.0
# Change data path
path.data: /mnt/disks/elk/elasticsearch

vim /etc/elasticsearch/jvm-options
# Adjust heap to 4G
-Xms4g
-Xmx4g

# Enable xpack.security
discovery.seed_hosts: [&amp;quot;10.140.0.10&amp;quot;]
discovery.type: &amp;quot;single-node&amp;quot;
xpack.security.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.license.self_generated.type: basic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。&lt;/p&gt;
&lt;p&gt;有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto

# Keep your passwords safe
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然後把啟動 Elasticsearch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl enable elasticsearch.service
sudo systemctl start elasticsearch.service
sudo systemctl status elasticsearch.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看一下 log，確定服務有在正常工作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/elasticsearch/elasticsearch.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在 node 上試打 Elasticsearch API&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl localhost:9200

{
  &amp;quot;name&amp;quot; : &amp;quot;elk&amp;quot;,
  &amp;quot;cluster_name&amp;quot; : &amp;quot;elasticsearch&amp;quot;,
  &amp;quot;cluster_uuid&amp;quot; : &amp;quot;uiMZe7VETo-H6JLFLF4SZg&amp;quot;,
  &amp;quot;version&amp;quot; : {
    &amp;quot;number&amp;quot; : &amp;quot;7.3.1&amp;quot;,
    &amp;quot;build_flavor&amp;quot; : &amp;quot;default&amp;quot;,
    &amp;quot;build_type&amp;quot; : &amp;quot;deb&amp;quot;,
    &amp;quot;build_hash&amp;quot; : &amp;quot;4749ba6&amp;quot;,
    &amp;quot;build_date&amp;quot; : &amp;quot;2019-08-19T20:19:25.651794Z&amp;quot;,
    &amp;quot;build_snapshot&amp;quot; : false,
    &amp;quot;lucene_version&amp;quot; : &amp;quot;8.1.0&amp;quot;,
    &amp;quot;minimum_wire_compatibility_version&amp;quot; : &amp;quot;6.8.0&amp;quot;,
    &amp;quot;minimum_index_compatibility_version&amp;quot; : &amp;quot;6.0.0-beta1&amp;quot;
  },
  &amp;quot;tagline&amp;quot; : &amp;quot;You Know, for Search&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;kibana&#34;&gt;Kibana&lt;/h1&gt;
&lt;p&gt;有了正常工作的 Elasticsearch，接下來要安裝 kibana，由於 apt repository 已經匯入，這邊直接&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install kibana
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;一樣快速設定一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ vim /etc/kibana/kinana.yml

# change server.host from localhost to 0.0.0.0 to allow outside requests
server.host: &amp;quot;0.0.0.0&amp;quot;

# Add elasticsearch password
elasticsearch.username: &amp;quot;kibana&amp;quot;
elasticsearch.password:

sudo systemctl enable kibana.service
sudo systemctl start kibana.service
sudo systemctl status kibana.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;檢查 log 並試打一下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl status kibana

$ curl localhost:5601
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;透過內網 ip 也可以用 browser 存取
使用 elastic 這組帳號密碼登入，可以有管理員權限
可以檢視一下 kibana 的頁面，看一下是否系統功能都上常上線
http://10.140.0.10/app/monitoring#&lt;/p&gt;
&lt;h1 id=&#34;filebeat&#34;&gt;Filebeat&lt;/h1&gt;
&lt;p&gt;以上是 ELK 最基本架構: elasticsearch 引擎與前端視覺化管理工具 Kibana。當然現在進去 kibana 是沒有數據的，所以我們現在來安裝第一個 beat，收集第一筆數據。&lt;/p&gt;
&lt;p&gt;你可能會覺得奇怪: 我現在沒有任何需要監控的應用，去哪收集數據?&lt;/p&gt;
&lt;p&gt;ELK 提供的自我監測 (self-monitoring) 的功能，也就是在 node 上部屬 filebeat 並啟用 modules，便可以把這台 node 上的 elasticsearch 運行的狀況，包含cpu 狀況、記憶體用量、儲存空間用量、安全性告警、&amp;hellip;都做為數據，傳到 elasticsearch 中，並在 Kibana monitoring 頁面製圖顯示。&lt;/p&gt;
&lt;p&gt;這邊也剛好做為我們 ELK stack 的第一筆數據收集。&lt;/p&gt;
&lt;p&gt;WARNING: 這邊一樣要提醒， production 環境多半會使用另外一組的 elasticsearch 來監控主要的這組 elastic stack，以維持 elk stack 的穩定性，才不會自己 monitoring 自己，結果 elastic 掛了，metrics 跟錯誤訊息都看不到。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-installation.html&#34;&gt;官方安裝文件&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get update
sudo apt-get install filebeat
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;預設的 filebeat.yml 設定檔案不是完整的，請到官網下載完整版，但官網沒給檔案連結(慘)，只有網頁版 &lt;a href=&#34;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&#34;&gt;https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我們上 github 把她載下來&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ wget https://raw.githubusercontent.com/elastic/beats/v7.3.1/filebeat/filebeat.reference.yml
$ sudo mv filebeat-reference-y
$ sudo vim /etc/filebeat/filebeat.yml

# Enable elasticsearch module and kibana module to process metrics of localhost elasticsearch &amp;amp; kibana
filebeat.modules:
- module: elasticsearch
  # Server log
  server:
    enabled: true

- module: kibana
  # All logs
  log:
    enabled: true

# The name will be added to metadata
name: filebeat-elk
fields:
  env: elk

# Add additional cloud_metadata since we&#39;re on GCP
processors:
- add_cloud_metadata: ~

# Output to elasticsearch
output.elasticsearch:
  enabled: true
  hosts: [&amp;quot;localhost:9200&amp;quot;]
  protocol: &amp;quot;http&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: 

# Configure kibana with filebeat: add template, dashboards, etc...
setup.kibana:
  host: &amp;quot;localhost:5601&amp;quot;
  protocol: &amp;quot;http&amp;quot;
  username: &amp;quot;elastic&amp;quot;
  password: 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;啟動 filebeat&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo systemctl start filebeat
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;看一下 log，filebeat 會開始收集 elasticsearch 的 log 與 metrics，可以在 log 上看到收集的狀況。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sudo journalctl -fu filebeat

Sep 15 06:28:50 elk filebeat[9143]: 2019-09-15T06:28:50.176Z        INFO        [monitoring]        log/log.go:145        Non-zero metrics in the last 30s        {&amp;quot;monitoring&amp;quot;: {&amp;quot;metrics&amp;quot;: {&amp;quot;beat&amp;quot;:{&amp;quot;cpu&amp;quot;:{&amp;quot;system&amp;quot;:{&amp;quot;ticks&amp;quot;:1670860,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:66}},&amp;quot;total&amp;quot;:{&amp;quot;ticks&amp;quot;:6964660,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:336},&amp;quot;value&amp;quot;:6964660},&amp;quot;user&amp;quot;:{&amp;quot;ticks&amp;quot;:5293800,&amp;quot;time&amp;quot;:{&amp;quot;ms&amp;quot;:270}}},&amp;quot;handles&amp;quot;:{&amp;quot;limit&amp;quot;:{&amp;quot;hard&amp;quot;:4096,&amp;quot;soft&amp;quot;:1024},&amp;quot;open&amp;quot;:11},&amp;quot;info&amp;quot;:{&amp;quot;ephemeral_id&amp;quot;:&amp;quot;62fd4bfa-1949-4356-9615-338ca6a95075&amp;quot;,&amp;quot;uptime&amp;quot;:{&amp;quot;ms&amp;quot;:786150373}},&amp;quot;memstats&amp;quot;:{&amp;quot;gc_next&amp;quot;:7681520,&amp;quot;memory_alloc&amp;quot;:4672576,&amp;quot;memory_total&amp;quot;:457564560376,&amp;quot;rss&amp;quot;:-32768},&amp;quot;runtime&amp;quot;:{&amp;quot;goroutines&amp;quot;:98}},&amp;quot;filebeat&amp;quot;:{&amp;quot;events&amp;quot;:{&amp;quot;active&amp;quot;:-29,&amp;quot;added&amp;quot;:1026,&amp;quot;done&amp;quot;:1055},&amp;quot;harvester&amp;quot;:{&amp;quot;open_files&amp;quot;:4,&amp;quot;running&amp;quot;:4}},&amp;quot;libbeat&amp;quot;:{&amp;quot;config&amp;quot;:{&amp;quot;module&amp;quot;:{&amp;quot;running&amp;quot;:0}},&amp;quot;output&amp;quot;:{&amp;quot;events&amp;quot;:{&amp;quot;acked&amp;quot;:1055,&amp;quot;active&amp;quot;:-50,&amp;quot;batches&amp;quot;:34,&amp;quot;total&amp;quot;:1005},&amp;quot;read&amp;quot;:{&amp;quot;bytes&amp;quot;:248606},&amp;quot;write&amp;quot;:{&amp;quot;bytes&amp;quot;:945393}},&amp;quot;pipeline&amp;quot;:{&amp;quot;clients&amp;quot;:9,&amp;quot;events&amp;quot;:{&amp;quot;active&amp;quot;:32,&amp;quot;published&amp;quot;:1026,&amp;quot;total&amp;quot;:1026},&amp;quot;queue&amp;quot;:{&amp;quot;acked&amp;quot;:1055}}},&amp;quot;registrar&amp;quot;:{&amp;quot;states&amp;quot;:{&amp;quot;current&amp;quot;:34,&amp;quot;update&amp;quot;:1055},&amp;quot;writes&amp;quot;:{&amp;quot;success&amp;quot;:35,&amp;quot;total&amp;quot;:35}},&amp;quot;system&amp;quot;:{&amp;quot;load&amp;quot;:{&amp;quot;1&amp;quot;:1.49,&amp;quot;15&amp;quot;:0.94,&amp;quot;5&amp;quot;:1.15,&amp;quot;norm&amp;quot;:{&amp;quot;1&amp;quot;:0.745,&amp;quot;15&amp;quot;:0.47,&amp;quot;5&amp;quot;:0.575}}}}}}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;如果數據都有送出，就可以回到 kibana 的頁面，看一下目前這個 elasticsearch 集群，有開啟 monitoring 功能的元件們，是否都有正常工作。&lt;/p&gt;
&lt;p&gt;http://10.140.0.10/app/monitoring#&lt;/p&gt;
&lt;p&gt;頁面長得像這樣&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;https://chechia.net/img/elk/kibana-monitoring.png&#34; width=&#34;100%&#34; height=&#34;100%&#34; &gt;


&lt;/figure&gt;

&lt;p&gt;Standalone cluster 中的 filebeat，是還未跟 elasticsearch 配對完成的數據，會顯示在另外一個集群中，配對完後會歸到 elk cluster 中，就是我們的主要 cluster。&lt;/p&gt;
&lt;p&gt;點進去可以看各個元件的服務情形。&lt;/p&gt;
&lt;h1 id=&#34;小結&#34;&gt;小結&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;簡單思考 self-host ELK stack 搭建的架構&lt;/li&gt;
&lt;li&gt;在單一 node 上安裝最簡易的 elastic stack&lt;/li&gt;
&lt;li&gt;設定元件的 output 位置&lt;/li&gt;
&lt;li&gt;設定 self-monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;恭喜各位獲得一個裸奔但是功能完整的 ELK, 我們下篇再向安全性邁進。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Redis Ha Topology</title>
      <link>https://chechia.net/post/redis-ha-topology/</link>
      <pubDate>Fri, 23 Aug 2019 16:12:10 +0800</pubDate>
      
      <guid>https://chechia.net/post/redis-ha-topology/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://ithelp.ithome.com.tw/2020ironman&#34;&gt;2020 It邦幫忙鐵人賽&lt;/a&gt; 系列文章&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 GKE 上部署 Redis HA
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://chechia.net/post/redis-ha-deployment/&#34;&gt;使用 helm 部署 redis-ha&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Redis HA with sentinel&lt;/li&gt;
&lt;li&gt;Redis sentinel topology&lt;/li&gt;
&lt;li&gt;Redis HA with HAproxy&lt;/li&gt;
&lt;li&gt;集群內部的 HA 設定，網路設定&lt;/li&gt;
&lt;li&gt;應用端的基本範例，效能調校&lt;/li&gt;
&lt;li&gt;在 GKE 上維運 redis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。&lt;/p&gt;
&lt;p&gt;寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。&lt;/p&gt;
&lt;p&gt;對我的文章有興趣，歡迎到我的網站上 &lt;a href=&#34;https://chechia.net&#34;&gt;https://chechia.net&lt;/a&gt; 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg&#34; alt=&#34;Exausted Cat Face&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;摘要&#34;&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Redis Sentinel Topology&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;topology&#34;&gt;Topology&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Masters: M1, M2, M3, &amp;hellip;, Mn.&lt;/li&gt;
&lt;li&gt;Slaves: R1, R2, R3, &amp;hellip;, Rn (R stands for replica).&lt;/li&gt;
&lt;li&gt;Sentinels: S1, S2, S3, &amp;hellip;, Sn.&lt;/li&gt;
&lt;li&gt;Clients: C1, C2, C3, &amp;hellip;, Cn.&lt;/li&gt;
&lt;li&gt;每個方格代表一台機器或是 VM&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-sentinels&#34;&gt;2 Sentinels&lt;/h3&gt;
&lt;p&gt;DON&amp;rsquo;T DO THIS&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+----+         +----+
| M1 |---------| R1 |
| S1 |         | S2 |
+----+         +----+

Configuration: quorum = 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這個設定下，如果 M1 掛了需要 failover，很有可能 S1 跟著機器一起掛了，S2 會沒有辦法取得多數來執行 failover，整個系統掛掉&lt;/p&gt;
&lt;h3 id=&#34;3-vm&#34;&gt;3 VM&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;       +----+
       | M1 |
       | S1 |
       +----+
          |
+----+    |    +----+
| R2 |----+----| R3 |
| S2 |         | S3 |
+----+         +----+

Configuration: quorum = 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;這是最基本的蛋又兼顧安全設定的設置&lt;/p&gt;
&lt;p&gt;如果 M1 死了 S1 跟著機器故障，S2 與 S3 還可以取得多數，順利 failover 到 R2 或是 R3。&lt;/p&gt;
&lt;h3 id=&#34;寫入資料遺失&#34;&gt;寫入資料遺失&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;         +----+
         | M1 |
         | S1 | &amp;lt;- C1 (writes will be lost)
         +----+
            |
            /
            /
+------+    |    +----+
| [M2] |----+----| R3 |
| S2   |         | S3 |
+------+         +----+
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;failover 之前，M1 是 master，Client 的寫入往 M1 寫&lt;/li&gt;
&lt;li&gt;M1 網路故障，M2 failover 後成為新的 master，可是 Client 往 M1 寫入的資料並無法 sync 回 M2&lt;/li&gt;
&lt;li&gt;等網路修復後，M1 回覆後會變成 R1 變成 slave，由 M2 去 sync R1，變成 R1 在 master 時收到的寫入資料遺失&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;為了避免這種情形，做額外的設定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;min-slaves-to-write 1&lt;/li&gt;
&lt;li&gt;min-slaves-max-lag 10&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當 master 發現自己再也無法 sync 到足夠的 slave，表示 master 可能被孤立，這時主動拒絕客戶端的寫入請求。客戶端被拒絕後，會再向 sentinel 取得有效的 master，重新執行寫入請求，確保資料寫到有效的 master 上。&lt;/p&gt;
&lt;h3 id=&#34;sentinel-放在-client-端&#34;&gt;Sentinel 放在 Client 端&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;            +----+         +----+
            | M1 |----+----| R1 |
            |    |    |    |    |
            +----+    |    +----+
                      |
         +------------+------------+
         |            |            |
         |            |            |
      +----+        +----+      +----+
      | C1 |        | C2 |      | C3 |
      | S1 |        | S2 |      | S3 |
      +----+        +----+      +----+
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有些情形，redis 這端只有兩台可用機器，這種情形可以考慮把 sentinel 放在客戶端的機器上&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仍然維持了獨立的 3 sentinels 的穩定&lt;/li&gt;
&lt;li&gt;sentinel 與 client 所觀察到的 redis 狀態是相同的&lt;/li&gt;
&lt;li&gt;如果 M1 死了，要 failover ，客戶端的 3 sentinel 可以正確地執行 failover，不受故障影響&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;客戶端又不足-3-個&#34;&gt;客戶端又不足 3 個&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;            +----+         +----+
            | M1 |----+----| R1 |
            | S1 |    |    | S2 |
            +----+    |    +----+
                      |
               +------+-----+
               |            |  
               |            |
            +----+        +----+
            | C1 |        | C2 |
            | S3 |        | S4 |
            +----+        +----+

      Configuration: quorum = 3

            +----+         +----+
            | M1 |----+----| R1 |
            | S1 |    |    | S2 |
            +----+    |    +----+
                      |
                      |        
                      |        
                   +----+      
                   | C1 |      
                   | S3 |      
                   +----+      

      Configuration: quorum = 2
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;跟上個例子類似，但又額外確保 3 sentinels&lt;/li&gt;
&lt;li&gt;如果 M1 死了，剩下的 sentinel 可以正確 failover&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Ithome Cloud Summit 2019: Service Mesh for Microservices on Kubernetes</title>
      <link>https://chechia.net/talk/service-mesh-for-microservices-on-kubernetes/</link>
      <pubDate>Wed, 15 May 2019 12:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/service-mesh-for-microservices-on-kubernetes/</guid>
      <description>&lt;h3 id=&#34;outlines&#34;&gt;Outlines&lt;/h3&gt;
&lt;p&gt;傳統的 Monolith被分解為分散的微服務，以取得更高的效能與更彈性的管理。當眾多的為服務同時運作，產生複雜的依賴與交流，網路層不再只是有『有通就好』，而是需要精細且彈性的流量管理與監控，來提供穩定的效能。本次主題將基於 Kubernetes 平台上的 Istio ，探討 Service Mesh 的概念與相關應用。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;何為 Service Mesh ？為何需要 Service Mesh ？&lt;/li&gt;
&lt;li&gt;Service Mesh 基本概念&lt;/li&gt;
&lt;li&gt;如何Service-to-Service的網路層管理監控&lt;/li&gt;
&lt;li&gt;導入 Istio 到 Kubernetes&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;目標聽眾&#34;&gt;目標聽眾&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;微運大量微服務，希望導入Service Mesh 的Operator&lt;/li&gt;
&lt;li&gt;想了解微服務生態中竄紅的 Service Mesh&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;收穫&#34;&gt;收穫&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;了解為服務的優勢與Cloud Native應用發展趨勢&lt;/li&gt;
&lt;li&gt;了解 Service Mesh 與 Istio 觀念&lt;/li&gt;
&lt;li&gt;能使用 Istio 於 Kubernetes，進行服務網路的管理。&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;你有聽過-microservice--istio有聽過嗎&#34;&gt;你有聽過 Microservice / Istio有聽過嗎？&lt;/h3&gt;
&lt;p&gt;今天來介紹一款好藥：Istio。如果你有以下問題：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;維運大量(成千上百)微服務&lt;/li&gt;
&lt;li&gt;需要服務對服務的流量控制，監控，管理&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;談 Service Mesh 之前，不免的要先談一下 Microservice，這個目前好像很夯的一個技術名詞。&lt;/p&gt;
&lt;p&gt;如果手上有一個 App，會希望依照 Monolith 的架構，或是 Microservices？
Microservices 聽起來又新又潮。相對於 Monolith有許多明顯的好處：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decoupling&lt;/li&gt;
&lt;li&gt;Scalability&lt;/li&gt;
&lt;li&gt;Performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也有明顯的壞處：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Development Complexity&lt;/li&gt;
&lt;li&gt;Operation Cost&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;沒事別挖坑跳&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;何為 Service Mesh？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service Mesh: Model / Pattern&lt;/li&gt;
&lt;li&gt;Implementations: linkerd, istio, &amp;hellip;&lt;/li&gt;
&lt;li&gt;基於底層的網路服務，在複雜的 topology 中可靠的傳遞&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用Microservie 可能會遇到的問題：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Traffic control&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;li&gt;A/B Testing&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Istio 三分鐘就入坑 佈署篇</title>
      <link>https://chechia.net/post/service-mesh-for-microservice-on-kubernetes/</link>
      <pubDate>Mon, 06 May 2019 18:12:15 +0800</pubDate>
      
      <guid>https://chechia.net/post/service-mesh-for-microservice-on-kubernetes/</guid>
      <description>&lt;h1 id=&#34;create-gke&#34;&gt;Create GKE&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;gcloud beta container --project &amp;quot;istio-playground-239810&amp;quot; clusters create &amp;quot;istio-playground&amp;quot; \
  --zone &amp;quot;asia-east1-b&amp;quot; \
  --username &amp;quot;admin&amp;quot; \
  --cluster-version &amp;quot;1.11.8-gke.6&amp;quot; \
  --machine-type &amp;quot;n1-standard-2&amp;quot; \
  --image-type &amp;quot;COS&amp;quot; \
  --disk-type &amp;quot;pd-standard&amp;quot; \
  --disk-size &amp;quot;100&amp;quot; \
  --preemptible \
  --num-nodes &amp;quot;1&amp;quot; \
  --enable-cloud-logging \
  --enable-cloud-monitoring \
  --no-enable-ip-alias \
  --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard,Istio \
  --istio-config auth=MTLS_PERMISSIVE \
  --no-enable-autoupgrade \
  --enable-autorepair
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;take-a-peek&#34;&gt;Take a Peek&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;$ kubectl get namespaces

NAME           STATUS    AGE
default        Active    2m
istio-system   Active    1m
kube-public    Active    2m
kube-system    Active    2m

$ kubectl get po -n istio-system
NAME                                      READY     STATUS      RESTARTS   AGE
istio-citadel-7f6f77cd7b-nxfbf            1/1       Running     0          3m
istio-cleanup-secrets-h454m               0/1       Completed   0          3m
istio-egressgateway-7c56db84cc-nlrwq      1/1       Running     0          3m
istio-galley-6c747bdb4f-45jrp             1/1       Running     0          3m
istio-ingressgateway-6ff68cf95d-tlkq4     1/1       Running     0          3m
istio-pilot-8ff66f8c4-q9chz               2/2       Running     0          3m
istio-policy-69b78b7d6-c8pld              2/2       Running     0          3m
istio-sidecar-injector-558996c897-hr6q4   1/1       Running     0          3m
istio-telemetry-f96459fb-5cbpg            2/2       Running     0          3m
promsd-ff878d44b-hv8nh                    2/2       Running     1          3m
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;deploy-app&#34;&gt;Deploy app&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;kubectl label namespace default istio-injection=enabled
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Bookinfo Application&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml

kubectl get pods
kubectl get services
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Gateway&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml

kubectl get gateways

kubectl get svc istio-ingressgateway -n istio-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Go to ingress public ip&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.status.loadBalancer.ingress[0].ip}&#39;)
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.spec.ports[?(@.name==&amp;quot;http2&amp;quot;)].port}&#39;)
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=&#39;{.spec.ports[?(@.name==&amp;quot;https&amp;quot;)].port}&#39;)

curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage

404 Not Found
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Apply destination rules&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/destination-rule-all.yaml

curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;brief-review&#34;&gt;Brief review&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl get virtualservices
kubectl get destinationrules
kubectl get gateways
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;istio-tasks&#34;&gt;Istio Tasks&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://istio.io/docs/tasks/traffic-management/&#34;&gt;https://istio.io/docs/tasks/traffic-management/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DevOps Taiwan Conference 2019: Jenkins X on Kubernetes</title>
      <link>https://chechia.net/talk/jenkins-on-kubernetes/</link>
      <pubDate>Sat, 20 Apr 2019 13:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/jenkins-on-kubernetes/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;How to deploy a cloud-native Jenkins with Jenkins X.&lt;/li&gt;
&lt;li&gt;A pipeline with Kubernetes based dynamics worker sclaing (jenkins-kubernetes).&lt;/li&gt;
&lt;li&gt;Give it a try.&lt;/li&gt;
&lt;li&gt;(Defered) Customized test reports for multiple language (ex. go-junit-report)&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Jenkins X on Kubernetes</title>
      <link>https://chechia.net/post/jenkins-x-on-kubernetes/</link>
      <pubDate>Fri, 19 Apr 2019 12:15:41 +0800</pubDate>
      
      <guid>https://chechia.net/post/jenkins-x-on-kubernetes/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://jenkins.io/&#34;&gt;Jenkins&lt;/a&gt; is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jenkins-x.io/&#34;&gt;Jenkins X&lt;/a&gt;, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.&lt;/p&gt;
&lt;p&gt;Here we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture&#34;&gt;Architecture of Jenkins X&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install&#34;&gt;Install Jenkins with jx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pipeline&#34;&gt;Create a Pipeline with jx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#client&#34;&gt;Develope with jx client&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more information about jx itself, check &lt;a href=&#34;https://github.com/jenkins-x/jx&#34;&gt;Jenkins-X Github Repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;
&lt;p&gt;Check this beautiful diagram.&lt;/p&gt;



  




&lt;figure&gt;

&lt;img src=&#34;https://chechia.net/img/jenkins/architecture-serverless.png&#34; &gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;a href=&#34;https://jenkins-x.io/architecture/diagram/&#34;&gt;https://jenkins-x.io/architecture/diagram/&lt;/a&gt;&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;install&#34;&gt;Install&lt;/h1&gt;
&lt;h3 id=&#34;create-gke-cluster--get-credentials&#34;&gt;Create GKE cluster &amp;amp; Get Credentials&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;gcloud init
gcloud components update
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;CLUSTER_NAME=jenkins-server
#CLUSTER_NAME=jenkins-serverless

gcloud container clusters create ${CLUSTER_NAME} \
  --num-nodes 1 \
  --machine-type n1-standard-4 \
  --enable-autoscaling \
  --min-nodes 1 \
  --max-nodes 2 \
  --zone asia-east1-b \
  --preemptible

# After cluster initialization, get credentials to access cluster with kubectl
gcloud container clusters get-credentials ${CLUSTER_NAME}

# Check cluster stats.
kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;install-jx-on-local-machine&#34;&gt;Install jx on Local Machine&lt;/h3&gt;
&lt;p&gt;[Jenkins X Release](&lt;a href=&#34;https://github.com/jenkins-x/jx/releases%5D(https://github.com/jenkins-x/jx/releases)&#34;&gt;https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases)&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;JX_VERSION=v2.0.2
OS_ARCH=darwin-amd64
#OS_ARCH=linux-amd64
curl -L https://github.com/jenkins-x/jx/releases/download/&amp;quot;${JX_VERSION}&amp;quot;/jx-&amp;quot;${OS_ARCH}&amp;quot;.tar.gz | tar xzv
sudo mv jx /usr/local/bin
jx version

NAME               VERSION
jx                 2.0.2
Kubernetes cluster v1.11.7-gke.12
kubectl            v1.11.9-dispatcher
helm client        v2.11.0+g2e55dbe
helm server        v2.11.0+g2e55dbe
git                git version 2.20.1
Operating System   Mac OS X 10.14.4 build 18E226
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;option-1-install-serverless-jenkins-pipeline&#34;&gt;(Option 1) Install Serverless Jenkins Pipeline&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;DEFAULT_PASSWORD=mySecretPassWord123
jx install \
  --default-admin-password=${DEFAULT_PASSWORD} \
  --provider=&#39;gke&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enter Github user name&lt;/li&gt;
&lt;li&gt;Enter Github personal api token for CI/CD&lt;/li&gt;
&lt;li&gt;Enable Github as Git pipeline server&lt;/li&gt;
&lt;li&gt;Select Jenkins installation type:
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Serverless Jenkins X Pipelines with Tekon&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Static Master Jenkins&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pick default workload build pack
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Library Workloads: CI+Release but no CD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select the organization where you want to create the environment repository:
&lt;ul&gt;
&lt;li&gt;chechiachang&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;Your Kubernetes context is now set to the namespace: jx
INFO[0231] To switch back to your original namespace use: jx namespace jx
INFO[0231] Or to use this context/namespace in just one terminal use: jx shell
INFO[0231] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/
INFO[0231] To import existing projects into Jenkins:       jx import
INFO[0231] To create a new Spring Boot microservice:       jx create spring -d web -d actuator
INFO[0231] To create a new microservice from a quickstart: jx create quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;option-2-install-static-jenkins-server&#34;&gt;(Option 2) Install Static Jenkins Server&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;DEFAULT_PASSWORD=mySecretPassWord123

jx install \
  --default-admin-password=${DEFAULT_PASSWORD} \
  --provider=&#39;gke&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enter Github user name&lt;/li&gt;
&lt;li&gt;Enter Github personal api token for CI/CD&lt;/li&gt;
&lt;li&gt;Enable Github as Git pipeline server&lt;/li&gt;
&lt;li&gt;Select Jenkins installation type:
&lt;ul&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Serverless Jenkins X Pipelines with Tekon&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Static Master Jenkins&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pick default workload build pack
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Library Workloads: CI+Release but no CD&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select the organization where you want to create the environment repository:
&lt;ul&gt;
&lt;li&gt;chechiachang&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;INFO[0465]Your Kubernetes context is now set to the namespace: jx
INFO[0465] To switch back to your original namespace use: jx namespace default
INFO[0465] Or to use this context/namespace in just one terminal use: jx shell
INFO[0465] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/
INFO[0465] To import existing projects into Jenkins:       jx import
INFO[0465] To create a new Spring Boot microservice:       jx create spring -d web -d actuator
INFO[0465] To create a new microservice from a quickstart: jx create quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Access Static Jenkins Server through Domain with username and password
Domain &lt;a href=&#34;http://jenkins.jx.11.22.33.44.nip.io/&#34;&gt;http://jenkins.jx.11.22.33.44.nip.io/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;uninstall&#34;&gt;Uninstall&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx uninstall
# rm -rf ~/.jx
&lt;/code&gt;&lt;/pre&gt;&lt;hr&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;setup-cicd-pipeline&#34;&gt;Setup CI/CD Pipeline&lt;/h1&gt;
&lt;h3 id=&#34;create-quickstart-repository&#34;&gt;Create Quickstart Repository&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --namespace jx --watch
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;# cd workspace
jx create quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which organisation do you want to use? chechiachang&lt;/li&gt;
&lt;li&gt;Enter the new repository name:  serverless-jenkins-quickstart&lt;/li&gt;
&lt;li&gt;select the quickstart you wish to create  [Use arrows to move, type to filter]
angular-io-quickstart
aspnet-app
dlang-http&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;golang-http
jenkins-cwp-quickstart
jenkins-quickstart
node-http&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;INFO[0121] Watch pipeline activity via:    jx get activity -f serverless-jenkins-quickstart -w
INFO[0121] Browse the pipeline log via:    jx get build logs chechiachang/serverless-jenkins-quickstart/master
INFO[0121] Open the Jenkins console via    jx console
INFO[0121] You can list the pipelines via: jx get pipelines
INFO[0121] Open the Jenkins console via    jx console
INFO[0121] You can list the pipelines via: jx get pipelines
INFO[0121] When the pipeline is complete:  jx get applications
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;check-log-of-the-first-run&#34;&gt;Check log of the first run&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx logs pipeline
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;add-step-to-pipeline&#34;&gt;Add Step to Pipeline&lt;/h3&gt;
&lt;p&gt;Add a setup step for pullrequest&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd serverless-jenkins-quickstart
jx create step --pipeline pullrequest \
  --lifecycle setup \
  --mode replace \
  --sh &amp;quot;echo hello world&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Validate pipeline step for each modification&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jx step validate
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A build-pack pod started after git push. Watch pod status with kubectl.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --namespace jx --watch
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;check-build-status-on-prow-serverless&#34;&gt;Check Build Status on Prow (Serverless)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://deck.jx.130.211.245.13.nip.io/&#34;&gt;http://deck.jx.130.211.245.13.nip.io/&lt;/a&gt;
Login with username and password&lt;/p&gt;
&lt;h3 id=&#34;import-existing-repository&#34;&gt;Import Existing Repository&lt;/h3&gt;
&lt;p&gt;In source code repository:&lt;/p&gt;
&lt;p&gt;Import jx to remote jenkins-server. This will apply a Jenkinsfile to repository by default&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jx import --url git@github.com:chechiachang/serverless-jenkins-quickstart.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Update jenkins-x.yml&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jx create step
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;git commit &amp;amp; push&lt;/p&gt;
&lt;h3 id=&#34;trouble-shooting&#34;&gt;Trouble Shooting&lt;/h3&gt;
&lt;p&gt;Failed to get jx resources&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jx get pipelines
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Make sure your jx (or kubectl) context is with the correct GKE and namespace&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kc config set-context gke_my-project_asia-east1-b_jenkins \
  --namespace=jx
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;why-not-use-helm-chart&#34;&gt;Why not use helm chart?&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s readlly depend on what we need in CI/CD automation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/jenkins&#34;&gt;Jenkins Helm Chart&lt;/a&gt; create Jenkins master and slave cluster on Kubernetes utilizing the Jenkins Kubernetes plugin.
Jenkin Platform with jx is Jenkins Platform native to Kubernetes. It comes with powerful cloud native components like Prow automation, Nexus, Docker Registry, Tekton Pipeline, &amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;check-jenkins-x-examples&#34;&gt;Check jenkins-x examples&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs&#34;&gt;https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;client&#34;&gt;Client&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;jx get urls
Name                      URL
jenkins                   http://jenkins.jx.11.22.33.44.nip.io
jenkins-x-chartmuseum     http://chartmuseum.jx.11.22.33.44.nip.io
jenkins-x-docker-registry http://docker-registry.jx.11.22.33.44.nip.io
jenkins-x-monocular-api   http://monocular.jx.11.22.33.44.nip.io
jenkins-x-monocular-ui    http://monocular.jx.11.22.33.44.nip.io
nexus                     http://nexus.jx.11.22.33.44.nip.io
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;get-cluster-status&#34;&gt;Get Cluster Status&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx diagnose
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;get-applications--pipelines&#34;&gt;Get Applications &amp;amp; Pipelines&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx get applications
jx get pipelines
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;get-ci-activities--build-log&#34;&gt;Get CI Activities &amp;amp; build log&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx get activities
jx get activities --filter=&#39;jenkins-x-on-kubernetes&#39;

jx get build log

INFO[0003] view the log at: http://jenkins.jx.11.22.33.44.nip.io/job/chechiachang/job/jenkins-x-on-kubernetes/job/feature-add-test/3/console
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;trigger-build--check-activity&#34;&gt;Trigger Build &amp;amp; Check Activity&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx start pipeline
jx start pipeline --filter=&#39;jenkins-x-on-kubernetes/feature-add-test&#39;

jx get activities --filter=&#39;jenkins-x-on-kubernetes&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;create-pull-request&#34;&gt;Create Pull Request&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;jx create pullrequest
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Jenkins X on Kubernetes</title>
      <link>https://chechia.net/project/jenkins-x-on-kubernetes/</link>
      <pubDate>Fri, 19 Apr 2019 11:11:59 +0800</pubDate>
      
      <guid>https://chechia.net/project/jenkins-x-on-kubernetes/</guid>
      <description>&lt;p&gt;An example project to demonstrate a working pipeline with jenkins-x on Kubernetes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Native Taiwan #13: ELK for Applications on Kubernetes</title>
      <link>https://chechia.net/talk/elk-on-kubernetes/</link>
      <pubDate>Tue, 22 Jan 2019 19:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/elk-on-kubernetes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Kubernetes Container Runtime Interface</title>
      <link>https://chechia.net/post/kubernetes-container-runtime-interface/</link>
      <pubDate>Sat, 06 Oct 2018 12:07:00 +0800</pubDate>
      
      <guid>https://chechia.net/post/kubernetes-container-runtime-interface/</guid>
      <description>&lt;p&gt;-&amp;gt; &lt;a href=&#34;https://chechia.net/slides/container-runtime-interface/&#34;&gt;Slides here&lt;/a&gt; &amp;lt;-&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ithome Kubernetes Summit 2018: Kubernetes Networking</title>
      <link>https://chechia.net/talk/kubernetes-networking/</link>
      <pubDate>Thu, 14 Jun 2018 09:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/kubernetes-networking/</guid>
      <description>&lt;p&gt;從系統管理層面看Kubernetes的網路架構&lt;/p&gt;
&lt;p&gt;網路實作為Kubernetes架構，也是開發過程中容易出錯的部分。本次演講將從群集管理員的角度，說明Kubernetes 中網路的實作。&lt;/p&gt;
&lt;p&gt;大綱:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Docker 與 Kubernetes 的網路架構&lt;/li&gt;
&lt;li&gt;不同層級的網路溝通實作&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;容器對容器&lt;/li&gt;
&lt;li&gt;Pod對Pod&lt;/li&gt;
&lt;li&gt;集群內部與Service&lt;/li&gt;
&lt;li&gt;集群外部對Service&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;以flannel為例講解網路實作&lt;/li&gt;
&lt;li&gt;開發過程中常遇到的網路問題&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;希望聽眾對Kubernetes的網路架構能有基礎的概念，並在開發過程中遇到問題時，有明確的除錯步驟來判定網路是否有問題。遇到網路的問題，也能明確的知道問題的核心，並找到解法。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ithome Cloud Summit 2018: Manage and Schedule GPU Computing Tasks on Kubernetes</title>
      <link>https://chechia.net/talk/gpu-computing-on-kubernetes/</link>
      <pubDate>Wed, 16 May 2018 11:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/gpu-computing-on-kubernetes/</guid>
      <description>&lt;p&gt;Manage and Schedule GPU Computing Tasks on Kubernetes&lt;/p&gt;
&lt;p&gt;使用Kubernets管理集群GPU機器，靈活的分配調度GPU資源，並自動排程GPU運算工作。
使用者如資料科學家，只需將運算工作實施到Kubernetes上，Kubernetes便會檢視機器上可用的GPU資源，將運算工作分配到合適的機器
上，並監控工作的狀況。如資源不足Kubernetes會自動將工作加入排程，當前面的工作完成，GPU資源釋放後，Kubernetes會自動將運算
工作，配置到合適的機器上。管理者如系統工程師，只需透過Kubernetes，將機器上的GPU資源加入到Kubernetes。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Why we need Kubernetes for GPUs computing? Pros &amp;amp; Cons&lt;/li&gt;
&lt;li&gt;How to deploy a GPU-enabled Kubernetes cluster&lt;/li&gt;
&lt;li&gt;Run GPU computing on Kubernetes cluster&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Azure Taiwan #12: Deploy Kubernetes With Kubespray</title>
      <link>https://chechia.net/talk/deploy-kubernetes-with-kubespray/</link>
      <pubDate>Wed, 28 Mar 2018 19:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/deploy-kubernetes-with-kubespray/</guid>
      <description>&lt;h1 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h1&gt;
&lt;p&gt;If you&amp;rsquo;re interested in building your own Kubernetes. Install the following tools we use.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.virtualbox.org/wiki/Downloads&#34;&gt;virtualbox 5.1+&lt;/a&gt; to create VMs, on which we deploy our Kubernetes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34;&gt;vagrant 2.0.x+&lt;/a&gt; to control virtualbox to build and manage vms.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.ansible.com/ansible/latest/intro_installation.html&#34;&gt;ansible-playbook&lt;/a&gt; to run Kubespray playbook to deploy Kuberentes&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl/&#34;&gt;kubectl&lt;/a&gt; to control Kubernetes cluster&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Ubuntu
apt-add-repository ppa:ansible/ansible \
  &amp;amp;&amp;amp; apt-get update \
  &amp;amp;&amp;amp; apt-get install -y python3 ansible
  &amp;amp;&amp;amp; pip install netaddr

# Mac
pip install ansible

port install py27-netaddr

# netaddr is required by Kubespray
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;lets-get-started&#34;&gt;Let&amp;rsquo;s get started&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;clone https://github.com/kubernetes-incubator/kubespray.git

cd kubespray
vagrant up
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That&amp;rsquo;s it!&lt;/p&gt;
&lt;p&gt;This gonna take a while. Let&amp;rsquo;s get to some details.&lt;/p&gt;
&lt;h3 id=&#34;virtualbox&#34;&gt;Virtualbox&lt;/h3&gt;
&lt;p&gt;Install &lt;a href=&#34;https://www.virtualbox.org/wiki/Downloads&#34;&gt;virtualbox 5.1+&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Disadvantage about vbox GUI:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Clicking is time-consuming and engineers are lazy.&lt;/li&gt;
&lt;li&gt;Bad for automation.&lt;/li&gt;
&lt;li&gt;Lack of Scalibility&lt;/li&gt;
&lt;li&gt;Manual operation could cause mistakes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A good practice is to Write shell script with VBoxManage, the client of virtualbox&lt;/p&gt;
&lt;p&gt;Or even better, use Vagrant&lt;/p&gt;
&lt;h3 id=&#34;vagrant&#34;&gt;Vagrant&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34;&gt;vagrant 2.0.x+&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Create you VMs with (ruby based) script.&lt;/p&gt;
&lt;p&gt;Bring VMs up &amp;amp; down within only one command&lt;/p&gt;
&lt;p&gt;Check the Vagrantfile&lt;/p&gt;
&lt;h3 id=&#34;ansible-playbook&#34;&gt;Ansible playbook&lt;/h3&gt;
&lt;p&gt;Ansible is a IT automation tools&lt;/p&gt;
&lt;p&gt;Basically, ansible playbook ssh and execute bash command on servers.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reduce manual efforts. Deliver and deploy faster&lt;/li&gt;
&lt;li&gt;Install K8s components to each servers and check components status on each step&lt;/li&gt;
&lt;li&gt;Come with lots of handy tools (like native array supports)&lt;/li&gt;
&lt;li&gt;Automation is everything&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;kubespay&#34;&gt;Kubespay&lt;/h3&gt;
&lt;p&gt;Deploy k8s with ansible-playbook&lt;/p&gt;
&lt;p&gt;Available on AWS, GCE, or baremetal&lt;/p&gt;
&lt;p&gt;High Available cluster&lt;/p&gt;
&lt;p&gt;Generate inventory file with inventory.py&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp -rfp inventory/sample inventory/mycluster

declare -a IPS=(10.10.1.3 10.10.1.4 10.10.1.5)
CONFIG_FILE=inventory/mycluster/hosts.ini python3 contrib/inventory_builder/inventory.py ${IPS[@]}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;(Optional) Change parameters&lt;/p&gt;
&lt;p&gt;deploy&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ansible-playbook -i inventory/myCluster/hosts.ini cluster.yml
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;kubectl&#34;&gt;Kubectl&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;kubectl config use-context

kubectl get po
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;destroy&#34;&gt;Destroy&lt;/h3&gt;
&lt;p&gt;Remember to suspend / destroy VMs&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vagrant suspend
vagrant destroy
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;more-about-kubernetes&#34;&gt;More about Kubernetes&lt;/h1&gt;
&lt;p&gt;Why k8s&lt;/p&gt;
&lt;h3 id=&#34;use-case-1-when-data-scientist-wants-gpu&#34;&gt;Use case 1: when data scientist wants GPU&lt;/h3&gt;
&lt;p&gt;Workflow dispatching and resouce management&lt;/p&gt;
&lt;h3 id=&#34;use-case-2-when-your-site-grows-bigger&#34;&gt;Use case 2: when your site grows bigger&lt;/h3&gt;
&lt;p&gt;Scalibility&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/&#34;&gt;FYI&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Native Taiwan #02: Kubernetes Storage and Glusterfs</title>
      <link>https://chechia.net/talk/kubernetes-storage-and-glusterfs/</link>
      <pubDate>Sat, 10 Feb 2018 10:00:00 +0800</pubDate>
      
      <guid>https://chechia.net/talk/kubernetes-storage-and-glusterfs/</guid>
      <description>&lt;h3 id=&#34;outlines&#34;&gt;Outlines&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Docker Storage&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kubernetes Storage&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GlusterFS for K8s&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;docker-storage&#34;&gt;Docker Storage&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/storage/&#34;&gt;Doc&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;within container: inside writable layer of a container&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;deleted with container&lt;/li&gt;
&lt;li&gt;couple with host machine&lt;/li&gt;
&lt;li&gt;require storage driver&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;docker ps -s
docker inspect ubuntu
dd if=/dev/zero of=1Mfile bs=1k count=1000
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Docker volume&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;a directory on host&lt;/li&gt;
&lt;li&gt;prepare: provision on host&lt;/li&gt;
&lt;li&gt;usage: set volume on docker run&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kubernetes-storage&#34;&gt;Kubernetes Storage&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/&#34;&gt;https://kubernetes.io/docs/concepts/storage/volumes/&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;On-disk files:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Deleted on container restart&lt;/li&gt;
&lt;li&gt;File sharing in Pod&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Kubernetes Volume:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;a directory&lt;/li&gt;
&lt;li&gt;Coexist with Pod&lt;/li&gt;
&lt;li&gt;Data preserved across container restarts&lt;/li&gt;
&lt;li&gt;Pod can use many volumes of different types&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Some of) Types of volumes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;emptyDir&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first created volume&lt;/li&gt;
&lt;li&gt;prepare: none&lt;/li&gt;
&lt;li&gt;usage: always&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;gcePersistentDisk&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;independent to pod&lt;/li&gt;
&lt;li&gt;prepare: gcp&lt;/li&gt;
&lt;li&gt;usage: claim by name&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;gcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PersistentVolumeClaim&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;prepare: provision by admin&lt;/li&gt;
&lt;li&gt;usage: add PVC request&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/&#34;&gt;Example&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;persistentvolume&#34;&gt;PersistentVolume&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&#34;&gt;Doc&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Persistent Volume&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a piece of provisioned storage&lt;/li&gt;
&lt;li&gt;Independent lifecycle&lt;/li&gt;
&lt;li&gt;abstract with k8s object API&lt;/li&gt;
&lt;li&gt;many implementations: ex. GCEPersistentDisk, NFS, GlusterFS&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why PersistentVolume&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;one APIs, many PV implementations&lt;/li&gt;
&lt;li&gt;Separates providers (admin) and consumers (users)&lt;/li&gt;
&lt;li&gt;PV subsystem API handles details of implementation&lt;/li&gt;
&lt;li&gt;Handle different need like size, access mode, performance&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PersistentVolumeClaim&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PV: a resource&lt;/li&gt;
&lt;li&gt;PVC: a request for storage&lt;/li&gt;
&lt;li&gt;Pods consume Node resources and PVCs consume PV resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PVC lifecycle&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Povisioning&lt;/li&gt;
&lt;li&gt;Binding&lt;/li&gt;
&lt;li&gt;Using&lt;/li&gt;
&lt;li&gt;Reclaiming&lt;/li&gt;
&lt;li&gt;Deleting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PV Access Modes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReadWriteOnce: 1 node R/W&lt;/li&gt;
&lt;li&gt;ReadOnlyMany: n node R, 1 node W&lt;/li&gt;
&lt;li&gt;ReadWriteMany: n node R/W&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;StorageClass&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;usage:PV.storageClassName&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;Doc&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;glusterfs&#34;&gt;GlusterFS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://docs.gluster.org/en/latest/Administrator%20Guide/GlusterFS%20Introduction/&#34;&gt;Doc&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Why glusterFS&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Network FS&lt;/li&gt;
&lt;li&gt;Distributed FS
&lt;ul&gt;
&lt;li&gt;High Availability&lt;/li&gt;
&lt;li&gt;Scalability&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;High performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Architecture: Types of Volumes&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Distributed&lt;/li&gt;
&lt;li&gt;Replicated&lt;/li&gt;
&lt;li&gt;Distributed Replicated&lt;/li&gt;
&lt;li&gt;Striped: file&lt;/li&gt;
&lt;li&gt;Distributed Striped&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;note: glusterFS Volume vs Kubernetes PV&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;glusterfs-for-k8s&#34;&gt;GlusterFS for k8s&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Heketi&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;REST storage management API&lt;/li&gt;
&lt;li&gt;Receive requests from k8s storage driver&lt;/li&gt;
&lt;li&gt;use secret to control glusterFS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Usage&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;has a glusterFS&lt;/li&gt;
&lt;li&gt;apply storage class and secret to k8s&lt;/li&gt;
&lt;li&gt;Create PV&lt;/li&gt;
&lt;li&gt;Request PVC with Pods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;demo&#34;&gt;Demo&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Env
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.9.2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
