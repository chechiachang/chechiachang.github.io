
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Che-Chia Chang 是一位專攻於DevOps、容器和Kubernetes運維的SRE（Site Reliability Engineer）專家。他是CNTUG（Container Native Taiwan User Group）、DevOps Taipei、GDS Taipei以及Golang Taiwan Meetup的積極成員。\n自2020年起，他獲得了Microsoft最有價值專家（MVP）的稱號。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh-hant","lastmod":1694442834,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Che-Chia Chang 是一位專攻於DevOps、容器和Kubernetes運維的SRE（Site Reliability Engineer）專家。他是CNTUG（Container Native Taiwan User Group）、DevOps Taipei、GDS Taipei以及Golang Taiwan Meetup的積極成員。\n自2020年起，他獲得了Microsoft最有價值專家（MVP）的稱號。","tags":null,"title":"張哲嘉","type":"authors"},{"authors":[],"categories":["kubernetes","workshop"],"content":"工作坊內容 上方投影片連結 https://github.com/chechiachang/etcd-playground Workshop Get started with Etcd \u0026amp; Kubernetes / 手把手搭建 Etcd 與 K8s\nOutline Etcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。\n預計內容：環境設定，Etcd 設定與部署，Etcd 基礎操作，部署 kube-apiserver / kube-controller-manager / kube-scheduler，使用 kubectl 操作 Kubernetes Cluster。讓參與者透過本次工作坊，可以有操作 k8s control plane 的經驗，更了解 Etcd 的基本操作，以及了解 Kubernetes 的基本架構。\n（規劃中）本次工作坊會提供參與者一個簡單的環境，讓參與者可以透過遠端操作來了解 Etcd 的基本操作。參與者必備個人筆電，透過 SSH 操控遠端機器。\n必備知識：Linux 操作基本知識，Docker 操作基本知識，會使用 SSH 連線 / Bash / docker。 工作坊時間不多，現場不會細講概念問題，參與者需要事前預習基本概念 https://etcd.io/ 與 http://play.etcd.io/play\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2024 Cloud Summit 2024 SRE Conference 2023 DevOpsDay Taipei 2023 Kubernetes Summit 2022 COSCUP 2022 Cloud Summit 2021 Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Cloud Summit 2019 Cloud Summit 2018 Cloud Summit 2018 Kubernetes Summit ","date":1729689600,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1722842897,"objectID":"0d720b793d241501cfc4809d7abec1e9","permalink":"https://chechia.net/zh-hant/talk/kubernetes-summit-get-started-with-etcd-kubernetes/","publishdate":"2024-08-05T00:00:00Z","relpermalink":"/zh-hant/talk/kubernetes-summit-get-started-with-etcd-kubernetes/","section":"event","summary":"Etcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。工作坊內容請見投影片","tags":["iac","aws","terraform","kubernetes","etcd"],"title":"Kubernetes Summit: Get started with Etcd \u0026 Kubernetes","type":"event"},{"authors":[],"categories":["kubernetes"],"content":"Presentation Upgrade A VM Based Clustermin\nTarget group 收穫 本次演講會講解升級操作，但不侷限在具體步驟，而是希望能講解更多 k8s 架構與設計，讓觀眾有以下收穫：如何升級 Kubernetes Cluster 的版本，升級時應考量的事項有哪些，有什麼工具可以協助升級流程，透過升級更了解 Kubernetes 的架構\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2024 Cloud Summit 2024 SRE Conference 2023 DevOpsDay Taipei 2023 Kubernetes Summit 2022 COSCUP 2022 Cloud Summit 2021 Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Cloud Summit 2019 Cloud Summit 2018 Cloud Summit 2018 Kubernetes Summit ","date":1729689600,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1722842897,"objectID":"27b54e0c923bff104f9b030c5cb95429","permalink":"https://chechia.net/zh-hant/talk/kubernetes-summit-upgrade-a-vm-based-cluster/","publishdate":"2024-08-04T00:00:00Z","relpermalink":"/zh-hant/talk/kubernetes-summit-upgrade-a-vm-based-cluster/","section":"event","summary":"分享如何升級 VM-based Kubernetes Cluster 的版本，包含 etcd，control plane，與 node。升級前如何規劃，升級步驟該如何操作，升級後應該如何檢查。","tags":["iac","aws","terraform","kubernetes","vault"],"title":"Kubernetes Summit: Upgrade A VM Based Cluster","type":"event"},{"authors":[],"categories":["kubernetes","vault"],"content":"Info 資料庫管理是一個很大的議題：如何管理資料庫的帳號密碼，如何精確的用戶設定權限，傳遞密碼給用戶，並自動化定期更新密碼。本場演講將分享如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫\n涵蓋以下內容：\ndatabase 帳號密碼管理的難題 簡介 Vault 與 database secret engine 在 vault 中設定 database secret engine vault 在需要時自動產生資料庫帳號密碼 vault 透過安全來源認證 app 身份(使用 k8s service account 與 public cloud 認證(aws iam role)) 完成 app 連線至 database 的工作週期 monitoring / audit：vault audit log + prometheus / grafana dashboard / alert manager 範例：如何使用 terraform 設定 vault 與 database secret engine Target group 有資料庫管理需求的工程師，想要學習如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，如何安全的傳遞密碼，並安全地連線到資料庫。\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","date":1724842800,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1719980793,"objectID":"d6e0424b910925b3c85229cbabc5a102","permalink":"https://chechia.net/zh-hant/talk/hashicorp-managed-database-credentials-with-hashicorp-vault/","publishdate":"2024-07-09T00:00:00Z","relpermalink":"/zh-hant/talk/hashicorp-managed-database-credentials-with-hashicorp-vault/","section":"event","summary":"分享如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，以及如何連線到資料庫，監控與審查。","tags":["iac","aws","terraform","kubernetes","vault"],"title":"Hashicorp: managed database credentials with Hashicorp Vault","type":"event"},{"authors":[],"categories":["kubernetes"],"content":" 投影片跟講稿我都放在我的網站上，如果有興趣可以參考 Managed Database Credentials with Hashicorp Vault Che Chia Chang\n關於我 Che Chia Chang SRE @ Maicoin Microsoft MVP 個人部落格chechia.net presentation and speaker notes 從零開始學Vault手把手入門 大綱 database 帳號密碼管理的難題 簡介 Vault 與 database secret engine 在 vault 中設定 database secret engine vault 在需要時自動產生資料庫帳號密碼 vault 透過安全來源認證 app 身份(使用 k8s service account 與 public cloud 認證(aws iam role)) 完成 app 連線至 database 的工作週期 monitoring / audit：vault audit log + prometheus / grafana dashboard / alert manager 範例：如何使用 terraform 設定 vault 與 database secret engine 參考資料 https://developer.hashicorp.com/vault/docs/secrets/databases ","date":1724151600,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1724151600,"objectID":"87d78709df150c864240220112223357","permalink":"https://chechia.net/zh-hant/slides/2024-08-28-hashicorp-vault-database/","publishdate":"2024-08-20T11:00:00Z","relpermalink":"/zh-hant/slides/2024-08-28-hashicorp-vault-database/","section":"slides","summary":"分享如何使用 Hashicorp Vault 管理資料庫的帳號密碼，並透過 AWS IAM Role 與 Kubernetes Service Account 進行驗證，以及如何連線到資料庫，監控與審查。","tags":["kubernetes"],"title":"Hashicorp: managed database credentials with Hashicorp Vault","type":"slides"},{"authors":[],"categories":["kubernetes"],"content":" Workshop: Get started with Etcd \u0026amp; Kubernetes 手把手搭建 Etcd 與 K8s 工作坊 Che Chia Chang https://chechia.net/\n關於我 Che Chia Chang SRE @ Maicoin Microsoft MVP 個人部落格chechia.net presentation and speaker notes 鐵人賽 (Terraform / Vault 手把手入門) 大綱 公有雲的費用 Q\u0026amp;A 參考資料 https://etcd.io/\nhttp://play.etcd.io/play\nhttps://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/\nhttps://github.com/guessi/docker-compose-etcd/blob/master/docker-compose.yml\nhttps://github.com/jpetazzo/container.training/blob/main/compose/simple-k8s-control-plane/docker-compose.yaml\nhttps://github.com/kelseyhightower/kubernetes-the-hard-way/blob/master/docs/07-bootstrapping-etcd.md\n","date":1722818700,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1722842897,"objectID":"0a3ef2843b40bfb0a308a7f2c536e6dc","permalink":"https://chechia.net/zh-hant/slides/2024-10-24-etcd-workshop/","publishdate":"2024-08-05T00:45:00Z","relpermalink":"/zh-hant/slides/2024-10-24-etcd-workshop/","section":"slides","summary":"Etcd 是 Kubernetes 的重要元件之一，本次工作坊將帶領觀眾初探 Etcd，包含安裝，設定，以及操作。並藉由本地的 Etcd 來架設一個最簡單的 Kubernetes Cluster。","tags":["etcd","kubernetes"],"title":"Kubernetes Summit: Get started with Etcd \u0026 Kubernetes","type":"slides"},{"authors":[],"categories":["kubernetes"],"content":" 投影片跟講稿我都放在我的網站上，如果有興趣可以參考 Upgrade A VM Based Cluster Che Chia Chang\n關於我 Che Chia Chang SRE @ Maicoin Microsoft MVP 個人部落格chechia.net presentation and speaker notes 鐵人賽 (Terraform / Vault 手把手入門) 大綱 Overview: Official doc Preflight Check: API Version etcd kube-apiserver kube-controller-manager kube-scheduler Q\u0026amp;A 參考資料 https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/ ","date":1722775500,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1722842897,"objectID":"57cae46599aaf4d3a9bc08863788afc8","permalink":"https://chechia.net/zh-hant/slides/2024-10-23-upgrade-k8s-cluster/","publishdate":"2024-08-04T12:45:00Z","relpermalink":"/zh-hant/slides/2024-10-23-upgrade-k8s-cluster/","section":"slides","summary":"分享如何升級 VM-based Kubernetes Cluster 的版本，包含 etcd，control plane，與 node。升級前如何規劃，升級步驟該如何操作，升級後應該如何檢查。","tags":["kubernetes"],"title":"Kubernetes Summit: Upgrade A VM Based Cluster","type":"slides"},{"authors":[],"categories":["kubernetes","aws"],"content":"Info Title: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\n使用 AWS 與 Kubernetes 就是要花錢，如何省錢？ 合理的設定資源，不僅省錢，還提升整體服務穩定度？ 既有服務已經存在，如何逐步改變，降低成本？\n本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan…，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\nTarget group AWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\n導入工具：有哪些工具可以使用 分析現況：精算各團隊與各專案成本 改變現況：從既有的架構中，找尋可以節省成本的地方 改善流程：有效率的維持省錢 workflow 改變文化：持續性低維持新服務與舊服務的合理成本 Author Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","date":1720010700,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1719980793,"objectID":"b8cb4492897366f8db7e55ad0be67693","permalink":"https://chechia.net/zh-hant/talk/cloud-summit-cloud-infrastructure-saving-engineering-%E9%9B%B2%E7%AB%AF%E7%9C%81%E9%8C%A2%E5%B7%A5%E7%A8%8B/","publishdate":"2024-07-01T00:00:00Z","relpermalink":"/zh-hant/talk/cloud-summit-cloud-infrastructure-saving-engineering-%E9%9B%B2%E7%AB%AF%E7%9C%81%E9%8C%A2%E5%B7%A5%E7%A8%8B/","section":"event","summary":"分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan","tags":["vault","iac","aws","terraform","kubernetes","cost"],"title":"Cloud Summit: Cloud Infrastructure Saving Engineering  雲端省錢工程","type":"event"},{"authors":[],"categories":["kubernetes"],"content":" 投影片跟講稿我都放在我的網站上，如果有興趣可以參考 雲端K8s省錢工程 Che Chia Chang\n兩個關鍵字：雲端 / k8s 如果你有在使用公有雲，而且有在跑 k8s，你是這篇演講的最主要對象 如果你是私有雲 k8s，或是其他的雲端服務，有一些相通的概念，會有一些參考價值 如果你是地端的機器，雖然概念相同，但成本控制的做法完全是另外一個故事，你可以當午休時間的故事聽聽看 關於我 Che Chia Chang SRE @ Maicoin Microsoft MVP 個人部落格chechia.net presentation and speaker notes 鐵人賽 (Terraform / Vault 手把手入門) 大綱 公有雲的費用 節省算力成本 監控是成本控管的基礎 導入成本分析與預測工具 自動化資源使用建議 Saving Plan/Committed Usage Spot Instance 與微服務 Autoscaling: HPA / VPA Q\u0026amp;A 這是我們今天的大綱\n我們會先講一下在公有雲上的成本 然後講一下 k8s 的運算資源管理 接著講一下監控，為何成本控管的基礎是監控 如何使用工具成本分析，未來成本預測 然後使用工具建議合適的資源 最後講實務上要如何降低成本，例如 saving plan / spot instance / HPA / VPA，依照執行的難度排序，有時間的話也會講如何從無到有開始進行\n動機 公司希望賺錢，節省成本 團隊希望控制成本 個人產生 Credit 與團隊 Impact 團隊而言，成本控制是一個很重要的議題，因為成本控制是一個很直接的影響公司獲利的方式 個人，如果為公司節省超過個人的薪水，那公司聘用你就是淨賺 節省公司雲端成本 50% 職涯上履歷很好看 公有雲的費用 打開公有雲的 billing\n算力成本 Compute Resource (cpu, memory) 儲存成本 Storage (Disk, EBS, S3…) 網路 Networking 有在使用公有雲的人，把公有雲的 billing 帳單打開來看，可能看到的大概是這幾個項目 當然因為不同團隊的服務不同，可能會有一些出入，但如果你的公司的產品是 web service，應該會有這幾個項目 今天只會講這個算力成本 算力成本 Compute Resource (cpu, memory) 今天只會講這個 Storage 不太好說省就省，因為你的資料量就在那邊，放不下就是要再買，所以 storage 的成本控制，可能是在資料的使用上，例如資料的壓縮，或是資料的備份，或是資料的存取方式，這些是另外一個故事 使用公有雲的服務，基本上 storage 都是依據使用的大小計價，而且需多服務都可以動態增長，例如動態增加 disk\nStorage / Database / Networking 這幾個項目，我們下集再來談\n算力成本 網頁服務的微服務系統，都需要算力\napi server cronjob business logic job database 如需節省，也可以從這裡下手 如果你跟敝社一樣是 web service，那麼 cpu / memory 可能是你最大的一筆成本\n你需要足夠運算能力 serve 客戶，不管是支持用戶的request，進行商業邏輯的運算，然後把運算完成的狀態回傳給客戶，或是存在資料庫中，每一個動作都需要算力\n足夠的 cpu 跟 memory，不是越多越好，而是足夠，不會浪費，也不會因為不足而影響服務品質 我們要省錢，就要從這裡下手\n尋找多餘的算力 已有足夠的 cpu / memory，多給也不會增加服務品質的多餘算力 多少才是夠？ 減去多少 cpu / memory，依然不改變服務品質 多少才是夠？這是一個複雜的問題，實務上通常使用 SLA / SLO 來衡量服務品質，然後根據服務品質的要求，來設定 cpu / memory 的配額\n我們今天要做的是成本優化，白話的說是降低 cpu / memory 的設定配額，但是我們不能降低到影響服務品質 甚至退一百步，防守性的來說，我們不希望因為降低 cpu / memory 的設定配額，為了省錢而導致負面的結果，甚至為服務的穩定度背鍋。\n維持SLO 以維持各個服務元件的 SLO為前提，降低 cpu / memory 使用量\n維持個個服務元件的 SLO 是一個很好的指標，如果你的服務是 99.9% 的 SLA，那麼你的服務就要保證 99.9% 的時間都是正常運作的，那麼你的 cpu / memory 就要足夠支持這個 SLA\n談 SLO 之前，你要先知道你的服務的 SLA 是多少，你要先知道當前的狀態是多少 這個在稍後的 monitoring 會提到，為什麼監控是成本管理的基礎\n基於 SLO 的成本調降 負載穩定的元件，可以抓過去 30d 的 p99 cpu time 或 p99 memory usage + buffer 負載不穩定的元件，例如 cpu usage 與受活躍用戶數量正比，需要搭配 HPA 水平拓展 負載不穩定的元件，但又不能水平拓展，例如 stateful service，可以考慮 VPA 負載穩定的元件，固定吃多少 cpu / memory，他的附載不太容易隨外部因素波動的服務，可以抓過去 30d 的 p99 cpu time 或 p99 memory usage + buffer，然後降低 cpu / memory 的設定配額 小結：成本控管的基本概念 尋找服務中多餘算力 以維持 SLO 為前提 降低 cpu / memory 使用量 依據元件負載特性，選擇適當的調降方式 k8s cpu / memory management https://kubernetes.io/docs/concepts/configuration/manage-resources-containers\nk8s 如何管理 workload 的 cpu 與 memory 調降多少會影響服務品質 這裡面再講一下 k8s 如何管理 cpu / memory\n這邊的重點是，怎麼樣的調整會影響到服務品質，那我們做成本控制的時候，就不要去踩到這個底線\nHow k8s manage cpu / memory scheduler 依據 cpu / memory request 調度 pod container runtime 設定 cgroup cpu 使用量依據 request 佔 node 比例分配 控制 cpu 用量不超過 limit memory 使用 limit，超過 limit 會被 oomkill，並依據設定重啟 當前 node memory 不足時，會依據 pod request Evict pod 文件講得很清楚 上面都是概念，底下進實作 Monitoring 監控是成本控管的基礎 監測是成本控管的基礎 沒有監測就沒有 p99 cpu time / memory usage，也沒有 SLI/SLO 沒有檢測下做成本精簡，會碰壞服務 如果沒有監測先補檢測 如果沒有 monitoring，上面的兩大前提都不存在 目前的 cpu / memory usage，runtime utilization 的資料 目前SLO，調降之後新的SLO\n沒有檢測下，還是可以做成本精簡 然而要馬兒好又要而不吃草，今天如果把 cpu / memory 降下來，有可能會餓到服務 cpu throttling / memory oomkill，這些都是可能發生的事情 會碰壞服務\n監測：調整前設定目標基準線 設定節省目標：ex. 過去 3 個月的 p99 資源用量 99% 的時間，memory 使用量都在這條線以下 99% 的時間，cpu throttling 在這條線以下 多餘的算力 = (分配的 resource - p99) 那如果我們把 cpu / memory 降低到 p99 / p99.9 服務品質會受到影響嗎？ 做之前要能評估做完大概能省多少 例如評估完後\n如果評估完發現省不了什麼錢，那當然團隊就不一定要做這件事\n監測: 調整後 資源用量是否有變化 確定沒有改壞東西，有壞要有及時的 alert 效能表現：有可能沒壞，但是變很慢 如果你改壞了，及時的 alert 會救你一命\nperformance 監測很重要\n推薦監測工具 prometheus.io Kubecost / Opencost 大家都知道 prometheus 是什麼嗎？\n知道的人請舉個手 不知道的我很簡單說一下\n我們想要知道一個 pod 會需要花多少 cpu / memory，你就把它跑起來，然後去紀錄跑起來的 pod 用了多少 cpu / memory，然後根據時間統計，你就可以拉出一張圖，看到這個 pod 用了多少 cpu / memory 你的 kubelet / cadvisor / container runtime 會知道你的 container 的 cpu / memory 使用量，包含 request \u0026amp; limit 然後是否有 throttling 或是 oomkill，這些資訊都會被 prometheus 收集起來，然後你可以透過其他工具，例如 kubecost 看到這些資訊\nhttps://grafana.com/grafana/dashboards/17375-k8s-resource-monitoring/\n這是 prometheus 收集的資料，vm 的 resource 使用量的 grafana dashboard 導入成本分析與預測工具 有了 prometheus 後，我們知道短期/長期的資源使用狀況 要把資源使用轉成成本，需要一個成本計算工具 評估是否要做成本精簡，能夠減少多少錢 管理上的考量：投資人力成本，與預期回報 政治上的考量，做成本節省需要時間跟人力資源 有一個精準的成本分析，節省空間估算的工具十分有說服力\n有這些資料，才可以科學化決策要不要做，該怎麼做 做之前就能評估做完大概能省多少\n如果評估完發現省不了什麼錢，那當然團隊就不一定要做這件事\n成本分析 VM 可以用各家公有雲的費用計算工具 Azure Cost Management AWS Cost Explorer GCP Cost Breakdown 成本分析：公有雲 billing 計算長時間的費用趨勢 適合當作成本精簡後的成果回報 不適合當做調整的依據 時間計算較長，反饋時間長，不及時，項目不夠精細 有無更即時的成本分析工具？ 成本分析: Kubecost / Opencost 有提供 UI 基於 prometheus 可以針對 allocation 做成本分析 Cost Allocation 可以透過 cloud provider 去撈雲端的使用資料 https://docs.kubecost.com/using-kubecost/navigating-the-kubecost-ui/cost-allocation\nhttps://docs.kubecost.com/using-kubecost/navigating-the-kubecost-ui/cost-allocation/efficiency-idle\n推薦工具 https://github.com/robusta-dev/krr Kubecost / Opencost Savings VPA recommendator 推薦工具: Kubecost / Opencost Savings 設定 target utilization request / allocatable dev 80%+ prod 60% 根據服務品質的要求，以及公司政策去做設定 SLA/SLO 開發與測試環境，在不影響工作的前提，都可以拉到 overcommit 推薦工具: KRR https://github.com/robusta-dev/krr 免安裝， …","date":1719837900,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1719980793,"objectID":"b0f9e57fc1ebd344be514e5f25fc24b9","permalink":"https://chechia.net/zh-hant/slides/2024-07-03-saving-money-on-cloud-k8s/","publishdate":"2024-07-01T12:45:00Z","relpermalink":"/zh-hant/slides/2024-07-03-saving-money-on-cloud-k8s/","section":"slides","summary":"分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan","tags":["kafka","kubernetes"],"title":"Cloud Summit: Cloud Infrastructure Saving Engineering  雲端省錢工程","type":"slides"},{"authors":[],"categories":[],"content":"Get-Started Google “how to contribute to kubernetes” click the first link you like in every page do what the page says Get-Started https://www.kubernetes.dev https://www.kubernetes.dev/docs/guide/ Contributor Playground Youtube: New Contributor Series 2018-2019 Join the Kubernetes Slack Contribute to kubernetes Documentation Join Group https://www.kubernetes.dev/community/community-groups/ SIG-DOCS Find Issue on Github https://github.com/kubernetes/website/issues/38681 Contributing to Kubernetes Documentation Join sig-docs google groupd Prerequisites In my opinion, the following are the prerequisites to contribute to Kubernetes:\nEnglish proficiency Unorganized Links Community-membership actively contributer Community Forums ","date":1714213123,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1719800115,"objectID":"f345716cecd78bb059146b4b34f1db6f","permalink":"https://chechia.net/zh-hant/post/2024-04-27-contribute-to-k8s/","publishdate":"2024-04-27T18:18:43+08:00","relpermalink":"/zh-hant/post/2024-04-27-contribute-to-k8s/","section":"post","summary":"Get-Started Google “how to contribute to kubernetes” click the first link you like in every page do what the page says Get-Started https://www.kubernetes.dev https://www.kubernetes.dev/docs/guide/ Contributor Playground Youtube: New Contributor Series 2018-2019 Join the Kubernetes Slack Contribute to kubernetes Documentation Join Group https://www.","tags":[],"title":"2024 04 27 Contribute to K8s","type":"post"},{"authors":[],"categories":["kubernetes","aws"],"content":"Info Title: SRE Conference: Cloud Infrastructure Saving Engineering 雲端省錢工程\n使用 AWS 與 Kubernetes 就是要花錢，如何省錢？ 合理的設定資源，不僅省錢，還提升整體服務穩定度？ 既有服務已經存在，如何逐步改變，降低成本？\n本次演講以實務的範例，分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan…，並分享如何改變文化，提高團隊的成本意識，實際的降低開銷\nTarget group AWS \u0026amp; Kubernetes User who want to decrease overall cost of infrastructure\n導入工具：有哪些工具可以使用 分析現況：精算各團隊與各專案成本 改變現況：從既有的架構中，找尋可以節省成本的地方 改善流程：有效率的維持省錢 workflow 改變文化：持續性低維持新服務與舊服務的合理成本 Author Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","date":1714137600,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1713932735,"objectID":"da8760839238a071aee0bb02b11bdee2","permalink":"https://chechia.net/zh-hant/talk/sre-conference-cloud-infrastructure-saving-engineering-%E9%9B%B2%E7%AB%AF%E7%9C%81%E9%8C%A2%E5%B7%A5%E7%A8%8B/","publishdate":"2024-04-01T00:00:00Z","relpermalink":"/zh-hant/talk/sre-conference-cloud-infrastructure-saving-engineering-%E9%9B%B2%E7%AB%AF%E7%9C%81%E9%8C%A2%E5%B7%A5%E7%A8%8B/","section":"event","summary":"分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan","tags":["vault","iac","aws","terraform","kubernetes","cost"],"title":"SRE Conference: Cloud Infrastructure Saving Engineering  雲端省錢工程","type":"event"},{"authors":[],"categories":["kubernetes"],"content":" 投影片跟講稿我都放在我的網站上，如果有興趣可以參考 Cost Management of Cloud Kubernetes 雲端K8s省錢工程 Che Chia Chang\n兩個關鍵字：雲端 / k8s 如果你有在使用公有雲，而且有在跑 k8s，你是這篇演講的最主要對象 如果你是私有雲 k8s，或是其他的雲端服務，有一些相通的概念，會有一些參考價值 如果你是地端的機器，雖然概念相同，但成本控制的做法完全是另外一個故事，你可以當午休時間的故事聽聽看 About Me Che Chia Chang SRE @ Maicoin Microsoft MVP 個人部落格chechia.net presentation and speaker notes 鐵人賽 (Terraform / Vault 手把手入門) Outline Cost on Public Cloud K8s Compute Resource Management Monitoring Cost Analysis and Prediction Resource Recommendation Saving Plan Spot Instance HPA / VPA Q\u0026amp;A 這是我們今天的大綱\n我們會先講一下在公有雲上的成本 然後講一下 k8s 的運算資源管理 接著講一下監控，為何成本控管的基礎是監控 如何使用工具成本分析，未來成本預測 然後使用工具建議合適的資源 最後講實務上要如何降低成本，例如 saving plan / spot instance / HPA / VPA，依照執行的難度排序，有時間的話也會講如何從無到有開始進行\nCost on Public Cloud open your cloud billing\nCompute Resource (cpu, memory) Storage (Disk, EBS, S3…) Database (Compute Resource + Storage) Networking 有在使用公有雲的人，把公有雲的 billing 帳單打開來看，可能看到的大概是這幾個項目 當然因為不同團隊的服務不同，可能會有一些出入，但如果你的公司的產品是 web service，應該會有這幾個項目 Today’s topic Today’s topic\nCompute Resource cpu \u0026amp; memory Maybe next time\nStorage (Disk, EBS, S3…) Database (Compute Resource + Storage) Networking Storage 不太好說省就省，因為你的資料量就在那邊，放不下就是要再買，所以 storage 的成本控制，可能是在資料的使用上，例如資料的壓縮，或是資料的備份，或是資料的存取方式，這些是另外一個故事 使用公有雲的服務，基本上 storage 都是依據使用的大小計價，而且需多服務都可以動態增長，例如動態增加 disk\nStorage / Database / Networking 這幾個項目，我們下集再來談\nCost on Public Cloud CPU / memory intense web service\nRESTful api long-connetion service business logic save state to disk / database 如果你跟敝社一樣是 web service，那麼 cpu / memory 可能是你最大的一筆成本\n你需要足夠運算能力 serve 客戶，不管是支持用戶的request，進行商業邏輯的運算，然後把運算完成的狀態回傳給客戶，或是存在資料庫中，每一個動作都需要算力\n足夠的 cpu 跟 memory，不是越多越好，而是足夠，不會浪費，也不會因為不足而影響服務品質 我們要省錢，就要從這裡下手\n尋找多餘的算力 已有足夠的 cpu / memory，多給也不會增加服務品質的多餘算力 多少才是夠？ 減去多少 cpu / memory，依然不改變服務品質 多少才是夠？這是一個複雜的問題，實務上通常使用 SLA / SLO 來衡量服務品質，然後根據服務品質的要求，來設定 cpu / memory 的配額\n我們今天要做的是成本優化，白話的說是降低 cpu / memory 的設定配額，但是我們不能降低到影響服務品質 甚至退一百步，防守性的來說，我們不希望因為降低 cpu / memory 的設定配額，為了省錢而導致負面的結果，甚至為服務的穩定度背鍋。\n維持SLO 以維持各個服務元件的 SLO為前提，降低 cpu / memory 使用量\n維持個個服務元件的 SLO 是一個很好的指標，如果你的服務是 99.9% 的 SLA，那麼你的服務就要保證 99.9% 的時間都是正常運作的，那麼你的 cpu / memory 就要足夠支持這個 SLA\n談 SLO 之前，你要先知道你的服務的 SLA 是多少，你要先知道當前的狀態是多少 這個在稍後的 monitoring 會提到，為什麼監控是成本管理的基礎\n基於 SLO 的成本調降 負載穩定的元件，可以抓過去 30d 的 p99 cpu time 或 p99 memory usage + buffer 負載不穩定的元件，例如 cpu usage 與受活躍用戶數量正比，需要搭配 HPA 水平拓展 負載不穩定的元件，但又不能水平拓展，例如 stateful service，可以考慮 VPA 負載穩定的元件，固定吃多少 cpu / memory，他的附載不太容易隨外部因素波動的服務，可以抓過去 30d 的 p99 cpu time 或 p99 memory usage + buffer，然後降低 cpu / memory 的設定配額 小結：成本控管的基本概念 尋找服務中多餘算力 以維持 SLO 為前提 降低 cpu / memory 使用量 依據元件負載特性，選擇適當的調降方式 Background Knowledge k8s cpu / memory management https://kubernetes.io/docs/concepts/configuration/manage-resources-containers\nk8s 如何管理 workload 的 cpu 與 memory 調降多少會影響服務品質 這裡面再講一下 k8s 如何管理 cpu / memory\n這邊的重點是，怎麼樣的調整會影響到服務品質，那我們做成本控制的時候，就不要去踩到這個底線\nHow k8s manage cpu / memory scheduler 依據 cpu / memory request 調度 pod container runtime 設定 cgroup cpu 使用量依據 request 佔 node 比例分配 控制 cpu 用量不超過 limit memory 使用 limit，超過 limit 會被 oomkill，並依據設定重啟 當前 node memory 不足時，會依據 pod request Evict pod 文件講得很清楚 上面都是概念，底下進實作 Monitoring Monitoring 是 cost management 的基礎 沒有 monitoring 就沒有 p99 cpu time / p99 memory usage，也沒有 SLI/SLO 如果沒有 monitoring 先補 monitoring 如果沒有 monitoring，上面的兩大前提都不存在 目前的 cpu / memory usage，runtime utilization 的資料 目前SLO，調降之後新的SLO Monitoring: 調整前 過去的 p99 資源用量 目前的效能表現 多餘的算力 = (分配的 resource - p99) 做之前要能評估做完大概能省多少 例如評估完後\n如果評估完發現省不了什麼錢，那當然團隊就不一定要做這件事\nMonitoring: 調整後 調整前後比較\n資源用量 目前的效能表現 確定沒有改壞東西 有改壞，要有及時的 alert 如果你改壞了，及時的 alert 會救你一命 Monitoring Tools prometheus.io Kubecost / Opencost 大家都知道 prometheus 是什麼嗎？\n知道的人請舉個手 不知道的我很簡單說一下\n我們想要知道一個 pod 會需要花多少 cpu / memory，你就把它跑起來，然後去紀錄跑起來的 pod 用了多少 cpu / memory，然後根據時間統計，你就可以拉出一張圖，看到這個 pod 用了多少 cpu / memory 你的 kubelet / cadvisor / container runtime 會知道你的 container 的 cpu / memory 使用量，包含 request \u0026amp; limit 然後是否有 throttling 或是 oomkill，這些資訊都會被 prometheus 收集起來，然後你可以透過其他工具，例如 kubecost 看到這些資訊\nhttps://grafana.com/grafana/dashboards/17375-k8s-resource-monitoring/\n這是 prometheus 收集的資料，vm 的 resource 使用量的 grafana dashboard Cost Analysis and Prediction 有了 prometheus 後，我們知道短期/長期的資源使用狀況 要把資源使用轉成成本，需要一個成本計算工具 評估是否要做成本精簡，能夠減少多少錢 管理上的考量：投資人力成本，與預期回報 政治上的考量，做成本節省需要時間跟人力資源 有一個精準的成本分析，節省空間估算的工具十分有說服力\n有這些資料，才可以科學化決策要不要做，該怎麼做 做之前就能評估做完大概能省多少\n如果評估完發現省不了什麼錢，那當然團隊就不一定要做這件事\nCost Analysis: billing 各家公有元都有自己的費用計算工具 Azure Cost Management AWS Cost Explorer GCP Cost Breakdown Cost Analysis: Cloud billing 計算長時間的費用趨勢 適合當作成本精簡後的成果回報 不適合當做調整的依據 時間計算較長，反饋時間長，不及時，項目不夠精細 有無更即時的成本分析工具？ Cost Analysis: Kubecost / Opencost 有提供 UI 基於 prometheus 可以針對 allocation 做成本分析 Cost Allocation 可以透過 cloud provider 去撈雲端的使用資料 https://docs.kubecost.com/using-kubecost/navigating-the-kubecost-ui/cost-allocation\nhttps://docs.kubecost.com/using-kubecost/navigating-the-kubecost-ui/cost-allocation/efficiency-idle\nRecommedation Tools https://github.com/robusta-dev/krr Kubecost / Opencost Savings VPA recommendator Recommedation Tools: Kubecost / Opencost Savings 設定 target utilization dev 80%+ prod 60% 根據服務品質的要求，以及公司政策去做設定 SLA/SLO 開發與測試環境，在不影響工作的前提，都可以拉到 overcommit Recommedation Tools: KRR …","date":1711411200,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1714110971,"objectID":"f6762a4ddacc51e570d92a180c8651af","permalink":"https://chechia.net/zh-hant/slides/2024-04-26-saving-money-on-cloud-k8s/","publishdate":"2024-03-26T00:00:00Z","relpermalink":"/zh-hant/slides/2024-04-26-saving-money-on-cloud-k8s/","section":"slides","summary":"分享幾個節省雲端開銷的方法，包含：導入 spot instance，成本計算與預測工具，動態資源調整HPA與VPA，saving plan","tags":["kafka","kubernetes"],"title":"SRE Conference: Cloud Infrastructure Saving Engineering  雲端省錢工程","type":"slides"},{"authors":[],"categories":["kubernetes","vault"],"content":"Info Title: Resource as Code for Kubernetes: stop kubectl apply\nhttps://k8s.ithome.com.tw/CFP\nInfrastrure as Code (IaC) 與 PaC，在萬物都該 as Code 得時代，你還在不斷的 kubectl apply 嗎？\n手動 apply 的痛點：\n人就是會忘：是誰 apply 這個在 k8s 上的？是誰上次漏 apply 所以壞了？ 人就是會寫錯：能否 apply 管理大量的 label, taint, annotation 安全：apply 變更內容是否有經過資訊安全的 review 當服務的 app code base 都已經用 chart 打包，使用 vcs 管理後，為何依賴的 k8s resource (namespace, secret, label, crd, …) 不需要推上 vcs 管理的？\n本次演講集合幾個管理 k8s 的範例，將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator, … 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率\ntarget group Kubernetes User who want to increase performance in k8s management\n","date":1698240000,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1713932423,"objectID":"52c6a9bbde2bd97e9bd818ac94ff1916","permalink":"https://chechia.net/zh-hant/talk/kubernetes-summit-resource-as-code-for-kubernetes-stop-kubectl-apply/","publishdate":"2023-09-10T00:00:00Z","relpermalink":"/zh-hant/talk/kubernetes-summit-resource-as-code-for-kubernetes-stop-kubectl-apply/","section":"event","summary":"將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率","tags":["vault","iac","aws","terraform","kubernetes"],"title":"Kubernetes Summit: Resource as Code for Kubernetes: Stop kubectl apply","type":"event"},{"authors":[],"categories":["kubernetes"],"content":" Q1: 有過使用 helm 跟 argocd 的人請舉手 Q2: k8s object 走 gitflow 管理的比例有超過 9 成的 Resource as Code for K8s Object 如何管理 k8s object Che Chia Chang\nOutline Manage Kubernetes Objects in Gitflow\nkubectl helm chart argocd (gitflow) applicationset test 今天的內容前半部像是講古\n首先會講 kubectl create / apply kubectl 使用，然後官方有提醒我們使用 kubectl 管理 k8s object 時的 trade-off，這些 trade-off 我們可以使用其他的工具來彌補\n導入 helm chart，來打包 k8s objects 變成一個完整地發布單位\nargocd 走 gitflow 來發布\n然後在 workflow 裡加入測試，確保 k8s objects 的交付品質\nKubectl # Imperative commands kubectl create deployment nginx --image nginx # Imperative object configuration kubectl create -f nginx.yaml # Declarative object configuration kubectl apply -R -f configs/ https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/#management-techniques\nkubectl 應該是所有人學 k8s 的第一個工具 作為官方的 cli 工具，kunectl 非常強大，可以控制幾乎大部分 k8s 的 api，也能對幾乎所有 k8s object 進行操作 首先，如同官方文件所描述，kubectl 的使用上，也有各種不同方法。ie. kubectl 交在不同人手上，使用方式是不同的 官方文件描述\nImperative commands 指令式 Imperative object configuration 指令物件 Declarative object configuration 宣告物件 kubectl create deployment nginx，一行命令告訴 k8s 你要 create deployment\nkubectl create -f nginx.yaml，使用者選擇要 create / apply / delete 而 nginx.yaml 裡面描述一個 nginx deployment 物件\nkubectl apply -f -R nginx/，使用者描述一個或多個物件，描述物件的狀態。apply 時，由 kubectl 決定要對 object 執行，create / update / delete\nIssue # Imperative commands kubectl create deployment nginx --image nginx # Imperative object configuration kubectl create -f nginx.yaml # Declarative object configuration kubectl apply -R -f configs/ kubectl 這麼好用，那為何不繼續用下去？\n的問題很明顯，不能 diff 2.3. 會有一個基礎的 spec，關於 deployment/nginx，因此可以在 重點\nchange review / diff source record other than live template，一個可重複使用的樣版 2.3. 的問題，你必須對 object 夠了解，才寫得出完整沒 bug 的 spec yaml\n聽起來是最完整的，他的問題就是要如何維持 local file 與 live 連結，或是說 sync 這個有使用 argocd 的人可能就會比較有感覺 大家有興趣可以去看官方描述的內容，這裡不贅述\nIssue change review / diff before apply source of live record template / repetitive apply sync local to live change review / diff before apply source of live record template / repetitive apply sync local to live Declarative object configuration nginx ├── deployment.yaml ├── ingress.yaml └── service.yaml redis ├── deployment.yaml ├── ingress.yaml └── service.yaml microservice-a b c ... 為了 change review，通常會走向 3. Declarative object configuration，可能會長這樣 一個 git repository 裡面有多個 directory，描述每一組服務所需要的資源 使用 kubectl 一次 apply 整個 directory，所以 local file 基本上也反應 live object 有 local file，很自然而然就會想要放到版本控制，例如 git，這樣又可以走 gitflow PR -\u0026gt; review -\u0026gt; merged -\u0026gt; apply master / release tag 有人在 2014 年前用過 k8s 嗎？\n古早時期，要用個 redis 還要自己包 service / ingress / deployment，先去 dockerhub 找 redis，然後依據 readme 自己包 deployment，自己測試看 redis 會不會動\n現在應該沒有人會因為要去使用 redis 或是 mysql，自己跑去寫 k8s object 了吧\nhelm v2.0-alpha 2016\n現在有 helm + chart\n如果只是使用低三方開源的 helm chart，社群幫你維護 service / ingress / deployment\n提供基本的 default value，預設就跑得起來 跑得起來後，跑得好，能使用 k8s orchestration 提供完整的功能，透過 value.yaml 控制 經過測試 issue tracking Helm chart k8s object 的開發，打包，測試，release\nk8s 十分強大，享受 orchestration k8s object 變得太複雜 標準化 template，release + upgrade chart 作為一個 k8s object 的 release / artifact，有開發流程，版本控管，測試，完整的發佈\napp 本身，例如 redis，當然是整個應用的核心。但要能夠在 k8s 執行，並正確地享受 k8s orchestration 的好處，k8s object 非常的重要\n甚至，k8s object 複雜度已經遠遠超過過去在 vm 上跑一個 redis，兜一個 systemd unit 就可以跑起來\nk8s object 需要 release / version，才能做 object 的固定版本 apply，upgrade 生版，有問題 rollback\n在 k8s 上要跑得穩邊的十分閫難，透過社群來維護大部分通運的第三方服務\nHelm Chart Library helm repo list NAME URL bitnami https://charts.bitnami.com/bitnami argocd https://argoproj.github.io/argo-helm chaos-mesh https://charts.chaos-mesh.org k8s objects 能動很簡單，但要跑得穩又能享受 orchestration 的便利，十分困難 透過社群來維護大部分通運的第三方服務，透過單一 value.yaml\n社群維護的 chart 不是就一勞永逸\n不熱門的 chart 特殊需求時，還是需要 fork chart 回來自己維護 但無論如何，都是大幅降低維運的複雜度\nhelm 生態系 ex. helmfile\nrepositories: - name: argocd url: https://argoproj.github.io/argo-helm helmDefaults: kubeContext: general #verify: true wait: true timeout: 300 context: general releases: - name: argocd namespace: argocd chart: argocd/argo-cd version: 5.31.0 values: - values/argocd.yaml - name: redis - name: mysql https://github.com/cdwv/awesome-helm\n更高層級的封裝 api-services ├── nginx ├── mysql └── ingress -\u0026gt; nlb daemon-services ├── redis ├── mysql └── kafka 底下的依賴標準化，依據穩定的 release 發布之後 可以再進行更高層級的封裝\n重複性的服務，例如每個 microservice 都需要 例如我有一百個 api service group，都是 restful api，都需要 ingress / service / nginx 等等\n底下的元件標準化，降低維護成本\n統一版本，醫病維護，升級，退版 例如 daemon service，底下依賴 queue / redis / db\n微服務 微服務不是問題，微服務底下的 k8s object 才是問題\n可以快速，標準化的產生經過測試，微服務單元 Issues V change review / diff before apply source of live record V template / repetitive apply sync local to live version control / gitflow\nhelm template 有提供許多語法，可以有系統化的產生重複的 k8s object ex. 可以跑 for loop / for each 這個在 IaC 或是 resource as code 的 xxx as code 都十分有利\nArgo CD Declarative GitOps CD for Kubernetes\nWhy Argo CD? Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.\napplication 應該是明確宣告的，清楚描述，並且有版本控管 描述 application 本身，附帶的設定 secret / …","date":1698019200,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1698209393,"objectID":"d6e2933d05346cab57f8fe3579d9e7b6","permalink":"https://chechia.net/zh-hant/slides/2023-10-25-k8s-resource-as-code/","publishdate":"2023-10-23T00:00:00Z","relpermalink":"/zh-hant/slides/2023-10-25-k8s-resource-as-code/","section":"slides","summary":"將 k8s resource 以 code 管理，推上 vcs，並使用 argoCD, secret operator 等工具進行管理，來讓避免低級的人工操作錯誤，降低團隊整體失誤率，並降低 k8s admin 管理的成本，提高管理效率","tags":["kafka","kubernetes"],"title":"Kubernetes Summit: Resource as Code for Kubernetes: Stop kubectl apply","type":"slides"},{"authors":[],"categories":["vault","docker"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay ??: Integrated Backend 集成存儲\nVault 支援多種存儲選項，用於持久存儲 Vault 資訊。自 Vault 1.4 起，提供了整合式存儲選項。此存儲後端不依賴於任何第三方系統，實現高可用性語義，支援企業複製功能，並提供備份/還原工作流。\n該選項將 Vault 的數據存儲在服務器的文件系統上，並使用共識協議將數據複製到集群中的每個服務器。有關整合式存儲內部的更多信息，請參閱整合式存儲內部文檔。此外，配置文檔可以幫助配置 Vault 以使用整合式存儲。\n以下各節將詳細介紹如何使用整合式存儲操作 Vault。\n服務器間通信 一旦節點加入到彼此，它們開始使用 Vault 的叢集端口進行 mTLS 通信。叢集端口的默認值為 8201。TLS 信息在加入時交換，並按一定的節奏進行輪換。\n整合式存儲的要求之一是必須設置 cluster_addr 配置選項。這允許 Vault 在加入時為節點 ID 分配地址。\n叢集成員資格 本節將概述如何引導和管理運行整合式存儲的 Vault 節點集群。\n整合式存儲在初始化過程中引導，並且結果是大小為 1 的集群。根據所需的部署大小，可以將節點加入到活動 Vault 節點中。\n加入節點 加入是將未初始化的 Vault 節點並使其成為現有集群成員的過程。為了將新節點驗證到集群，它必須使用相同的密封機制。如果使用自動解封，則必須配置新節點以使用與其嘗試加入的集群相同的 KMS 提供程序和密鑰。如果使用 Shamir 密封，則必須在加入過程完成之前為新節點提供解封密鑰。一旦節點成功加入，來自活動節點的數據就可以開始複制到它。一旦節點加入，則不能重新加入到不同的集群。\n您可以通過配置文件自動加入節點，也可以通過 API 手動加入（下面描述了這兩種方法）。在加入節點時，必須使用領導節點的 API 地址。我們建議在所有節點上設置 api_addr 配置選項，以使加入過程更簡單。\nretry_join 配置 此方法允許在配置文件中設置一個或多個目標領導節點。當未初始化的 Vault 服務器啟動時，它將嘗試加入每個已定義的潛在領導者，直到成功。當指定的領導者之一變為活動狀態時，此節點將成功加入。當使用 Shamir 密封時，已加入的節點仍然需要手動解封。當使用自動解封時，節點將能夠自動加入並自動解封。\n下面是一個示例 retry_join 配置：\nstorage \u0026#34;raft\u0026#34; { path = \u0026#34;/var/raft/\u0026#34; node_id = \u0026#34;node3\u0026#34; retry_join { leader_api_addr = \u0026#34;https://node1.vault.local:8200\u0026#34; } retry_join { leader_api_addr = \u0026#34;https://node2.vault.local:8200\u0026#34; } } storage \u0026#34;raft\u0026#34; { path = \u0026#34;/var/raft/\u0026#34; node_id = \u0026#34;node3\u0026#34; retry_join { auto_join = \u0026#34;provider=aws region=eu-west-1 tag_key=vault tag_value=... access_key_id=... secret_access_key=...\u0026#34; } } chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/concepts/integrated-storage 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026#34;您\u0026#34;，請用\u0026#34;你\u0026#34;取代\u0026#34;您\u0026#34;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;數據庫\u0026#34; 改為 \u0026#34;資料庫\u0026#34;，將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;訪問\u0026#34; 改為 \u0026#34;存取\u0026#34;，將 \u0026#34;源代碼\u0026#34; 改為 \u0026#34;原始碼\u0026#34;，將 \u0026#34;信息\u0026#34; 改為 \u0026#34;資訊\u0026#34;，將 \u0026#34;命令\u0026#34; 改為 \u0026#34;指令\u0026#34;，將 \u0026#34;禁用\u0026#34; 改為 \u0026#34;停用\u0026#34;，將 \u0026#34;默認\u0026#34; 改為 \u0026#34;預設\u0026#34;。 ","date":1696884146,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695693546,"objectID":"5722c092452f4f8235aabe5724a0d084","permalink":"https://chechia.net/zh-hant/post/2023-10-10-vault-workshop-integrated-storage/","publishdate":"2023-10-10T04:42:26+08:00","relpermalink":"/zh-hant/post/2023-10-10-vault-workshop-integrated-storage/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay ??: Integrated Backend 集成存儲\nVault 支援多種存儲選項，用於持久存儲 Vault 資訊。自 Vault 1.4 起，提供了整合式存儲選項。此存儲後端不依賴於任何第三方系統，實現高可用性語義，支援企業複製功能，並提供備份/還原工作流。\n該選項將 Vault 的數據存儲在服務器的文件系統上，並使用共識協議將數據複製到集群中的每個服務器。有關整合式存儲內部的更多信息，請參閱整合式存儲內部文檔。此外，配置文檔可以幫助配置 Vault 以使用整合式存儲。\n以下各節將詳細介紹如何使用整合式存儲操作 Vault。\n服務器間通信 一旦節點加入到彼此，它們開始使用 Vault 的叢集端口進行 mTLS 通信。叢集端口的默認值為 8201。TLS 信息在加入時交換，並按一定的節奏進行輪換。","tags":["vault","iac","workshop","docker","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop ??: Integrated Storage","type":"post"},{"authors":[],"categories":null,"content":"HashiCorp Vault 自建金鑰管理最佳入坑姿勢\n本次演講從導入 HashiCorp Vault 作為起點，直接提供實務上經驗，分享建議的設定與路上可能有的雷。\nVault 入坑的困難 Vault + Terraform 一入坑就 IaC mount path + role + policy 命名與管理 升級與維護 會依據企業需求提供實際用例 demo，當天提供 github code example 中階\n預期聽眾是有 Vault 使用經驗，希望能更有效率管理 Vault 的人 不會講太多基本功能介紹 vault 介紹 ","date":1695734400,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1713932423,"objectID":"4a0a8e731612ddbde164c1bfa90b86f2","permalink":"https://chechia.net/zh-hant/talk/devopsday-hashicorp-vault-%E8%87%AA%E5%BB%BA%E9%87%91%E9%91%B0%E7%AE%A1%E7%90%86%E6%9C%80%E4%BD%B3%E5%85%A5%E5%9D%91%E5%A7%BF%E5%8B%A2/","publishdate":"2023-09-09T00:00:00Z","relpermalink":"/zh-hant/talk/devopsday-hashicorp-vault-%E8%87%AA%E5%BB%BA%E9%87%91%E9%91%B0%E7%AE%A1%E7%90%86%E6%9C%80%E4%BD%B3%E5%85%A5%E5%9D%91%E5%A7%BF%E5%8B%A2/","section":"event","summary":"從導入 HashiCorp Vault 作為起點，直接提供實務上經驗，分享建議的入坑設定","tags":[],"title":"DevOpsDay: HashiCorp Vault 自建金鑰管理最佳入坑姿勢","type":"event"},{"authors":[],"categories":["vault","terraform","kubernetes"],"content":" Q1: 有過使用 hashicorp vault 的人請舉手\n沒使用過的人不是這個 session 的目標聽眾，可以 QR code 拍下來去聽別場。例如對面同題材的session QR code 有投影片，範例 github repo，投影片裡還有講稿，所以我今天在這裡的用處就是念稿，真的可以 qrcode 拍了回家看\nQ2: 有使用過 infrastructure as code / terraform 的人請舉手\n有使用 vault 但是沒有使用 IaC 的朋友，才是這場 session 的主要受眾\nQ3: 有使用 iac deploy vault stack，或是有使用 iac / vcs 管理 vault 內的 policy 的人請舉手\n這些人可以出去吃零食，今天講的內容你們都會了，我沒什麼東西可以跟你們分享\nHashiCorp Vault 自建金鑰管理最佳實踐 Che Chia Chang | Vault 鐵人賽 workshop\ns 標題有稍微修改 About Me\nOutline from dev to prod-ready terraform deployment IaC on aws / azure / gcp / k8s 如何開始 terraform configuration IaC IaC everything secret backends / auth method / role / policy / audit … 工作流程自動化 gitflow / tested / automation manage vault infra \u0026amp; vault configuration from a aspect of devops Vault 基礎的學習資源 2023-05-10 雲端地端通吃的私鑰管理平台 2023 鐵人賽: vault 10- day workshop 2021 鐵人賽: terraform 30 day workshop 範例 Github 同一時間，Vault + Kubernetes 請出門對面DE會議室 不會講 vault 的基礎操作，但如果你需要學習資源，你可以來這邊找 第一個 google slides 是我在其他場合的演講，適合第一次接觸 vault，或是正在評估是否要導入 vault 的團隊\n第二個今年的 ithome 2023鐵人賽，我寫的內容就是 vault workshop，雖然寫到第十篇就因故停更，但前面 1-8 篇剛好是 vault 操作基礎，使用 chatgpt 翻譯 vault official tutorial，也是適合第一次使用 vault 的人\n第三個是如果沒接觸過 infrastructure as code IaC，這個也是鐵人賽的 30 day workshop，這個有完賽佳作\nproduction deploy checklist End-to-End TLS Single Tenancy Enable Auditing Immutable Upgrades Upgrade Frequently Restrict Storage Access 進入 production 前的維運問題\nvault 官方建議的 prod checklist 大多需要定期更新，維護，而非一勞永逸一次性作業\n例如 end-to-end TLS certificate 需要管理與更新\nauditing 對 vault 的存取紀錄需要安全的輸出，並且能夠檢核。設定做一次就可以生效。但是實務上未來會需要不斷的根據外部稽核需求調整\nimmutable upgrades 指的是當你使用 vault server 與 storage backend，vault server 本身是 immutable 的，你可以自己使用 official binary build VM Image (ex. aws ami)，或是透過 container image release 來更新\nupgrade frequently，但要做的安全，而且有效率，最好是做到半自動化或是全自動化，如果沒有 IaC 會比較耗工\nInfrastructure as Code 導入 IaC ，做到頻繁且安全的 vault 更新，但同時又要有效率甚至全自動化\nmultiple env: dev, stag, prod programable / reusable: 標準化，可重複使用的 code tested infrastructure 以 vault 為目標 維運 vault 的第一目標：安全第一，不求高效能，但是追求安全 是我們要將 deploy / release / upgrade vault 中的風險降到最低\ninfra: 升級 vault 版本，調整 VM / container，調整 load balancer，除錯 config: 更改 auth-method / policy\npolicy 容易改壞但無法及時發現，要用時才發現權限壞了 如何開始 IaC for Vault immutable Vault server storage backend load balancer security group / firewall rule IaC 好棒，那有沒有什麼資源可以幫助我們開始 deploy IaC Vault 答案是有的 https://www.terraform.io/\nDeploy on public cloud https://github.com/hashicorp/terraform-aws-vault https://github.com/hashicorp/terraform-aws-vault-starter https://github.com/hashicorp/terraform-azurerm-vault https://github.com/terraform-google-modules/terraform-google-vault 在公有雲 deploy Vault，雲服務商有提供既有的terraform module 不一定要照單全收，可以試著架起測試環境，然後調整terraform module，成為適合自己產品的架構 調整架構需要考量的點，底下分析 Deploy terraform-aws-vault hashicorp 官方提供的 terraform-aws-vault 是一個不錯的開始 terraform-aws-vault ELB -\u0026gt; AWS Autoscaling Group -\u0026gt; EC2 Backend: consul cluster Vault AMI security group tested by hashicorp with terratest goto https://github.com/hashicorp/terraform-aws-vault terraform-aws-vault 如何使用\n可以參考 main.tf 範例 可以檢視 ./modules 內說明逐步操作 terraform-aws-vault-starter ELB -\u0026gt; AWS Autoscaling Group -\u0026gt; EC2 security group Integrated storage (raft) goto https://github.com/hashicorp/terraform-aws-vault-starter Integrated storage integrated-storage Raft Consensus protocol 出incident要有辦法解 stateful server 備份，管理(資料遷移) util 工具未必夠成熟 選一個會維護的storage DynamoDB MySQL / PostgreSQL 注意storage是否能夠為維護，重啟，備份 On Kubernetes argocd + vault helm chart\nserver 與 injector 建議分開兩個 argocd applicatoin / helm release獨立deploy\nserver 選一個會維護的storage backend injector goto https://github.com/hashicorp/vault-helm/blob/main/values.yaml#L357\n注意server 是否 ha\nDeploy On Kubernetes secure your k8s goto https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/\nFAQ: on VM or on K8s? 問題不是 vault 在 VM 上安全，還是在 k8s 上安全 而是團隊能不能 secure 底下的 infra，如果熟 VM 就會覺得 VM 好做 secure k8s 是 VM + k8s 都要\n回到 Outline: prod-ready terraform configuration IaC auth method / config / role secret mounts policy team / people application / service account audit log Vault Configuration IaC VCS / PR reviewed well-formed / linted / no-typo multiple env: dev, stag, prod programable / reusable: 標準化，可重複使用的 code tested 可以為 terraform code 寫測試 可以寫整合測試腳本測試 vault dev server automation auditable version control 有多重要？\n有 / 沒有 review 的 code 品質，天差地遠 分享團隊知識，分享變更資訊 避免最雷的同事出包，全 team 的效能瓶頸 change management 對於 vault 內的的更改要最嚴格控制 沒有『誒是誰改了這個我怎麼不知道』這回事 Linter / formated 是工作效率的根本\n多環境測試\n有安全的測試環境，才能促進團隊創新 有 / 沒有 經過完整環境測試的 release 品質，天差地遠 Programable / Reusable\nvariable / for loop reusable module / don’t repeat your self tested policy code\npolicy as code terratest automation\ngitflow secured admin access only to workflow. developer don’t have admin access. auditable\naudit in code / review live server generate audit log 外部稽核 hashicorp official tutorial tee admin-policy.hcl \u0026lt;\u0026lt;EOF # Read system health check path \u0026#34;sys/health\u0026#34; { capabilities = [\u0026#34;read\u0026#34;, \u0026#34;sudo\u0026#34;] } # List existing policies path \u0026#34;sys/policies/acl\u0026#34; { capabilities = [\u0026#34;list\u0026#34;] } EOF configure vault policy with CLI hashicorp official tutorial vault policy …","date":1695254400,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695705440,"objectID":"86c60c34cc522e669267a036580d9278","permalink":"https://chechia.net/zh-hant/slides/2023-09-26-devopsday-2023-vault/","publishdate":"2023-09-21T00:00:00Z","relpermalink":"/zh-hant/slides/2023-09-26-devopsday-2023-vault/","section":"slides","summary":"從導入 HashiCorp Vault 作為起點，直接提供實務上經驗，分享建議的入坑設定","tags":["vault","terraform","iac","kubernetes"],"title":"DevOpsDay: HashiCorp Vault 自建金鑰管理最佳入坑姿勢","type":"slides"},{"authors":[],"categories":["vault","docker"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Local with Docker minikube r=https://api.github.com/repos/kubernetes/minikube/releases curl -LO $(curl -s $r | grep -o \u0026#39;http.*download/v.*beta.*/minikube-darwin-arm64\u0026#39; | head -n1) sudo install minikube-darwin-arm64 /usr/local/bin/minikube 如何選擇 VM or Docker or Kubernetes 假議題\nchatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/install 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026#34;您\u0026#34;，請用\u0026#34;你\u0026#34;取代\u0026#34;您\u0026#34;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;數據庫\u0026#34; 改為 \u0026#34;資料庫\u0026#34;，將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;訪問\u0026#34; 改為 \u0026#34;存取\u0026#34;，將 \u0026#34;源代碼\u0026#34; 改為 \u0026#34;原始碼\u0026#34;，將 \u0026#34;信息\u0026#34; 改為 \u0026#34;資訊\u0026#34;，將 \u0026#34;命令\u0026#34; 改為 \u0026#34;指令\u0026#34;，將 \u0026#34;禁用\u0026#34; 改為 \u0026#34;停用\u0026#34;，將 \u0026#34;默認\u0026#34; 改為 \u0026#34;預設\u0026#34;。 ","date":1694644946,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695693546,"objectID":"1c39a5994f9f38f15204da63004165db","permalink":"https://chechia.net/zh-hant/post/2023-09-21-vault-workshop-deploy-on-kubernetes/","publishdate":"2023-09-14T06:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-21-vault-workshop-deploy-on-kubernetes/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Local with Docker minikube r=https://api.github.com/repos/kubernetes/minikube/releases curl -LO $(curl -s $r | grep -o 'http.*download/v.*beta.*/minikube-darwin-arm64' | head -n1) sudo install minikube-darwin-arm64 /usr/local/bin/minikube 如何選擇 VM or Docker or Kubernetes 假議題","tags":["vault","iac","workshop","docker","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 10: Deploy with Docker","type":"post"},{"authors":[],"categories":["vault","docker"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Vault configuration chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/install 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\u0026#34;您\u0026#34;，請用\u0026#34;你\u0026#34;取代\u0026#34;您\u0026#34;。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;數據庫\u0026#34; 改為 \u0026#34;資料庫\u0026#34;，將 \u0026#34;數據\u0026#34; 改為 \u0026#34;資料\u0026#34;，將 \u0026#34;訪問\u0026#34; 改為 \u0026#34;存取\u0026#34;，將 \u0026#34;源代碼\u0026#34; 改為 \u0026#34;原始碼\u0026#34;，將 \u0026#34;信息\u0026#34; 改為 \u0026#34;資訊\u0026#34;，將 \u0026#34;命令\u0026#34; 改為 \u0026#34;指令\u0026#34;，將 \u0026#34;禁用\u0026#34; 改為 \u0026#34;停用\u0026#34;，將 \u0026#34;默認\u0026#34; 改為 \u0026#34;預設\u0026#34;。 ","date":1694641346,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695693546,"objectID":"35cac6b0ffb7996b911978440f6a4c58","permalink":"https://chechia.net/zh-hant/post/2023-09-20-vault-workshop-ha-docker/","publishdate":"2023-09-14T05:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-20-vault-workshop-ha-docker/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 09: Deploy with Docker Vault configuration chatGPT 本段部分內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/install 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。 很重要：不要使用敬語，翻譯結果中若出現\"您\"，請用\"你\"取代\"您\"。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，value，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary，prefix，instance，metadata。 修正下列翻譯：將 \"數據\" 改為 \"資料\"，將 \"數據庫\" 改為 \"資料庫\"，將 \"數據\" 改為 \"資料\"，將 \"訪問\" 改為 \"存取\"，將 \"源代碼\" 改為 \"原始碼\"，將 \"信息\" 改為 \"資訊\"，將 \"命令\" 改為 \"指令\"，將 \"禁用\" 改為 \"停用\"，將 \"默認\" 改為 \"預設\"。 ","tags":["vault","iac","workshop","docker","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 09: Vault HA","type":"post"},{"authors":[],"categories":["vault","docker"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 08: Vault in Docker and Initialization Vault in container 前幾天我們使用 vault dev Server 來啟用測試用的 Vault server。\n在 production 環境我們不會使用 dev Server。Vault 提供許多安裝方法\n可以使用 binary 直接安裝在VM上 也可以透過 hashicorp/vault official docker image，在container 環境中執行 或是使用 hashicorp official helm chart，安裝在kuberntes上 使用範例 repository 你可以使用筆者準備的範例 repository https://github.com/chechiachang/vault-playground\ngit clone git@github.com:chechiachang/vault-playground.git cd deploy/00-docker-dev/ 你可以使用下列指令啟動 vault in docker\n並使用 -v flag 來掛載 ./config/ volume 到 container 中的 /vault/config.d/ docker run --cap-add=IPC_LOCK \\ --volume ./vault/config/:/vault/config.d \\ --volume ./vault/file/:/vault/file \\ --volume ./vault/logs/:/vault/logs \\ -p 8200:8200 \\ --name vault_1 \\ -d \\ hashicorp/vault:1.14.3 \\ vault server -config=/vault/config.d/vault.hcl 然後使用 docker logs 指令檢視 vault server log\ndocker logs -f vault_1 output\n==\u0026gt; Vault server configuration: Administrative Namespace: Api Address: https://0.0.0.0:8200 Cgo: disabled Cluster Address: https://0.0.0.0:8201 Environment Variables: GODEBUG, GOTRACEBACK, HOME, HOSTNAME, NAME, PATH, PWD, SHLVL, VERSION Go Version: go1.20.8 Listener 1: tcp (addr: \u0026#34;0.0.0.0:8200\u0026#34;, cluster address: \u0026#34;0.0.0.0:8201\u0026#34;, max_request_duration: \u0026#34;1m30s\u0026#34;, max_request_size: \u0026#34;33554432\u0026#34;, tls: \u0026#34;disabled\u0026#34;) Log Level: Mlock: supported: true, enabled: false Recovery Mode: false Storage: file Version: Vault v1.14.3, built 2023-09-11T21:23:55Z Version Sha: 56debfa71653e72433345f23cd26276bc90629ce ==\u0026gt; Vault server started! Log data will stream in below: 2023-09-20T13:38:25.914Z [INFO] proxy environment: http_proxy=\u0026#34;\u0026#34; https_proxy=\u0026#34;\u0026#34; no_proxy=\u0026#34;\u0026#34; 2023-09-20T13:38:25.920Z [INFO] core: Initializing version history cache for core 你會發現，vault server 不是以 dev server 的狀態啟動的，需要進行額外的初始化設定\nexport VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; 檢查 vault status\nvault status output，顯示一個上未初始化的 vault server 與其 storage backend “filesyste”\n可以在 ./vault/config/vault.hcl 上看到我們使用新的 Storage Backend “file” 設定 vault server 的 storage backend 目前是 sealed 狀態 不是 vault server sealed，而是沒有提供 vault server 初始化設定 / unseal keys，所以 vault server 無法解密 storage backend Key Value --- ----- Seal Type shamir Initialized false Sealed true Total Shares 0 Threshold 0 Unseal Progress 0/0 Unseal Nonce n/a Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type file HA Enabled false Storage Backend: filesystem https://developer.hashicorp.com/vault/docs/configuration/storage/filesystem\nfilesystem storage backend將 Vault 的資料存儲在文件系統上，使用標準的目錄結構。\n它可以用於持久的Single Server情況，或者在本地開發時，耐久性不是關鍵問題的情況下使用。\nfilesystem storage backend 不支援高可用性 - 檔案系統後端不支援高可用性。\nfilesystem storage backend 由HashiCorp 官方支援 - 檔案系統後端是由 HashiCorp 官方支援維護的。\n初始化 Vault 初始化是配置 Vault 的過程。僅對第一次使用在 Vault server 上的 Backend 上執行一次。在高可用(HA)模式下運行時，這僅在每個叢集(Vault Cluster)中執行一次，而不是每台Server。\n在初始化期間，將生成加密金鑰、創建 unseal keys，並創建 init root token。\n初始化不需身份驗證，僅適用於全新的 Vault，有資料存在的 Storage Backend 無法用初始化解鎖。\n要初始化 Vault，使用以下命令\nvault operator init output\nUnseal Key 1: 9AYJ...kNCn Unseal Key 2: RqDx...odRU Unseal Key 3: uHUv...lws4 Unseal Key 4: f+KD...aHgL Unseal Key 5: AKyF...e6vd Initial Root Token: hvs.8yU3...7esX Vault initialized with 5 key shares and a key threshold of 3. Please securely distribute the key shares printed above. When the Vault is re-sealed, restarted, or stopped, you must supply at least 3 of these keys to unseal it before it can start servicing requests. # Vault 使用了 5 個金鑰份額encryption key share和 3 的 key threshold 進行初始化。請安全地分發上面的金鑰份額。 # 當 Vault 被重新密封、重新啟動或停止時，你必須提供至少 3 個這些金鑰來對其進行解封，然後它才能開始處理請求。 Vault does not store the generated root key. Without at least 3 keys to reconstruct the root key, Vault will remain permanently sealed! # Vault 不存儲生成的根金鑰。如果沒有至少 3 個金鑰來重建根金鑰，Vault 將保持永久密封！ It is possible to generate new unseal keys, provided you have a quorum of existing unseal keys shares. See \u0026#34;vault operator rekey\u0026#34; for more information. # 如果你有足夠的現有解封金鑰份額，則可以生成新的解封金鑰。有關更多信息，請參閱 \u0026#34;vault operator rekey\u0026#34;。 初始化過程輸出了兩個非常重要的信息：解封金鑰和 init root token。這是唯一一次 Vault 顯示這些資料。\n為了這個入門課程，請將這些金鑰保存在某個地方，然後繼續進行操作。\nVault backend storage 認 key 不認人，請收好這五隻 unseal key，放置在五個不同安全的地方 有 3/5 的 unseal key 就能重建 encryption key，也就是能夠解密 storage backend 可以用 3/5 unseal key 產生 root token vault operator generate-root 換句話說，unseal key 是比 root 還大的超級管理員 key 在實際的部署情況下，你永遠不應該將這些金鑰保存在一起。 你可能會使用 Vault 的 PGP 和 Keybase.io 支援，使用使用者的 PGP 金鑰加密每個解封金鑰。 這可以防止單個人擁有所有的解封金鑰。 vault server log，可以看到 vault server 開始進行初始化流程\n2023-09-20T13:47:05.113Z [INFO] core: security barrier not initialized 2023-09-20T13:47:05.117Z [INFO] core: seal configuration missing, not initialized 2023-09-20T14:03:28.506Z [INFO] core: security barrier not initialized 2023-09-20T14:03:28.512Z [INFO] core: seal configuration missing, not initialized 2023-09-20T14:03:28.521Z [INFO] core: security barrier not initialized 2023-09-20T14:03:28.557Z [INFO] core: security barrier initialized: stored=1 shares=5 threshold=3 …","date":1694637746,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695224937,"objectID":"59374be8f87e0b492479a15fb6977f17","permalink":"https://chechia.net/zh-hant/post/2023-09-19-vault-workshop-docker-and-initialization/","publishdate":"2023-09-14T04:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-19-vault-workshop-docker-and-initialization/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 08: Vault in Docker and Initialization Vault in container 前幾天我們使用 vault dev Server 來啟用測試用的 Vault server。\n在 production 環境我們不會使用 dev Server。Vault 提供許多安裝方法","tags":["vault","iac","workshop","docker","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 08: Vault in Docker and Initialization","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 07: Policy \u0026amp; Approle 在 Vault 中，策略（Policies）控制著使用者可以訪問的資源。在上一篇身份驗證中，你已經學到了身份驗證方法(authentication method)。而這一節是關於授權(Authorization)，也就是合法的用戶登入後，應該能夠取得怎樣的權限。\n在身份驗證方面，Vault 提供了多種啟用和使用的選項或方法。Vault 在授權和 policy 方面也使用相同的設計。所有身份驗證方法都將登入者的身份 map 回與 Vault 配置的核心 policy。\n準備環境 在 vault 官方網站文件中，Hashicorp 官方準備了 https://instruqt.com/的 session lab\n筆者個人覺得 interactive lab 的 browser tab 很難用，也不喜歡 Terminal session 連 remote VM 的延遲，以下內容還是會使用 local dev Server 說明。\n觀眾們可以自己斟酌使用。\nPolicy 格式 策略（Policies）是以HCL撰寫的，但也兼容JSON格式。以下是一個範例策略：\n# Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026#34;secret/data/*\u0026#34; { capabilities = [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;] } path \u0026#34;secret/data/foo\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } 這個範例 policy 授予了對 KV v2 秘密管理（secrets engine）的資源的能力。如果你對於與這個秘密管理引擎相關的路徑不熟悉，可以查閱該秘密管理引擎文件中的 ACL 規則部分。\n根據這個 policy，使用者可以對 secret/data/ 中的任何秘密進行寫入操作，但對於 secret/data/foo 的存取僅允許讀取。policy 的預設行為是 Deny，因此不允許對未指定路徑的資源進行任何存取。\npolicy 格式使用 prefix 匹配系統來確定對 API 路徑的訪問控制。使用最具體的已定義策略，即精確匹配或最長 prefix 匹配。也就是存取 secret/data/foo 路徑符合上面兩條規則，但是由於第二條規則的路徑 match 最長最精確，所以最終拿到的權限是 “read”\n由於 Vault 中的一切都必須通過 API 進行訪問，這使得對 Vault 的每個方面都有嚴格的控制，包括啟用秘密管理引擎、啟用身份驗證方法、身份驗證，以及存取秘密等。\n有一些內建的策略無法被移除。例如，root policy 和 default policy 是必需的策略，無法被刪除。\ndefault policy 提供了一組常見的權限，並且 default 包含在所有 token 中。 root policy 給予 token 超級管理員權限，類似於Linux機器上的 root 用戶。 啟動本地開發環境 步驟在前幾篇已經出現過數次，這邊就簡單帶過\nvault server -dev -dev-no-store-token export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; unset VAULT_TOKEN vault login Defult policy 你可以用下列指令列出預設的 policy\nvault policy list output\ndefault root 可以用以下指令讀取 default policy 的內容\nvault policy read default output 回傳許多條 policy rule，主要是給予 token 基本的操作權限\n# Allow tokens to look up their own properties path \u0026#34;auth/token/lookup-self\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } # Allow tokens to renew themselves path \u0026#34;auth/token/renew-self\u0026#34; { capabilities = [\u0026#34;update\u0026#34;] } # Allow tokens to revoke themselves path \u0026#34;auth/token/revoke-self\u0026#34; { capabilities = [\u0026#34;update\u0026#34;] } ... 熟悉 default policy 內容後，可以斟酌使用。或是選擇從頭編寫自己的 policy。\n編寫第一個 policy 要撰寫 policy ，使用 vault policy write 指令。請查閱該指令的幫助文件以進一步了解用法。\nvault policy write -h Usage: vault policy write [options] NAME PATH Uploads a policy with name NAME from the contents of a local file PATH or stdin. If PATH is \u0026#34;-\u0026#34;, the policy is read from stdin. Otherwise, it is loaded from the file at the given path on the local disk. # 使用 vault policy write 指令，可以從本地文件 PATH 或標準輸入（stdin）的內容上傳一個名為 NAME 的策略。如果 PATH 是 \u0026#34;-\u0026#34;，則策略將從 stdin 讀取。否則，它將從本地 disk 上給定路徑的文件中載入。 Upload a policy named \u0026#34;my-policy\u0026#34; from \u0026#34;/tmp/policy.hcl\u0026#34; on the local disk: $ vault policy write my-policy /tmp/policy.hcl Upload a policy from stdin: $ cat my-policy.hcl | vault policy write my-policy - 使用以下指令創建名為 “my-policy” 的 policy，並將其內容來自 stdin（標準輸入）\nvault policy write my-policy - \u0026lt;\u0026lt; EOF # Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026#34;secret/data/*\u0026#34; { capabilities = [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;] } path \u0026#34;secret/data/foo\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } EOF output\nSuccess! Uploaded policy: my-policy 使用下列指令列出 policy\nvault policy list output\ndefault my-policy root 讀取 my-policy 的內容\nvault policy read my-policy output\n# Dev servers have version 2 of KV secrets engine mounted by default, so will # need these paths to grant permissions: path \u0026#34;secret/data/*\u0026#34; { capabilities = [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;] } path \u0026#34;secret/data/foo\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } 測試 policy 你所建立的 policy ，提供對 KV-V2 秘密引擎所定義的秘密進行管理。policy 被附加到 Vault 直接生成的 otoken，或透過其各種授權方法生成的 token 上。\n創建一個 token ，添加 my-policy。-field flag 設定只回傳部分 key-vault data，而不是傳整個 metadata table。-policy 設定連結 token 的 policy。\nvault token create -field token -policy=my-policy hvs.CAESIIDh...UxbUU 為了簡化，本內容使用 dev 模式伺服器，直接從 token 授權方法創建 token 。請記住，在大多數 production 環境部署中，token 將由已啟用的授權方法(ex. github auth method)創建。\n可以使用 token login\nvault login output 回傳登入資訊，以及連結 token 的 policy，policy 包含 default 與 my-policy\nSuccess! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026#34;vault login\u0026#34; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.CAESIIDh...UxbUU token_accessor vY3aLwMSezlxeOn7YwcjxuhZ token_duration 767h56m52s token_renewable true token_policies [\u0026#34;default\u0026#34; \u0026#34;my-policy\u0026#34;] identity_policies [] policies [\u0026#34;default\u0026#34; \u0026#34;my-policy\u0026#34;] 你可以執行 vault token lookup，查找目前 token 的完整資訊\nvault token lookup output\nKey Value --- ----- accessor vY3aLwMSezlxeOn7YwcjxuhZ creation_time 1694917062 creation_ttl 768h display_name token entity_id n/a expire_time 2023-10-19T10:17:42.723364+08:00 explicit_max_ttl 0s id hvs.CAESIIDh...UxbUU issue_time 2023-09-17T10:17:42.723365+08:00 meta \u0026lt;nil\u0026gt; num_uses 0 orphan false path auth/token/create policies [default my-policy] renewable true ttl 767h55m45s type service 這個policy 啟用了 secret/ 秘密引擎內每個路徑的創建和更新功能，除了一個例外路徑 secret/data/foo  …","date":1694634146,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695049410,"objectID":"e51bdf75acbc70ef48688971a226ba31","permalink":"https://chechia.net/zh-hant/post/2023-09-18-vault-workshop-policy-and-approle/","publishdate":"2023-09-14T03:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-18-vault-workshop-policy-and-approle/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 07: Policy \u0026 Approle 在 Vault 中，策略（Policies）控制著使用者可以訪問的資源。在上一篇身份驗證中，你已經學到了身份驗證方法(authentication method)。而這一節是關於授權(Authorization)，也就是合法的用戶登入後，應該能夠取得怎樣的權限。\n在身份驗證方面，Vault 提供了多種啟用和使用的選項或方法。Vault 在授權和 policy 方面也使用相同的設計。所有身份驗證方法都將登入者的身份 map 回與 Vault 配置的核心 policy。\n準備環境 在 vault 官方網站文件中，Hashicorp 官方準備了 https://instruqt.","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 07: Policy \u0026 approle","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 06：Github auth method 不需要使用 root token 的 auth method: github Vault 支援用於人員使用者的身份驗證方法。GitHub 身份驗證，使用戶可以通過提供他們的 GitHub 憑證來驗證 Vault，並收到一個 Vault token。\n簡單來說，github organization chechia-net，可以設定適當的權限給成員 chechiachang，讓 chechiachang 可以透過 github 取得有權限的 token。\n注意\n在練習中所描述的這種身份驗證方法，需要你擁有 GitHub、屬於 GitHub org 中的一個 team ，並生成了具有 read:org scope 的 GitHub personal access token。你可以於 github 創建一個 free plan 的 org，並設定一個 team，然後透過個人 Settings / Developer Settings 來產生一把具有 read:org 權限的 personal access token\n啟用 github auth method 你可以使用下列指令，在 path=github/ 下啟用 GitHub 身份驗證方法。\nexport VAULT_TOKEN=hvs.qCqY...9KYv vault auth enable github 這個指令使用 root token 設定 auth method，也是整個 github auth methods 中，唯一需要用到 root token 設定的地方，而且只需要設定一次\noutput\nSuccess! Enabled github auth method at: github/ vault server log 顯示 credential backend 已啟用\n2023-09-16T21:40:10.444+0800 [INFO] core: enabled credential backend: path=github/ type=github version=\u0026#34;\u0026#34; 你可以下列指令，列出所有的 auth methods\nvault auth list output\nPath Type Accessor Description Version ---- ---- -------- ----------- ------- github/ github auth_github_9bc96e5f n/a n/a token/ token auth_token_b7984c52 token based credentials n/a github 身份驗證方法已啟用，並位於路徑 auth/github/。\n設定 github auth method 此身份驗證方法需要你在配置中設置 GitHub organization。GitHub organization中，維護了一個允許與 Vault 驗證的使用者列表。\n設置 GitHub 身份驗證的組織。\nvault write auth/github/config organization=chechia-net output\nSuccess! Data written to: auth/github/config 現在，chechia-net GitHub 組織中的所有使用者都可以進行身份驗證。\n設定 github auth method，給不同 team 不同的 policy GitHub 組織可以定義團隊(team)。每個團隊可能可以在組織維護的所有 repository 中執行不同的操作。這些團隊也可能需要訪問 Vault 內的特定密碼。\n配置 GitHub sre 團隊的身份驗證，以獲得 default 和 application 兩個 policy的授權。\nvault write auth/github/map/teams/sre value=default,application output\nSuccess! Data written to: auth/github/map/teams/sre GitHub sre 團隊中 chechia-net 組織的成員，將使用 default 和 application policy 進行身份驗證和授權。\n備註：application policy 尚未在 Vault 中定義。在該策略定義之前，Vault 仍然允許使用者進行身份驗證，但會產生警告。\n你可以使用指令顯示 Vault 啟用的所有身份驗證方法\nvault auth list 回傳顯示兩個 auth methods，github/ 與 token/\nPath Type Accessor Description Version ---- ---- -------- ----------- ------- github/ github auth_github_9bc96e5f n/a n/a token/ token auth_token_b7984c52 token based credentials n/a 使用 github auth method login 使用 help 指令來了解 github auth method 的說明\nvault auth help github output\nUsage: vault login -method=github [CONFIG K=V...] The GitHub auth method allows users to authenticate using a GitHub personal access token. Users can generate a personal access token from the settings page on their GitHub account. # GitHub 身份驗證方法允許使用者使用 GitHub personal access token進行身份驗證。使用者可以從其 GitHub Settings -\u0026gt; Developer Settings -\u0026gt; Personal access tokens (classic) 生成個人訪問令牌。 Authenticate using a GitHub token: $ vault login -method=github token=abcd1234 Configuration: mount=\u0026lt;string\u0026gt; Path where the GitHub credential method is mounted. This is usually provided via the -path flag in the \u0026#34;vault login\u0026#34; command, but it can be specified here as well. If specified here, it takes precedence over the value for -path. The default value is \u0026#34;github\u0026#34;. # GitHub 憑證方法掛載的路徑。通常透過 \u0026#34;vault login\u0026#34; 指令的 -path flag 提供，但這裡也可以指定。如果在這裡指定，則優先於 -path 的值。預設的路徑值為 \u0026#34;github\u0026#34;。 token=\u0026lt;string\u0026gt; GitHub personal access token to use for authentication. If not provided, Vault will prompt for the value. # 用於身份驗證的 GitHub personal access token。如果未提供，Vault 將提示輸入此值。 輸出顯示了使用 GitHub 方法進行 login 的例子。該方法要求必須定義該方法，並由使用者提供 GitHub personal access token。\n由於你將嘗試使用身份驗證方法進行登錄，請確保在此 shell session 中未設置 VAULT_TOKEN 環境變數，因為其值將優先於你從 Vault 獲取的任何 token。\n取消設定該環境變數。\nunset VAULT_TOKEN 嘗試使用 github auth methods 登入\nvault login -method=github output，使用者使用 github personal access token，取得有效的 vault token\ntoken 格式不同 token 的效期為 768h，短效期 token 的曝險程度較低 這個 token 的 policy 權限只有 default policy metadata 是 github 回傳的，vault 中並沒有建立 username=chechiachang 的資料 org 是 chechia-net username 是 chechiachang GitHub Personal Access Token (will be hidden): Success! You are now authenticated. The token information displayed below is already stored in the token helper. You do NOT need to run \u0026#34;vault login\u0026#34; again. Future Vault requests will automatically use this token. Key Value --- ----- token hvs.CAESI...ncU0 token_accessor lKVpMawotXg1fXcgRvpAOAwQ token_duration 768h token_renewable true token_policies [\u0026#34;default\u0026#34;] identity_policies [] policies [\u0026#34;default\u0026#34;] token_meta_org chechia-net token_meta_username chechiachang 當未提供 GitHub personal access token 給命令時，Vault CLI 會提示使用者輸入。\n如果提供了有效的 GitHub personal access token，則使用者將成功登錄 vault，並且輸出會顯示一個 Vault token。使用者可以使用這個 Vault token，直到該 token 被撤銷或其有效期超過了 token_duration。\n你可以使用 vault token lookup 來檢視當前的 token\nvault token lookup output，顯示使用中的 token，使用的 path 與 methods 是 github/，效期 ttl 在倒數中逐漸減少 767h54m27s\nKey Value --- ----- accessor lKVpMawotXg1fXcgRvpAOAwQ creation_time 1694873434 creation_ttl 768h display_name github-chechiachang entity_id 5d8af1ab-53a2-e1c5-61bf-bb0b6c7d190f expire_time …","date":1694630546,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695049680,"objectID":"fd8b4581fef04cb3cd24151b55ee5223","permalink":"https://chechia.net/zh-hant/post/2023-09-17-vault-workshop-authentication-github/","publishdate":"2023-09-14T02:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-17-vault-workshop-authentication-github/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 06：Github auth method 不需要使用 root token 的 auth method: github Vault 支援用於人員使用者的身份驗證方法。GitHub 身份驗證，使用戶可以通過提供他們的 GitHub 憑證來驗證 Vault，並收到一個 Vault token。\n簡單來說，github organization chechia-net，可以設定適當的權限給成員 chechiachang，讓 chechiachang 可以透過 github 取得有權限的 token。","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 06: Github Auth method","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 05：Authentication 在前面的文章中，你已經創建了第一個秘密，了解了秘密引擎，並在以開發模式啟動的 Vault 服務器中探索了動態秘密。\n接下來的內容中，我們將深入研究使用 Vault token 和 GitHub 憑證進行身份驗證。\nToken 身份驗證 Token 身份驗證已自動啟用。當你啟動開發模式服務器時，輸出顯示了 Root token。Vault CLI 從 $VAULT_TOKEN 環境變數中讀取 Root token。這個根 token 可以在 Vault 內執行任何操作，因為它被分配了 Root policy 權限。允許的權限中包含創建新的 token。\n現在，讓我們創建一個新的 token。\n啟動全新的 dev Vault Server\nvault server -dev output，server log 回傳 dev 模式的警告\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. # 警告！已啟用開發模式！在此模式下，Vault完全運行在內存中，使用單個unseal key解封。 # root token已被CLI驗證，因此你可以立即開始使用Vault。 You may need to set the following environment variables: $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; # 以下顯示了unseal key 和 root token，以防你想要封存/解封Vault，或重新進行身份驗證。 The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. # dev 模式預設配置的 unseal key 與 root token Unseal Key: QDUAuY7Kltsc/3bVwUYF39u8aEFgWNRs/1D5yxFtim4= Root Token: hvs.J30e...0DJaN # 開發模式絕不應在生產安裝中使用！ Development mode should NOT be used in production installations! 依據提示 export 需要的變數\nexport VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; 在 dev mode 下，未使用 token 也可以存取 Vault Server 中的資料，這是因為 vault server 在啟用時，預設使用 token helper，將 root token 寫入到 local filesystem\nVault token helper 說明\n你可以使用以下指令，檢查儲存於本地的 vault token\ncat ~/.vault-token output\nhvs.J30e...0DJaN% vault CLI 自動取用本地儲存的 root token，所以已經自動完成 authentication，不用額外進行 authentication，也可以使用 Vault\n使用 Vault 時，須額外注意本地儲存的 token\n避免 dev 模式下儲存 root token Vault dev server 自動寫入 root token，讓開發 Vault 功能時十分便利。然而在資訊安全的領域，便利往往代表著風險。在非 dev Server 環境中，我們會嚴格控制所有 token 的曝險程度，特別是 root token，根本不該被 print 出，當然也萬萬不能儲存在本地電腦。\n依據你的開發需求，你可能不希望 dev 模式下，long-live 永久有效的 root token 儲存在本地電腦中，你可以使用 -dev-no-store-token flag 來避免 Vault dev server 暴露 root token。\nvault server -dev -dev-no-store-token output\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variables: $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: hNi8T3YctTZrLYlKezLAJttfiF97D1Vy7Tq+HMM3y9w= Root Token: hvs.qCqY...p89KYv Development mode should NOT be used in production installations! 你可以檢查本地使否有儲存的的 vault token\ncat ~/.vault-token output\ncat: /Users/che-chia/.vault-token: No such file or directory 然後試圖在沒有 .vault-token 的狀態下，存取 Vault Server\nvault secrets list output，沒有合法的 access token，Vault server 回傳 403 權限被拒\nvault secrets list Error listing secrets engines: Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/mounts Code: 403. Errors: * permission denied Authentication 你可以使用 vault dev Server log 中回傳的 root token 來存取 Vault server\n你可以將 token 在寫回 .vault-token，這是最方便，也最危險\n建議：永遠不要儲存 root token 在本地電腦上。非常容易遺忘自己本地有儲存 root token。\necho hvs.qCqY...p89KYv \u0026gt; ~/.vault-token 另一個方式是使用環境變數 VAULT_TOKEN\nVAULT_TOKEN=hvs.qCqY...p89KYv vault secrets list 上面是在 CLI 前面插入環境變數，下面是 export VAULT_TOKEN 到當前 session 的環境變數\nexport VAULT_TOKEN=hvs.qCqY...p89KYv vault secrets list output\nPath Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9cc57bcf per-token private secret storage identity/ identity identity_41faabef identity store secret/ kv kv_fca914b5 key/value secret storage sys/ system system_5dd83198 system endpoints used for control, policy and debugging 你可以檢查目前環境變數中，與 Vault 有關的 env\nenv | grep VAULT output\nVAULT_ADDR=http://127.0.0.1:8200 VAULT_TOKEN=hvs.qCqY...p89KYv 當然，VAULT_TOKEN 的存在時間越久，token 曝險的機率就越高。\n也很容易忘記 VAULT_TOKEN 有存在的環境變數。\nVAULT TOKEN 雷點 workshop 至今，你目前只有一台 vault dev server，而且裡面並沒有任何的機密資料，這個 vault server 可以隨時拋棄。\n然而，在實務上，Vault 管理員手上可能會有多個 Vault server 需要管理。\n想像你有 dev / stag / prod 的 server，某一天你正在 dev 開發一個新功能，由於本地有 VAULT_TOKEN 與 VAULT_ADDR，你不疑有他的在這台 server 上運行許多測試的指令，包含新增一對測試 secret，刪刪改改現有的 secret。\n然後你的本能忽然覺得怪怪的，我沒有設置任何 vault env，為何可以直接存取 dev server？\n你拉出 env 看，發現是 production server 的 addr + token，上次進入 production server 時的 session，還存有有效的 addr 與 token\nVAULT_ADDR=http://vault.prod.chechia.net VAULT_TOKEN=this-is-prod-token 你麻煩大了\n心得：永遠不要儲存長效期的 token 在本地。本 workshop 會不斷地強調，提醒各種操作中的資安風險，請各位學習過程中就養成良好習慣，以免方便一時，遺憾終身。\nroot token https://developer.hashicorp.com/vault/docs/concepts/tokens#root-tokens\nroot token是附帶root policy的token。root token可以在Vault中執行任何操作。任何操作。\n此外，它們是Vault中唯一可以設置為永不過期，且無需進行任何續訂的 token 類型。由於 root token 權限如此大，Vault 設計上刻意讓創建root token 很困難；實際上只有三種方法可以創建root token：\n在 vault operator init 時生成的初始root token - 此token不會過期\n使用另一個root token 創建 root token。這點有個限制：使用有限期的root token，無法創建永不過期的root token\n在擁有 unseal keys quorum的權限下，使用 vault operator generate-root 來創建root token\nroot …","date":1694626946,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695049680,"objectID":"b4a38f7af09257f7c24eff5155088a26","permalink":"https://chechia.net/zh-hant/post/2023-09-16-vault-workshop-authentication/","publishdate":"2023-09-14T01:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-16-vault-workshop-authentication/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 05：Authentication 在前面的文章中，你已經創建了第一個秘密，了解了秘密引擎，並在以開發模式啟動的 Vault 服務器中探索了動態秘密。\n接下來的內容中，我們將深入研究使用 Vault token 和 GitHub 憑證進行身份驗證。\nToken 身份驗證 Token 身份驗證已自動啟用。當你啟動開發模式服務器時，輸出顯示了 Root token。Vault CLI 從 $VAULT_TOKEN 環境變數中讀取 Root token。這個根 token 可以在 Vault 內執行任何操作，因為它被分配了 Root policy 權限。允許的權限中包含創建新的 token。","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 05: Authentication","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 04：Secret Engine KV V2 KV秘密引擎，用於在Vault的已配置物理Storage中，存儲任意秘密。\nkey 名稱必須始終是字符串。如果你直接通過CLI編寫非字串 value，它們將被轉換為字串。但是，你可以通過從JSON文件中將key-value pair寫入Vault，或使用HTTP API來保留非字串 value。\n此秘密引擎遵照ACL policy中，創建(create)和更新(update)權限之間的設定。它還支持patch功能，用於表示部分更新(patch)，而更新功能(update)則表示完全覆蓋。\n設置 大多數秘密引擎必須在執行其功能之前事先配置。這些步驟通常由操作人員或配置管理工具完成。\n啟用v2 kv秘密引擎：\n啟動一個乾淨的本地的開發模式 Vault Server，全新的 Vault Server 包含底下預設啟用的引擎\nvault server -dev export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_57a4ca51 per-token private secret storage identity/ identity identity_c2ecbd49 identity store secret/ kv kv_c41afde8 key/value secret storage sys/ system system_c063e514 system endpoints used for control, policy and debugging 你可以在指定的路徑下，啟用一個新的 v2 kv 秘密引擎。若不指定 -path 參數，則預設 path 為 type，也就是 path=kv。\nvault secrets enable -version=2 -path=kv kv Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_57a4ca51 per-token private secret storage identity/ identity identity_c2ecbd49 identity store kv/ kv kv_904eee17 n/a secret/ kv kv_c41afde8 key/value secret storage sys/ system system_c063e514 system endpoints used for control, policy and debugging 或者，你可以將kv-v2作為秘密引擎類型，以參數傳遞：\nvault secrets enable kv-v2 上面的指令與下面的指令等效，重複執行會出現路徑重複錯誤\nvault secrets enable -path kv-v2 -version=2 kv-v2 Error enabling: Error making API request. URL: POST http://127.0.0.1:8200/v1/sys/mounts/kv-v2 Code: 400. Errors: * path is already in use at kv-v2/ 你可以停用一個秘密引擎\nvault secrets disable kv-v2 為了學習效果，往後課程都會使用更完整的指令，作為示範。\n更多啟用秘密引擎的指令，你可以參考 vault secrets enable –help 的協助指令\nvault secrets enable --help Usage: vault secrets enable [options] TYPE Enables a secrets engine. By default, secrets engines are enabled at the path corresponding to their TYPE, but users can customize the path using the -path option. 啟用秘密引擎。預設情況下，秘密引擎會在與其類型相對應的路徑啟用，但使用者可以使用 -path 選項自訂路徑。 Once enabled, Vault will route all requests which begin with the path to the secrets engine. 一旦啟用，Vault 將將所有以該路徑開頭的請求路由到秘密引擎。 Enable the AWS secrets engine at aws/: $ vault secrets enable aws Enable the SSH secrets engine at ssh-prod/: $ vault secrets enable -path=ssh-prod ssh Enable the database secrets engine with an explicit maximum TTL of 30m: $ vault secrets enable -max-lease-ttl=30m database Enable a custom plugin (after it is registered in the plugin registry): $ vault secrets enable -path=my-secrets -plugin-name=my-plugin plugin OR (preferred way): $ vault secrets enable -path=my-secrets my-plugin 從上面的內容，可以看到秘密引擎支援各種不同的秘密類型，例如以下都是一個秘密引擎類型：aws，ssh，database，plugin。\n注意，請不要弄混 path=aws 的秘密引擎，以及 type=aws 的秘密引擎。底下是一個十分混淆的例子：\nvault secrets enable --version=2 --path=kv-v1 kv vault secrets enable --version=1 --path=kv-v2 kv vault secrets list --detailed Path Plugin Accessor Default TTL Max TTL Force No Cache Replication Seal Wrap External Entropy Access Options Description UUID Version Running Version Running SHA256 Deprecation Status ---- ------ -------- ----------- ------- -------------- ----------- --------- ----------------------- ------- ----------- ---- ------- --------------- -------------- ------------------ cubbyhole/ cubbyhole cubbyhole_57a4ca51 n/a n/a false local false false map[] per-token private secret storage cee0001e-95f8-efaa-6a9a-a3d5c3573abc n/a v1.14.3+builtin.vault n/a n/a identity/ identity identity_c2ecbd49 system system false replicated false false map[] identity store 3b481e0d-1c6d-6698-85c3-c52839b4d6e4 n/a v1.14.3+builtin.vault n/a n/a kv-v1/ kv kv_c8de7a39 system system false replicated false false map[version:2] n/a 98289587-5b5d-06f3-c3cf-469e830eda9e n/a v0.15.0+builtin n/a supported kv-v2/ kv kv_f83d9acd system system false replicated false false map[version:1] n/a 5fc 請不要做這種不良的命名，誤導自己也誤導別人。不建議把 engine type 當作 path，實務上也沒有必要這個做。\nvault secrets disable kv-v1 vault secrets disable kv-v2 使用公司組織，團隊，專案名稱，作為 engine path 的命名，是個不錯的起點。\norg = chechia.net team = sre project = workshop vault secrets enable --version=2 --path=chechia-net/sre/workshop kv 使用 v2 kv 在秘密引擎配置完成，且用戶/機器具備具有適當權限的Vault token後，它可以生成憑證。KV秘密引擎允許將任意vvalue寫入指定的key。\n在KV-v2中仍然可以使用類似路徑的KV-v1語法來引用秘密（secret/foo），但我們建議使用-flag的語法以避免將其錯誤認為是秘密的實際路徑（secret/data/foo是真正的路徑）。\n寫入/讀取任意資料 寫入任意資料：\nvault kv put -mount=chechia-net/sre/workshop my-secret foo=a bar=b ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 讀取任意資料：\nvault kv get -mount=chechia-net/sre/workshop my-secret ============= Secret Path ============= chechia-net/sre/workshop/data/my-secret ======= Metadata ======= Key Value --- ----- created_time 2023-09-15T16:48:45.817265Z custom_metadata \u0026lt;nil\u0026gt; deletion_time n/a destroyed false version 1 === Data === Key Value …","date":1694623346,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695049428,"objectID":"dc2687c559eb0003de9fff7dd72f0685","permalink":"https://chechia.net/zh-hant/post/2023-09-15-vault-workshop-secret-engine-kv-v2/","publishdate":"2023-09-14T00:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-15-vault-workshop-secret-engine-kv-v2/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 04：Secret Engine KV V2 KV秘密引擎，用於在Vault的已配置物理Storage中，存儲任意秘密。\nkey 名稱必須始終是字符串。如果你直接通過CLI編寫非字串 value，它們將被轉換為字串。但是，你可以通過從JSON文件中將key-value pair寫入Vault，或使用HTTP API來保留非字串 value。\n此秘密引擎遵照ACL policy中，創建(create)和更新(update)權限之間的設定。它還支持patch功能，用於表示部分更新(patch)，而更新功能(update)則表示完全覆蓋。\n設置 大多數秘密引擎必須在執行其功能之前事先配置。這些步驟通常由操作人員或配置管理工具完成。\n啟用v2 kv秘密引擎：\n啟動一個乾淨的本地的開發模式 Vault Server，全新的 Vault Server 包含底下預設啟用的引擎","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 04: Secret Engine KV V2","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 03：細探 Secret Engine 秘密引擎 什麼是秘密引擎？ 秘密引擎是Vault的組件，用於存儲、生成或加密秘密。在前面的內容中，你使用了Key/Value v2 秘密引擎來存儲數據。一些秘密引擎，比如鍵/值秘密引擎，僅僅是用來存儲和讀取數據的。其他秘密引擎則連接到其他服務並根據需求生成動態憑證。還有一些秘密引擎提供加密作為服務。\n前面的內容中，默認情況下，key/value v2 秘密引擎已啟用，並準備在 secret/ 下接收請求，因為我們在-dev 模式下啟動了Vault Server。\n在底下我們使用 kv v1 做簡單的範例。\nmount path 建議在使用 KV v2 秘密引擎時，使用可選的 -mount flag 語法，例如\nvault kv get -mount=secret foo 請嘗試以下命令，這將導致錯誤：\nvault kv put foo/bar a=b Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/internal/ui/mounts/foo/bar Code: 403. Errors: * preflight capability check returned 403, please ensure client\u0026#39;s policies grant access to path \u0026#34;foo/bar/\u0026#34; Path prefix 路徑前綴告訴 Vault 應該將流量 route 到哪個秘密引擎\n當請求到達 Vault 時，它會使用最長前綴匹配來匹配初始路徑部分，然後將請求傳遞給在該路徑啟用的相應秘密引擎。\n如果 mount 設置為 foo/bar 則會在 foo/bar 這個 path 下的 secret engine，儲存 a=b 如果 mount 設置為 foo 則會在 foo 這個 path 下的 secret engine，儲存在 path /bar，a=b Vault 將這些秘密引擎呈現得類似於文件系統 (ex. )/usr/local/bin/vault)\n在 linux 上存取一個 mount path 不存在的 directory path 在 vault 中存取 foo 處未掛載秘密引擎，所以上面的命令返回了錯誤。 對於 vault kv 命令，也可以使用 -mount flag\n啟用一個秘密引擎 要開始，請在路徑 kv 啟用一個新的 KV 秘密引擎。每個路徑都是完全隔離的，無法與其他路徑通信。例如，\n啟用在 foo 的 KV 秘密引擎無法與啟用在 bar 的 KV秘密引擎通信。\n/foo a=b /bar c=d 啟用新的秘密引擎以前，先查看一下目前 vault server 中已經啟用的引擎。\n除了 default 存在的引擎，還有在 dev 模式下自動建立的 secret/\ndefault\ncubbyhole/ identity/ sys/ dev mode\nsecret/ vault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9c6d82c2 per-token private secret storage identity/ identity identity_8feb8f49 identity store secret/ kv kv_6f946f62 key/value secret storage sys/ system system_b45bc416 system endpoints used for control, policy and debugging 然後在 path=kv 下，啟用一個 secret engine\nvault secrets enable -path=kv kv Success! Enabled the kv secrets engine at: kv/ 秘密引擎啟用的路徑默認為秘密引擎的名稱。因此，以下命令等效於執行上面的命令。\nvault secrets enable kv 重複執行這個命令會拋出“路徑已在kv/ 中使用”錯誤。\n為了驗證我們的成功並獲取有關秘密引擎的更多信息，使用以下的 vault secrets list 命令：\nvault secrets list Path Type Accessor Description ---- ---- -------- ----------- cubbyhole/ cubbyhole cubbyhole_9c6d82c2 per-token private secret storage identity/ identity identity_8feb8f49 identity store kv/ kv kv_2b6528af n/a secret/ kv kv_6f946f62 key/value secret storage sys/ system system_b45bc416 system endpoints used for control, policy and debugging 這顯示了在這個Vault伺服器上已啟用的5個秘密引擎。\n你可以看到秘密引擎的類型、相應的路徑以及可選的描述（如果沒有提供描述，則為“n/a”）。\n使用帶有 -detailed 標誌運行上述命令可以顯示KV秘密引擎的版本和更多信息。\nvault secrets list --detailed Path Plugin Accessor Default TTL Max TTL Force No Cache Replication Seal Wrap External Entropy Access Options Description UUID Version Running Version Running SHA256 Deprecation Status ---- ------ -------- ----------- ------- -------------- ----------- --------- ----------------------- ------- ----------- ---- ------- --------------- -------------- ------------------ cubbyhole/ cubbyhole cubbyhole_9c6d82c2 n/a n/a false local false false map[] per-token private secret storage 6087f484-a02c-f36c-0a1a-aa07840f988c n/a v1.14.3+builtin.vault n/a n/a identity/ identity identity_8feb8f49 system system false replicated false false map[] identity store c3a1e4ae-09c6-29b9-906a-8281b46690f3 n/a v1.14.3+builtin.vault n/a n/a kv/ kv kv_2b6528af system system false replicated false false map[] n/a 5ff9bc4c-6d2c-5fd7-cbe0-e9b7fbf03ee5 n/a v0.15.0+builtin n/a supported secret/ kv kv_6f946f62 system system false replicated false false map[version:2] key/value secret storage ed28307e-0472-c0a7-b7a9-65543a63e68c n/a v0.15.0+builtin n/a supported sys/ system system_b45bc416 n/a n/a false replicated true false map[] system endpoints used for control, policy and debugging 9712d56d-95e3-6c07-796f-d44565de5c07 n/a v1.14.3+builtin.vault n/a n/a sys/ 路徑對應到系統後端。這些路徑與Vault的核心系統交互，對於初學者來說不是必需的。\n建立 Secret 要創建私鑰，使用 kv put 命令。\nvault kv put kv/hello target=world Success! Data written to: kv/hello 要讀取存儲在kv/hello路徑中的私鑰，使用 kv get 命令。\nvault kv get kv/hello ===== Data ===== Key Value --- ----- target world 嘗試建立第二個 kv/my-secret\nvault kv put kv/my-secret value=\u0026#34;s3c(eT\u0026#34; Success! Data written to: kv/my-secret 讀取位於 kv/my-secret 的資料\nvault kv get kv/my-secret ==== Data ==== Key Value --- ----- value s3c(eT 刪除位於 kv/my-secret 的資料\nvault kv delete kv/my-secret Success! Data deleted (if it existed) at: kv/my-secret 列出位於 kv/my-secret 的資料\nvault kv list kv/ Keys ---- hello 停用秘密引擎 當不再需要秘密引擎時，可以將其停用。\n當停用秘密引擎時，所有私鑰都將被撤銷，相應的Vault數據和配置將被刪除。\nvault secrets disable kv/ Success! Disabled the secrets engine (if it existed) at: kv/ 請注意，此命令將路徑作為參數，而不是秘密引擎的類型。\n對原始路徑的任何數據路由請求都將導致錯誤，但現在可以在該路徑啟用另一個秘密引擎。\nvault kv get kv/hello Error making API request. URL: GET http://127.0.0.1:8200/v1/sys/internal/ui/mounts/kv/hello Code: 403. Errors: * preflight capability check returned 403, please ensure client\u0026#39;s policies grant access to path \u0026#34;kv/hello/\u0026#34; 秘密引擎是一個抽象 Vault的行為類似於虛擬文件系統。讀取/寫入/刪除/列出操作將轉發到相應的秘密引擎，秘密引擎決定如何對這些操作做出反應。\n這種抽象非常強大。它使Vault能夠直接與物理系統、資料庫、HSM 等進行交互。\n但除了這些物理系統外，Vault還可以與更多獨特的環境進行交互，比如AWS IAM、動態SQL …","date":1694616146,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695049428,"objectID":"c63a0db7bb8cfcf7073b7e8d60e8c068","permalink":"https://chechia.net/zh-hant/post/2023-09-14-vault-workshop-secret-engine/","publishdate":"2023-09-13T22:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-14-vault-workshop-secret-engine/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\n整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\nDay 03：細探 Secret Engine 秘密引擎 什麼是秘密引擎？ 秘密引擎是Vault的組件，用於存儲、生成或加密秘密。在前面的內容中，你使用了Key/Value v2 秘密引擎來存儲數據。一些秘密引擎，比如鍵/值秘密引擎，僅僅是用來存儲和讀取數據的。其他秘密引擎則連接到其他服務並根據需求生成動態憑證。還有一些秘密引擎提供加密作為服務。\n前面的內容中，默認情況下，key/value v2 秘密引擎已啟用，並準備在 secret/ 下接收請求，因為我們在-dev 模式下啟動了Vault Server。\n在底下我們使用 kv v1 做簡單的範例。","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 03: Secret Engine","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 02 準備：執行一個本地開發用途的 Vault 整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\n準備工作 在開始進行 Vault 30 天 workshop 之前，有一些準備工作需要完成：\n步驟1：下載 Vault Binary\n首先，你需要下載HashiCorp Vault的最新版本。你可以在HashiCorp的官方網站上找到Vault的下載鏈接。請確保下載適用於你操作系統的版本。\nhttps://developer.hashicorp.com/vault/downloads\nMacOS amd64 MacOS arm64 Linux 步驟2：安裝Vault\n下載完成後，根據你的操作系統進行安裝。對於大多數Linux和Unix系統，你c可以通過解壓縮壓縮文件來安裝Vault。將Vault Binary文件移至你的PATH中，以便在終端中輕鬆訪問\n筆者使用的是 Mac M1，使用以下指令進行安裝\nwget https://releases.hashicorp.com/vault/1.14.3/vault_1.14.3_darwin_arm64.zip unzip vault_1.14.3_darwin_arm64.zip sudo mv vault /usr/local/bin/vault Vault v1.14.3 (56debfa71653e72433345f23cd26276bc90629ce), built 2023-09-11T21:23:55Z 你不需要使用最新版本的 Vault 也可以完成這次 workshop\n啟動 Vault server 使用以下指令在本地啟動 Vault Server。請確保 Vault 伺服器已正確啟動，並且未出現錯誤消息。\nvault server -dev 使用 -dev 選項可以啟動Vault的開發模式，該模式不需要身份驗證即可運行Vault。\n確認Vault運行 打開你的瀏覽器，瀏覽到 http://127.0.0.1:8200\n或者使用 curl 命令發送 GET 請求：\ncurl http://127.0.0.1:8200/v1/sys/health | jq { \u0026#34;initialized\u0026#34;: true, \u0026#34;sealed\u0026#34;: false, \u0026#34;standby\u0026#34;: false, \u0026#34;performance_standby\u0026#34;: false, \u0026#34;replication_performance_mode\u0026#34;: \u0026#34;disabled\u0026#34;, \u0026#34;replication_dr_mode\u0026#34;: \u0026#34;disabled\u0026#34;, \u0026#34;server_time_utc\u0026#34;: 1694693898, \u0026#34;version\u0026#34;: \u0026#34;1.14.3\u0026#34;, \u0026#34;cluster_name\u0026#34;: \u0026#34;vault-cluster-3273d150\u0026#34;, \u0026#34;cluster_id\u0026#34;: \u0026#34;a32d555e-3346-0757-1d57-21e2f646aaf3\u0026#34; } 如果Vault正常運行，你應該會收到一個包含Vault訊息的JSON response。\n也可使用 Vault Binary 來存取 Vault API\nexport Vault Server 的 http endpoint (我們尚未啟動 tls listener，所以沒有 https endpoint)\nexport VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; vault status Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 1 Threshold 1 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem Cluster Name vault-cluster-3273d150 Cluster ID a32d555e-3346-0757-1d57-21e2f646aaf3 HA Enabled false 可以看到本地的 Vault server\n已初始化 已解封 版本 未啟用 HA 也可以使用 Vault Binary 中的 operator 指令檢測\nvault operator members Host Name API Address Cluster Address Active Node Version Upgrade Version Redundancy Zone Last Echo --------- ----------- --------------- ----------- ------- --------------- --------------- --------- CheChias-MacBook-Pro.local http://127.0.0.1:8200 https://127.0.0.1:8201 true 1.14.3 n/a n/a n/a 初始化Vault 在開發模式下運行的Vault可能不需要初始化，但如果你使用的是 production 模式，請參考Vault的官方文檔，進行初始化和設置。\nVault server 在 dev 模式下啟動後，會自動產生一隻單一個 unseal key，與一把 Root Token。\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variables: $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: JbKrVzhH4VO14nPBRvWE1qxJz9CVhnOFJT0pYLsx9LU= Root Token: hvs.0pmmzGSZ8S7w8vP2E4sm7SqE Development mode should NOT be used in production installations! 你可以使用 vault operator 指令，嘗試 seal Vault server\nvault operator seal Success! Vault is sealed. 再次檢查 status\nvault status Key Value --- ----- Seal Type shamir Initialized true Sealed true Total Shares 1 Threshold 1 Unseal Progress 0/1 Unseal Nonce n/a Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem HA Enabled false 顯示為密鑰密封（Sealed）狀態\n你可以使用 vault operator 解封 Vault\nvault operator unseal JbKrVzhH4VO14nPBRvWE1qxJz9CVhnOFJT0pYLsx9LU= Key Value --- ----- Seal Type shamir Initialized true Sealed false Total Shares 1 Threshold 1 Version 1.14.3 Build Date 2023-09-11T21:23:55Z Storage Type inmem Cluster Name vault-cluster-3273d150 Cluster ID a32d555e-3346-0757-1d57-21e2f646aaf3 HA Enabled false 顯示為解封狀態\n密鑰安全 本 workshop 中使用的 vault server / seal key / root token 都是測試用途的暫時性存在，筆者寫完文章後，這些 server 與密鑰早已經清理完畢，所以沒有資安風險。\n為了示範方便，workshop 會以明碼 plain text 形式表現\n如果你是在公司的 production 環境使用，務必要確保密鑰的安全\n不要在 local 電腦上存放 token 或 seal key 不要在沒有密碼學與資安檢測的平台上儲存 token 或 seal key 不要透過通訊軟體(ex. slack) 明碼傳送 token 或 seal key … 你可能會問，本地不能存，遠端也不能存，也不能隨意傳送給同事，那應該如何協作與管理？\n在後面的 workshop 內容會與大家一一示範解說\nDev 模式 你可以關閉執行 vault server 的 terminal session 來關閉 vault server\nvault server -dev ctrl + c https://developer.hashicorp.com/vault/docs/concepts/dev-server\n“Dev” 伺服器模式\ndev 模式的伺服器不需要進一步的設定，且你本地的 vault CLI 已經被認證。這使得使用 Vault 或啟動用於開發的 Vault instance 變得容易。Vault 的每一個功能在 “dev” 模式中都是可用的。-dev flag 只是將很多設置簡化為不安全的默認值。\n警告：永遠、永遠、永遠不要在生產環境中運行 “dev” 模式伺服器。這是不安全的，且每次重新啟動都會丟失資料（因為它將資料存儲在記憶體中）。它僅供開發或實驗使用。\ndev 模式特性 dev 伺服器的屬性（有些可以使用命令行 flag 或指定配置文件來覆寫）：\n初始化且未封印: 伺服器將自動初始化且未封印。你不需要使用 vault operator unseal。它立即可用。\n記憶體存儲: 所有資料（加密後）都存儲在記憶體中。Vault 伺服器不需要任何檔案權限。\n綁定到本地地址而不使用 TLS: 伺服器正在監聽 127.0.0.1:8200（默認的伺服器地址）而不使用 TLS。\n自動認證: 伺服器存儲了你的 root access token，所以 vault CLI 存取已經準備好。如果你透過 API 存取 Vault，你需要使用打印出的 token 進行認證。\n單一解封鑰匙: 伺服器使用單一的 unseal key 進行初始化。Vault 已經是解封的，但如果你想嘗試 seal/unseal，那麼只需要輸出的單一鑰匙。\n安裝 key value storage: 在 secret/ 處安裝了一個 v2 KV secret engine。請注意 v1 KV 有所不同。如果你想使用 v1，使用此旗標 -dev-kv-v1。\n第一次存取 vault 資料 建立通用 Key-Value 存儲： 使用以下命令在 Vault 中創建通用的 Key-Value 存儲。在這個例子中， …","date":1694612546,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694876681,"objectID":"ad2a1383e84754656adf59306e7b8928","permalink":"https://chechia.net/zh-hant/post/2023-09-13-vault-workshop-get-started/","publishdate":"2023-09-13T21:42:26+08:00","relpermalink":"/zh-hant/post/2023-09-13-vault-workshop-get-started/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 02 準備：執行一個本地開發用途的 Vault 整篇 Workshop 會使用的範例與原始碼，放在 Github Repository: vault-playground\n準備工作 在開始進行 Vault 30 天 workshop 之前，有一些準備工作需要完成：\n步驟1：下載 Vault Binary\n首先，你需要下載HashiCorp Vault的最新版本。你可以在HashiCorp的官方網站上找到Vault的下載鏈接。請確保下載適用於你操作系統的版本。\nhttps://developer.hashicorp.com/vault/downloads\nMacOS amd64 MacOS arm64 Linux 步驟2：安裝Vault","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 02: Get Started","type":"post"},{"authors":[],"categories":["vault"],"content":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 01 前言：從零開始的 Hashicorp Vault Workshop 從零開始 workshop 系列已經做了四年，內容包含 k8s, terraform, aws 等。\n深深覺得要學習一個工具，還是要動手做。\n所以有了 30 天手把手 workshop 系列文，讓有興趣接觸的朋友，能以相對低的門檻入門。\n關於內容\n無基礎的手把手的基礎教學 完整的範例，提供原始程式碼，也提供 production 的經驗與範例 官方最新文件繁中翻譯 (chatGPT based) 建議讀者務必跟著操作，不要只是看過\n其他文章 https://chechia.net/\n過去的 Workshop\n2022 鐵人賽: Terraform IaC Best Practice on AWS Cloud / 在 aws 公有雲上找 IaC 最佳實踐 (因故退賽) 2021 鐵人賽: Terraform Workshop - Infrastructure as Code for Public Cloud 2020 鐵人賽: Kubernetes X DevOps X 從零開始導入工具 X 需求分析 2019 鐵人賽: Kubernetes 預定內容與許願清單 本 workshop 預計有底下內容\nDay 01 前言：從零開始的 Hashicorp Vault Workshop Day 02 準備：執行一個本地開發用途的 Vault Day 03：細探 Secret Engine 秘密引擎 Day 04：Secret Engine KV V2 Day 05：Authentication Day 06: Github auth method Day 07: Policy Day 08: Docker and Initialization 本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDynamic Secrets tls certificate management configuration docker initialization ha storage backend filesystem postgresql Infrastructure as Code for Vault deploy policy auth method Vault Kuberntes integration Azure integration AWS integration token in detail 關於翻譯工具 chatGPT 本系列文章大量使用 chatGPT 翻譯官方文章。目的是使用最新版官方文件內容，提供第一手且精準的資料給讀者，但又能降低非母語讀者得學習門檻。\n有翻譯的段落都會標示出處，著作權皆屬於原出處所有。\n本系列文章以分享資訊，貢獻社群，提高國內整體技術能力為目的，並不用於商業用途。\n使用翻譯工具並不代表作者（譯者）沒有付出相當時間心力，包含規劃文章大綱，測試 prompt engineering，調整參數，並人工校正產生的內容。\n本系列作品在於教導讀者使用工具 Hashicorp Vault，而 chatGPT 等大語言模型工具就是近年最值得學習的工具。如果讀者還不會使用 chatGPT，本系列文章都附上 Prompt 提示詞，可以參考，學習，並自由使用。也許學會使用 chatGPT 所帶來的價值，會比學習 Vault 帶來的還多。\n很重要所以再說一遍，chatGPT 是近年最值得學習的工具，沒有之一。\nhttps://github.com/f/awesome-chatgpt-prompts/blob/main/prompts.csv What is Vault? 本段內容使用 chatGPT-3.5 翻譯 https://developer.hashicorp.com/vault/docs/what-is-vault 內容，並由筆者人工校驗\nbase context\n我希望你能充當一名繁體中文翻譯，拼寫修正者和改進者。我將用英文與程式語言與你對話，你將翻譯它，並以已糾正且改進的版本回答，以繁體中文表達。我希望你能用更美麗和優雅、高級的繁體中文詞語和句子替換我簡化的詞語和句子。保持意義不變。我只希望你回答糾正和改進，不要寫解釋。不要使用敬語，請用你取代您。 result correction\n部分英文內容為專有名詞，產生的繁體中文翻譯結果中，這些名詞維持英文，不需要翻譯成中文：key，certificate，token，policy，policy rule，path，path-based，key rolling，audit，audit trail，plain text，key value，Consul，S3 bucket，Leasing，Renewal，binary 修正下列翻譯：秘密改為私鑰，數據改為資料，數據庫改為資料庫，數據改為資料，訪問改為存取，源代碼改為原始碼。 What is Vault? HashiCorp Vault 是一套基於身份的私鑰和加密管理系統。所謂的\u0026#34;私鑰\u0026#34;，是你希望嚴格控制存取的資訊，例如 API key、密碼和certificate。Vault 提供加密服務，並透過認證和授權方法進行管控。使用 Vault 的使用者介面、命令列介面或 HTTP API，可以安全地儲存和管理私鑰及其他敏感資料，並能嚴格控制（限制）其存取權，並可進行 audit。\n現代系統需要存取大量的私鑰，包括資料庫憑證、外部服務的 API key、面向服務的架構通訊憑證等。了解誰正在存取哪些私鑰可能相當困難，尤其當這種存取可能因平台而異。而在此基礎上加入key rolling、安全儲存和詳細的audit trail，若無自訂的解決方案幾乎是不可能的。這正是 Vault 發揮作用的地方。\nVault 會驗證並授權客戶端（用戶、機器、應用程序）在提供他們存取秘密或儲存的敏感資料之前。\nVault 如何運作： Vault 主要使用 token，且 token 與客戶端的 policy 相關聯。每一 policy 都是基於 path-based 的，policy rule 限制每個客戶端對於每一 path 的行動和存取能力。使用 Vault，你可以手動建立 token 並指派給你的客戶端，或客戶端可以登入並獲得 token。下面的插圖顯示了 Vault 的核心工作流程。\nVault 工作流程 Vault 的核心工作流程包含四個階段：\n認證：在 Vault 中的認證是指客戶端提供資訊，Vault 使用此資訊來判定他們是否是他們所聲稱的那個人。一旦客戶端根據某種認證方式成功認證，將生成一個與策略相關聯的 token。\n驗證：Vault 根據第三方受信賴的來源，例如 Github、LDAP、AppRole 等，對客戶端進行驗證。\n授權：客戶端根據 Vault 安全策略進行匹配。此策略是一套規則集，定義客戶端使用其 Vault token 可存取哪些 API 端點。策略提供了一種宣告方式來授予或禁止存取 Vault 中的特定路徑和操作。\n存取：根據與客戶端身份相關聯的策略，Vault 通過發出 token 來授予存取秘密、金鑰和加密能力的權限。然後，客戶端可以使用他們的 Vault token 進行未來的操作。\n為什麼選擇 Vault？ 現今的大多數企業都有憑證散布在其組織中。密碼、API 金鑰和憑證存儲在明文中、應用源代碼、配置文件和其他位置。由於這些憑證存在於各處，因此可能很難真正知道誰有存取和授權權限。明文中的憑證也增加了內部和外部攻擊者進行惡意攻擊的可能性。\n考慮到這些挑戰，Vault 被設計出來。Vault 採取所有這些憑證並集中管理，這樣他們就只在一個位置定義，從而減少了憑證的不必要暴露。但 Vault 更進一步，確保用戶、應用程序和系統都被認證並明確授權存取資源，同時也提供一個審計跟踪，捕捉並保留客戶端操作的歷史記錄。\nVault 的主要功能包括 安全的秘密存儲：可以在 Vault 中存儲任意的金鑰/值秘密。Vault 在將這些秘密寫入持久性存儲之前會對其進行加密，因此獲取原始存儲並不足以存取你的秘密。Vault 可以寫入磁盤、Consul 等。\n動態秘密：Vault 可以為某些系統即時生成秘密，例如 AWS 或 SQL 數據庫。例如，當應用程序需要存取 S3 桶時，它會要求 Vault 提供憑證，Vault 將即時生成具有有效權限的 AWS 密鑰對。在創建這些動態秘密後，Vault 也會在租期到期後自動撤銷它們。\n數據加密：Vault 可以在不存儲數據的情況下加密和解密數據。這允許安全團隊定義加密參數，並允許開發人員將加密數據存儲在如 SQL 數據庫之類的地方，而不必設計他們自己的加密方法。\n租賃和續期：Vault 中的所有秘密都有一個與之相關的租期。在租期結束時，Vault 會自動撤銷該秘密。客戶端可以通過內置的續期 API 來續租。\n撤銷：Vault 具有內置的秘密撤銷支援。Vault 不僅可以撤銷單一的秘密，還可以撤銷秘密樹，例如由特定用戶讀取的所有秘密，或特定類型的所有秘密。撤銷在金鑰滾動以及在入侵情況下鎖定系統時都很有幫助。\n什麼是 HCP Vault？ HashiCorp Cloud Platform (HCP) Vault 是 Vault 的託管版本，由 HashiCorp 運營，使組織能夠快速啟動並運行。HCP Vault 使用與自託管的 Vault 相同的二進制文件，這意味著你將擁有一致的用戶體驗。你可以使用相同的 Vault 客戶端與 HCP Vault 通信，就像你使用自託管的 Vault 一樣。請參考 HCP Vault 文檔以獲得更多資訊。\n","date":1694522119,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1695224937,"objectID":"65feca0329c2cf52a51a072f2efd39dd","permalink":"https://chechia.net/zh-hant/post/2023-09-12-vault-workshop-introduction/","publishdate":"2023-09-12T20:35:19+08:00","relpermalink":"/zh-hant/post/2023-09-12-vault-workshop-introduction/","section":"post","summary":"如果你希望追蹤最新的草稿，請見鐵人賽2023\n本 workshop 也接受網友的許願清單，如果有興趣的題目可於第一篇底下留言，筆者會盡力實現，但不做任何保證\nDay 01 前言：從零開始的 Hashicorp Vault Workshop 從零開始 workshop 系列已經做了四年，內容包含 k8s, terraform, aws 等。\n深深覺得要學習一個工具，還是要動手做。\n所以有了 30 天手把手 workshop 系列文，讓有興趣接觸的朋友，能以相對低的門檻入門。\n關於內容\n無基礎的手把手的基礎教學 完整的範例，提供原始程式碼，也提供 production 的經驗與範例 官方最新文件繁中翻譯 (chatGPT based) 建議讀者務必跟著操作，不要只是看過","tags":["vault","iac","workshop","terraform","鐵人賽2023","chatgpt"],"title":"Vault Workshop 01: Introduction","type":"post"},{"authors":null,"categories":null,"content":"","date":1694304000,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694692432,"objectID":"fb93c27fe10b079a575ad02284ccff89","permalink":"https://chechia.net/zh-hant/project/vault-playground/","publishdate":"2023-09-10T00:00:00Z","relpermalink":"/zh-hant/project/vault-playground/","section":"project","summary":"","tags":["鐵人賽2023","iac","vault","terraform"],"title":"從零開始的 30 天 Hashicorp Vault Workshop","type":"project"},{"authors":[],"categories":["kubernetes","vault"],"content":"Info 現代網路應用需要處理許多私密金鑰的管理，例如：user 的密碼、server 的資料、database 的資料、microservices 彼此 authentication…。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。 如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。\nHashiCorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，還有許多特點如：\n不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統 server/Kubernetes/VM 支援跨環境的應用，可以串連混合雲中的應用，作為私鑰認證的中心\nTarget group 本次活動將簡介 Hashicorp Vault，並以 AWS Cloud 與本地 Kubernetes 為例，提供幾個基本的操作範例，適合初次接觸 HashiCorp Vault 與尋找私鑰管理平台的團隊。\nAuthor Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2023 DevOpsDay 2023 Ithome Kubernetes Summit 2022 COSCUP 2022 Ithome Cloud Summit 2021 Ithome Cloud Summit 2020 DevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2020 Cloud Native Taiwan 年末聚會 2020 Ithome Cloud Summit 2019 Ithome Cloud Summit 2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit ","date":1683716400,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1683716400,"objectID":"d209db151869cc26e808a5bd6d0ee9e9","permalink":"https://chechia.net/zh-hant/talk/hashicorp-vault-on-aws-k8s-%E9%9B%B2%E7%AB%AF%E5%9C%B0%E7%AB%AF%E9%80%9A%E5%90%83%E7%9A%84%E7%A7%81%E9%91%B0%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/zh-hant/talk/hashicorp-vault-on-aws-k8s-%E9%9B%B2%E7%AB%AF%E5%9C%B0%E7%AB%AF%E9%80%9A%E5%90%83%E7%9A%84%E7%A7%81%E9%91%B0%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0/","section":"event","summary":"HashiCorp Vault 為一款開源的私密資料管理平台。本次活動將簡介 Hashicorp Vault，並以 AWS Cloud 與本地 Kubernetes 為例，提供幾個基本的操作範例，適合初次接觸 HashiCorp Vault 與尋找私鑰管理平台的團隊。","tags":["iac","aws","terraform","kubernetes","vault"],"title":"HashiCorp Vault on AWS \u0026 K8s - 雲端地端通吃的私鑰管理平台","type":"event"},{"authors":null,"categories":["vault"],"content":"target group 金融客戶 vault 4/15 上架\naccupass 5/10 (Wed) 11:00-12:00\nwebbase link (online)\n10:30 上線設備測試\n11:00 Ming 開場\n11:05 主講\n12:00 Q\u0026amp;A (留言)\n當天錄影會上線\n演講大綱 基本介紹 vault 架構 企業需求 self-host 複雜的 policy 用例 demo aws auth k8s auth policy 當天提供 github example Q\u0026amp;A (10mins) 內容 演講主題：Hashicorp vault 雲端地端通吃的私鑰管理平台\n現代網路應用需要處理許多私密金曜的管理，例如：user 的密碼，server 的資料，database 的資料，microservices 彼此 authentication…。加上駭客團體猖獗，許多國內外知名企業紛紛遭駭，導致公司與使用者的損失。 如何系統化且自動化管理大量的私密資料，成為系統整體安全性的關鍵。 Hashicorp Vault 為一款開源的私密資料管理平台，除了保障系統安全性，比起市面上的其他管理工具，有許多特點\n不依賴外部服務，適合自行架設在內部公有雲/私有雲/傳統server/Kubernetes/VM 支援跨環境的應用，可以串連混合雲中的應用，作為私要認證的中心 本次演講簡介 Hashicorp Vault，以 aws cloud 與本地 kubernetes 為例，提供幾個基本的操作範例 適合初次接觸的 Hashicorp Vault，與尋找私要管理平台的團隊 講者簡介\nChe-Chia Chang，SRE，喜歡研究公有雲/容器化應用/Kubernetes Microsoft MVP，Ithome 雲端大會/COSCUP講師，常出現 CNTUG / DevOpsTW / Golang Taipei 技術 blog 收錄過往演講與文章 https://chechia.net\nPresentation https://docs.google.com/presentation/d/1iex9lm89OCIR8IAoD1RPe4vcW--bcKBmMHoixDybqP8/edit#slide=id.g2403737215e_0_147\n","date":1683692342,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"28f24b96533b13a26af4fb714b6facd2","permalink":"https://chechia.net/zh-hant/post/2023-05-10-hashicorp-comminity-presentation-vault-introduction/","publishdate":"2023-05-10T12:19:02+08:00","relpermalink":"/zh-hant/post/2023-05-10-hashicorp-comminity-presentation-vault-introduction/","section":"post","summary":"target group 金融客戶 vault 4/15 上架\naccupass 5/10 (Wed) 11:00-12:00\nwebbase link (online)\n10:30 上線設備測試\n11:00 Ming 開場\n11:05 主講\n12:00 Q\u0026A (留言)\n當天錄影會上線\n演講大綱 基本介紹 vault 架構 企業需求 self-host 複雜的 policy 用例 demo aws auth k8s auth policy 當天提供 github example Q\u0026A (10mins) 內容 演講主題：Hashicorp vault 雲端地端通吃的私鑰管理平台","tags":["vault","iac","aws"],"title":"Hashicorp Comminity Presentation Vault Introduction","type":"post"},{"authors":null,"categories":["terraform"],"content":"AWS cross account delegation 依舊 WIP，今天延續昨天內容，使用 github action 做 terraform module 的 CI/CD\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nTerraform Github Action Jobs 我們可以在 terraform repository 中增加 tool checks 到 CI/CD 中，加強 code 的品質控管\nterragrunt-infrastructure-modules PR 在此\n這個 PR 包含幾個 Github Action Workflow\nls .github/workflows checkov.yaml format.yaml plan.yaml security-scan.yaml validate.yaml format.yaml 中執行 terraform fmt -recursive，需求與目的已在昨天說明 validate.yaml 中執行 terraform validate，用來驗證 module 內的 terraform syntax 是否符合語法 checkov.yaml 是多語言的 policy as code 工具，在這邊執行掃描 terraform code 的安全性與 CVEs 檢查 security-scan.yaml 中也是 Policy As Code 使用 tfsec 工具掃描 plan.yaml 中在 pipeline 執行自動化 terraform plan 由於 plan 需要存取 state 與 provider API，設定上有許多權限設定需要開啟，目前是一個 dummy workflow policy as code 內容可以將一篇演講，有興趣請見底下投影片: 2022 DevOpsDay: Policy As Code for Terraform: https://docs.google.com/presentation/d/1yawazO1B_sP5Yiav-XLGJXW3ZS2JTV0wGuJwhrUKQ3A\n對於 Policy as Code 有興趣的朋友請見今年 DevOpsDay 的演講 https://devopsdays.tw/session-page/1146\nTODO 與進度 root 中設定 IAM User aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role cross account iam role iam_cross_account_roles TODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy Optional: Cloudtrail Optional: terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role ","date":1664109543,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"dd2f2867b25fc954a3882c597084ebb4","permalink":"https://chechia.net/zh-hant/post/2022-09-25-14th-ithome-ironman-iac-aws-workshop-11-aws-github-action-ci-cd/","publishdate":"2022-09-25T20:39:03+08:00","relpermalink":"/zh-hant/post/2022-09-25-14th-ithome-ironman-iac-aws-workshop-11-aws-github-action-ci-cd/","section":"post","summary":"AWS cross account delegation 依舊 WIP，今天延續昨天內容，使用 github action 做 terraform module 的 CI/CD\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nTerraform Github Action Jobs 我們可以在 terraform repository 中增加 tool checks 到 CI/CD 中，加強 code 的品質控管","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Terraform Github Action CI/CD","type":"post"},{"authors":null,"categories":["terraform"],"content":"TODO 與進度 root 中設定 IAM User aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM role 允許 security assume dev IAM role iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nAWS cross account with iam roles 要做跨 AWS account 的 IAM Roles access control，我們先看官方文件理解這個功能\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\n回憶我們在 day-04 提到的 multi-accounts 架構\naccount/security 有所有 iam-users 其他 child-account ex. account/test 中會有 iam-roles 需求：security/iam-user 可以使用 test/iam-role 的身份，存取 test 底下的 resource ex.test/ec2 好處 admin 不需要到每一個 child-account 下面去開每一個人的 iam user，統一到 account/security 管理 user 只要登入一個 security 帳號，就可以控制多個 child-account，不用登出又登入不同 account 在 aws official doc 上，也是舉幾個 account 作為 cross account delegation 的範例。由於本 workshop 已經有現存的 child-account，我們會直接使用 workshop 的 accounts 做說明。完成這次 iam role delegation 的設定後，我們會得到以下成果\naccount/security 下面 IAM User 可以 assume 到 account/test 下面的特定的 IAM role account/test 下面有一個 IAM role 可以存取 S3 Bucket 工程師可以使用 web console 登入 security/user，然後 assume role 為 test/role，取得查看 s3 bucket 的權限 工程師也可使用以 security/user 的身份， call aws API 取得 test/role 的暫時的 credential 步驟 我們會先過一次 aws 官方文件上所述的步驟，讓大家了解整個設定步驟，弄清整個 delegation 的流程與概念。之後會轉而使用 Terraform 設定所有的元件。\n首先要在 test (child-account) 設定 iam role 將 account/security 作為 trusted entity 設定 role 的 policy，增加可以存取 s3 bucket 的權限給 test/role 調整 iam role，可以設定需要給予權限，或是 deny 某些權限 最後做測試，是否可以完成 switch role AWS 的範例使用 aws web console 做範例，這個 workshop 後面我們會使用 terraform 來實作。\n外出取材 上面的範例還在 WIP，請當作者外出取材一天，今天先講另外一個工具\nTerraform fmt \u0026amp; lint 要提升 terraform code 品質，有許多工具非常值得在 CI/CD 過程中使用\n例如 terraform 內建的 fmt 與 terragrunt 內建的 hclfmt\n在 module 的 repository 中會需要跑 terraform fmt，確保每個 module release 出去前都有經過 fmt 在使用 terragrunt 的 root module 會需要 hclfmt .hcl 檔案 cd terragrunt-infrastructure-modules terraform fmt -recursive cd terragrunt-infrastructure-live-example terragrunt hclfmt fmt 對於程式碼的品質是基礎但十分重要的\n沒有固定 format 的程式碼會造成 git 使用出現過多 diff，造成團隊協作的困難 format 也會影響自動化，例如 templating / variable expension fmt / lint 是我們第一個帶入 CI/CD 的工具\n手動 fmt fmt 一下\ncd terragrunt-infrastructure-modules terraform fmt -recursive cd terragrunt-infrastructure-live-example terragrunt hclfmt commit 如下\nterragrunt-infrastructure-modules commit terragrunt-infrastructure-live-example Git precommit hook 既然是 code 品質的基礎，應該每次 commit 之前都觸發檢查，這個階段適合使用 git pre-commit hook 前執行\nhttps://github.com/antonbabenko/pre-commit-terraform\n以 terragrunt-infrastructure-live-example 為例\nPR 在此 啟用前需要 install remote script 到本地 git add 後，使用 run 來手動觸發 pre-commit hook 會根據 commit hook 執行腳本，以這邊的範例會執行 id: terraform-fmt id: terraform-validate id: tflint id: terraform_checkov id: terrascan id: terraform_tfsec id: infracost_breakdown 除了 fmt 有說明過以外，其他的 tool 我們後續再說明 pre-commit install pre-commit install pre-commit installed at .git/hooks/pre-commit pre-commit run 如果通過 run 測試，就可以進行 git commit commit 之前會在跑一次 script，所以稱作 pre-commit hook 如果通過測試變化自動 commit 如果沒有通過，則退回這次 commit 如果想要跳過 pre-commit check，可以使用 git commit –no-verify 通過上述步驟，來確保工程師在本地發出的 commit 有經過基礎的驗證，非常值得團隊導入\nGithub Action ","date":1663772141,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"8de2d22651c7a119d2c32206633f5cfb","permalink":"https://chechia.net/zh-hant/post/2022-09-21-14th-ithome-ironman-iac-aws-workshop-10-aws-cross-account-delegation-and-pre-commit-hook/","publishdate":"2022-09-21T22:55:41+08:00","relpermalink":"/zh-hant/post/2022-09-21-14th-ithome-ironman-iac-aws-workshop-10-aws-cross-account-delegation-and-pre-commit-hook/","section":"post","summary":"TODO 與進度 root 中設定 IAM User aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026 Group dev 中設定 IAM role 允許 security assume dev IAM role iThome 鐵人賽好讀版","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Aws Cross Account Delegation \u0026 pre-commit hook","type":"post"},{"authors":null,"categories":["terraform"],"content":"使用 aws module 的好處 為何許多開源的 terraform module 內部使用的都是其他的 module，而不是從 resource 單位開始？\nTerraform 官方文件，如何建立 module\n如何建立一個 module\n根據最常出現的使用情劇與需求 專注於業務的需求與抽象，雃後把實作(terraform resource) 在 module 中組合實作出來 module 也需要考慮 resource 之間的 architecture 也要考慮到觀禮是否方便？使用上是否安全？ ex. 我們需求是產生一個 IAM User\n正常的實作就是寫一個 resource.aws_iam_user 達成需求 更好的實作是：除了 iam_user 外，加上 一定會需要配權限，應搭配 iam_group + iam_policy pgp encrypt 讓資料更安全 password_policy 增加密碼安全 …等 比起單一一個 iam_user 思考的更全面，更接近最佳實踐 在這次的 Best Practice 追尋之旅中，我們會帶大家去看其他團隊所寫出的 terraform module\n目前已經看了 aws 的 terraform module gruntwork 的 terraform module design (我們沒看到 private module，只是跟隨文件自己刻) 這些有多年企業解決方案經驗的團隊，寫出來的 terraform module 都會考量許多 security 方面的問題 這也是我們前十篇都在專注的方向 一般來說，一個好 module 會帶來很多好處\n精簡 .tf code，透過 terraform function 與判斷式來產生 resource 整合不同 resource 使用時要輸入的 input 可以引導使用者的 architecture 設計 能符合需求，參數方便使用，內容邏輯清楚的 module 就是好 module\n然而要寫好一個 module 需要很多經驗，不僅要對 aws 元件，架構都很熟悉，還要考量管理與安全。我們有機會再來分享。\n為什麼要花這麼多時間講 account / iam / security 的基礎設定？ 因為人家寫出來的就是這麼的安全，開頭直接立於不被駭之地\n昨天使用 reset root IAM user 的密碼，並使用 pgp key 加密保護，今天要進一步強化 IAM 的安全性，包括\n強化 password policy 增加跨帳號 iam role 的 assume permission 增加 MFA 本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\npassword policy 密碼強度的重要性不需要再強調，強迫 user 使用高強度的密碼並且定期更新，才能將地安全風險 想要透過 web console 修改 password policy 的朋友可以看aws doc: setting account password policy。我們這邊會使用 terraform 配置\n修改 terragrunt-infrastructure-modules\n注意：昨天在 module.iam_user 設定的是 login profile 的 password，是管理員配給 user 的第一把 password，並且登入後必須更換密碼 今天要設置的是之後的重設密碼都要遵循的規範 如果想要強化密碼管理\n更長的密碼 更多限制 定期更新 首先先更改 terraform module，開啟 password policy\n增加 resource.aws_iam_account_password_policy 增加 input variable，將 variable 往從上層 input variable 接進來，讓我們可以在最上層調整 可以設定密碼壽命，default 90 天 可以設定最小密碼長度，default 32 字元 其他 policy 參數都寫死固定 需要數字，大小寫英文 允許使用者在密碼過期前重設密碼 密碼過期就鎖住帳號不給登入，要請 admin 來 reset # aws_iam_account_password_policy.tf resource \u0026#34;aws_iam_account_password_policy\u0026#34; \u0026#34;strict\u0026#34; { allow_users_to_change_password = true minimum_password_length = var.minimum_password_length hard_expiry = true max_password_age = var.max_password_age require_lowercase_characters = true require_numbers = true require_uppercase_characters = true require_symbols = true password_reuse_prevention = 0 } # variables.tf variable \u0026#34;minimum_password_length\u0026#34; { type = number description = \u0026#34;The number of days that an user password is valid.\u0026#34; default = 32 } variable \u0026#34;max_password_age\u0026#34; { type = number description = \u0026#34;Minimum length to require for user passwords.\u0026#34; default = 90 } 進行 terragrunt plan，沒問題的話就可以直接 apply\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_iam_account_password_policy.strict will be created + resource \u0026#34;aws_iam_account_password_policy\u0026#34; \u0026#34;strict\u0026#34; { + allow_users_to_change_password = true + expire_passwords = (known after apply) + hard_expiry = true + id = (known after apply) + max_password_age = 90 + minimum_password_length = 32 + password_reuse_prevention = 0 + require_lowercase_characters = true + require_numbers = true + require_symbols = true + require_uppercase_characters = true } Plan: 1 to add, 0 to change, 0 to destroy. aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt apply 注意更改密碼的副作用\n由於更改 password policy 是整個 account 通用的，所以也會影響到 administrator 的 password 雖然影響 password，但不影響 access key，所以 terraform 可以正常工作 Apply passsword policy 後，我們使用原先的密碼走 aws web console 登入看看\n可以使用現有密碼登入 嘗試從右上角 User -\u0026gt; security credentials -\u0026gt; change password，可以發現新的 password policy 已經更新了 我們便手動更改密碼，以符合新的 password policy 更改密碼完成記得存在密碼儲存器中 個人習慣耕買密碼完成後，都重新登入，再登入一次，確定密碼正確，權限也沒問題 MFA for me 接著我們可以為自己的 IAM User 啟用 MFA 裝置\n在右上角 User -\u0026gt; security credentials -\u0026gt; MFA\n點選 Assign MFA Device 選擇 virtual MFA Device，我們這邊示範使用 google authenticator app 畫面上出現 MFA key 的 QRcode，這個 QRcode == key，不能洩漏，離開這個畫面就不會再出現了 使用 google authenticator，新增一組 account，然後掃描 QRCode 底下填入新 account 產生的 6 位數字 token 等待 60 秒 底下填入第二組新 account 產生的 6 位數字 token，確定真的能夠取得正確的 token 之後每次登入都需要輸入 MFA NOTE: 這裡是 IAM User login 時需要輸入 MFA，我們之後會設定 child account 下 iam-role assume 時都需要輸入 MFA\n明天會前十篇的重點之一：cross account 的 iam role 配置\nTODO 與進度 透過 root account 設定一組 IAM User 透過 root account 設定多個 aws child accounts root 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key root account password policy aws cross account iam role delegation root account MFA policy (Optional) Cloudtrail (Optional) terraform aws config security 中設定 IAM User security 設定 password policy security 設定 MFA policy security 中設定 IAM Policy \u0026amp; Group dev 中設定 IAM …","date":1663689674,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"997aec17b48e979502eb3a0c1f01b6e8","permalink":"https://chechia.net/zh-hant/post/2022-09-20-14th-ithome-ironman-iac-aws-workshop-09-terraform-module-and-password-security/","publishdate":"2022-09-21T00:01:14+08:00","relpermalink":"/zh-hant/post/2022-09-20-14th-ithome-ironman-iac-aws-workshop-09-terraform-module-and-password-security/","section":"post","summary":"使用 aws module 的好處 為何許多開源的 terraform module 內部使用的都是其他的 module，而不是從 resource 單位開始？\nTerraform 官方文件，如何建立 module\n如何建立一個 module\n根據最常出現的使用情劇與需求 專注於業務的需求與抽象，雃後把實作(terraform resource) 在 module 中組合實作出來 module 也需要考慮 resource 之間的 architecture 也要考慮到觀禮是否方便？使用上是否安全？ ex. 我們需求是產生一個 IAM User","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Modules and password security","type":"post"},{"authors":null,"categories":["terraform"],"content":"昨天處理完 Accounting 的 reset password，今天要來 reset root account Administrator 的權限\n本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nReset IAM root user Administrator Password 依照上面的步驟，我們確定可以正常登入，於是可以來可以新建 Administrator 的 login profile\n由於 Administrator 會需要 access key，所以我們也把他設成 true # terragrunt.hcl users = { Administrator = { groups = [\u0026#34;full-access\u0026#34;] pgp_key = \u0026#34;keybase:chechiachang\u0026#34; create_login_profile = true create_access_keys = true }, Accounting = { groups = [\u0026#34;billing\u0026#34;] pgp_key = \u0026#34;keybase:chechiachang\u0026#34; create_login_profile = true create_access_keys = false # accounting always use web console, won\u0026#39;t use access key } } 記得更改 module/output.tf，增加 secret key 的 output，而不要只留在 terraform state 裡面\n# output.tf output \u0026#34;keybase_secret_key_decrypt_command\u0026#34; { description = \u0026#34;Decrypt access secret key command\u0026#34; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_secret_key_decrypt_command } } output \u0026#34;keybase_secret_key_pgp_message\u0026#34; { description = \u0026#34;Encrypted access secret key\u0026#34; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_secret_key_pgp_message } } 試著 plan 一下，預期\nAdministrator 會產生一組新的 login profile Administrator 會產生一組新的 access key aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0] will be created + resource \u0026#34;aws_iam_access_key\u0026#34; \u0026#34;this\u0026#34; { + create_date = (known after apply) + encrypted_secret = (known after apply) + encrypted_ses_smtp_password_v4 = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + pgp_key = \u0026#34;keybase:chechiachang\u0026#34; + secret = (sensitive value) + ses_smtp_password_v4 = (sensitive value) + status = \u0026#34;Active\u0026#34; + user = \u0026#34;Administrator\u0026#34; } # module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_user_login_profile.this[0] will be created + resource \u0026#34;aws_iam_user_login_profile\u0026#34; \u0026#34;this\u0026#34; { + encrypted_password = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 + password_reset_required = false + pgp_key = \u0026#34;keybase:chechiachang\u0026#34; + user = \u0026#34;Administrator\u0026#34; } Plan: 2 to add, 0 to change, 0 to destroy. Changes to Outputs: + iam_access_key_id = { + Accounting = \u0026#34;\u0026#34; + Administrator = null -\u0026gt; (known after apply) } ~ keybase_password_decrypt_command = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } ~ keybase_password_pgp_message = { ~ Administrator = null -\u0026gt; (known after apply) # (1 unchanged element hidden) } + keybase_secret_key_decrypt_command = { + Accounting = null + Administrator = (known after apply) } + keybase_secret_key_pgp_message = { + Accounting = null + Administrator = (known after apply) } 符合預期!! 可喜可賀\n但是我們已經有 access key 在使用了，我又不想要走 day 3 aws-vault add key 的步驟換一組新的，這時該怎辦\n沒錯，我們可以像 Day3 一樣，使用 import 匯入已經存在的 resource 到 address 裡面\ngoogle “terraform aws iam user access key”，找到 aws iam user access key 的說明頁面 address 是 module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0] access key ID 可以在密碼管理器查看 或是在 aws web console -\u0026gt; IAM -\u0026gt; User -\u0026gt; Administrator -\u0026gt; Security Credential -\u0026gt; Access Keys 查到 aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt import \u0026#39;module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0]\u0026#39; AKIA1234567890 module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0]: Importing from ID \u0026#34;AKIA2I2H7ERWKI4RIF4Z\u0026#34;... module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0]: Import prepared! Prepared aws_iam_access_key for import module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0]: Refreshing state... [id=AKIA2I2H7ERWKI4RIF4Z] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Releasing state lock. This may take a few moments... 順便 login profile 也 import 近來\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt import \u0026#39;module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_user_login_profile.this[0]\u0026#39; Administrator 再次 plan 就不需要新建一把 access key 了…吧？！\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create -/+ destroy and then create replacement Terraform will perform the following actions: # module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_access_key.this[0] must be replaced -/+ resource \u0026#34;aws_iam_access_key\u0026#34; \u0026#34;this\u0026#34; { ~ create_date = \u0026#34;2022-09-16T13:05:35Z\u0026#34; -\u0026gt; (known after apply) + encrypted_secret = (known after apply) + encrypted_ses_smtp_password_v4 = (known after apply) …","date":1663689407,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"2a383c951b1367a21cd12cdeb545d392","permalink":"https://chechia.net/zh-hant/post/2022-09-19-14th-ithome-ironman-iac-aws-workshop-08-reset-iam-user-administrator/","publishdate":"2022-09-20T23:56:47+08:00","relpermalink":"/zh-hant/post/2022-09-19-14th-ithome-ironman-iac-aws-workshop-08-reset-iam-user-administrator/","section":"post","summary":"昨天處理完 Accounting 的 reset password，今天要來 reset root account Administrator 的權限\n本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026 pgp key iThome 鐵人賽好讀版","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Reset Iam User Administrator","type":"post"},{"authors":null,"categories":["terraform"],"content":"昨天我們建立 IAM Group 與 policy，並說明 policy 管理原則。\n然而昨天最後創建 user 時，我們關閉了 create_login_profile = false 的選項 這是為了避免當前的登入機制被覆蓋掉，影響 root account Administrator 的使用 更改 login 方式，在某些極端的情形下，有可能讓 Administrator 自己覆蓋自己的 login 設定後，讓自己無法登入 如果你是管理員，在更改 admin 帳號的權限與登入設定時，一定要多加注意\n如果不幸改壞，無法登入，就只能再去找出 root account root user 帳號來解救了 如果是 root account root user 把自己的 login 改壞，就會很痛苦，要請 aws support 來救你 本日進度\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group reset root account IAM user login profile \u0026amp; pgp key iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nEnhance User Login Security Gruntwork Guide 在完成 group 與 policy 設定後，下一個需要做的是加強 user 登入的安全性\n需要做的事情有\n啟用 MFA policy 設定嚴格的 password policy 建立 login profile: password 後，使用各個成員的 gpg key 加密 (encrypt) password，只有成員自己可以解開自己的 password MFA Policy Multi-factor authentication 是常用的認證方式\n在使用 AWS API 的時候，除了帳號密碼 / access key 以外，需要第二層登入裝置做驗證 AWS 支援許多不同形式的 MFA 裝置 虛擬的 MFA 裝置 ex. 跟 google authenticator 一起使用 硬體的 MFA 裝置 ex. 符合 FIDO2 標準的 YubiKey 等等 本 workshop 會實作虛擬的 MFA 裝置，讓使用者使用 Google Authenticator 登入才能使用 API 然而 如 Gruntwork Guide 在 MFA Policy所述，要 enforce MFA 到所有的 AWS API 會有諸多困難\n如果我們要求所有 AWS API 都過 MFA，有些來源可能很難做 MFA ex. EC2 VM 上的一個 application 也需要打 AWS API，但在 VM 上就很難取得 Google Authenticator 的認證碼 ex. 新的 IAM User 剛新建，沒有 MFA，也就無法登入設定自己的 MFA，變成雞生蛋蛋生雞 Gruntwork Guide 中建議在以下情境中啟用 MFA 就好\nsecurity 以外的 child-account (dev/stag/prod) 的所有 iam-role 都開啟 MFA 當我們要使用 security/iam-user assuem 到另外一個 account/iam-role 時，需要 MFA 認證才可 assume role 然後把 security 以外的 child-account 開啟 global 的 trust policy ，這樣設定就很單純 不會 block iam-user 登入，因為所有 user 都在 security 底下，其他 child-account 沒有 user 在 root 與 security account 內，才有 IAM User 與 IAM Group，在這邊啟用 MFA 將 MFA 依據 policy 去設定，透過 policy - group - user 去 enforce user 使用 MFA 新 user 建立時，需要額外建立 “self-management” permissions policy，讓新用戶可以不用 MFA，進行第一次的 MFA 設定 Password Policy Password Policy 指的是對於 user 自己設定的密碼要符合一定的規範\nAWS official doc: 如何設定 password policy ex. 密碼至少要 32 字元長度 ex. 密碼要包含英文，數字，大小寫，或特殊字元 ex. 密碼多久需要更新一次 這些 password policy 可以大幅強化所有 User 的密碼安全性 由於我們使用的 external module terraform-aws-iam 已經整合了 password policy，所以我們這邊直接在 input 中開啟即可以使用\nPGP key pgp (Pretty Good Privacy) 是非常常用的非對稱加密工具\n比較常用的工具如 OpenPGP 或是 gpg (GnuPG) 為何 iam user 創建會牽扯到 pgp？\n不管是 gruntwork module 或是 aws terraform-aws-iam module 都支援 pgp 在 admin 新建 User 與 login profile 後，aws api 產生 password，回傳給 admin terraform，這時 admin 需要把第一把 password 傳給 user，讓 user 做第一次登入 這整個傳遞過程，不管從 aws api -\u0026gt; terraform state -\u0026gt; terraform output -\u0026gt; admin -\u0026gt; user，如果是明碼未加密的狀態下，實在不宜傳遞，很有可能被有心中中途攔截 因此我們可以使用 pgp 相關工具來加密 admin 以 gpg(GnuPG) 為例非對稱加密 (local exposure)\nadmin 取得新 user (ex. accounting 人員) 的 gpg public key admin 產生 user password 後，使用 gpg 工具，以 account 人員的 public key 加密 password 獲得一串 encrypted text，傳給 accounting 人員 accounting 人員使用自己的 private key 用 gpg 工具解密，即可取得明碼 password 只有有 private key 的人可以解開 encrypted text accounting 人員使用 password 登入後，會因為 login profile 要求，而被迫更改新密碼，捨棄 admin local 有曝險過的 password 上面的做法 terraform state 與 admin local 會看到明碼 password，不夠安全。\nPGP \u0026amp; keybase.io aws 提供一個改進的做法，terraform state 只存 encrypted text\n這樣可以避免在 terraform state 中與 admin local 的 password 曝險 pre-requisite\ngpg tool keybase.io 註冊 安裝 gpg\nsudo port install gpg gpg --version 如果第一次使用，可以創建一對 key pair\ngpg --full-generate-key 列出自己有的 secret key (private key)\ngpg --list-secret-keys --keyid-format=long export public key\ngpg --armor --export 3AA5C34371567BD2 -----BEGIN PGP PUBLIC KEY BLOCK----- mQINBF9oN7ABEADeJ5BO3RsvfB0RpU3ZtI3AZmLmMfoaQ41QtkLoFEhF0XnSBNhH .... ARFfAR39B4hHAnA+/+scVFdGT8i9kuYxk8Ocb7zHgULrOBfKEPEQ5XLmdiqAD+DN jO2IFPM= -----END PGP PUBLIC KEY BLOCK----- NOTE: private key 是鑰匙，public key 是鎖頭，永遠不要把 private key 傳出去，而是把 public key 傳出去讓別人加密，用 public key 加密過的東西，只能用手上這把 private key 解開，也就是專屬你的\n在 keybase.io 上面註冊後，上傳你的 pgp public key\n依照指示完成步驟 (optional) 認證 github / domain / 其他網站讓其他人確定這個是你本人 實際操作：password policy \u0026amp; pgp key NOTE: 如果不啟用 pgp-key，admin terraform apply 新 user 後拿到的 password 就會是明碼，可以透過 gpg 工具加密，在透過網路傳給對方\n這邊我們先使用 Accounting 先做測試，啟用 login profile\nNOTE: 你知道自己是 admin，就請不要拿自己的 User 做測試，壞了很麻煩\n將 terragrunt.hcl 改成\nAccounting = { groups = [\u0026#34;billing\u0026#34;] pgp_key = \u0026#34;keybase:chechiachang\u0026#34; create_login_profile = true create_access_keys = false # accounting always use web console, won\u0026#39;t use access key } } 接著要改 module 中的 code，將 module 的 output 接出來到上層的 root module\noutput.tf\n# https://github.com/terraform-aws-modules/terraform-aws-iam/blob/master/modules/iam-user/outputs.tf output \u0026#34;keybase_password_pgp_message\u0026#34; { description = \u0026#34;Encrypted password\u0026#34; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_password_pgp_message } } output \u0026#34;keybase_password_decrypt_command\u0026#34; { description = \u0026#34;Decrypt user password command\u0026#34; value = { for key, value in module.iam_user : key =\u0026gt; value.keybase_password_decrypt_command } } 然後我們試著 plan\n預期：產生 accounting login profile，產生 pgp encrypted password text (而不是明碼 password) aws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform will perform the following actions: # …","date":1663669806,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"6f5b87cae3d99396b95c73ccf8087ddb","permalink":"https://chechia.net/zh-hant/post/2022-09-18-14th-ithome-ironman-iac-aws-workshop-07-reset-iam-user/","publishdate":"2022-09-20T18:30:06+08:00","relpermalink":"/zh-hant/post/2022-09-18-14th-ithome-ironman-iac-aws-workshop-07-reset-iam-user/","section":"post","summary":"昨天我們建立 IAM Group 與 policy，並說明 policy 管理原則。\n然而昨天最後創建 user 時，我們關閉了 create_login_profile = false 的選項 這是為了避免當前的登入機制被覆蓋掉，影響 root account Administrator 的使用 更改 login 方式，在某些極端的情形下，有可能讓 Administrator 自己覆蓋自己的 login 設定後，讓自己無法登入 如果你是管理員，在更改 admin 帳號的權限與登入設定時，一定要多加注意\n如果不幸改壞，無法登入，就只能再去找出 root account root user 帳號來解救了 如果是 root account root user 把自己的 login 改壞，就會很痛苦，要請 aws support 來救你 本日進度","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Reset Iam User","type":"post"},{"authors":null,"categories":["terraform"],"content":"昨天我們將 root account IAM user import 到 terraform 中\n示範 terraform import 增加 iam user 的功能到 module repository 中 今天要完成 root account 中 IAM Group + Policy，順便聊聊 aws IAM policy 管理原則\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User import terraform 中 補上 root account IAM Policy 補上 root account IAM Group iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n承接昨天的 plan 結果，我們今天要把 IAM policy 與 group 開出來\nIam User 首先 review aws_iam_user 的 resource\n# module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_user.this[0] will be updated in-place ~ resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;this\u0026#34; { + force_destroy = true id = \u0026#34;Administrator\u0026#34; name = \u0026#34;Administrator\u0026#34; tags = {} # (4 unchanged attributes hidden) }` force_destroy 這個參數我們需要嗎？可以一找以下的步驟查文件判斷\n直接 google “terraform aws iam user”，找到 Terraform registry 上 AWS iam user 的說明 這裡有說明，刪除 user 時，如果有 non-terraform-managed 的 access key, login profile, MFA devices，是否仍要強制刪除 這裡我們希望如果有 access key 或 MFA device 存在的話，不要直接刪除 User 將 https://github.com/chechiachang/terragrunt-infrastructure-modules/pull/1/files#diff-b15a741b1129d8f5451060653922d85c934792a1dd41c7fae1b02f3a6398094aR8 改成 false Iam Group \u0026amp; Policy 今天要來調整 Iam Group \u0026amp; Policy\nGruntwork 文件: Root Account 說明，root account 下應該有兩個 policy group group/full-access 給予 root account 超級管理員完整的控制權限 billing 給予 billing 的會計人員進來報帳 首先 full-access 的 iam_group plan 後的結果如下\n由於我們使用的 module 是 terraform-aws-iam 的 group with policies module.iam_group_with_policies_full_access 裡面有 group 也有 policy .aws_iam_group.this[0] 是主要的 group # module.iam_group_with_policies_full_access.aws_iam_group.this[0] will be created + resource \u0026#34;aws_iam_group\u0026#34; \u0026#34;this\u0026#34; { + arn = (known after apply) + id = (known after apply) + name = \u0026#34;full-access\u0026#34; + path = \u0026#34;/\u0026#34; + unique_id = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group_membership.this[0] will be created + resource \u0026#34;aws_iam_group_membership\u0026#34; \u0026#34;this\u0026#34; { + group = (known after apply) + id = (known after apply) + name = \u0026#34;full-access\u0026#34; + users = [ + \u0026#34;Administrator\u0026#34;, ] } 有了 group，接下來就是要配 policy\n一樣透過 group + policy -\u0026gt; attachment 的 resource，把 group 跟 policy 綁在一起 我們這邊的程式碼 直接使用 aws 預先定義好的 policy arn:aws:iam::aws:policy/AdministratorAccess # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026#34;aws_iam_group_policy_attachment\u0026#34; \u0026#34;custom_arns\u0026#34; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } IAM AWS 預先定義的 policy 有哪些 aws 已經預先定義好的 policy 可以使用？\n可以上 aws web console -\u0026gt; iam -\u0026gt; policies 下查詢 這些預先定義的 policy，是 aws 依照最常出現的使用情境，事先建立的 policy\n使用者不用再建議 ex. AWS 覺得 administrator 大概會需要這些權限，都先開到這個 policy/admin 上 ex. AWS 覺得 billing 大概會需要這些權限，都先開到這個 policy/billing 上 由於是 AWS 依照大部分人的需求開的 policy，所以會多開許多權限 這也是用預先定義 policy 的缺點，就是為了滿足很多人的需求，權限太大 違反最小權限原則，給予權限過多也造成安全性的風險 不使用預先定義的 policy 的話，我們可以自己寫 policy\n一個 policy 開出來 default 沒有任何 permission 依據最小權限原則，一句一句增加 permission 到 policy 上 如此可以確保 policy 上的 permission 都是需要的，而不會有多開但是用不到的權限 權限愈大，安全性風險越高，所以最佳實踐會希望所有 policy 都是配得剛剛好夠用就好\n然而配 policy 很花時間，用 aws 已經寫好的 policy 馬上可以用 實務上可以考量專案時間與整體人力，以及需要的安全等級，來考慮要不要做到這麼細 AWS policy access advisor 通常寫 policy 很難配到非常完美剛剛好，通常都會多開 permission\n權限多開功能不會壞，少開直接 permission denied 那多開的權限沒有用到，之後要如何把沒有用到的權限收回來？ AWS web console 提供根據使用紀錄，提建議修改 policy\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html\naws 會統計各個 IAM 元件，存取 AWS API 使用/沒使用的權限\n例如這個 User 過去 90 天使用了哪些 permission 去打 API 以及 User 還有哪些 policy permission，是過去 90 天都沒有用到的 這些長時間都沒有用到的 policy，我們就應該定期 review，然後移除這些不必要的權限\nUser / Group / … 等等都需要做一樣的事情 module 作為一個組成元件使用 由於我們在 module input 中開啟了 attach_iam_self_management_policy = true 參數，在 module 中便連帶產生\n.aws_iam_policy.iam_self_management[0]，裡面定義的要做 self management 所需要的 permission 再把 policy 透過 .aws_iam_group_policy_attachment.iam_self_management[0] 綁到 group 上 # module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management[0] will be created + resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;iam_self_management\u0026#34; { ... } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026#34;aws_iam_group_policy_attachment\u0026#34; \u0026#34;iam_self_management\u0026#34; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } 上面就是我們的 root account group/full-access 與對應的 policy\nbilling account 接著是 root account group/full-access 與對應的 policy\n我們可以自己手寫 policy，依照最小權限原則，一條一條加入 permission 或是我們可以上 aws web console 找看看有無預先定義好的 policy 我們搜尋 billing 有看到四個 policy，每個 policy 都有附上 description 說明使用的目的\nBilling AWSBillingConductorReadOnlyAccess AWSBillingConductorFullAccess AWSBillingReadOnlyAccess 這裡就要依據使用的需求選擇\n給予 billing 管理員的權限，就會是 ConductorFullAccess 或 ConductorReadOnlyAccess 給予 billing 報帳人員，應該不用更改 aws 的其他設定，只需要讀取跟輸出，就會是 AWSBillingReadOnlyAccess 我們這邊選擇 AWSBillingConductorFullAccess，做個示範\n點擊 AWSBillingConductorFullAccess，跳到 Policies » AWSBillingConductorFullAccess 頁面 可以看到 policy 的完整 arn，Amazon Resource Name (ARN) 唯一識別AWS 資源，類似於 ID 然後把 arn 填入 module.am_group_with_policies_billing plan \u0026amp; apply 上面的變更我們的 PR …","date":1663641577,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"3de821611ca5806b0d201e57b829ddbd","permalink":"https://chechia.net/zh-hant/post/2022-09-17-14th-ithome-ironman-iac-aws-workshop-06-provision-iam-group-policy/","publishdate":"2022-09-20T10:39:37+08:00","relpermalink":"/zh-hant/post/2022-09-17-14th-ithome-ironman-iac-aws-workshop-06-provision-iam-group-policy/","section":"post","summary":"昨天我們將 root account IAM user import 到 terraform 中\n示範 terraform import 增加 iam user 的功能到 module repository 中 今天要完成 root account 中 IAM Group + Policy，順便聊聊 aws IAM policy 管理原則","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Provision Iam Group Policy","type":"post"},{"authors":null,"categories":["terraform"],"content":"昨天我們為每個環境(dev / stag / prod …) 設定一個 aws organization account\n今天要使用 terraform 設定 AWS IAM User\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User 加到 terraform 中 security 中設定 IAM User security 設定 password policy security 設定 MFA policy iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nAccounts \u0026amp; IAM Users 今天要使用 Terraform 設定 IAM Users。\n未來所有的 User 都會透過 terraform 設定並管理 Day02 設定的 root account IAM User: Administrator 雖然是手動建立的，我們一樣需要把他匯入到 terraform 中 然而上個 PR 中，我們的 terraform module 中並沒有設定 IAM User 的功能，也就是這段 code 是沒有發揮功能 https://github.com/chechiachang/terragrunt-infrastructure-live-example/pull/1/files#diff-62920ff868733e1c625c23fe7ffd6c93bebd87ae16b865869bf682e29b082a99R54-R67\nusers = { alice = { groups = [\u0026#34;full-access\u0026#34;] pgp_key = \u0026#34;keybase:alice\u0026#34; create_login_profile = true create_access_keys = false }, bob = { groups = [\u0026#34;billing\u0026#34;] pgp_key = \u0026#34;keybase:bob\u0026#34; create_login_profile = true create_access_keys = false } } 我們這邊一樣嘗試把這個 users (map) 的功能補上\n使用開源 module 實作 IAM User 使用的開源 terraform module 是 terraform-aws-modules/terraform-aws-iam 是 aws 官方維護的開源 terraform module，品質很好，放心使用 需求整理\n要產生 IAM User input: 一個 map users = {} output: 多個 user 要產生 IAM Group 與 IAM Policy 增加 IAM User PR 的 commit 在此 https://github.com/chechiachang/terragrunt-infrastructure-modules/pull/1\n於是我們試著執行 terraform plan\naws-vault exec terraform-30day-root-iam-user --no-session -- terragrunt plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.iam_group_with_policies_full_access.aws_iam_group.this[0] will be created + resource \u0026#34;aws_iam_group\u0026#34; \u0026#34;this\u0026#34; { + arn = (known after apply) + id = (known after apply) + name = \u0026#34;full-access\u0026#34; + path = \u0026#34;/\u0026#34; + unique_id = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_group_membership.this[0] will be created + resource \u0026#34;aws_iam_group_membership\u0026#34; \u0026#34;this\u0026#34; { + group = (known after apply) + id = (known after apply) + name = \u0026#34;full-access\u0026#34; + users = [ + \u0026#34;Administrator\u0026#34;, ] } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.custom_arns[0] will be created + resource \u0026#34;aws_iam_group_policy_attachment\u0026#34; \u0026#34;custom_arns\u0026#34; { + group = (known after apply) + id = (known after apply) + policy_arn = \u0026#34;arn:aws:iam::aws:policy/AdministratorAccess\u0026#34; } # module.iam_group_with_policies_full_access.aws_iam_group_policy_attachment.iam_self_management[0] will be created + resource \u0026#34;aws_iam_group_policy_attachment\u0026#34; \u0026#34;iam_self_management\u0026#34; { + group = (known after apply) + id = (known after apply) + policy_arn = (known after apply) } # module.iam_group_with_policies_full_access.aws_iam_policy.iam_self_management[0] will be created + resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;iam_self_management\u0026#34; { + arn = (known after apply) + id = (known after apply) + name = (known after apply) + name_prefix = \u0026#34;IAMSelfManagement-\u0026#34; + path = \u0026#34;/\u0026#34; + policy = jsonencode( { + Statement = [ + { + Action = [ + \u0026#34;iam:UploadSigningCertificate\u0026#34;, + \u0026#34;iam:UploadSSHPublicKey\u0026#34;, + \u0026#34;iam:UpdateUser\u0026#34;, + \u0026#34;iam:UpdateLoginProfile\u0026#34;, + \u0026#34;iam:UpdateAccessKey\u0026#34;, + \u0026#34;iam:ResyncMFADevice\u0026#34;, + \u0026#34;iam:List*\u0026#34;, + \u0026#34;iam:Get*\u0026#34;, + \u0026#34;iam:GenerateServiceLastAccessedDetails\u0026#34;, + \u0026#34;iam:GenerateCredentialReport\u0026#34;, + \u0026#34;iam:EnableMFADevice\u0026#34;, + \u0026#34;iam:DeleteVirtualMFADevice\u0026#34;, + \u0026#34;iam:DeleteLoginProfile\u0026#34;, + \u0026#34;iam:DeleteAccessKey\u0026#34;, + \u0026#34;iam:CreateVirtualMFADevice\u0026#34;, + \u0026#34;iam:CreateLoginProfile\u0026#34;, + \u0026#34;iam:CreateAccessKey\u0026#34;, + \u0026#34;iam:ChangePassword\u0026#34;, ] + Effect = \u0026#34;Allow\u0026#34; + Resource = [ + \u0026#34;arn:aws:iam::706136188012:user/*/${aws:username}\u0026#34;, + \u0026#34;arn:aws:iam::706136188012:user/${aws:username}\u0026#34;, + \u0026#34;arn:aws:iam::706136188012:mfa/${aws:username}\u0026#34;, ] + Sid = \u0026#34;AllowSelfManagement\u0026#34; }, + { + Action = [ + \u0026#34;iam:List*\u0026#34;, + \u0026#34;iam:Get*\u0026#34;, ] + Effect = \u0026#34;Allow\u0026#34; + Resource = \u0026#34;*\u0026#34; + Sid = \u0026#34;AllowIAMReadOnly\u0026#34; }, + { + Action = \u0026#34;iam:DeactivateMFADevice\u0026#34; + Condition = { + Bool = { + \u0026#34;aws:MultiFactorAuthPresent\u0026#34; = \u0026#34;true\u0026#34; } + NumericLessThan = { + \u0026#34;aws:MultiFactorAuthAge\u0026#34; = \u0026#34;3600\u0026#34; } } + Effect = \u0026#34;Allow\u0026#34; + Resource = [ + \u0026#34;arn:aws:iam::706136188012:user/*/${aws:username}\u0026#34;, + \u0026#34;arn:aws:iam::706136188012:user/${aws:username}\u0026#34;, + \u0026#34;arn:aws:iam::706136188012:mfa/${aws:username}\u0026#34;, ] + Sid = \u0026#34;AllowDeactivateMFADevice\u0026#34; }, ] + Version = \u0026#34;2012-10-17\u0026#34; } ) + policy_id = (known after apply) + tags_all = (known after apply) } # module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_user.this[0] will be created + resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;this\u0026#34; { + arn = (known after apply) + force_destroy = true + id = (known after apply) + name = \u0026#34;Administrator\u0026#34; + path = \u0026#34;/\u0026#34; + tags_all = (known after apply) + unique_id = (known after apply) } # module.iam_user[\u0026#34;Administrator\u0026#34;].aws_iam_user_login_profile.this[0] will be created + resource \u0026#34;aws_iam_user_login_profile\u0026#34; \u0026#34;this\u0026#34; { + encrypted_password = (known after apply) + id = (known after apply) + key_fingerprint = (known after apply) + password = (known after apply) + password_length = 20 + password_reset_required = false + pgp_key = \u0026#34;keybase:alice\u0026#34; + user = \u0026#34;Administrator\u0026#34; } …","date":1663559967,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"19de429dcd5b3f0fc76036459753eab2","permalink":"https://chechia.net/zh-hant/post/2022-09-16-14th-ithome-ironman-iac-aws-workshop-05-provision-iam-user/","publishdate":"2022-09-19T11:59:27+08:00","relpermalink":"/zh-hant/post/2022-09-16-14th-ithome-ironman-iac-aws-workshop-05-provision-iam-user/","section":"post","summary":"昨天我們為每個環境(dev / stag / prod …) 設定一個 aws organization account\n今天要使用 terraform 設定 AWS IAM User\nroot 中設定 IAM User 將手動產生的 Administrator 的 IAM User 加到 terraform 中 security 中設定 IAM User security 設定 password policy security 設定 MFA policy iThome 鐵人賽好讀版","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Provision Iam User","type":"post"},{"authors":null,"categories":["terraform"],"content":"昨天設定了多個 aws organization accounts，但我們還沒有說明為什麼要這樣做\n今天會先講 Gruntwork 提出的 Production-Grade Design: multi-account security strategy\n拆分多個 account 的用意 多 account 下如何統一管理 IAM User access control 與安全性控管 iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nProduction-grade Design IAM Gruntwork 提出的 Production-Grade Design: multi-account security strategy，我們這邊一一講解\nRoot Account 的用途 位於 diagram 圖中的最上層 是整個架構的創世帳號，權限最大 root account 裡面沒有任何 infrastructure 元件， i.e. aws ec2 不會長在裡面，是一個空 account root account 的存取最嚴格 只有很少數的 admin 可以存取 i.e 只有 SRE 部門主管有自己的 IAM User，其他 SRE 沒有權限存取 root account root account 的工作最少 設定 child account 管理 billing IAM policy 管理，不要把 policy 直接綁在 User 上，應該要\n為不同權責的成員設定 IAM Group，ex. dev-admin / dev-user / stag-admin / qa / billing / … 把 policy 綁在 Group 上，ex. ReadPermission -\u0026gt; dev-user / Read+Write -\u0026gt; dev-admin 把 User 加進 / 移除 Group，來管理人員權限 Group 通常會接近公司團隊的職責分配 root account 內部只會有 full-access 的管理員 group billing 的會計出帳 group Child Account 的用途 透過 root account 創建 child accounts 後，這裡說明各個 account 的設計用途\nSecurity 用來管理 authentication 與 authorization\n底下一樣沒有任何 infrastructure 元件 定義所有 IAM User 與 IAM Group，所有團隊成員的 User 在這邊設定 所有的 User 放在 Security 底下統一管理 其他的 child account 不會設定 IAM User，ex. dev / stag / prod 裡面都沒有 IAM User child account 中設定 IAM roles，讓 security/user assume role 存取其他 child account ex. security/dev-admin/chechia assume role dev/admin，透過 aws STS(Security Token Service) 暫時取得 dev/admin 這個 role 的權限來控制 dev 團隊主管也會有一個 security IAM User，平時開發 infrastructure 使用這個 IAM User，只有需要調整 root account 時才會動用 root account IAM User。反正 root account 是能不用就不用 這個做法有非常多好處，但又不會造成工作上的負擔\n使用方面：所有的團隊成員只會有一個 User，透過 security 登入。不會一個人有好幾個帳號密碼很煩 管理方面：管理員只要在 security 內管理 User，而不用一直切換 account 去調整 IAM User 安全方面：只要在 security 內落實安全性設定，就可以很好的控制所有 account 的存取權限 dev / stag / prod 用來存放 infrastructure 與跑 application\n開發時使用 dev / stag 環境，測試都完成後，才自動推倒 prod 公開給用戶使用 dev / stag 環境是內部環境，架構與 prod 類似，來模擬 prod 的環境方便開發與測試 可能機器數量或規格比 prod 小很多，來節省成本 有些團隊會有更多 account，例如 qa / uat / …，都可以依照需求建立 prod 有些團隊會有複數 prod 環境，可以供災難發生時做備援切換 shared 裡面放跨 application account 共用的元件\n例如 AWS ECR 可能不想要每個 account 都開，想放一起的話可以放在 shared，然後讓其他 account 存取 shared 版本控制系統 Git / github 可能會只有一組大家共用 CI/CD pipeline / Jenkins …等等跨環境共用的資源，可以放 shared 每個 account 都開也是有許多好處，例如更獨立更安全，這就看看各個團隊災安全與成本上的取捨 管理上由於 prod 會使用到 shared 的元件，建議要把 shared 的安全等級當作 prod 來管理，嚴格限制 shared 的存取，以保護 prod 環境的安全 很多個 sandbox account 讓工程師自由的測試功能\n如果有需求與預算，一個完全獨立的 sandbox 讓工程師可以做各種實驗，對於促進團隊內部的創新會有非常大的幫助 如果在 dev 上做這測試，有可能會影響到其他團隊成員，而覺得綁手綁腳的事情，都可以盡情在 sandbox 上操作 可以為 sandbox 的帳號設定更嚴格的 billing / cost 設定，避免工程師玩太嗨超過預算 黃金準則是一個工程師一個 sandbox 來達到 100% 開發獨立 testing account 用來測試 infrastructure 的測試\n如果有使用 Gruntwork/Terratest 來測試 terraform tf code 的朋友，應該知道 terratest 會把 terraform module 中的 infrastrcuture 真的在 aws 開出來，做功能測試，然後測試完成後再把 infrastrcture 全部刪掉 是的 IaC terraform module 也是需要單元測試 testing 就是用來讓 IaC 做自動化測試的環境，裡面的 infrastructure 都是常常 create + destroy，不會有常駐的服務 與 sandbox 不同的是 testing 是跑自動化 CI/CD pipeline 的測試 sandbox 是讓工程師做規模較小的手動測試與開發，因為我們也不希望一堆測試用 infrastructure 散落在 sandbox 裡面，感覺很貴 logs 用來收集 log 到單一 account 底下，方便查閱\n所有 child account 的 log 都收到這裡，而不用跑到各個 account 去查 log aws account 內部的 log，cloud trail 都可以透過客訂與工具轉發，集中收集 如果公司規模很大，有多個 business unit 的話，也可以多建幾個 organization，來對應公司組織\n透過 web console switch role 我是一個開發工程師（不是 admin），那在 multi-account 下的環境我應該如何工作？\n如何存取 aws\nIAM User 可以透過帳號密碼來存取 aws web console，登入之後會發現自己處在 security account 底下 第一次登入的話需要重設密碼，需要符合 IAM Password Policy 會需要輸入 MFA，會需要符合 IAM MFA Policy 兩面的安全性 policy 在實際公司帳號裡是必備，但在 workshop 中，我們還沒做，先留個坑之後來補 programatic access 如同 day03 的 root account IAM User 步驟，我們也是使用 aws-vault 搭配 aws access key，來讓 terraform 執行 web console 如何 switch role\n可以在 aws web console -\u0026gt; 右上角 user -\u0026gt; switch role 填入\n目標 account ID (ex dev: 123456789012) 目標的 role 給自己看的好辨識名稱 如果 admin 有設定 IAM role 與 assume role 權限，就可以切換到 dev account 下的 IAM role 身分 透過 web console switch role 如同 day03 使用 root account IAM User，我們也是使用 aws-vault 搭配 aws access key，來讓 terraform 執行) 來跑 terraform 建立 child accounts 一樣，我們也需要設定 aws-vault，讓我們可以在 local 機上 assume 不同 account role，來使用各個 child account\naws-vault Roles and MFA 需要調整 ~/.aws/config 的設定\nsecurity 管理員設定完 iam role + assume policy 後，會提供登入資訊給其他團隊成員 cat ~/.aws/config # 這是 root account IAM User 不要再拿來用了 # aws access key 收在本機的 aws-vault 內 [profile terraform-30day-root-iam-user] region=ap-northeast-1 # 這是 security account (111111111111) IAM User（還沒建立） # aws access key 收在本機的 aws-vault 內 [profile chechia-security-iam-user] region=ap-northeast-1 # 這是 dev account (333333333333) IAM Role（還沒建立） [profile dev-admin] source_profile = chechia-security-iam-user role_arn = arn:aws:iam::333333333333:role/dev-admin mfa_serial = arn:aws:iam::111111111111:mfa/chechia-security-iam-user 上面這些都還沒建立，所以都還不能用，只是先說一下之後要怎麼操作\nsecurity 管理員要設定一大堆東西，但工程師使用非常簡單，只要透過 aws-vault 切換就可以在不同 account 下操作 terraform\naws-vault exec dev-admin -- aws sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;xxxxxxxxxxxxxxxx\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;333333333333\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::333333333333:role/dev-admin\u0026#34; } aws-vault exec dev-admin -- terragrunt plan aws-vault exec stag-admin -- terragrunt plan …","date":1663464925,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"ace90bff86625ba1b3ca74db6c3e4380","permalink":"https://chechia.net/zh-hant/post/2022-09-15-14th-ithome-ironman-iac-aws-workshop-04-aws-multi-account-structure/","publishdate":"2022-09-18T09:35:25+08:00","relpermalink":"/zh-hant/post/2022-09-15-14th-ithome-ironman-iac-aws-workshop-04-aws-multi-account-structure/","section":"post","summary":"昨天設定了多個 aws organization accounts，但我們還沒有說明為什麼要這樣做\n今天會先講 Gruntwork 提出的 Production-Grade Design: multi-account security strategy\n拆分多個 account 的用意 多 account 下如何統一管理 IAM User access control 與安全性控管 iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\nProduction-grade Design IAM Gruntwork 提出的 Production-Grade Design: multi-account security strategy，我們這邊一一講解","tags":["terraform","iac","aws","鐵人賽2022"],"title":"AWS Multi-Accounts Structure","type":"post"},{"authors":null,"categories":["terraform"],"content":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nProvision AWS Account\n今天要把 aws account 設定完成，並且開始使用 live-repository 來搭建 aws 元件\n我們自己的 module repository 開一個權限的 repository fork 我自幹的 modules repository terragrunt-infrastructure-modules https://github.com/chechiachang/terragrunt-infrastructure-modules 上面這個是放 terraform module 的 repository，是兩個 repository 不要跟執行 terragrunt repository 搞混，是兩個 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example 今天的進度與 code base\nterraform-live-example PR 在此 terragrunt-infrastructure-modules 請直接看 tag v0.0.1 因為有地方嚴重寫錯，我有 force push 東西 :tear:，如果已經提早下載的朋友可能會需要清掉 repository main branch + tag 重拉 orz 對，我們且戰且走常常會這樣，發現昨天寫的東西寫錯臨時改，請大家見諒\n為何使用 aws modules X) 因為我們沒有付錢，不能用 gruntwork 的 priviate repository (大誤\nO) 因為我們想要使用開源版本的 terraform module (XD)\n我們快速看一下 module 裡面的內容 https://github.com/chechiachang/terragrunt-infrastructure-modules/tree/v0.0.1/aws/modules/account-baseline-root\nresource \u0026#34;aws_organizations_organization\u0026#34; \u0026#34;org\u0026#34; { aws_service_access_principals = [ \u0026#34;cloudtrail.amazonaws.com\u0026#34;, \u0026#34;config.amazonaws.com\u0026#34;, ] feature_set = \u0026#34;ALL\u0026#34; } resource \u0026#34;aws_organizations_account\u0026#34; \u0026#34;account\u0026#34; { for_each = var.child_accounts name = each.key email = each.value[\u0026#34;email\u0026#34;] depends_on = [ aws_organizations_organization.org ] } 我們的需求\n在 root account 下 provision 一個 aws organization 為每個環境 provision 一個 aws organization account 使用 terraform for_each function 來迭代 variabel 傳入的 organization accounts (型別是 map) each.key 會拿到每個 map element 的 key，例如 child_accounts = { logs = {}, security = {}, shared = {}, dev = {}, stage = {}, prod = {} } 跑出來 each.key 就會是 [logs, security, shared, dev, stage, prod] 正好是我們這次需要 provision 的六個 aws organization accounts，滿足目前的需求 each.value[“email”] 則依序帶入各自的 email 到 account 中 NOTE: 這些 email 請使用你收得到的 email\n在現實中這會是六組不同的 email，email 重複的話會被 aws api reject，所以需要使用不同的 email 或是跟 gruntwork guide 一樣，使用 gmail 帳號後+ 的字元會 ignore 如果設定了收不到信的 email 作為 account email 就會有點麻煩 不要像我一樣設了 6 個收不到 email，只好再多開一個 account XD，還好我們是在 workshop 這個 terraform module 的寫法完全忽略其他 each.value 內的值\n我們並不清楚 gruntwork 在 private 的 terraform module 中，遮些參數的確切用途，以及對 aws organization account 設定有何影響 但以筆者使用 aws 的經驗，大概猜得出來這些參數的用途，以及 terraform module 應該如何調整，為了課程進度安排，我們先暫時忽略。繼續照著 gruntwork 的文件做下去，之後有用到再來補 Terragrunt Plan \u0026amp; apply 在跑 terragrunt 之前，我們先要切換成 root account 的 iam User，昨天新建的 IAM User，存放在密碼企理，還記得嗎？\n使用 aws-vault 拿 IAM User 的 access key 做 programatic 登入，提供 terraform credential。而非透過 aws web console 記住：我們不能用 root account 做這些工作\naws-vault 由於我們現在要使用 IAM User 做 terraform 來創建新的 child accounts，未來會使用新的 child accounts 來做各個環境中的 terraform 元件操作，我們需要一個工具來協助切換這些 account\n我們使用的工具是 aws-vault\nhttps://github.com/99designs/aws-vault 支援很多安裝平台，選自己喜歡用的，或是直接在 github release 頁面下載 binary $ aws-vault --help usage: aws-vault [\u0026lt;flags\u0026gt;] \u0026lt;command\u0026gt; [\u0026lt;args\u0026gt; ...] A vault for securely storing and accessing AWS credentials in development environments. ... 使用 Root Account IAM User 身份執行 terragrunt\n在本機紀錄 access key (請好好保護本機的安全)\naws-vault add terraform-30day-root-iam-user Enter Access Key Id: XXXXXXXXXXXX Enter Secret Key: YYYYYYYYYYYY 透過 aws-cli 取得 caller identity，也就是現在的身份是誰，確定是 Administrator\n從 aws 的角度，所有操作都是 IAM User: Administrator 操作的 只能操作當初建立 IAM User 時 attach 的權限（蘇然也是很大）但已經比 root account 小很多 之後我們會進一步限縮每一個 IAM User 的 permission aws-vault exec terraform-30day-root-iam-user -- aws sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;AIDAxxxxxxxxxxxKGW\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;706136188012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::706136188012:user/Administrator\u0026#34; } 然後透過 aws-vault 來執行 terragunt，這樣 terragrunt 就會使用 IAM User 的 credential 來操作 provider 去呼叫 aws api\naws-vault exec terraform-30day-root-iam-user -- terragrunt init aws-vault exec terraform-30day-root-iam-user -- terragrunt plan Terraform will perform the following actions: # aws_organizations_account.account[\u0026#34;dev\u0026#34;] will be created + resource \u0026#34;aws_organizations_account\u0026#34; \u0026#34;account\u0026#34; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026#34;terraform30days@outlook.com\u0026#34; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026#34;dev\u0026#34; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026#34;logs\u0026#34;] will be created + resource \u0026#34;aws_organizations_account\u0026#34; \u0026#34;account\u0026#34; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026#34;terraform30days@outlook.com\u0026#34; + govcloud_id = (known after apply) + id = (known after apply) + joined_method = (known after apply) + joined_timestamp = (known after apply) + name = \u0026#34;logs\u0026#34; + parent_id = (known after apply) + status = (known after apply) + tags_all = (known after apply) } # aws_organizations_account.account[\u0026#34;prod\u0026#34;] will be created + resource \u0026#34;aws_organizations_account\u0026#34; \u0026#34;account\u0026#34; { + arn = (known after apply) + close_on_deletion = false + create_govcloud = false + email = \u0026#34;terraform30days@outlook.com\u0026#34; + govcloud_id = (known after apply) …","date":1663319781,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"10581b524e3d36685d7bd357048bbb0e","permalink":"https://chechia.net/zh-hant/post/2022-09-14-14th-ithome-ironman-iac-aws-workshop-03-provision-aws-account/","publishdate":"2022-09-16T17:16:21+08:00","relpermalink":"/zh-hant/post/2022-09-14-14th-ithome-ironman-iac-aws-workshop-03-provision-aws-account/","section":"post","summary":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nProvision AWS Account\n今天要把 aws account 設定完成，並且開始使用 live-repository 來搭建 aws 元件\n我們自己的 module repository 開一個權限的 repository fork 我自幹的 modules repository terragrunt-infrastructure-modules https://github.","tags":["terraform","iac","aws","鐵人賽2022"],"title":"Provision Child AWS Accounts","type":"post"},{"authors":null,"categories":["terraform"],"content":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nAWS Account structure AWS account structure 是 AWS IAM 提供的 feature 之一，可以讓用戶從一個 account login，然後透過授權操作其他 account 的元件。\n然而有這個 feature 不代表用戶就要買單，有實際需求我們才來用 feature。使用 account structure 的理由是什麼？\nAccount structure 的需求 AWS 官方 whitepaper: 使用 multiple aws account 的好處\n依據職務需求將 workload 分組 為不同 environment 設定獨立的 security control 限制敏感資料的存取 提升創新與開發的敏捷 控制事故的影響程度與衝擊 impact 精細的拆分 IT 的工作 成本控管 分散 aws quotas 使用限額與 API rate limit AWS 官方的 organization best practice AWS 官方 OU 管理的 best practice\nGruntwork Doc 建議的 IaC best practice 中的 aws account structure，提到三個主要的目的\nIsolation (AKA compartmentalization) 不同環境，事故發生時，影響範圍不會太大 (ex. 工程師 terraform 誤刪 infra) 安全性，dev / stag 有安全性漏洞時，attacker 不會攻擊到 prod 環境 Authentication and authorization 把 user 都放在一起管理，透過更精細的 access control 去控制其他 account 內部的元件 Auditing and reporting 依據環境與用途拆分 account，讓監測與成本控制更精細 從實務上的經驗，有以下狀況就考慮要拆分 aws account\n如果 IT 部門很大人數很多，那將所有工作內容不同的 user 都放在同一個 aws account 下導致管理麻煩 大多數的 incident 都是工程師 change 造成的，拆分 dev / stag 的 change 曾經影響到 prod 不同團隊對於負責的元件範圍區分不清 工作掉球，該做的事沒人處理 工作重功，做了一樣的事情導致衝突 小故事：技術問題解決人為問題（？） iThome DevOpsDay 有一個聽眾聽完2022 DevOpsDay PaC for IaC 演講 後跑來找我，他問說： 講師你好，我們 terraform 常常誤刪東西，deploy dev / stag 結果 prod 壞掉（汗 如果不是技術的問題，是人的問題，不知道有沒有方法解？\n要治本還是要加強內部教育訓練，很基礎的重大錯誤是很難用技術擋的 但短期要治標，可以\n先區隔 dev stag 與 prod 的 terraform repo 跟 aws account 把 prod 權限鎖起來，先不要讓 junior 的成員去觸碰 所有 prod apply 都要兩到三個 senior 去 approve 也許你們的生活會快樂一點(好血淚)\nworkshop 我們會根據 Gruntwork Landing Zone Guide 的文字敘述來進行今天的 workshop\nCAUTION You must be a Gruntwork subscriber to access the Gruntwork Infrastructure as Code Library. 是的，這份 Gruntwork Landing Zone Guide 內部的範例有使用許多 gruntwork-io 的 private repository，這部分是需要付費。gruntwork subscription 一個月 $795 起跳，有興趣的朋友可以參考 Gruntwork subscription checkout\n我們這篇 workshop 並不使用 Gruntwork 的付費功能，所有 private repository 我們就會使用其他開源的 modules 來替代，例如使用 aws + terraform team 官方維護的 aws modules\n由於缺少部分 repository 的內容，這份 Gruntwork Landing Zone Guide 會常常看到很多 github external link 是 404 not found，表示是 private repository。看起來會有點不舒服，但沒關係我們就見招拆招自由發揮。\n建立 workspace repository repository 的內容可以直接參照 Gruntwork Guide 的內容\n本次 workshop 我們使用的 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example\nterragrunt 都會在這個 repository 裡面運作 各位可以直接 copy 來改 Root account root account 是整個 account structure 最上層的 account，創世 account。由於是創世 account，權限也是最大，因此會需要加上許多限制:\n使用高強度的密碼 使用密碼儲存器保管密碼，ex 1password / lastpass / …，這些密碼儲存器都有自動加密，且有通過安全風險測試 不要存在電腦上 / email / google drive / 紙上 / 或是其他沒有經過安全驗證工具上 務必開啟 MFA 刪除 root account 的 aws access key，只允許透過 aws web console 存取 自己再也不要使用這個帳號，降低洩漏的風險 AWS 官方建議，只有這些工作需要使用 root account ，其他工作都不應該使用 root account Root Account IAM user 我們使用 root account 的最後一件工作，就是建立一個 IAM User 作為 admin，之後使用這個 IAM User 操作。文件與步驟在此\nName: Administrator 勾選: programmatic access AWS Management Console access Permission 頁面選擇 Attach existing policies to user directly 勾選 AdministratorAccess policy. 點下一步，直到產生出 IAM User 將登入資訊存在密碼儲存器 login url name password Access key ID Secret access key 接著也是要鎖住 root account 的 IAM User\n使用高強度的密碼 使用密碼儲存器保管密碼 務必開啟 MFA 為 Root account 設定 Security baseline 依照 gruntwork guide 接下來需要設定底下的元件\n建立所有的 child accounts 設定 AWS Organization IAM ROles IAM Users IAM Groups IAM Password Policies Amazon GuardDuty AWS CloudTrail AWS Config. CAUTION You must be a Gruntwork subscriber to access terraform-aws-service-catalog. 由於 terraform-aws-service-catalog 是 private repository，需要付費訂閱才能使用，所以我們無法取得裡面的內容\n為了避免這個 workshop 卡在這裡，我們就來自幹這些 module 吧ＸＤ\n直接使用其他的開源 modules 替代 依照 guide 把元件生出來 然後依照自己的需求去客製化 NOTE: Gruntwork 的 security module 有其 enterprise solution 的經驗在裡面，其實公司有預算還是相當值得參考的。只是我們 workshop 為了壁面大家破費，先當一回免費仔。\n我們自己的 module repository 開一個權限的 repository fork 我自幹的 modules repository terragrunt-infrastructure-modules 上面這個是放 terraform module 的 repository，是兩個 repository 不要跟執行 terragrunt repository 搞混，是兩個 repository https://github.com/chechiachang/terragrunt-infrastructure-live-example 寫到這邊就 6000 字了，還沒開始碰 terragrunt\naws account structure 與 terragrunt directory structure 的概念非常重要，有好的基礎會讓後面的工作快樂很多 明天會繼續今天的工作，把 aws account / iam policy / permission 配好\n","date":1663233381,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d9b9f9e8b77dc857ce82518398c4c770","permalink":"https://chechia.net/zh-hant/post/2022-09-13-14th-ithome-ironman-iac-aws-workshop-02-aws-account-structure/","publishdate":"2022-09-15T17:16:21+08:00","relpermalink":"/zh-hant/post/2022-09-13-14th-ithome-ironman-iac-aws-workshop-02-aws-account-structure/","section":"post","summary":"今天要使用 terraform 設定 AWS account structure\niThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nAWS Account structure AWS account structure 是 AWS IAM 提供的 feature 之一，可以讓用戶從一個 account login，然後透過授權操作其他 account 的元件。\n然而有這個 feature 不代表用戶就要買單，有實際需求我們才來用 feature。使用 account structure 的理由是什麼？","tags":["terraform","iac","aws","鐵人賽2022"],"title":"AWS Account Struture","type":"post"},{"authors":null,"categories":["terraform"],"content":"iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nAWS 為例，實現 production framework 的最佳實踐的 30 天 workshop\n準備 Production environmtn Devops 面臨的問題 DevOps 的工作內容繁雜\n今天想要 Deploy 一個 backend API deployment 到 k8s 上 一轉眼發現自己在修 port / servie / ingress / alb 一轉眼發現自己在修 tls certificate 一轉眼發現自己在修 node linux 上的一個 bug 一轉眼發現自己在修 monitoring / metrics + alert / log collection … 我只是想 deploy 一個 backend deployment 修infra 花了好幾天，而寫 deployment 只要 2 hr Devops 的工作，是處理 developer 與 operator 中間的繁雜的細節 換個角度思考：上面這些元件全都是存在許久的功能，應該有一套統一介面隨叫即用這些服務，ex.\n今天想要 Deploy 一個 backend API deployment 到 k8s 上 承上，同值性高，coding 講求 DRY 原則 (Don’t Repeat Youself)，infra 管理上卻會有大量重複的工作\n開一個 Restful API deployment 開一個 GRPC deployment 開一個 Kafka message Queue / redis / RDS / … 希望這些元件都有寫好的 unit 可以重複使用 但實務上還是會有細節需要調整 自動化\n標準化功能後，應該要可以自動部署 甚至是當 Backend API deployment 部署上去時，自動調用產生對應的元件 Backend API deployment 有變化時應該要跟據 deployment 需求去做 reconcile (k8s controller) 缺乏測試\nVM / alb / RDS / … 等公有雲服務需要測試嗎 A: AWS / Google / Azure 都測過了，我們不用再測 然而 SRE 卻發現時常需要花時間 debug 這些元件 不是 functional issue，而是 configuration 問題，或是不同元件設定造成交互作用 B: 要上 production 的元件都要測試 使用 terraform provision 一台 EC2 後，在使用 terratest (Golang) 打 aws API 進行測試 標準化，自動化後，應該有大量的自動化測試，來確保每一個元件的 configuration 都是正確的，而且有信心維持穩定度 infra 時常改變\n技術元件時常改變：terraform 升級，aws 升級，k8s 升級，linux 升級與 patch infra 沒有射後不理這種事，都需要時常且穩定的更新 為了滿足上游服務(前後端/DB/…)的需求 調整 VM spec / networking / routing / security rules / 不斷的改變耗費大量的人工時間 production ready 需要準備的\nproduction -\u0026gt; 真金白銀，公司的業務與名聲 production ready 應該是有明確的表準 True / False，而不是『我覺得現在是 production ready 了』 DevOps / SRE 都知道 production ready 的標準，只是沒有把他明確寫出來 production ready 應該就跟 unit test 一樣，一個 checklist 跑下去檢查，通過就是 production-ready，不通過就是 not-ready，且會說明不通過的原因 Gruntwork production ready checklist 人工介入工作太多\n新增元件需要人工處裡 定期的 patch \u0026amp; update / 安全性更新 / deploy 都需要人工介入的話就太浪費時間了 人工工作太多容易造成人為錯誤(human error) 多環境\nSRE 需要提供大量的環境 dev / qa / stag / prod … 或是提早 scan 到更前面的工作流程，例如在 code base / reposiroty 中就先 scan Gruntwork solution 為何要使用 gruntwork framework\nWhy you need a framework\nGruntwork 提出兩個 solution\nReference Architecture Build your own architecture Prerequisites 由於本次分享不希望有多餘的費用，所以內容依據 gruntwork 公開網站與開源的 repository 內容講解使用\n不會涉及付費版的 terraform code library 內容與教材(subscription $795/month) aws 服務也盡量使用 free tier 中的額度 Prerequisites\n基本 Terraform 經驗，或是參考2021 年的鐵人賽系列文章: 30 天學 Terraform on Azure 基本 AWS 服務的觀念 都沒有也沒關係，會沿路在幫各位複習 30 天的目標 使用 terraform 設定所有 aws account 的 resource 參考 gruntwork-io 的開源 repository，設定 如果是測試或學習，可以參考 https://github.com/gruntwork-io/terraform-aws-service-catalog 學習目標\n熟悉 terraform 基本觀念 學會使用 terraform IaC 控制所有 aws service Repository NOTE: 如果是有購買 gruntwork subscription 的朋友，請使用私有的 IaC library repository，有更完整且經過測試的架構\n本課程使用 gruntwork 開源的 repository 架構\n請 copy 需要的部分（而不要 fork，因為這裡不需要過去的 commit history 與未來的 update)\n去年iThome 鐵人賽使用的範例 repository 我自己使用的 repo 今年 iThome 會使用的 repository 今天要做的事 請準備好以下 prerequisite\n設定 / 註冊 aws account 併取得 aws 12-month free-tier (預期是免費的預期 12-month free tier 即可包含所有內容) 然而成本控制也是課程的一環，各位要注意使用的資源，不要超過 aws free tier 太多，理論上也是最多幾美金的花費 安裝與設定 terraform tools terraform 1.2.9 terragrunt 0.38.9 使用舊版本的朋友請盡量使用 terraform \u0026gt;1.x 與 terragrunt \u0026gt;0.35.8 的版本 Aws account\n如果已經有 aws cloud service account 可以直接使用，本課程會使用全新的 aws account 使用 email 註冊 aws cloud service email 認證 信用卡登記 手機認證 Install terraform\nterraform 1.2.9 terragrunt 0.38.9 設定 Github repository 準備一個 github repository 來存放ㄏ的 terraform code\n可以使用我的範例 repository，也是 fork 底下 gruntwork 的 repository 稍作整理 或參考 gruntwork Github repository example git clone git@github.com:chechiachang/terragrunt-infrastructure-live-example.git 明天 帶著大家在 terraform code 中設定 aws account 與 IAM 權限\n參考資源 不熟悉 terraform 的朋友不妨參考去年的佳作 Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天，雖然是在 Azure 上 ，但許多基本概念都是相通的\n過去的鐵人賽文章\n13th 佳作 Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天 12th 佳作 Kubernetes X DevOps X 從零開始導入工具 X 需求分析＊從底層開始研究到懷疑人生的體悟＊ 11th 優勝 其實我真的沒想過只是把服務丟上 kubernetes 就有這麼多問題只好來參加30天分享那些年我怎麼在 kubernetes 上踩雷各項服務 kkk 推薦一下 Gruntwork 他們家的 blog: Gruntwork devops as a service\nQ\u0026amp;A 過去參賽都會有許多朋友提出問題，今年若有疑問可以集中在第一篇文章，我會跟大家一起討論。有想要看的題目也可以留言，我們看時間做安排。\nreferences https://docs.gruntwork.io/intro/overview/how-it-works ","date":1662964308,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"97c2f2c800290596eab5d3a84d18c24d","permalink":"https://chechia.net/zh-hant/post/2022-09-12-14th-ithome-ironman-iac-aws-workshop-01-introduction/","publishdate":"2022-09-12T14:31:48+08:00","relpermalink":"/zh-hant/post/2022-09-12-14th-ithome-ironman-iac-aws-workshop-01-introduction/","section":"post","summary":"iThome 鐵人賽好讀版\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n–\nAWS 為例，實現 production framework 的最佳實踐的 30 天 workshop\n準備 Production environmtn Devops 面臨的問題 DevOps 的工作內容繁雜\n今天想要 Deploy 一個 backend API deployment 到 k8s 上 一轉眼發現自己在修 port / servie / ingress / alb 一轉眼發現自己在修 tls certificate 一轉眼發現自己在修 node linux 上的一個 bug 一轉眼發現自己在修 monitoring / metrics + alert / log collection … 我只是想 deploy 一個 backend deployment 修infra 花了好幾天，而寫 deployment 只要 2 hr Devops 的工作，是處理 developer 與 operator 中間的繁雜的細節 換個角度思考：上面這些元件全都是存在許久的功能，應該有一套統一介面隨叫即用這些服務，ex.","tags":["terraform","iac","aws","鐵人賽2022"],"title":"IaC best practice workshop on aws","type":"post"},{"authors":null,"categories":["Technology"],"content":"Titleq 從零導入 Policy as Code 到 terraform 甘苦談 - Introducing policy as code for terraform\nPresentation Google Slides\nDescription Terraform 是一個很棒的 Infrastructure as Code 的工具，能夠以現代的軟體工程流程來穩定的建構 infrastructure，並隨著需求變更自動化迭代，將新的 feature 安全地更新到既有的 infrastructure 上。只要是 Infrastructure 有關的問題，我一律推薦 Terraform。\n這也意味 Terraform 的品質就等於 infrastructure 的品質，好的 terraform 帶你上天堂，不好的 terraform 全 team 火葬場。\nInfrastructure 有太多資安的考量，新的風險不斷被檢查出來，連帶要不斷的 patch terraform code 來避免潛在的資安風險 Infrastructure 會不斷地更新，例如公有雲的 api 不斷更新，去年的 code 已經跟不上今年的最佳實踐了 好的工程師寫出的 terraform code 屌打菜鳥工程師，要如何讓團隊依循更好的實踐，避免寫出爛 code 維護 code 有 clean code / best practice，Terraform 也有 clean code 與 best practice，工程師要如何工具來輔助，是 Policy as Code 的一大課題。 本演講聚焦於實際經驗，從一大堆 terraform modules 開始，沒有 Policy as Code，到一步步評估、導入、實作、改進與迭代，逐漸的提升團隊 Terraform code 品質，提升 infrastructure 交付品質，並避免到未來潛在的風險。\n本次演講的 Terraform 版本為從 0.12, 0.13, 一路推進 1.0+。\nContent 從零導入 Policy as Code 到 terraform 甘苦談\n簡介 Terraform 與 Policy as Code 當你手上有數不完的 terraform modules，管理 terraform 品質是個大問題 如何管理 Terraform 自動化: 改變從 Jenkins pipeline 開始，gitflow 改進，pre-commit hook Policy as Code 校驗 使用工具量化 terraform code 品質 完整導入 Policy as Code 小結: 玩 infra 必用 terraform，玩 terraform 必做 policy as code About me Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit 2019 Ithome Cloud Summit 2020 Ithome Cloud Summit 2020/12/18\tCloud Native Taiwan 年末聚會 2020/8/17\tDevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2021 Ithome Cloud Summit 2022 Ithome Cloud Summit 2022 COSCUP\nSuggested Audience 推薦有 Terraform 使用經驗，特別是公有雲經驗，對 Policy as Code 有興趣的人 想要寫出 Terraform clean code 的人 ","date":1654707852,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"b1e366b15d0a114530fef8154d410d7f","permalink":"https://chechia.net/zh-hant/post/2022-06-09-ithome-devopsday-2022-proposal-introduce-policy-as-code-for-terraform-/","publishdate":"2022-06-09T01:04:12+08:00","relpermalink":"/zh-hant/post/2022-06-09-ithome-devopsday-2022-proposal-introduce-policy-as-code-for-terraform-/","section":"post","summary":"Titleq 從零導入 Policy as Code 到 terraform 甘苦談 - Introducing policy as code for terraform\nPresentation Google Slides\nDescription Terraform 是一個很棒的 Infrastructure as Code 的工具，能夠以現代的軟體工程流程來穩定的建構 infrastructure，並隨著需求變更自動化迭代，將新的 feature 安全地更新到既有的 infrastructure 上。只要是 Infrastructure 有關的問題，我一律推薦 Terraform。","tags":["pac","devops","terraform"],"title":"IThome 2022 DevOpsDay Introducing policy as code for terraform","type":"post"},{"authors":null,"categories":["Technology"],"content":"Titleq 在 k8s 上跑 time series database 甘苦談 - Operating Time Series Database in K8s\nGoogle Slides Youtube live record: https://youtu.be/YexUnVOZC8M?t=9421\nDescription Influxdb 為市占最高的 time series DBMS 之一，使用上與 RDBMS 有不同優劣勢。\n在維運方面，database 有許多相似需求：穩定性、高可用性、備份、還原、資源管理、調度、災難復原…等。社群常聽到有人問：可不可以在 K8s 上跑 database。\n本演講會分享在 k8s 中維運，實務上所遇到的問題，提供一些思考方向。\n本次演講的 influxdb 版本為 Influxdb OSS / enterprise 1.9+\nInfluxDB is one the the most popular time series database management system (DBMS) and is a powerful platform when dealing with time series data.\nDBMSs, including RDBMS, have many similar requirements on aspect of infrastructure operation. They all require stability, high availability, backup and restore utilities, cpu / memory resource management, disaster recovery…etc. People often ask about that whether is it ok to run DBMS in kubernetes cluster or not. This lecture try to provide some points from our experiences dealing with real-world issues.\nThe version of InfluxDB discussed in the lecture is InfluxDB OSS / Enterprise 1.9+\nContent 在 k8s 上跑 time series database 甘苦談\n簡介 Influxdb，time series 與 RDBMS 的差異 使用 time series 的幾個情境: 在 k8s 上跑 DB 穩定性 Availability、HA (enterprise)、faillure recovery 資源管理 OOMKilled、cpu throttling、OutOfDisk DB management: data migration、backup \u0026amp; restore、data retention 小結: 你該不該用 cloud service / VM / 在 K8s 上跑 database Operating Time Series Database in K8s\nA brief introduction about InfluxDB and the differences with RDBMS Some scenario to use a time series database (other than RDBMS) Operate InfluxDB in K8s Stability, availability, disaster recovery Resource management, OOMKilled, cpu throttling, out of disk DB management: data migration, retention, rotation, backup \u0026amp; restore summary: whether or not to run a database in k8s About me Che-Chia Chang，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。 Microsoft 最有價值從業人員 MVP。\n目前為 Golang Taiwan Meetup Organizer，常出現於 CNTUG，DevOps Taipei，GDG Taipei， Golang Taipei Meetup。\nChe-Chia Chang, an SRE specialize in container and Kubernetes operation. An active member of CNTUG, DevOps Taipei, GDS Taipei, Golang Taiwan Meetup. Microsoft Most Valuable Professional since 2020.\nhttps://chechia.net\n2018 Ithome Cloud Summit 2018 Ithome Kubernetes Summit 2019 Ithome Cloud Summit 2020 Ithome Cloud Summit 2020/12/18\tCloud Native Taiwan 年末聚會 2020/8/17\tDevOps Taiwan Meetup #26 - 從零開始導入 Terraform 2021 Ithome Cloud Summit\nSuggested Audience 推薦有 k8s 使用經驗的從業人員，對 k8s 有上手經驗 問可不可以在 k8s 上面跑 database 的人 ","date":1653066252,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"56672428516db290abe4cd5185237096","permalink":"https://chechia.net/zh-hant/post/2022-05-21-coscup-proposal-operating-time-series-database-in-k8s/","publishdate":"2022-05-21T01:04:12+08:00","relpermalink":"/zh-hant/post/2022-05-21-coscup-proposal-operating-time-series-database-in-k8s/","section":"post","summary":"Titleq 在 k8s 上跑 time series database 甘苦談 - Operating Time Series Database in K8s\nGoogle Slides Youtube live record: https://youtu.be/YexUnVOZC8M?t=9421\nDescription Influxdb 為市占最高的 time series DBMS 之一，使用上與 RDBMS 有不同優劣勢。","tags":["kubernetes","devops","terraform"],"title":"2022 05 21 COSCUP Operating Time Series Database in K8s","type":"post"},{"authors":null,"categories":["Technology"],"content":"各位好\n關於這個 QRcode\n每次上台前，我都會想要帶什麼樣的內容給觀眾，讓觀眾值得花 30 分鐘在底下聽。後來就習慣先發表一篇文章，把對觀眾有幫助的資源包成一包：\n首先是完整投影片：https://slides.com/chechiac…/terraform-introduction-a56697 逐字講稿：(最後校稿中） 然而只有本次演講內容，回去可能還是不太容易操作。所以這次附上使用 Terraform 一鍵部署 vault 的 Github Repository：https://github.com/…/southe…/chechia_net/vault/singleton 如果不熟 Terraform，再附上 30 天手把手 Terraform 教學文章，只要願意花時間，全篇中文一個月帶你上手 Terraform。 IThome 鐵人賽好讀版：https://ithelp.ithome.com.tw/users/20120327/ironman/4057 Github Repository 完整版：https://github.com/…/terraform-30…/tree/main/lecture/zh 如果遇到問題還是需要找人發問，所以再推薦兩個社群，可以來這邊發問，要找我本人也找得到。甚至只是加入潛水不講話，都可以被動吸收許多新知。 Cloud Native Taiwan User Group https://t.me/cntug https://fb.cloudnative.tw DevOps Taiwan Meetup Group https://t.me/devopstw 大家可以手機拍完這個就出去吃午餐了。 或是拍回家，然後傳給一個同事叫他花 30 天把 Terraform 跟 Vault 這些都學會。\n總之希望對各位有幫助，讓國內技術力能持續進步成長。\n回到本次演講。\n本次演講有三個關鍵字\nKubernetes Vault Terraform 這個是隱藏的 請問平時工作會用到這三個技術之一的朋友，請舉個手，好讓我知道一下觀眾的分布，等等分享的內容會照比例做一些調整。\n我們今天不會講太多 Kubernetes 的內容，重點放在 Vault，以及如何設定 Vault，所以 Terraform Infrastructure as Code 或是 configuration as Code 會在這邊跑出來。\n關於我，我是哲嘉。我在 Maicoin 當 SRE。常出現的社群是 CNTUG 與 DevOpsTW。\n我們今天談的更大的主題其實是 Key Management 私鑰管理，或是密碼管理。這是一個很大的題目，今天演講內容只是其中的一個實作案例。\n舉個例子，一邊是 API Server，另一邊 Database，或是第三方服務\nDatabase 來 Authenticate 合法的 Client 用戶端，可能是 username + password，或是 API Key + Secret，或是 Access Token，或是 Private Key，Client Certificate，都可以。\n在跨不同平台或是介面的服務，我們常用的 Auth 方法。認 Key 不認人。\n那中間這些 Key 要怎麼管理，就有很多學問 其中最基本的，是怎麼配置給 API Server 讓他使用 注意：讓 API Server 使用，隱含的意思是，其他人不管是其他微服務，或是開發工程師，閒雜人等都不能看到。\n這邊我們假設 API Server 是在 Kubernetes 裡面跑，微服務架構，所以 API Server 可能是一個 Pod，我們 SRE 要為這組 Pod 配置密碼。\n這邊列出的應該是 K8s 比較常見的幾種做法。\nplain text，直接寫進 file 讓 Pod 去讀取 k8s secret 做 base64 encode 安全一點的透過外部機制作加密解密 或是寫在 API Server 的 memory 中 事實上如果沒有使用 K8s，使用 VM 或是公有雲 Container Service，應該都是類似的原理，大家都是 Linux base，secret 放進來看要放在 disk 或是 memory 裏面。\n實際放到 K8s 大概會長長這些 yaml\n最簡單，就直接把 secret 壓到 Pod env 裏面，\n最簡單也最危險\n所有看得到 yaml 的人都明碼看見密碼 所有能進到 file system 提外話\nAPI Server 被從正面打穿，滲透到拿到這組密碼的機會多高？\n如果 API Server golang library 有在跟安全性更新 Kubernetes 用公有雲的 Kubernetes Service，有在更新 OS ami 跟 docker image 都有在更新 然後有功能正常的防火牆 打掛蠻有可能的，但打穿服務到拿到密碼，是有難度的。\n更多時候，至少在幣圈有被爆出來的資安事件，大多是是公司員工被釣魚信掉到，被植入惡意軟體在 local 電腦，然後他又看得到明碼的密碼，直接爆炸。\n明碼糟糕的地方，大家都看得到，一開始就是 exposed 的狀態，風險不可控，也無從管制。\n然後是 K8s secret，也是從 env 掛進去 Pod\n放在 k8s secret，是 base64 的格式\n看起來跟原先內容不一樣了，有人就跟我說，他們家的 k8s secret 有用 base64 加密。\nencode 跟 encrypt\nk8s secret 有什麼問題\n明碼 plain text 的問題，不該看到的人很容易就看到 根本的問題還是 RBAC 懶得設定，大家都用 default role 進來 要用可以，先看 k8s secret 後面的儲存實作是什麼？\n是 k8s etcd 透過 k8s API server 存取 etcd 跟 kube-api 一般來說是夠安全的。官方文件還建議\nsecret 要加密 encrypt 增強 RBAC 控制，只有特定 role 才看得到 secret，，而不是每個人都用 default role 近來 k8s，然後進到 namespace 全部 secret 就看光光 RBAC 有設定好，有加密，是可以做到安全。當然還是沒有專門 key Management 工具，如 Vault 有額外管理上的優化功能。\n加密範例可以看強者我朋友的文章\n加密完可能長這樣，還要額外透過其他機制才能進行解密，拿到原始資料\n例如透過 k8s controller 解密 或是透過 vault server 或是透過公有雲的 Key Management Service 做解密 其他的 k8s secret 加密解決方案\n今天我們使用 Vault，其中一個目的就是要坐中間這段 Safe Magic\n給這個 Pod secret 然後只給這個 Pod Secret 當然 Key Management 其他還有一堆事情要處理\n密碼洩漏的話有沒有 revoke 機制\n能不能定時 Rotate 汰換密碼？\n改架構，底下的設定好不好耕著動態調整，還是要跟著 rename / mv k8s secret\n怎麼做稽核，怎麼檢查內容。我看不到 vault 幫我看一下內容對不對，這個很容易發生。\n臆想頭就很痛\n今天的目的很單純\n密碼的露出盡量小 最好密碼是有期限的，逾期自動失效 暴露了可以 revoke Vault\n有人用過？這邊簡介一下\n在 CNCF 的 landscape 漱渝 Key Management，應該是裡面市佔最高的開源項目\n重點就是保護與管理 secret\nVault 的核心概念，這個影片是 Hashicorp 官方介紹的影片，講得很好，大家自己回去看一下\n這邊簡單 vault 101\n現在有一台 vault server 已經設定好了，我們可以使用 vault client 連線\n例如這邊\n使用 VAULT TOKEN 告訴 vault 你是什麼身份的用戶 或是進行 login，Vault 會呸發一組臨時的 TOKEN 給你 拿著組 TOKEN 去問 Vault，請問我可以要 /user/mysql 這個路徑下的資料嗎？\nVault 檢查 TOKEN 的 role 與 permission，可以就回傳值 不行就 permission denied\nVault 就是金庫，真正重要的 key 存在裡面，使用這要來問 Vault，要先過 Vault 這關\nVault 實際存放 Secret Engine，這邊也跳過，大家先把他當 key / value 存放好了 XD\n等等，這樣 vault Auth 有點怪\n本來拿 username / password 去控制 Mysql 先在多一步，先拿到 VAULT TOKEN，再拿 TOKEN 去跟 Vault 拿 Mysql password，再去連 MySQL\n咦這樣不是很怪？我如果 Vault token 暴露了，有心人士還是可以從 vault 拿到資料啊\n這樣有比較安全嗎？還是只是花式被駭\n對，只是做 token 交換的話，還是不夠，所以 Vault 有提供許多 Auth method\ntoken 只是其中一種 有很多認證方法不用 token 交換，但也能讓 vault 認得 k8s pod 與 api server 這是今天的重點之一，token / 密碼傳遞不安全，那就用其他手段 auth\n以 AWS IAM auth 為例\n如果今天是在 aws ec2 上跑，那可以透過 aws internal api 去取得身份認證資料，也就是 ec2 instance metadata\n拿這個 metadata 去問 vault，vault 再透過 aws api 去確認，這個 ec2 instance 真的是合法的\n然後依據 ec2 instance 身份，配發權限跟資料\napi server ec2 就給 api server secret frontend server ec2 就給 frontend secret 中間沒有多餘的密碼 / token 交換\n來往的對話大概是這樣\nAWS API 是可信的 Vault 自己維護是可信的 服務透過三方交握去認證，認 runtime 環境的 metadata，不再是認 key 不認人 至於 api server ec2 近來 vault 後，應該有什麼樣的權限，在 vault 內部透過 policy 配置\n設定 path 設定允許的 operation 例如 read write list delete … 等 runtime 動態調整\n當然 Vault 還有提供很多更加安全的功能\n例如如何安全地存放 secret Storage 這邊跳過\nSecret + auth 搭配跳過\n以及 dynamic secret，來解認 key 不認人的問題\n例如 mysql 的靜態 username / password，透過 vault 設定可以變成動態的\n回到 k8s，使用 token auth，然後把 VAULT TOKEN 放在 k8s secret 裏面，只有比較安全一點點\n就是 VAULT TOKEN 可以快速 rotate 跟 revoke\n這邊可以搭配其他 auth 方式來解決\n剛剛不是讓 vault 去認 aws ec2？\n在 k8s 中，可以讓 vault 去認 k8s cluster / service account\n路徑圖長這樣\npod runtime 都會有 service account 使用 service account 的 metadata 去問 vault vault 去問 k8s，這個 service account 是真的假的 k8s 回答 vault，Pod 跟service account 是合法的 vault 再配權限給 Pod …","date":1636995852,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"dd1551f4eb238bd451484b5d83882cbe","permalink":"https://chechia.net/zh-hant/post/2021-11-16-ithome-cloud-summit-vault/","publishdate":"2021-11-16T01:04:12+08:00","relpermalink":"/zh-hant/post/2021-11-16-ithome-cloud-summit-vault/","section":"post","summary":"各位好\n關於這個 QRcode\n每次上台前，我都會想要帶什麼樣的內容給觀眾，讓觀眾值得花 30 分鐘在底下聽。後來就習慣先發表一篇文章，把對觀眾有幫助的資源包成一包：\n首先是完整投影片：https://slides.com/chechiac…/terraform-introduction-a56697 逐字講稿：(最後校稿中） 然而只有本次演講內容，回去可能還是不太容易操作。所以這次附上使用 Terraform 一鍵部署 vault 的 Github Repository：https://github.com/…/southe…/chechia_net/vault/singleton 如果不熟 Terraform，再附上 30 天手把手 Terraform 教學文章，只要願意花時間，全篇中文一個月帶你上手 Terraform。 IThome 鐵人賽好讀版：https://ithelp.ithome.com.tw/users/20120327/ironman/4057 Github Repository 完整版：https://github.com/…/terraform-30…/tree/main/lecture/zh 如果遇到問題還是需要找人發問，所以再推薦兩個社群，可以來這邊發問，要找我本人也找得到。甚至只是加入潛水不講話，都可以被動吸收許多新知。 Cloud Native Taiwan User Group https://t.","tags":["kubernetes","vault","terraform"],"title":"2021 11 16 Ithome Cloud Summit Vault","type":"post"},{"authors":null,"categories":["kubernetes","terraform"],"content":"2021 ithome 鐵人賽開跑了～\n今年的題目是「Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天」，是一個長達 30 天的 terraform workshop，對 terraform 有興趣的朋友歡迎追縱\nDay 01 - 引言：Terraform 是個好東西。\n課程內容與代碼會放在 Github 上: https://github.com/chechiachang/terraform-30-days\n賽後文章會整理放到個人的部落格上 http://chechia.net/\n追蹤粉專可以收到文章的主動推播\n","date":1630470306,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"4564241cb1a4b220e0de74caa8dd4a1d","permalink":"https://chechia.net/zh-hant/post/2021-09-01-ithome-ironman-go/","publishdate":"2021-09-01T12:25:06+08:00","relpermalink":"/zh-hant/post/2021-09-01-ithome-ironman-go/","section":"post","summary":"2021 ithome 鐵人賽開跑了～\n今年的題目是「Terraform Workshop - Infrastructure as Code for Public Cloud 疫情警戒陪你度過 30 天」，是一個長達 30 天的 terraform workshop，對 terraform 有興趣的朋友歡迎追縱\nDay 01 - 引言：Terraform 是個好東西。\n課程內容與代碼會放在 Github 上: https://github.com/chechiachang/terraform-30-days","tags":["terraform","iac","鐵人賽2021"],"title":"2021 09 01 Ithome Ironman Go","type":"post"},{"authors":null,"categories":null,"content":"Event https://devops.kktix.cc/events/meetup33-maicoin-devops-taiwan https://youtu.be/8hEifTAWnY0?t=3269 audience: 90/90 resources this presentation\nhttps://slides.com/chechiachang/terraform-introduction-a56697 github\nhttps://github.com/chechiachang/terraform-azure https://github.com/chechiachang/vault-playground/tree/master/deploy/v0.8.0 ","date":1620894566,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"35859312eb3813377645389ac87c1439","permalink":"https://chechia.net/zh-hant/post/2021-05-13-devops-taiwan-33-hashicorp-vault-for-kubernetes-apps/","publishdate":"2021-05-13T16:29:26+08:00","relpermalink":"/zh-hant/post/2021-05-13-devops-taiwan-33-hashicorp-vault-for-kubernetes-apps/","section":"post","summary":"Event https://devops.kktix.cc/events/meetup33-maicoin-devops-taiwan https://youtu.be/8hEifTAWnY0?t=3269 audience: 90/90 resources this presentation\nhttps://slides.com/chechiachang/terraform-introduction-a56697 github\nhttps://github.com/chechiachang/terraform-azure https://github.com/chechiachang/vault-playground/tree/master/deploy/v0.8.0 ","tags":null,"title":"DevOps Taiwan Meetup #33: Hashicorp Vault for Kubernetes","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n需求與問題 隨著 terraform 在團隊內的規模持續成長，團隊需要讓工作流程更加順暢，來面對大量的 tf 變更查核與變更請求。想像幾十個工程師同時在修改幾十個不同的 terraform projects / modules，這時可能會有幾個問題\n需要一個穩定乾淨的環境執行 terraform 工程師的開發本機不是個好選擇 需要 24/7 的 terraform 執行中心 執行中心會有各個環境 (dev / stage / prod) 的存取權限，希望設置在內部 下列兩個工作會切換工作平台，例如 Github review，檢視 difference，與討論 PR review 完有時會忘記 merge，merge 完有時會忘記 apply repository 越多，忘得越多… 團隊已經導入 Git-flow，希望把工作流程做得更完整自動化更加便利\nAtlantis 解決方案是與版本控制整合，提供 terraform 執行的 worker，病可以與 Git Host (e.g. Github) 整合做 PR + Review，來達成持續的 CI/CD 遠端執行，自動 plan 與自動 apply merge。也就是 Atlantis 幫我們處理以下幾件事\nGit-flow TODO 自動化 plan TODO Webhook 回傳 plan 結果 TODO 透過 bot 控制 apply TODO 自動 merge 需求與工作流程 以下的範例使用\nGithub 作為版本控管工具 是整個工作流程的控制中心 tf code，review，plan，apply 的結果都會在 Github 進一步整合到 Slack 上也非常方便 Atlantis 放在 Kubernetes 上 使用 helm chart 部署 如果需要其他的版本控制工具或是安裝方式，請見Atlantis 官方文件，Atlantis 支援非常多版本控制工具，例如 Github，Gitlab，Bitbucket，…等\n整個工作流程\nGithub commit push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Github PR push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Github Merge event / main branch push Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Atlantis acquire access to site (dev / stage) Atlantis apply to site Github release tag push (eg. 1.2.0-rc) Github event -\u0026gt; Atlantis webhook Atlantis run terraform validate plan Atlantis acquire access to site (dev/stage) Atlantis apply to site (release-candidate / prod) 依據上面的環境，安裝需要準備以下幾個東西\nGit Git Host 上設定 Atlantis 存取 Git Repository 的權限 為 Git Host 設定存取私鑰，讓 Atlantis 認證 webhook Terraform Backend State storage: Atlantis 需要存取外部的 state storage Terraform 使用的版本要注意一下。Atlantis 可以支援不同 repository / project 使用不同版本 Environment credential (provider credential) Atlantis 會需要存取不同的環境 (dev / stage / prod) 為這些環境獨立配置 credential 讓 Atlantis 存取 實際 Atlantis 的環境會有\nDocker 作為孤城師本機開發測試使用 Kubernetes Atlantis 我們的團隊會是一個環境一個獨立的 Atlantis 安全性：切分各個環境的權限與 Access 各個 Atlantis webhook 只接收屬於自己的 event Docker 使用 Docker 作為本地開發與測試的容器 runtime：\ndocker pull runatlantis/atlantis:v0.15.0 docker run runatlantis/atlantis:v0.15.0 atlantis \\ --gh-user=GITHUB_USERNAME \\ --gh-token=GITHUB_TOKEN Helm Chart Helm 的安裝十分簡易\nhelm repo add runatlantis https://runatlantis.github.io/helm-charts helm inspect values runatlantis/atlantis \u0026gt; values.yaml 更改 values.yaml\ngithub: user: chechiachang token: ... secret: ... orgWhitelist: github.com/chechiachang/* 安裝到 Kubernetes 上\nhelm install atlantis runatlantis/atlantis -f values.yaml 結果 工程師透過 Github 就可以完成所有工作 加上 Github Slack 整合，就完全不用離開 Slack，跟 bot 聊天就可以完成所有 terraform 工作 安全穩定的 terraform 執行環境 獨立的 in-cluster credential，集群的存取權限都控制在集群內，不會暴露到外部環境 自動 plan 與 apply，不會遺忘 優劣 優點\n可以 self-hosted，credential 不外洩，確保最高的安全性 已經整合 Kubernetes, helm chart webhook 整合許多版本控制庫，例如 github, gitlab,… 實現安全的遠端執行 本地執行還是會有諸多問題 terraform cloud 提供的遠端執行 缺點，其實沒什麼缺點\n就需要多養一組 Atlantis 使用 stateless deployment，其實是應該不會有什麼負擔 Terraform 系列至此應該是全部完結，感謝各位\n","date":1602040548,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"358feb9ca2bde56b386e8fcd36eebbae","permalink":"https://chechia.net/zh-hant/post/2020-10-07-terraform-infrastructure-as-code-atlantis/","publishdate":"2020-10-07T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-07-terraform-infrastructure-as-code-atlantis/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac","aws"],"title":"Terraform Infrastructure as Code: Atlantis","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt 問題與需求 當團隊已經開始大規模使用 terraform，tf 檔案越來越多。我們為了增加程式碼的重複利用性，會使用 terraform module 將常用的 tf 檔案封裝。\n隨著導入的規模越來越大，這些 module 會越來越多，而且使用這些 module 的 project 也增加 隨著管理的 infrastructure 越來越複雜，為了描述這些本來就很複雜的 infrastructure，module 不只越來越多，還會出現 module 引用其他 module，大 module 使用小 module 的 nested module 這點在複雜的 provider 中常出現，例如 使用 terraform 描述網路架構 描述複雜的 IAM \u0026amp; Role terraform vault-provider 管理的環境越來越多，例如 dev, staging, prod,…，每個環境都需要獨立的工作空間，造成大量重複的 tf code 使用一段時間 terraform ，很快就會發現的第一個問題是：不論怎麼從 tf 檔案中提取重複部分，做成 module，還是有很多部分的 tf code 是完全重複的，例如：\n每個 module 會定義的 variable (argument)，使用這個 module 的上層會需要提供傳入 variable 每個 module 都會需要提供 provider Terraform 的語法要求上面這些參數都明確的定義，這讓整個 module 或資料夾的描述非常清楚。但會處就是，這些描述的參數道出都是，而且不斷的重複，每個 module 或資料夾都要提供，不然在 terraform validate 時會因為應提供參數未提供，導致錯誤。雖然立意良好，但卻嚴重違反 DRY (Don’t Repeat Yourself) 的原則\nmodule 多層引用，project 引用大 module，大 module 又引用小 module 開始感覺 backend 與 provider 的 code 比其他功能 code 多了 有沒有可能用其他工具，避免這些重複的參數，backend，provider 等等？\nTerragrunt 推薦有大量使用的團隊，直接使用 Terragrunt 這款工具。\n他是一層 terraform 的 wrapper\ni.e. 執行 terraform plan -\u0026gt; terragrunt plan terragrunt 會先解析，產生重複的 code，然後再執行 terraform 編輯時只要寫一次，terragrunt 代為在其他地方產生重複的 code 產生的 code 會被隱藏與 cache Terragrunt 有許多功能，例如\n處理 Backend，provider，與其他不斷重複的 tf code 處理多環境下重複的 tf …完整功能請見官方文件 導入 Terragrunt 可以避免重複代碼，降低維護成本，提升生產效率\n使用情境 上述說明可能不太清楚，我們實際看範例，以前幾篇我們使用的範例 repository 為例子，把 gcp 的資料夾目錄應該長這樣：\n可以很清楚見到底下幾個東西不斷重複 由於 my-new-project, gke-playground, national-team-5g 三個專案是獨立的專案，各自可以獨立運行。但其實同屬於同公司的專案，可能許多內容都是重複的。\ntree gcp gcp ├── README.md ├── Makefile ├── my-new-project/ │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── gke-playground/ │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── national-team-5g/ │ ├── dev │ │ ├── terraform.tf │ │ ├── terraform.tfvars │ │ ├── variable.tf │ │ └── provider.tf │ ├── stag │ │ ├── terraform.tf │ │ ├── terraform.tfvars │ │ ├── variable.tf │ │ └── provider.tf │ └── prod │ ├── terraform.tf │ ├── terraform.tfvars │ ├── variable.tf │ └── provider.tf ├── templates/ └── modules/ 可以很清楚見到底下幾個東西不斷重複\nprovider.tf 這裡描述使用的 provider，這邊只有使用 gcp provider，所以是完全一樣重複 variable.tf 是定義的參數 terraform.tfvars 是傳入的參數，也就是實際各專案各自的執行參數 national-team-5g 的專案下，又各自拆分多個執行環境，每個環境獨立，所以又有許多重複的 code 這邊的目的，是有系統化的處理這些重複的代碼\ngcp ├── terraform.tfvars # gcp 共用的參數 ├── terragrunt.hcl # gcp 共用的程式碼 ├── national-team-5g/ │ ├── terraform.tfvars # national-team-5g 共用的參數 │ ├── terragrunt.hcl # national-team-5g 共用的程式碼 │ ├── dev │ │ ├── terraform.tfvars # dev 的參數 │ │ └── terragrunt.hcl # dev 環境自己的程式碼 │ ├── staging │ │ ├── terraform.tfvars # staging 的參數 │ │ └── terragrunt.hcl # staging 環境自己的程式碼 │ └── prod │ ├── terraform.tfvars # prod 的參數 │ └── terragrunt.hcl # prod 環境自己的程式碼 差異非常多是吧，但這邊只是從資料目錄結構看，其實如果從 tf 檔案內部的程式碼看，裡面的代碼更是精簡到不行，用起來非常的爽XD\n安裝 去 release 頁面找適合的執行檔案 下載 chmod u+x terragrunt mv terragrunt /usr/local/bin/terragrunt 詳細文件請見\n範例 Terragrunt 的 DRY feature，其實內容都大同小異\n├── national-team-5g/ │ ├── dev │ │ ├── terraform.tfvars # staging 的參數 │ │ └── provider.tf │ ├── stag │ │ ├── terraform.tfvars # staging 的參數 │ │ └── provider.tf │ └── prod │ ├── terraform.tfvars # prod 的參數 │ └── provider.tf\n例如\nnational-team-5g/dev/provider.tf\nprovider \u0026#34;google\u0026#34; { version = \u0026#34;~\u0026gt;v3.25.0\u0026#34; credentials = file(var.credential_json) project = var.project region = var.region } 改成\nnational-team-5g/dev/terragrunt.hcl\ngenerate \u0026#34;provider\u0026#34; { path = \u0026#34;provider.tf\u0026#34; if_exists = \u0026#34;overwrite\u0026#34; contents = \u0026lt;\u0026lt;EOF provider \u0026#34;google\u0026#34; { version = \u0026#34;~\u0026gt;v3.25.0\u0026#34; credentials = file(var.credential_json) project = var.project region = var.region } EOF } 執行\ncd national-team-5g/dev terragrunt plan 關於參數\nvar.region 是本地參數，可以依據 dev/staging/prod 參數各自設定 var.project 這個是整個 national-team-5g 都共用的參數，可以進一步提高到更上層 i.g. 放在 national-team-5g/terraform.tfvars 檔案裡面，透過 terragrunt.hcl 傳遞。 Functions tf 檔案，在許多 block 中禁止使用 variable，這層限制有其考量，但是缺點是造成許多 hard coded 的程式碼。\nTerragrunt 產生的程式碼，由於是在 terragrunt 執行後，terraform 執行前，因次沒有這層限制，可以做許多事情，例如 code generate 以及 function 運算，terragrunt 提供許多內建 function，這邊只介紹常用幾個。\nfind_in_parent_folder() path_relative_to_include() 範例兩個檔案\nproject/dev/terraform.hcl\ninclude { path = find_in_parent_folders() } project/terraform.hcl\nenv = path_relative_to_include() 最後 parse 後的結果\nproject/dev/terraform.hcl\nenv = \u0026#34;dev\u0026#34; 做了兩件事\n要求 project/dev 去上層尋找 terraform.hcl， …","date":1601954148,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"4931936b48903ad9f4ca7c041c2d11cb","permalink":"https://chechia.net/zh-hant/post/2020-10-06-terraform-infrastructure-as-code-terragrunt/","publishdate":"2020-10-06T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-06-terraform-infrastructure-as-code-terragrunt/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac","aws"],"title":"Terraform Infrastructure as Code: Terragrunt","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 工具 Terraform Atlantis Terragrunt 工具與文化 新工具提供解決方案，然而單純導入工具後不是就一勞永逸，許多實務上的問題，還是要依賴改善工作流程，並且避免整體運作的錯誤。\n其次，不同團隊已有既有的團隊文化，整合新的工具後還是需要磨合，不一定要照單全收。換句話說，工作流程的不斷改進也是解決方案的一環。\n建議實踐 Recommended Practices Terraform 官網有許多建議的實作與導入流程，其中大部分的建議我們都已經在前面的幾篇文章中提到，這邊要來說明一下，並補充其他官方推薦的實踐。\n技術複雜度: 隨著架構的複雜增加，維護整體架構的困難逐漸增加 組織複雜度: 隨著團隊規模增長，分工與權責越顯複雜，團隊的協作困難逐漸增加 Terraform 的目的，在於減少上面兩者的複雜度。\n工作目錄結構 工作目錄 (workspace) 是 terraform 運作的基準點，容納 tf 檔案，通常會使用版本控制工具管理。\n一個環境一個工作目錄 這個我們在前面的範例已經實踐，基本上目錄為\nproject (git repository) ├── api-server/ │ ├── dev │ │ └── terraform.tf │ ├── stage │ │ └── terraform.tf │ └── prod │ └── terraform.tf ├── grpc-server/ │ 將產品或專案切割成各自獨立的環境，以某專案的某環境作為管理單位\nproject-api-server-dev project-api-server-stage project-api-server-prod project-grpc-server-dev … 這麼做有幾個好處：\n確保產品與環境的獨立 彼此不互相影響 但又可確保彼此的關連性，例如架構相同 以環境的為管理單位，換句話說，一組完整的環境 直接對應到 一組完整的工作目錄 管理上非常直觀明確 使用工作目錄分配權限與權責 直接為不同團隊分配不同工作目錄的存取權限 權責與工作目錄的明確對應 持續評估與改進 自動化程度也是工作流程進步的指標：\n手動控制 半自動化 全自動化 導入 terraform 後的我們已經描述過了，內容詳情請見上上篇文章。下面描述官方推薦的從最開始，完全沒有 terraform 經驗開始導入流程，以筆者個人於公司從零開始導入的經驗，非常直得參考。\n如何從完全手動操作，變成半自動操作 如何從半自動，變成 IaC (infrastructure as code) 如何從 IaC，變成 IaC 多人協作 進階改進 IaC 多人協作 如何從完全手動操作，變成半自動操作 如果團隊應該是還在完全手動控制 infrastructure\n查驗 (audit) 困難 無法複現 (reproduce) 拓展 (scale) 困難 很難分享 infrastructure 的知識 第一步要執行的是：選擇少部分，可以控制的 infrastructure 開始導入 terraform。所以要做的就是\n開始使用 terraform 如果需要可以參考一些範例專案 Terraform: Get Started collection 開始進入半自動階段後，團隊中的少部分人員開始使用 terraform，手上也有了可運作的少部分 infrastructure as code，可以作為 demo ，或是其他團隊成員的教育訓練，這個可以幫助下一階段的演進。\n如何從半自動，變成 IaC (infrastructure as code) 目前半自動的工作專案內容應該是：\nTerraform code 手動操作的流程 一些輔助腳本 下個階段希望繼續推展 terraform 的使用，降低手動步驟與腳本執行。這個階段可以做的事情\n使用版本控制管理 tf code 這邊假設團隊本來就有使用版本控制，開始將 tf code 導入版本控制的工作流程 開始將先產生的 tf code 移入版本控制 所有團隊都能共享新 tf code 的知識 開始使用第一個 terraform module 一個簡單的方式是將重複使用的 infrastructure 抽出，減少重複的 tf code 這邊需要提醒，盡量以完整個 infrastructure 作為單位抽離 tf code 作為 module 完整的 infrastructure 也是在 provider 中間轉成的單位 持續推廣團隊內 terraform 的使用 開始設定工作準則 (Guidelines) 來描述並規範工作流程 在團隊上不完全熟悉 terraform 前，可以提升工作效率，推廣最佳實踐，並且降低錯誤風險 團隊的架構師可以依據團隊文化形塑工作流程，更符合團隊需求 開始導入 configuration management 例如 Chef cookbook 私鑰與隱秘資訊管理 導入 vault 來管理 terraform 整合 vault，可以使用 terraform 管理 vault 的結構，使用 vault 來管理 terraform 所需的 credentials 如何從 IaC，變成 IaC 多人協作 導入版本控制可以降低堆人協作的複雜度，下個階段需要\n統一跨團隊的工作流程 使用 terraform 管理團隊的 IAM 權限 可以執行的改進如下：\n官方推薦使用 Terraform Cloud 來做後台，我們稍候推薦的幾個免費工具也有相似功能，團隊可以參考使用。這邊專注在需求與方法。 開始設計整個組織間的工作目錄結構 工作目錄要反映 獨立的環境與獨立的專案 負責管理的團隊組織，已分配存取權限 這部分的實作後面的文章也會提到 官方推薦的 Terraform Cloud 可見官方文件 進階改進 IaC 多人協作 如今團隊已經有多人協作的介面，也有完整的工作流程，我們可以藉由以下改進，達成更堅固的框架\n官方建議大量導入 Terraform Cloud，但這會超出免費額度 我們之後的文章會提供開源版本的解法 簡化工作目錄的管理 明確的 review 與 audit 工作流程 增加 infrastructure 的監測與效能監控，這些都可以使用 terraform 設置 具體實作，請見下篇文章\n","date":1601867748,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"e7204c2d088ce225d600bffe6c70e999","permalink":"https://chechia.net/zh-hant/post/2020-10-05-terraform-infrastructure-as-code-recommended-practices/","publishdate":"2020-10-05T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-05-terraform-infrastructure-as-code-recommended-practices/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac","aws"],"title":"Terraform Infrastructure as Code: Recommended Practices","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt Terraform Backend 剛開始使用 terraform 的時候，大家的第一個範例應該都是 local backend 吧，就是直接在本地 terraform apply，在目前的工作目錄下產生 state 檔案。這個 state 檔案直接 cat 打開來看後，可以發現裡面一切都是明碼的。初學時筆者感覺所謂的 Terraform backend 只是一個存放中繼的 state 資料的 workspace，後來發現完全不是這麼回事，便立刻棄用了 local backend。\n之後依照官方推薦就使用了 Terraform Cloud，後來便出現許多問題，等等會分析。\n最後團隊選用了自家公有雲的 backend，例如\nAWS S3 作為 state storage，DynamoDB 作為中心化的 workflow lock GCP 完整 terraform backend 支援清單可以見官方網站。\n這篇文章要來仔細探討所謂的 terraform backend，backend 的重要性，與如選擇適合自己團隊的 backend。\n問題 如果使用 tf random 產生亂數密碼，直接去 cat state 檔案就可以看到明碼的 random 數值。\n多人協作 lock\n另外一個解法是，根本不使用 backend，terraform 也支援這樣的做法。雖然官方也明講 backend are completely optional，但依照筆者的經驗，強烈建議多人團隊務必去啟用，並找尋適合自己的 Backend。\n需求 資訊安全，希望 terraform 使用是安全的，不會暴露敏感資訊 多人協作，希望可以同時工作，但又不會互相衝突 遠端操作，避免本地操作 State 存放與鎖 預設的 backend 是 loal backend，也就是執行 terraform apply 後，本地會出現一個 JSON 格式的 state 檔案。然而 local state 會立刻遇到的問題，就是\n第一個是協作困難，apply 的結果別人看不到，不能接續著做 每個人都可以 apply 但不同人的 apply 沒有相依性 可是遠端的 infrastructure 有，A B 不同人一起 apply 到 infrastructure 上，可寧就會衝突，或是產生不可預期的錯誤 所以筆者強烈推薦使用外部的 backend 來取代 local backend。多半 backend 會多做許多事，透過控制 state 來確保 infrastrure 的完整性，例如對 state 存取有以下的限制:\nＡ透過鎖來控制，禁止多人同時存取，例如同時有兩個人 apply 相同或不同檔案，先取得 backend lock 的人執行，後來的人會被 terraform 阻擋 避免多頭馬車的問題 確保 state 的唯一性，使用另外一個 repo 的 state 檔案，遠端的 state 會拒絕存取 確保 state 的順序，每次 apply 的 state 都是依序產生 如果 state 是舊的，可能就會被遠端的 backend 拒絕，避免使用舊的 apply 覆蓋新的 infrastructure 這些限制，如果使用 local backend 才會容易遇到，使用外部的 backend 其實不容易會發生。Terraform 基於 Infrastructure as Code 實現，可以將整個 terraform repo 視為 code 一部分，這樣就可以想像為何這些 state 限制是重要的。\nstate 跟隨 tf 檔案，隨著 tf 檔案的 commit 推進，state 也跟著推進 commit 1 的 tf 檔案，apply 後產生 state 1 如果今天有團隊 force push 了一個 conflict 的 tf 檔案，硬要 apply 的結果也可能造成 infrastructure conflicts 如果今天團隊有多個 branch 同時開發，branch A apply 的 state 會與 branch B apply 的 state 也可能造成衝突 State 手動更改 在很特殊的狀況下，你也可以手動更改 state file，然後 push 上傳，但這點非常不建議，terraform 會自動維護 state 的完整性，手動更改可能會直接破壞正常的 state。什麼情形會用到？就是你的 state 因為某個原因被玩壞，大部分是人為弄壞的。這時候才被迫要手動更改 state。手動更改 state ，講白了不做死就不會死。\nvault state pull vim your-local-state-file # Increment Serial if needed vault state push # vault state push -force Hosted: Terraform Cloud Terraform Cloud，是官方提供的解決方案，有提供較多功能，例如在 terraform cloud 的網站上遠端 plan。有提供免費版，提供最多 5 人團隊使用。也有提供進階的方案 $20/user 或是 $70/user，以及 enterprise 版本，可以本地安裝。\n使用很簡單，我在前幾篇提供的範例 repository全部都是使用 terraform cloud 作為 Backend。提供\n遠端的 state 儲存庫 所有團隊成員使用各自帳號登入 plan 之前會 sync terraform cloud 上面的 state 遠端的 state lock 如果 lock 被人佔據，表示有其他人正在使用，會取消新的操作，避免衝突 也提供線上檢視 state ，或是線上修改 state 的功能 貼心小功能，但多半用不到 託管的 Terraform Cloud 作為 backend 他有幾個問題\n如果要啟用遠端 plan，需要綁定版本控制服務器，例如綁定 Github 或 Gitlab 基於安全性考量，這點很多公司就直接打槍了 暴露原始碼給第三方公司，可能會讓公司的安全性檢驗不過 加上 terraform 的 Git Code，基本上就是所有 infrastructure 的資訊 加上如果有使用 terraform 編輯 IAM 或是 provision vault，等於許多敏感資料都會出現在 terraform repository 中 如果是重視安全性的團隊，則至少要使用可以 self-hosted 的 Terraform Enterprise，而不要使用公有的 Terraform Cloud。然而 Enterprise 我是沒用過，請不要問我價錢。\nConsul Consul 嚴格來說不是單純的儲存庫，他是 Service Networking 設定與服務發現的解決方案，只是本身帶有 key value 的儲存功能，就被自家整合。換句話說，他不是專門拿來做 terraform backend 的，比較像是如果團隊本來就有使用 consul，可以考慮公用儲存庫，作為 terraform backend。\n這是相同公司 Hashicorp 提供的自家的 backend，整合的很完整。但就只是單純的儲存庫，沒有 terraform cloud 的遠端執行啊，或是線上檢視 state 檔案的功能。\n可以在公司內部自行架設一組 cluster，然後就可以作為 backend。\n問題是\nConsul 這麼大一包，結果只使用了裡面的 kv store Consul 是分散式的 Key-value store，不熟悉的話不太好養 換句話說，如果死了救得起來嗎 Etcd Etcd 跟 consul 類似，雖然是專業的儲存庫，但是使用這麼複雜的分散式儲存庫，只作為 terraform 的 backend，顯然很不經濟。\n可以在公司內部自行架設一組 cluster，然後就可以作為 backend。\n一樣只建議已經有在使用 etcd ，且熟悉維運 etcd 的團隊，才考慮使用 etcd 兼作為 terraform backend。\n這些分散式的儲存庫，不太容易死，然而萬一死了可能不太好救。\nPublic Cloud AWS S3 + DynamoDB GCP gcs + pg Azurerm + pg\n只有單純的 state storage 與 lock 的功能，沒有什麼花俏的線上執行或是快速 review。\n好處是\n使用非常單純 也是處在安全的內網環境中 有於是公有雲提供的服務，基本的 IAM 與權限控管可以直接應用 Postgresql Postgrel 也是一個不錯的選擇\nTerraform 並不會帶來大量的資料庫負擔，所以可能會把 terraform 與附載較低的應用，共用資料庫。\n使用上作為 state storage 與 lock 很單純沒什麼問題\n其他 官方還有提供所有支援的 backend\n如果 Kubernetes 還有做其他事情的話，請不要用 Kubernetes secret 作為 terraform 的 backend\n","date":1601781348,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"1a9f99fd076abd2eb693e41d09ddcdf0","permalink":"https://chechia.net/zh-hant/post/2020-10-04-terraform-infrastructure-as-code-backend/","publishdate":"2020-10-04T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-04-terraform-infrastructure-as-code-backend/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac","aws"],"title":"Terraform Infrastructure as Code: Backends","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講解 Terraform 的基本操作流程，提供範本原始碼，以及一步一步導入的詳細步驟。各位應該都可以依照上面幾篇的說明，開始快樂的使用 Terraform 了。\n而當使用 Terraform 的規模越來越大，管理的資料越來越多時，開始會出現一些問題，例如重複的 terraform code 越來越多，協同工作 review 不太容易，state 的內容管理與鎖管理，等等。這些問題可以透過一些工作流程的改進，或是導入新的小工具，來改善工作效率。\n接下來筆者推薦幾個心得與工具，希望能提升使用 Terraform 的效率與產值\n以下幾篇文章，適合已經使用過 terraform 一點時間，有經驗的團隊，並打算更大規模導入 terraform，正在尋求改善的方向。\n心得 CI/CD 全自動化 State backend 選擇 最佳實踐 https://www.terraform.io/docs/cloud/guides/recommended-practices/index.html 工具 Terraform Atlantis Terragrunt CI/CD 全自動化 當公司成功導入 terraform ，並且整合 Git-flow 的工作流程後，應該可以很明顯的感受到，整體的 infrastructure 產出穩定度有大幅提升，畢竟軟體工程中 code review 是穩定度的核心關鍵之一，而導入 terraform 與 infrastructure as code 讓 infrastructure 也能在合理的工作流程中被層層 review。\nstable master -\u0026gt; feature/add-new-infra -\u0026gt; PR feature/add-new-infra -\u0026gt; master\nfeature branch merge 進去 master 之後，就確定是 review 過而且穩定的程式碼，再由工程師 pull 最新的 master ，然後在本地執行 terraform apply，把 infrastructure 生出來。\n這樣做本質不會有什麼大問題，畢竟 code 已經是穩定。然而，實務上卻還是會出現偶發的問題。例如：使用錯誤的 credentail 或 context，導致 dev 或 staging 的 infrastructure apply 到 prod 上，直接 p0 issue 大爆發，SRE 都跪著上班。又例如：使用錯誤的 master 版本 apply，結果也是服務掉線，整個 team 跪著上班。\n關鍵：團隊整體的安全性，是由程度最菜的同事決定。\n對我就是在說你XD。然而人都會菜，而人事成本也是公司經營的關鍵考量，相信所有同事都大神是不切實際的，不如改進工作流程，近一步降低人為操作失誤的可能。人的問題，根本之道還是教育，然而我們可以試著用技術與工具降低風險。\n以下的做法，可能會協助避免這個問題。我們要做的就是 CI/CD 的全自動化。\n需求 避免 apply 失誤 避免愚蠢的錯誤 環境切換錯誤 apply 錯版本 (愚蠢的錯誤比你想的要多，出現後會讓你三觀大開) 加速工作流程 PR 結束後，在合適的時間自動 apply 自動回報結果 出錯自動 rollback 上個穩定版本 最小權限原則 (least privilege access) 原本工程師為了 apply ，會有 admin 權限的 credential 移轉到安全的 CI/CD server 上，在 server 上執行 工程師不再握有這些超級管理員權限，避免工作機被駭的安全隱憂 NOTE: 工程師被釣魚 (phishing) 或是社交工程攻擊 (social engineering attacks) 才是導致公司服務被害的主因，不可不防 解決方案 選擇安全的 CI/CD server，例如在內網的 self-hosted Jenkins server 將 terraform 的執行點，從工程師本機移轉到 CI/CD 服務器上 更改 CI/CD ，執行以下步驟 Terraform validate Terraform plan 的結果輸出到 Github / Slack plan 結束後停住 CI/CD，發送一個 apply request 到 Github comment，不再繼續執行 apply SRE 主管只要透過 Github comment 或是 Slack bot 就可以選擇合適的時間，approve apply 最後移除大多數工程師的 terraform 權限 範例 事實上，不同家的 CI/CD server，工作流程都是類似\ncheckout initialize tools / SDK (ex. az/aws/gcloud client) inject public cloud credential (ex. Azure/AWS/GCP key) terraform validete terraform plan terraform apply (option) require manual approve 如果是 Jenkins 的使用者，可以參考 Azure 提供的範例\n環境管理 在人工 apply 的工作流程中，工程師需要自行切換環境，例如 git repo 工作目錄如下\ntree project ├── dev ├── staging └── prod cd project/dev; terraform plan # plan staging cd project/prod; terraform plan # plan prod 這是相對比較安全的做法，在對的資料夾目錄下，就會 apply 到正確的環境，程式碼與環境有緊密的對映。除了以下幾種情形\n想要部署 dev，結果沒注意到自己在 staging 或是 prod 有一部分的 input variable 會影響結果，然後又輸入錯誤的 input 到錯誤的環境上 對很愚蠢，但我都見過（氣血攻心）。\n既然導入了自動化，環境的切換可以自動切換，例如\n所有 feature branch 執行 CI/CD server 上的本地測試 lint init validate 所有 PR 都會觸發新的 dev 環境部署 plan apply 測試腳本 所有 master merge / push 都會觸發 staginge 部署 QA 測試 壓力測試 (optional) 所有 release candidate tag 會部署到 release candinate release management 所有發布版本的 tag (ex. 1.1.0 / 1.2.0-release) 會部署 prod 當然要事先通知相關人士 stack holders 資深工程師只要控制 branch / tag 就可以控制發布。\n你說這樣，萬一天兵去自己打 tag 打錯 commit，或是推錯 branch 推到 master或是 release candidate，還不是依樣爆掉。那你可以把 master 與 release branch 鎖起來 (protected branch) ，然後把 tag push 權限鎖住。\n你說還是錯 那我… …們看下一段 orz\n安全性 雖然說是自動化改善工作流程，然而收回存取權限，對於服務的整體安全性大幅提升，畢竟是可以更改 infrastructure 的管理員帳戶。Terraform 既然能夠新增修改雲端的 infrastructure，這個帳號的權限是相當大的，萬一金鑰(GCP/AWS/Azure credentail) 流出或遭駭，後果都是毀滅性的，例如可以直接刪除服務的 infrastructure，或是修改防火牆的規則，偷埋其他金耀，…等於是整座公有雲送給駭客。所以我們使用 Terraform 應該要慎重考慮存取權限的安全性。\n本來是每個 SRE 的本機電腦上，可能都會有這把帳戶權限。\n如果是 self-hosted Jenkins server，或是 Github enterprise server，把服務權限移轉到這些服務器，便可以確保金鑰永遠都在公司的防火牆內部網路，更加大幅度的提升整體的安全性。\n其他 可以進一步做金鑰權限分割，將底下四個權限透過公有雲的 IAM role 去切割。要是萬一金鑰還是外洩了，可以降低損失。\n讀取 新增 修改 刪除 你說這個不用自動化就可以做，我說如果分割金鑰然後人工自己切換操作，反而會增加操作的複雜度，增加錯誤的機會。然後工程師的痛苦程度，與手上的金鑰數量成正比。\n或是利用環境存取金耀分割，把金耀進一步切割成不同的權限，萬一掉了，損害也控制在一個環境之內。例如：\ndev staging prod 不同環境的 infrastructure ，在創建初期就是透過不同的帳號產生的，彼此不會有不乾淨殘留的帳號權限。\n小結 這邊就講兩件事\n自動化可以防呆 自動化可以增加安全 參考文件 Terraform Official doc: Running Terraform in Automation ","date":1601694948,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"ee597de7e0d59a4adfd309e27f519bd3","permalink":"https://chechia.net/zh-hant/post/2020-10-03-terraform-infrastructure-as-code-automation/","publishdate":"2020-10-03T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-03-terraform-infrastructure-as-code-automation/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac","aws"],"title":"Terraform Infrastructure as Code: CI/CD automation","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n上面講了很多 terraform 的操作範例，應該看到這裡，對於 terraform 基本上是什麼東西，應該有些概念了。然而這樣還不能算是學會 terraform，這種工具的東西一定要有實際操作過的經驗才算是學會。\n可以直接參考 Terraform 官方的 Get-started 文件來操作學習，我這邊也提供一個 Git repository 讓大家上手，當作初次操作的框架。\n提供做為範例的原始碼 這個 Github Repository 是我給社群演講所使用的範例，第一次使用的可以參考\nhttps://github.com/chechiachang/terraform-playground\ntree ├── README.md ├── SOP.md ├── aws/ ├── azure/ └── gcp/ TL;DR 選擇使用的雲平台，這邊提供三家範例，例如我這邊使用 gcp，當然你就要準備 GCP 的帳號，並且下載有執行權限的用戶 credential json key 等等。\n雖然我沒收 gcp 錢，這邊還是推廣一下 gcp 的 free credit 試用。阿要用 Azure Cloud 的 free credit 來執行這個範例也是完全沒問題，非常夠用。唯有 AWS 的試用方案跟剩下兩家不太一樣，這個 repository 起的服務可能會超過 AWS 的免費額度涵蓋範圍，總之請自己注意。\ngit clone https://github.com/chechiachang/terraform-playground cd gcp DIR=my-new-project make project cd my-new-project vim *tf terraform init terraform plan terraform apply 這樣應該就會跑完。然後我們講解幾個地方。\n工作目錄 Terraform 預設是以當下執行的目錄作為基準，掃描資料夾中的 .tf 檔案。所以可以把一個一個獨立的專案先用資料夾裝好，彼此內容互不干涉。\n我們這邊創建新的 subdirectory，這邊是以 my-new-project 為範例。這邊指的 project 只是一個 terraform resource 範圍，可以但不用是一個真實的 gcp project。terraform 執行是以一個 directory 為範圍，不同 project directory 可以透過不同 terraform 指令控制。如果是獨立的服務希望獨立管理也可以切開。\n我寫了一個簡單的 Makefile，進一步封裝基本的指令，基本上不需要 Makefile 以外的操作。 make project 是其中一個指令，幫忙創建資料夾、布置 Makefile 與基本的 terraform 設定等等。\n如果團隊有多人協作，非常推薦使用統一 Makefile / 或是 bash script 封裝，統一這些編輯的雜事，降低不同人編輯出錯的風險。\n目錄結構 總之我們現在 cd 到 my-new-project 的工作目錄下，這個目錄代表一個專案。其他的 gke-playgound 與 national-team-5g 也是其他的專案，先忽略他。\ngcp ├── README.md ├── Makefile ├── my-new-project/ ├── gke-playground/ ├── national-team-5g/ ├── templates/ └── modules/ cd my-new-project 進入 my-new-project 下，可以看到裡面已經有一些檔案，我們首先要編輯的是這個 terraform.tf\nmy-new-project ├── Makefile ├── provider.tf ├── terraform.tf └── variables.tf vim terraform.tf terraform.tf 是 terraform 本身的設定 這邊是 Terraform Backend 的設定，如果不知道什麼是 terraform backend 這個我們明天的文章會講。這邊使用的 backend 是 terraform 官方自家的 terraform cloud，可以在網站上\n註冊使用者，填到底下 organization 這裡 創建一個 workspace，填到 workspace.name 這裡 terraform { # Create a remote organization on https://app.terraform.io backend \u0026#34;remote\u0026#34; { # Provide terraform credential by # - terraform login (suggested) # - use User API Token #token = \u0026#34;\u0026#34; hostname = \u0026#34;app.terraform.io\u0026#34; organization = \u0026#34;chechia\u0026#34; workspaces { name = \u0026#34;terraform-playground\u0026#34; } } } provider.tf 是 provider 的設定 terraform client 會把 tf 檔案拿來運算，透過 Provider ，將需求實際轉化成 API call ，然後送到公有雲或是其他目標。這邊就只講到這樣。\nprovider 為了工作，可能會需要提供一些參數，例如 google 的 provider 會需要在這裡提供 credential_json 的路徑，請把它放在適合的地方，然後用絕對路徑指向 google\nNOTE: 不要 commit credential key 到 git repository 裡面。可以放到外層資料夾，或至少要 gitignore 掉。\nprovider \u0026#34;google\u0026#34; { version = \u0026#34;~\u0026gt;v3.25.0\u0026#34; credentials = file(var.credential_json) project = var.project region = var.region } variable.tf 做參數的存放點 雖然上面 tf 檔案使用了 terraform / provider / variable ，但 terraform 掃描檔案時，檔名本身並不會影響。也就是說，參數你想擺哪就擺哪。不過上面是常見的命名慣例，這樣擺人類比較容易找得到。\nvariable 這邊設定的參數，比較像是 arguments，也就是當其他位置的 tf 檔案，引用這個資料夾作為 module 的時候，作為參數輸入的 placeholder，其他 tf 檔案可以使用 variable 關鍵字定義的參數，例如: var.project，或是 provider.tf 裡的 var.credential_json。\nvariable 關鍵字也可以定義 default 預設值，如果沒有定義 default，也沒有從外部傳入 argument，會在 validate 時造成 error。\n# Global variable \u0026#34;project\u0026#34; { type = string default = \u0026#34;myproject\u0026#34; } variable \u0026#34;credential_json\u0026#34; { type = string default = \u0026#34;../credentials/gke-playground-0423-aacf6a39cc3f.json\u0026#34; } variable \u0026#34;region\u0026#34; { type = string default = \u0026#34;asia-east1\u0026#34; } Create 這裡我們試著創建一台 GCE，使用下列指令，會發現多了一個檔案 compute_instance.tf。\nNAME=my-new-gce make gce my-new-project ├── Makefile ├── compute_instance.tf ├── provider.tf ├── terraform.tf └── variables.tf 內容大概是\nmodule \u0026#34;my-new-gce\u0026#34; { source = \u0026#34;../modules/compute_instance\u0026#34; providers = { google = google } project = var.project name = \u0026#34;my-new-gce\u0026#34; image = \u0026#34;https://www.googleapis.com/compute/v1/projects/centos-cloud/global/images/centos-7-v20200429\u0026#34; machine_type = \u0026#34;n1-standard-1\u0026#34; network = \u0026#34;projects/${var.project}/global/networks/myproject\u0026#34; subnetwork = \u0026#34;projects/${var.project}/regions/asia-east1/subnetworks/myproject\u0026#34; } module 關鍵字定義一組資源，具體的內容是這裡，簡單來說可以把一堆 tf 檔案放在一塊，然後把需要的參數使用 variable.tf 拉出去，讓其他地方引用。\nsource = \u0026#34;\u0026#34; 是實際引用的 module 來源\n底下是這個 module 需要用到的參數，例如 project, name, image, machine_type,… 等是 gce 這個 module 需要的參數。\nMakefile NAME=my-new-gce make gce 這行指令與 terraform 無關，只是一個快速生成 compute_instance.tf 的小腳本。使用這個腳本可以\n快速產生 tf 檔案 產生標準化的 tf 檔案，所有 project 的 compute_instance 都長一樣 抽換名子 name 使用 symbolic link 讓所有 project 資料夾使用同一個 Makefile，keep your code DRY。\n最後就是常規的 plan 與 apply，這邊沒有什麼特別的。\nterraform plan terraform apply 小結 有規律地整理 project 可以降低維護成本 善用 module 封裝，可以提高整體的重用性與易用姓，提高開發效率 使用 template tf 可以加速重複的資源產生步驟 問題: 此時的資料夾中還是充滿大量重複的 code，例如到處都需要 provider、重複的 module，一大堆重複的東西。有沒有可能再讓我們的程式碼更 DRY 一點呢?\nTerragrunt 幫我們實現這點，非常值得使用的工具。請見下篇分享。\n","date":1601608548,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"ac35504a295f1db57a557e760dfaab0e","permalink":"https://chechia.net/zh-hant/post/2020-10-02-terraform-infrastructure-as-code-repository-example/","publishdate":"2020-10-02T11:15:48+08:00","relpermalink":"/zh-hant/post/2020-10-02-terraform-infrastructure-as-code-repository-example/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","鐵人賽2020","iac"],"title":"Terraform Infrastructure as Code: Example repository","type":"post"},{"authors":[],"categories":null,"content":"先占虛擬機與 Kubernetes 在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制\n資料中心會 (可預期或不可預期地) 終止先占虛擬機 先占虛擬機不能自動重啟，而是會被資料中心終止後回收 GCP 不保證有足夠的先占虛擬機 節點的終止會造成額外的維運成本，例如\n管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機 管理多個應用複本，節點終止時，維護整體應用的可用性 將移除節點上的應用，重新排程到其他可用節點 動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints) 意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效 配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量 這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要\n補足被移除的 15 個節點 計算被移除的應用，補足移除的應用數量 移除失效的應用端點，補上新的應用端點 持續監控應用狀態 … 沒有自動化管理工具，看了心已累 (貓爪掩面)\n我們使用 Kubernetes 協助維運自動化，在 GCP 上我們使用 GKE，除了上述提到的容器應用管理自動化外，GKE 還額外整合先占虛擬機的使用\n啟用先占虛擬機的節點池 (node-pool)，設定節點池的自動拓展，自動補足先占節點的數量 GKE 自動維護先占虛擬機的 labels 關於 GKE 的先占虛擬機的完整細節，請見GCP 官方文件。這份文件底下也提供了 GCP 官方建議的先占虛擬機最佳實踐\n架構設計需要假設，部分或是全部的先占虛擬機都不可用的情形 Pod 不一定有時間能優雅終止 (graceful shutdown) 同時使用隨選虛擬機與先占虛擬機，以維持先占虛擬機不可用時，服務依然可用 注意節點替換時的 IP 變更 避免使用有狀態的 Pod 在先占虛擬機上 (這點稍後的文章，我們會試圖超越) 使用 node taint 來協助排程到先占虛擬機，與非先占虛擬機 總之，由於有容器自動化管理，我們才能輕易的使用先占虛擬機。\nGKE 然而，決定使用 GKE 後，就有許多關於成本的事情需要討論\n先看 GKE 的計費方式 pricing\n每個 GKE 集群管理費用 $0.1/hr = $72/hr 這個費用是固定收費，只要開一個集群，不論集群的節點數量。所以在節點多、算力大的集群裡，這個費用會被稀釋，但在節點少的集群裡比例會被放大。\n然後 GKE 還是會有一些自己的毛，俗話說有一好沒兩好，我們使用它的好處同時，也要注意許多眉眉角角。再來爬文件。如同最前面宣導，用產品就要乖乖把文件看完，不過這裡先針對與先占虛擬機相關的議題\nAllocatable Resource Regional Cluster Cluster autoscaler Allocatable Resource 在網路上看到這篇好文 GKE 上的可使用的資源 Allocatable Resource。啥意思呢？難道還有不能使用的資源嗎？\n沒錯，GKE 會保留一定的機器資源 (e.g. cpu, memory, disk)，來維持節點的管理元件，例如 container runtime (e.g. Docker)、kubelet、cAdvisor。\n也就是說，就算我們跟 GCP 購買了算力，有一個比例的資源我們是使用不到的。細節請見 理解 GKE 集群架構。這會影響我們單一節點的規格，我們也需要一並計算，能實際使用的資源 (allocatable resource)。\nAllocatable = Capacity - Reserved - Eviction Threshold\nCapacity，是機器上實際裝載的資源，例如 n1-standard-4 提供 4 cpu 15 Gb memory Reserved，公有雲代管集群，預保留的資源 Eviction Threshold：Kubernetes 設定的 kubelet 驅逐門檻 驅逐門檻 (Eviction threshold) Kubelet 會主動監測節點上的資源使用狀況，當節點發生資源不足的狀況時，kubelet 會主動終止某些 Pod 的運行，並回收節點的資源，來避免整個節點資源不足導致的系統不穩定。被終止的 Pod 可以再次排程到其他資源足夠的節點上。細節請見 官方文件 Scheduling and Eviction\n在 Kubernetes 上，我們可以進一步設定驅逐門檻，當節點的可用資源低於驅逐的門檻，kubelet 會觸發 Pod 驅逐機制\nGKE 上每個節點會額外保留 100 MiB 的記憶體，作為驅逐門檻，意思是當節點耗盡資源，導致剩餘記憶體低於 100 MiB 的時候，會直接觸發 GKE 的 Pod Eviction，終止並回收部分的 Pod。換句話說，這 100 MiB 是不能被使用的資源。細節請見官方文件\n集群保留資源精算 資源的定義，使用雲平台的一般費用大多來自此\ncpu memory storage 然後是這個表，注意保留的資源是累進級距\n255 MiB of memory for machines with less than 1 GB of memory 25% of the first 4GB of memory 20% of the next 4GB of memory (up to 8GB) 10% of the next 8GB of memory (up to 16GB) 6% of the next 112GB of memory (up to 128GB) 2% of any memory above 128GB\n值計上能夠用到的資源，底下 GCP 也整理好了，例如 n1-standard-4 實際使用的是 memory 12.3/15，cpu 3.92/4。\n在維持合理的使用率下，開啟大的機器，可以降低被保留的資源比例，依照筆者公司過去經驗，GKE 起跳就是 n1-standard-4 或是以上規格，如果低於這個規格，可調度的資源比例真的太低，應該重新考慮一下這個解決方案是否合乎成本。\n但究竟什麼規格的機器適合我們的需求，說實在完全要看執行的應用而定。\n","date":1601112260,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"650c28bef3fe40dc9d01be07c20dfe80","permalink":"https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/","publishdate":"2020-09-26T17:24:20+08:00","relpermalink":"/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/","section":"post","summary":"先占虛擬機與 Kubernetes 在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制\n資料中心會 (可預期或不可預期地) 終止先占虛擬機 先占虛擬機不能自動重啟，而是會被資料中心終止後回收 GCP 不保證有足夠的先占虛擬機 節點的終止會造成額外的維運成本，例如\n管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機 管理多個應用複本，節點終止時，維護整體應用的可用性 將移除節點上的應用，重新排程到其他可用節點 動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints) 意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效 配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量 這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Kubernetes","type":"post"},{"authors":[],"categories":null,"content":"先占虛擬機，技術文件二三事 第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。\nGoogle 先占虛擬機官方文件\n使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。\n以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要\n多作功課，仔細查閱官方文件，理解服務的規格 深入分析自身的需求 基於上面兩者，量化分析 什麼是先占虛擬機器(Preemptible Instance) 先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如\n目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用 目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go) 由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。\n先占機器的優缺點 除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點\n比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由 先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。\nGCP 不保證會有足夠的先占虛擬機 先占虛擬機不能直接轉換成普通虛擬機 資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉 先占機器排除在 GCP 的服務等級協議 (SLA)之外 先占虛擬機不適用GCP 免費額度 費用粗估試算 至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。\n以常用的 N1 standard 虛擬機：https://cloud.google.com/compute/vm-instance-pricing#n1_standard_machine_types\nHourly Machine type\tCPUs\tMemory\tPrice (USD)\tPreemptible price (USD) n1-standard-1\t1\t3.75GB\t$0.0550\t$0.0110 n1-standard-2\t2\t7.5GB\t$0.1100\t$0.0220 n1-standard-4\t4\t15GB\t$0.2200\t$0.0440 n1-standard-8\t8\t30GB\t$0.4400\t$0.0880 n1-standard-16\t16\t60GB\t$0.8800\t$0.1760 n1-standard-32\t32\t120GB\t$1.7600\t$0.3520 n1-standard-64\t64\t240GB\t$3.5200\t$0.7040\n如果是用 GPU 運算：https://cloud.google.com/compute/gpus-pricing\nModel\tGPUs\tGPU memory\tGPU price (USD)\tPreemptible GPU price (USD) NVIDIA® Tesla® T4\t1 GPU\t16 GB GDDR6\t$0.35 per GPU\t$0.11 per GPU NVIDIA® Tesla® V100\t1 GPU\t16 GB HBM2\t$2.48 per GPU\t$0.74 per GPU\n依據虛擬機規格的不同，先占虛擬機大約是隨選虛擬機價格的 2 到 3 折。在 AWS 與 Azure，由於計費方式不同，有可能拿到 1 折左右的浮動價格。從各種角度來說，都是非常高的折數。\n不妨說，這整系列文章，都是衝這著個折數來的 XD。畢竟成本是實實在在的花費，工作負載 (workload) 合適的話，應該盡量嘗試導入。\n這個折數還有另外一個效果是，可以在相同成本下，添增更多資源算力，作為解決方案。什麼意思呢？就是如果工作負載合適的話，可以使用更高規格的先占節點，整體成本反而會下降。\n至於究竟差多少，需要依據規格與定價詳細試算才知道。底下我們就來算算看。\n","date":1601089420,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"7a2edcde9c17465b24dde23d7621e8c5","permalink":"https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/","publishdate":"2020-09-26T11:03:40+08:00","relpermalink":"/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/","section":"post","summary":"先占虛擬機，技術文件二三事 第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。\nGoogle 先占虛擬機官方文件\n使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。\n以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要\n多作功課，仔細查閱官方文件，理解服務的規格 深入分析自身的需求 基於上面兩者，量化分析 什麼是先占虛擬機器(Preemptible Instance) 先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如\n目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用 目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go) 由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。\n先占機器的優缺點 除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點\n比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由 先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。\nGCP 不保證會有足夠的先占虛擬機 先占虛擬機不能直接轉換成普通虛擬機 資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉 先占機器排除在 GCP 的服務等級協議 (SLA)之外 先占虛擬機不適用GCP 免費額度 費用粗估試算 至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Introduction","type":"post"},{"authors":[],"categories":null,"content":"關於資源評估 架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？\n以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。\n導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：\n每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載 如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載 此外還有機會因為上篇提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低 只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源 例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu 希望整體資源的使用率夠高，當然預留太多的資源會造成浪費 要控管 Kubernetes 的資源使用量也可設定資源需求與資源限制，延伸閱讀。\n估計得越準確，當然實際部署的資源掌握度就越高，然而筆者過去的經驗，團隊在交付源碼時未必就能夠做出有效的資源消耗評估，那有沒有什麼辦法可以幫助我們？\n資源需求估估看 如果應用開發團隊，有先作應用的 profiling，然後 release candidate 版本有在 staging 上作壓力測試的話，維運團隊這邊應該就取得的數據，做部署前的資源評估。\n應用在不同狀態或是工作階段，會消耗不同的資源\n例如：運算密集的 batch job 可能會有\n控制節點 (master node) 啟動後會佔有一定的資源，一般來說不會消耗太多，只是需要為控制節點優先保留資源 工作節點 (worker node) 啟動時會需要預留足夠的資源，接收工作後會逐漸增加資源使用，拉到滿載 例如：面向用戶的服務，可能會有\n啟動應用所需的資源 沒有大量請求，只維持基本應用運行所使用的資源 負載壓力灌進來時，消耗資源隨用戶請求數量的成長曲線，設定的安全上界 如果沒有這些數據，其實維運很難事前估計資源，變成要實際推上線後見招拆招，基於實際的資源消耗去做自動擴容，其實有可能會造成資源的浪費，因此我建議如果開發團隊沒有作 profiling，維運團隊可以在工作流程內簡單加一步 profiling，目前主流語言都有提供相關工具，簡單的執行就可以獲得很多資訊。\n至於壓力測試，也是可以使用基本的工具(例如 Artilery)簡單整到工作流程。特別是面對客戶的應用，務必要進行壓力測試。\n有了上述的資源需求數據，才能事先安排機器的規格。例如\n應用是面對客戶的 API server 基本資源是 200m cpu 1Gi memory，這部分直接寫進 Kubernetes resource request，在排程時就先結點上預留。 負載拉到 1,000 RPS，latency 95% 20ms 99% 30ms，這時的資源需求的上界大約 2 cpu 4 Gi memory 超過 1,000 RPS 就應該要透過水平擴展去增加更多 instamce 如果單跑這個 API server，就可以安排 memory 8，cpu 4 的 GKE node-pool，讓負載落在可用資源的 60%-70%，這樣還有餘裕可以承受大流量，給自動水平擴展做動的時間。\n資源調整參考 當然這些數據都可以依照時實際需求調整，資源要壓縮得更緊或更鬆都是可行的。\n如果應用有整合分析後台，例如 Real-time Uer Monitoring、或是基本的 Google Analytics，都可以觀察這些調整實際對用戶帶來的影響，用戶行為改變對公司營收的影響，全都可以量化。例如\n機器負載拉到 80%，cpu 的壓力，導致 API latency 增加到 95% 50ms 99% 100ms 此時用戶已經很有感了，會導致 0.1% 用戶跳轉離開 而這 0.1% 的用戶，以往的平均消費，換算成為公司營收，是 $1,000/month 把機器負載壓到 60%，只計算 cpu 的數量的話，需要多開 3 台 n1-standard-4 機器，共計 $337.26/month 提供老闆做參考，老闆可能會趨向加開機器 當然上面的例子都非常簡化，變成國中數學問題，這邊只是提供一個估算的例子。現實中的問題都會複雜百倍，例如機器規格拉上去出現新的瓶頸、例如依賴的服務，message queue、database 壓力上升，或是公司內部問題，就拿不到預算(血淚 SRE)。如果要減少機器，也可以參考，一般來說聽到關機器省錢的話，老闆都會接受的 XD。\n回到先占機器 根據上面的國中數學，把應用一個一個都計算清楚，需求逐漸明確了。假設，架構團隊拿到開機器的工單，掐指一算，決定\n1 GKE cluster 6 n1-standard-4 這些都是隨選虛擬機，價格大約是 $747/month (含集群管理費 $73/month)。今天有人腦動大開，那如果全部換成先占節點呢？變成 $265/month，虛擬機費用 $192 / $674 = 0.28 直接打超過三折。\n有人就擔心，這樣真的可以嗎？真的沒問題嗎？會不會影響用戶阿。\n答案是會，就是會影響用戶 XD\n聽到這邊很多人就怕了。但是怎麼個影響法呢，還需要看底下幾個段落，如果換成先占虛擬機，用戶會怎麼受到影響。我們也要試圖量化這個影響，當作要不要導入的判斷依據。\n句個反例，「我覺得可以」「你覺得不行」，或是「某某公司的某某團隊可以啊」「我們公司也來做吧」，這些都是很糟糕的理由。除了對內部毫無說服力之外，也沒有辦法作為導入成效的指標，會讓團隊陷入「導入了也不知道有比導入前好？」或是「具體導入後成效量化」，會影響團隊做出真正有效的判斷，應極力避免。\n此外，問行不行之前，其實需要知道團隊願意為了三折機器，付出多少成本。如果只是每月省個台幣 15,000，工程師薪水都超過了。但如果手下有三十台或三百台以上，也許就非常值得投資。成本不是絕對的，很多時候要與其他成本 (e.g. 開發人力時間成本) 一起考量。\n以上都是說明導入的動機，以下說明先占虛擬機的各種機制，以及對應用的實際影響。\n","date":1601007722,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"887128d2f5d034ae96eca13ed213831a","permalink":"https://chechia.net/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/","publishdate":"2020-09-25T12:22:02+08:00","relpermalink":"/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/","section":"post","summary":"關於資源評估 架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？\n以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。\n導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：\n每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載 如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載 此外還有機會因為上篇提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低 只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源 例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu 希望整體資源的使用率夠高，當然預留太多的資源會造成浪費 要控管 Kubernetes 的資源使用量也可設定資源需求與資源限制，延伸閱讀。","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Resource Calculation","type":"post"},{"authors":[],"categories":null,"content":"需求規劃 使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 Google Cloud Pricing Calculator 精算成本。\n另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。\n這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。\n所以這邊先定義我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 Batch Job 常見的範例，例如\n使用網路爬蟲 (crawler) 去抓取許多網站的所有內容 使用 GPU 進行機器學習的 Model Training 大數據計算 MapReduce 這些任務的核心需求，很簡單直接\n盡快完成整體工作 盡可能節省大量算力成本 例如：我手上的機器學習 Model 粗略估計 10000 小時*GPU 的算力需求，才能產出一個有效的Model。由於大量的算力需求，一般來說都會選擇分散式的運算框架 (ex. MapReduce) ，將真正消耗算力的工作，使用分而化之 (divide and conquer) 的架構設計，將分配任務的控制節點 (master)，與實際進行運算的工作節點(worker) 拆分。基於原本的分散式架構，幾乎可以無痛地將工作節點轉移到先占虛擬機上。\n根據上述的需求，這類的工作特性可能有\nCPU / GPU 算力需求高的運算節點 (Worker) Worker 本身是無狀態的 Stateless 可控的即時負載 將整體工作切分成任務單元 (task)，分配給工作節點 任務單元的狀態外部保留，工作節點可容錯 (fault-tolerent)，任務單元可復原 由於先占虛擬機可能是浮動價格，這類工作可以根據優先程度，調整合適的工作時間，例如在資料中心算力需求低，先占虛擬機的費用低廉時，啟用較多的工作節點加快運算，如果費用高時，可以降低先占虛擬機的使用，延後工作，甚至是調用不同區域，費用低的工作節點，來降低整體的成本。\n執得注意的是，這類任務的控制節點 (master)，也許是集中式的，也許是分散式的，需要根據性質考量，是否適合放在先占虛擬機上。有些架構控制節點可以容錯，然而錯誤發生後會需要復原狀態，這時會消耗額外的算力，可能會拖緩整體進度，造成算力的消耗。也許就可以考量使用隨選虛擬機配合使用。\nUser-facing services 常見的範例，例如\nRestful API server Websocket Server TCP/HTTP reverse proxy 這些工作的核心需求如下：\n整題服務的高可用性 (high availability) 承受不可預期的負載高峰 (load spikes) 整體表現需要低延遲 (low latency) 可以水平擴展 (horizontal scaling)，支撐用戶的成長 最終平衡效能呈現與成本 由於會面對使用者，需要能支持使用者的壓力，又同時需要有一定的服務效能，來維持使用者體驗。實務上設計可能採用無狀態應用 (stateless)，多副本 (replica) 部署到集群中。需要儲存的狀態（如用戶狀態），使用外部的共用儲存 (例如：Redis，RDBMS，或是 Non-SQL DB)。請求的流量，透過上游的負載均衡器 (Load Balancer)，送進多個後端，處理完成請求後，再返回給使用者。\n這樣的設計，使用先占虛擬機也不會有太多的問題\n現代的附載均衡器多半都能像後端做可用性檢測 (health check)，可以把流量導向工作正常的節點，如果後端的虛擬機被資料中心收回了，流量也會移轉到其他節點上，不會遺失用戶請求 配合自動水平拓展工具 (Auto horizontal scaler)，可以設定期望的服務節點數量，如果資料中心回收先占節點，拓展工具可以同時去取得新的先占節點，或是取得隨選隨用的虛擬機 配合流量監測，也可以動態調整期望的服務節點數量，例如：偵測到大量用戶連線數時，增加更多服務節點，待流量下降後，再降低服務節點數量 這樣的設計實務上有幾點注意\n雖然說應用後端本身是無狀態的，但面對用戶也許還是會有部分狀態存在應用外部，例如：User session，或是 websocket 的長連線。特別注意這些服務斷線的時候，對於使用者的影響，配合前端增強使用者體驗 後端水平拓展後，瓶頸會轉移到其他地方，例如 Database 成為效能瓶頸，應用這邊需要做一定程度的自律( ex. connection limit，rate limit)，避免不斷增長的應用壓垮依賴的服務，如 MessageQueue 或是 Database 分散式的儲存中心 (distributed DB)，如：cassandra 或是小強 DB。而這樣類型的服務，是否適合放在先占節點上？\n","date":1600925952,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d179df620641a513191bb3e0445ce8f4","permalink":"https://chechia.net/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/","publishdate":"2020-09-24T13:39:12+08:00","relpermalink":"/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/","section":"post","summary":"需求規劃 使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 Google Cloud Pricing Calculator 精算成本。\n另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。\n這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。\n所以這邊先定義我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 Batch Job 常見的範例，例如\n使用網路爬蟲 (crawler) 去抓取許多網站的所有內容 使用 GPU 進行機器學習的 Model Training 大數據計算 MapReduce 這些任務的核心需求，很簡單直接","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Requirement","type":"post"},{"authors":[],"categories":null,"content":"先占虛擬機終止流程 (Preemption process) 子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。\n這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。\n如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如\n應用重啟初始化時 cpu memory 突然拉高 承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance …等等 如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。\n這邊有幾個面向要注意\nGCP 如何終止先占節點 GCP 移除節點對 GKE 、以及執行中應用的影響 GKE 集群如何應對的節點失效 GCP 自動調度補足新的先占節點 GKE 集群如何應對節點補足 三個重點\n先占虛擬機終止對集群的影響 Pod 隨之終止對應用的影響，是否能夠優雅終止 有沒有方法可以避免上面兩者的影響 劇透一下：有的，有一些招式可以處理。讓我們繼續看下去。\nGCP 如何終止虛擬機 先占虛擬機的硬體終止步驟與一般隨選虛擬機相同，所以我們要先理解虛擬機的停止流程\n這裡指的終止 (Stop) 是虛擬機生命週期 的 RUNNING -\u0026gt; instances.stop() -\u0026gt; STOPPING -\u0026gt; TERMINATED 的步驟。\ninstances.stop() ACPI shutdown OS 會進行 shutdown 流程，並嘗試執行各個服務的終止流程，以安全的終止服務。如果虛擬機有設定Shtudown Script 會在這步驟處理 等待至少 90 秒，讓 OS 完成終止的流程 逾時的終止流程，GCP 會直接強制終止，就算 shutdown script 還沒跑完 GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內 虛擬機變成 TERMINATED 狀態 GCP 如何終止先占虛擬機 與隨選虛擬機不同\n先占虛擬機的時間 30 秒 搭配 GKE 使用 Managed Instance Group，終止的虛擬機會被刪除，Autoscaler 會啟動新的虛擬機 一樣先看先占虛擬機的說明文件：終止流程\n資料中心開始回收先占虛擬機，選中我們專案其中的一台先占虛擬機 Compute Engine 傳送 ACPI G2 Soft Off，這裡 OS 會試圖安全關變服務，也會執行 shutdown script，可以做簡短的優雅終止 30秒後，ACPI G3 Mechanical off 但 30 秒能做什麼？只能快速的交代當前進度。如果應用需要花時間收尾，保存工作進度，可能會產生許多問題\nGCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內 在面對大量 IO 的工作，可能會導致者台虛擬機的大量應用一起進入優雅終止，先占虛擬機最後耗盡資源，來不及做完 如果可能會超時，或是沒完成會有資料遺失風險，就不能在這個階段處理 依賴 shutdown script 做收尾是危險的，我們之後要想辦法處理這個不保證做完的優雅終止。\n如果應用本身有容錯的框架，或是有容錯機制，我們這邊要額外做的工作就會少很多。例如許多程式框架提供自動重啟的功能，在外部保存 checkpoint，worker 只負責運算，終止信號一進來，也不用保存，直接拋棄未完成的工作進度，留待繼起的 worker 從 checkpoint 接手。\nPreemption selection 除了 24 小時的壽命限制會終止虛擬機，資料中心的事件也會觸發主動的虛擬機回收，由 GCP 主動觸發的回收機制機率很低，會根據每日每個區域 (zone) 的狀態而定。這裡描述資料中心啟動的臨時回收。\nGCP 不會把所你手上的 preemptible 機器都收走，而是依照一定的規則，選擇一個比例撤換的機器。\n先看文件敘述\nCompute Engine 會避免從單一客戶移除太多先占虛擬機 優先移除新的虛擬機，偏向保留舊的虛擬機 (但最多仍活不過 24hr) 開啟後馬上被移除的先占虛擬機不計費 機器尺寸較小的機器，可用性較高。例如：16 cpu 的先占虛擬機，比 128 cpu 的先占虛擬機容易取得 GCP 每天平均會移除一個專案中 5% - 15% 的虛擬機，從用戶的角度我們需要預期至少這個程度的回收。不過這個比例，GCP 也不給予任何保證。以筆者經驗，只能說絕大部分的時候，都不會擔心超過這個比例的回收，但是還是要做好最壞的打算，如果臨時無法取得足夠的先占虛擬機，要有方法暫時補足隨選虛擬機。\nGKE 使用先占虛擬機會違反 Kubernetes 的設計\nPod grace period 會被忽略 Pod disreuption budget，不會被遵守 (可能會超過) 對應用的影響 GCP 觸發的 Preemptible process，對應用的影響\ninvoluntary disruptions，GCP 送 ACPI G2 Soft Off OS 終止服務，包含執行 Kubernetes 的 container runtime 容器內的應用會收到 SIGTERM，啟動 graceful shutdown Kubernetes 提供的 Graceful-shutdown 可能會跑不完 實務上只是中斷當前工作 https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters\n大量節點同時回收 由於 GCP 並不保證收回的機器的數量，同時回收的機器量大，還是會衝擊到服務。例如：一次收回 15% 的算力，當然服務還是會受到衝擊。當然這樣事件的機率並不高，但我們仍是需要為此打算。這邊有幾個做法\n預留更多的算力 使用 regional cluster，在多個 zone 上分配先占虛擬機 我們自行控制，提前主動回收虛擬機 預留更多資源 這點很直觀，由於使用了更加便宜的機器，我們可以用同樣的成本，開更多的機器。\n退一步說，使用打三折的先占虛擬機，然後開原本兩倍的機器數量\n總成本是 0.3 * 2 = 0.6 倍 同時間可用資源是 2 倍 由於先占節點回收的單位是一個一個虛擬機\n安排合適的機器尺寸 尺寸較小的先占虛擬機，可用性較高。意思是零碎的先占虛擬機容易取得 但當然也不能都開太小的機器，這會嚴重影響應用的分配。至於具體需要開多大，可以根據預計在機器上運行的應用，做綜合考量，例如有以下影用需要執行：\napp A: 1 cpu 5 replica app B: 3 cpu 5 replica app C: 5 cpu 5 replica 總共至少 45 cpu ，預期機器負載8 成的話，需要總共 45 / 0.8 = 56 cpu。也許可以考慮\n8 cpu * 7 先占虛擬機 4 cpu * 14 也舉幾個極端不可行的例子\n56 cpu * 1 這樣的虛擬機回收時的影響範圍 (blast radius) 就是 100% 服務 1 cpu * 56 機器太瑣碎，可能超出 Qouta (節點數量，IP 數量…) 應用會更分散，節點間的內部網路流量會增加 前幾篇提到的 reserved resource 比例高，會影響應用的部署 如果希望更保險，可以再補上隨選虛擬機混合搭配，例如\n8 cpu * 7 5 先占虛擬機 2 隨選虛擬機 4 cpu * 14 10 先占虛擬機 4 隨選虛擬機 虛擬機區域 虛擬機的回收觸發，也是會依據服務的區域 (zone) 回收。意思是節點回收不會同時觸發 asia-east1 中所有 zone 的節點回收，一般來說時間是錯開的 (不過GCP 也不保證這點 XD)。為了維持 GKE 的可用性，我們都會開多個 node-pool 在多個區域下。\n總之避免把機器都放在同個區域中。\n自行控制的虛擬機汰換 簡單來說我們在 24 hr 期限之前，先分批自盡 XD，打散個各個虛擬機的 24 小時限制。\n使用這個有趣的工具 estafette-gke-preemptible-killer，自動汰換先占虛擬機，讓整個繼起虛擬機都分散在 24 小時間。\nestafette-gke-preemptible-killer ，使用上簡單，大家自己看著辦 XD。如果大家有興趣，留言的人多的話，我再另外開一篇細講。\n小結 為了使用先占虛擬機，我們要多做以下幾件事\n為應用設計可容錯分散式架構，例如應用可以同時執行一樣的 API server 3 個 replica 分散 Pod 到合適的機器上，例如設置 PodAntiAffinity 設定合適的虛擬機大小，合適的分散應用 使用 estafette-gke-preemptible-killer，自動汰換先占虛擬機 不依賴應用的 Graceful-shutdown 流程 ","date":1600849394,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"f8a9bd9ab0bd2bef9c340229ff9d702a","permalink":"https://chechia.net/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/","publishdate":"2020-09-23T16:23:14+08:00","relpermalink":"/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/","section":"post","summary":"先占虛擬機終止流程 (Preemption process) 子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。\n這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。\n如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如\n應用重啟初始化時 cpu memory 突然拉高 承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance …等等 如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。\n這邊有幾個面向要注意\nGCP 如何終止先占節點 GCP 移除節點對 GKE 、以及執行中應用的影響 GKE 集群如何應對的節點失效 GCP 自動調度補足新的先占節點 GKE 集群如何應對節點補足 三個重點","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Speficication","type":"post"},{"authors":[],"categories":null,"content":"我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 該不該在 Kubernetes 上面跑 database？\nTL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的\n等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。\n應用是否適合部署到 Kubernetes 上，可以看這篇 Google Blog: To run or not to run a database on Kubernetes: What to consider，如果大家有興趣，再留言告訴我，我再進行中文翻譯。\n文中針對三個可能的方案做分析，以 MySQL 為例：\nSass，GCP 的 Cloud SQL 最低的管理維運成本 自架 MySQL 在 GCP 的 VM 上，自行管理 自負完全的管理責任，包含可用性，備份 (backup)，以及容錯移轉 (failover) 自架 MySQL 在 Kubernetes 上 自負完全的管理責任 Kubernetes 的複雜抽象層，會加重維運工作的複雜程度 然而 RDBMS 的提供商，自家也提供 Operator\nOracle 自家提供的 MySQL Operator CrunchyData 也有提供的 Postgres Operator 你就想，所以這些人是想怎樣，RDBMS 放 Kubernetes 上到底是行不行 XD。Google 的文章說明：如果應用本身並不符合 Kubernetes 的工作流程 (Pod life-cycle)，可以透過上述的 Operator 來自動化許多維運的作業，降低維運的困難。\n然而 DB 有千百種，除了 RDBMS 以外，還有另外一批 Database 天生就具有分散式的架構，這些儲存庫部署到 Kubernetes 上，並不會太痛苦 (還是要付出一定的成本XD)，但是卻可以得益於 Kubernetes 的諸多功能。\n底下我們先根據分散式的儲存庫做概觀描述，本系列文的最後，會根據時間狀況，做實例分享：Cassandra 或是 CockroachDB。提供各位一點發想，並根據需求去選擇需要的儲存庫\n「行不行要問你自己了施主，技術上都可以，維運上要看看你的團隊有沒有那個屁股吃這份藥 XD」\nDistributed Database 底下非常粗淺的簡介分散式儲存庫的概念，提供一個基準點，幫助接下來討論是否可以使用先占虛擬機。這邊要強調，儲存庫的類型千百種，底層的各種實作差異都非常大，底下的模型是基於 cassandra 但不會走太多細節。 cassandra 的規格有機會再細聊。\n當後端應用已經順利水平拓展之後，整體服務的效能瓶頸往往都壓在後端 DB 上。這些不同的 DB 面向不同的需求，當需求符合時，可以考慮使用這些解決方法。\n這邊要強調，不是放棄現有的 RDBMS ，完全移轉到新的資料庫，這樣的成本太高，也沒有必要性。更好的做法，是搭配既有的關聯性資料庫，將不是核心業務的資料處理抽出，移轉到合適的資料庫上。讓不同需求的資料儲存到更合適的儲存庫，是這段話要強調的重點，關連式資料庫也不是唯一選擇。\n分散式的資料庫有以下特徵\n分散式節點集群 (Cluster)：資料庫是多個節點共存，而非 single master, multiple slaves 的架構 配合共識算法 (consensus algorithm) 溝通節點之間的資訊 無單點錯誤 (Single-point failure)：e.g. 不會因為 master 錯誤導致整個服務失效 高可用(High Availitility：可以承受集群中一定數量虛擬機故障，服務仍然可用 資料 sharding 到不同節點上 複本 (replica) 節點複本 (node replicas)：多個節點提供服務，提供流量的帶寬與可用性 資料複本 (data replicas)：在多個節點上儲存資料，提供資料的備份，同時也提供讀取帶寬與可用性 從以上特徵來說，使用此架構的服務可以承受先占虛擬機的不定時終止，或許可以使用。\n實務上有非常多需要注意，需要依據各自服務的性質，各自處理。常見的問題舉例如下：\n應用可以容錯 (fault-tolerent)，然而錯誤發生後，會需要消耗復原成本，例如重啟後需要花時間初始化，或是在多節點上進行 data rebalance。 可以承受突然的錯誤，使用先占虛擬機，變成每日固定會承受必然的錯誤。這裡犧牲了部分算力，甚至造成隱性的維護成本，最後是否符合節省成本的需求。 都是需要仔細了解解決方案，並且分析需求，來評估是否有合乎成本。\nGKE 以上分析了三種常見需求例子：從 batch job，user-facing service，與 distributed database。明天會實際搬出 GKE 與 GCP Preemptible Instance 的技術規格，與大家實際討論。\n","date":1600756740,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"e14c46689d3578ca310e4410bc46bbf0","permalink":"https://chechia.net/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/","publishdate":"2020-09-22T14:39:00+08:00","relpermalink":"/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/","section":"post","summary":"我們以下幾個需求：\n執行短期的 batch job 執行長期的 user-facing API server 執行長期的 stateful 資料庫、儲存庫 該不該在 Kubernetes 上面跑 database？\nTL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的\n等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。\n應用是否適合部署到 Kubernetes 上，可以看這篇 Google Blog: To run or not to run a database on Kubernetes: What to consider，如果大家有興趣，再留言告訴我，我再進行中文翻譯。","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance Requirement Distributed","type":"post"},{"authors":[],"categories":null,"content":"前言 鐵人賽的第二部分，要帶來公有雲省錢系列文章。\n架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。\n一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。\nKubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。\n「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。\n這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。\n這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。\n先占虛擬機 TL;DR 先占虛擬機為隨選虛擬機定價的 2-3 折，使用先占虛擬機可能可以節省 7 成的雲平台支出 先占虛擬機比起隨選虛擬機，外加有諸多限制，e.g. 最長壽命 24 hr、雲平台會主動終止先占虛擬機…等 配合使用自動水平擴展 (auto-scaler)，讓舊的先占虛擬機回收的同時，去購買新的先占虛擬機 配合可容錯 (fault-tolerent) 的分散式應用，讓應用可以無痛在虛擬機切換轉移，不影響服務 要讓應用可以容錯，需要做非常多事情 搭配 kubernetes ，自動化管理來簡化工作 配合正確的設定，可以穩定的執行有狀態的分散式資料庫或儲存庫 或是看 Google 官方 Blog：Cutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs\n預計內容\n需求假設、釐清需求，並且精準計價 精準計價使用先占虛擬機的節省成本 先占虛擬機的規格、額外限制 額外限制，造成技術要多做很多額外的事情 實務經驗分享：API server 實務經驗：從使用隨選虛擬機，移轉到先占虛擬機，公司實際導入經驗 實務經驗：Elasticsearch 實務經驗分享：其他分散式資料庫，也許是 Cassandra 或是 cockroachDB 上面的內容不曉得會寫幾篇看感覺 XD\n有寫過鐵人賽的都知道 30 篇真的很漫長，一篇文章幾千字，都要花好幾個小時。我去年後半，真的都會看讀者的留言跟按讚，取暖一波，才有動力繼續寫。留言的人多就會多寫，留言的人少就會少寫，各位覺得文章還看得下去，請務必來我粉專按讚留個言，不管是推推、鞭鞭、或是有想看的文章來許願，都十分歡迎。有你們的支持，我才有動力繼續寫。\n請大家務必以實際行動支持好文章，不要讓劣幣驅逐良幣。不然 iThome 上面之後只剩洗觀看數的熱門文章了 XD\n當然，沒人留言我就會當作自己才是垃圾文 (自知之明XD)，就會收一收回家嚕貓睡覺，掰掰~\n-\u0026gt;我的粉專，等你來留言\n","date":1600651337,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"8b6f6fe6a62ceb767543681a22a85162","permalink":"https://chechia.net/zh-hant/post/2020-09-21-gcp-preemptible-instance/","publishdate":"2020-09-21T09:22:17+08:00","relpermalink":"/zh-hant/post/2020-09-21-gcp-preemptible-instance/","section":"post","summary":"前言 鐵人賽的第二部分，要帶來公有雲省錢系列文章。\n架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。\n一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。\nKubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。\n「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。\n這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。\n這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。","tags":["kubernetes","gcp","preemptible","spot-instance","鐵人賽2020"],"title":"Gcp Preemptible Instance","type":"post"},{"authors":[],"categories":null,"content":"前言 這是原文完整版本。太長不讀 (TL;DR) 請見Borg Omega and Kubernetes 前世今生摘要\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n摘要 在 container 技術夯起來前，Google 已經做了 container 十幾年，過程中發展出需三套容器管理系統。雖然每一代系統的開發需求不同，但每一代都深受上一代影響。這篇文章描述 Google 開發這些系統時，學到的經驗。\n第一套 container management 系統是 Borg，為了管理 1. 長期執行的服務 2. 批次的短期工作 (batch job)，原本分別是由 Babysitter 與 Global Work Queue 兩套系統分開管理。後者的架構深刻影響 Borg，但 Global Work Queue 專注於 batch job。兩套系統都在 Linux control groups 之前。Borg 將上述兩種應用放在共享的機器上，來增加資源的使用率，以節省成本。這種共享基於支援 container 的 Linux Kernel (Google 也貢獻許多 Linux kernel container 程式碼)，提供更好的隔離 (isolation) 給延遲敏感的使用者服務 (latency-sentitive user-facing services)，以及消耗大量 cpu 的 batch 程式。\n越來越多應用都在 Borg 上開發執行， Google 的應用與 infratructure 團隊開發許多工具與服務，形成 Borg 生態系。這些系統提供設定 (configure) 與更新 (update) 工作、預測資源需求、動態推送設定到執行中的工作、服務發現 (service discovery) 與負載均衡 (Load balancing)，等等功能。這些生態系的開發基於 Google 不同團隊的需求，產生一個不同起源 (heterogeneous)、只針對各別需求的 (ad-hoc) 一個堆不同系統，Borg 的使用者需要使用不同的程式語言與程序，來與這些系統互動。Borg 仍然是 Google 主要的容器管理系統，因為他規模 (scale) 巨大，功能多樣，而且極度堅固 (robustness)。\nOmega 是 Borg 的下一代，目的是改善 Borg 生態系的軟體工程。Omega 承襲許多 Borg 測試成功的模式，但不同於 Borg，Omega 有完整的架構設計，整體更加一致。Omega 將集群狀態 (cluster state) 存放在中心化 (centralized)、基於 Paxos 算法、交易導向 (transaction-oriented) 的儲存系統，讓集群的控制面板 (control panel) 存取，例如 scheduler。Omega 使用樂觀的併發控制 (optimistic concurrency control) 來處理偶發的衝突，這一層解藕 (decoupling) 的設計，使得原先的 Borgmaster 的功能可以拆分成多個元件，取代原本單一 (monolithic) 集中 (centralized) 的 master，被所有變更請求堵塞。許多 Omage 成功的創新也會被迭代回去 Borg 中。\n第三套 Google 開發的容器管理系統是 Kubernetes，這時外界工程師也開始對 Linux 容器有興趣，而 Google 同時在開發並推展自己的公有雲架構。Kubernetes 在這樣的背景下構思並開發。與 Borg 及 Omega 不同，Kubernetes 是開源軟體，不限於 Google 內部開發。Kubernetes 內部有共享的持久層儲存 (persistent store)，服務元件持續監測有關物件，與 Omega 類似。不同的是，Omega 允許信任的控制面板的元件直接存取儲存庫，Kubernetes 則透過 domain-specific 的 REST API 存取，來提供高階 (higher-level) 的 API 版本控制、驗證、語意處理 (semantics)、以及存取政策 (policy)，來支援更廣泛的用戶端。更重要的是 Kubernetes 著重工程師在 cluster 上開發與執行應用的體驗，簡化複雜分散式系統 (distributed system) 的管理與部屬 (deploy)，同時仍能透過容器來提升資源的使用率。\n這篇文章描述 Google 從 Borg 到 Kubernetes 獲得的知識與經驗。\n容器 (Containers) 回顧歷史，第一個容器只提供 root file system (透過 chroot)。FreeBSD jail 進一步延伸出額外的命名空間 (namespace) 例如 process ID。Solaris 大幅地探索相關的新功能。Linux control groups (cgroups) 吸收這些想法，直到今日仍持續開發。\n容器提供資源隔離 (resource isolation)，Google 得以大幅提升資源使用率 (utilization) 超出當時產業平均值，例如 Borg 使用容器，將批次暫時工作、與面對用戶需要注意延遲的應用，兩者放在同樣的物理機器上。用戶應用需要預留更多額外的資源，來處理突然產生的負載高峰 (load spikes) 以及錯誤處理 (fail-over) ，這些預留的資源常常都不會用到，可以轉讓批次工作使用。容器提供的資源管理工具實現此類需求，kernel-level 的資源隔離也確保程序之間不會互相干擾。Google 開發 Borg 中，同時也提交新功能給 Linux 容器，來滿足上述的需求。然而目前的隔離並不完整，如果 Linux kernel 不管理的資源，容器自然也無法格理，例如 level-3 的處理器快取 (level-3 processor cache) 與記憶體帶寬 (memory bandwith)，容器還需要增加一層安全保護層 (例如虛擬機器 Virtual Machine) 才能對付公有雲上出現的惡意行為。\n現代的容器不只提供隔離機制，更提供映像檔 (image)，在這個檔案上建構容器的應用。Google 使用 Midas Package Manager (MPM) 來建構並部屬容器映像檔，隔離機制與 MPM package 的關係，可以對比 Docker daemon 與 Docker image registry。這個章節描述的「容器」同時包含兩個概念：隔離、映像檔。\n應用導向的架構 (Application-oriented infrastructure) 隨著時間證明，容器不只能提供高階的資源使用率，容器化 (containerization) 使資料中心 (data center) 從原本機器導向 (machine-oriented) 變成應用導向 (application-oriented)，這個段落提供兩個例子：\n容器封裝 (encapsulate) 應用的環境 (environment)，在機器與作業系統的細節上，增加一層抽象層，解藕兩件事情：應用開發、部屬到架構。 良好設計的容器與映像檔只專注在單一個應用，管理容器意味管理應用，而不是管理機器。這點差異使得管理的 API，從機器導向變成影用導向，大幅度的改善應用的部屬與監控。 應用的環境 (Application environment) 原本 kernel 內的 cgroup、chroot、與命名空間，是用來保護應用不被旁邊其他應用的雜訊影響。將這些工具與容器映像檔一起使用，產生一層抽象層，分離應用、與底下的 (各種不同家的) 作業系統。解藕映像檔與作業系統，進一步提供相同的環境給開發環境 (development) 與生產環境 (production)，降低環境間的不一致性造成的問題，最終提升開發的可靠程度、並加速開發的流程。\n建構這層抽象的關鍵是，使用密閉的容器映象檔 (hermetic container image) ，將所有應用有關的依賴 (dependencies) 都打包，整包部屬到容器中。正確執行的話，容器對外部的依賴只剩下 Linux kernel 系統調用介面 (system-call interface)。這層介面大幅改善映象檔的部屬方便性 (protability)，但目前機制仍不完美，應用仍然暴露在某些作業系統的介面上，特別是 socket options、/proc、以及 ioctl 的調用參數。Google 希望透過推廣開放容器倡議 (Open Container Initiative) 來清楚界定，上述介面與容器的抽象層。\n次外，容器提供的隔離與最小依賴，在 Google 證明非常有效，容器是 Google 架構上唯一的可執行單位 (runnable entity)，進一步使 Google 只提供少數的作業系統版本給所有機器，只需要少數的維護人員來負責管理版本。\n密閉容器映想檔有很多方式達成，在 Borg 建構映想檔時，應用的執行檔 (binary) 靜態連結到可用的程序庫 (library) 版本，程式庫在公司內部存放。至此 Borg 容器映像檔還不是完全密閉，底下還有依賴一層基底映象檔 (base image)，直接事先安裝到機器上，而不是隨映像檔部屬，基底映象檔包含通用工具例如 tar 與 libc 程式庫，因此更新基底映象檔仍會影響應用，偶爾會產生大麻煩。\n現代的容器映像檔格式，如 Docker 與 ACI，都加強這層抽象、提供更緊密的封裝，移除特定作業系統的依賴性，並要求使用者在分享映象檔內容時，必須明確宣告。\n容器作為管理單位 (Container as the unit of management) 圍繞容器建構管理 API，而非圍繞機器，使得資料中心的「Primary key」從機器變成應用。帶來許多好處：\n應用工程師與維運團隊 (operation team) 不需再煩惱機器與作業系統的細節 架構團隊 (infrastructure team) 獲得更多升版或是調度硬體的彈性，實際對應用與開發團隊的影響非常小 管理系統收集應用的監測數據 (例如 CPU 與 Memory 使用 metrics)，而非機器的監測數據，直接提升應用的監測與自我檢查 (introspection)，特別是擴展 (scale-up)、機器錯誤、或是維護時，造成應用移到其他位置時，監測依然可用。 容器提供許多切入點給泛用 API (generic API)，泛用 API 使資訊在管理系統與應用之間流動，但其實兩者互相不清楚對方的實作細節。在 Borg 中有一系列的 API 連結到所有容器，例如 /healthz 端點回報應用的健康程度給協調管理者 (orchestrator)，當發現不健康的應用，自動終止並重啟應用。這個自動修復功能是可靠的分散式系統的基礎。(Kubernetes 也提供類似功能，透過 HTTP 端點或是 exec command 來檢查容器內部的應用)\n容器提供、或是提供給容器的資訊，可以透過許多使用者介面呈現。Borg 提供可以動態更新的文字狀態訊息，Kubernetes 提供 key-value 的 annotation 存放在給個物件的 metadata，這些都能溝通應用。annotation 可以是容器自行設定、或是由管理系統設定 (例如滾動更新版本時標註新版本)。\n容器管理系統可以取得容器內部訊息，例如資源使用狀況、容器的 metadata，並傳播給日誌 (logging) 或是監控 (monitoring)，例如使用者名稱、監控任務名稱、用戶身分。甚至進一步在節點維護時， …","date":1599889852,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"395acbd23bcba1c70eb2e6ba3ed2556a","permalink":"https://chechia.net/zh-hant/post/2020-09-12-borg-omega-and-kubernetes/","publishdate":"2020-09-12T13:50:52+08:00","relpermalink":"/zh-hant/post/2020-09-12-borg-omega-and-kubernetes/","section":"post","summary":"前言 這是原文完整版本。太長不讀 (TL;DR) 請見Borg Omega and Kubernetes 前世今生摘要\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n摘要 在 container 技術夯起來前，Google 已經做了 container 十幾年，過程中發展出需三套容器管理系統。雖然每一代系統的開發需求不同，但每一代都深受上一代影響。這篇文章描述 Google 開發這些系統時，學到的經驗。\n第一套 container management 系統是 Borg，為了管理 1. 長期執行的服務 2. 批次的短期工作 (batch job)，原本分別是由 Babysitter 與 Global Work Queue 兩套系統分開管理。後者的架構深刻影響 Borg，但 Global Work Queue 專注於 batch job。兩套系統都在 Linux control groups 之前。Borg 將上述兩種應用放在共享的機器上，來增加資源的使用率，以節省成本。這種共享基於支援 container 的 Linux Kernel (Google 也貢獻許多 Linux kernel container 程式碼)，提供更好的隔離 (isolation) 給延遲敏感的使用者服務 (latency-sentitive user-facing services)，以及消耗大量 cpu 的 batch 程式。","tags":["鐵人賽2020","kubernetes","borg"],"title":"Borg Omega and Kubernetes Translation 全文翻譯","type":"post"},{"authors":[],"categories":null,"content":"這是原文翻譯的太長不讀 (TL;DR) 版本。完整翻譯請見Borg Omega and Kubernetes 前世今生浩文完整翻譯\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n前言 Borg 以前就有應用管理系統，那時還沒有 Linux control group Borg 是第一套統一的 container-management system Borg 仍被大規模的使用，有許多功能而且非常堅固 Omega 繼承 Borg 上成功的設計，並希望改進 Borg 的生態系 Kubernetes 開源 透過 REST API 溝通 client 應用開發導向，著重於開發者的需求，希望能簡單的部署複雜的系統 Container Google 使用 Container 來提昇 utilization 把 batch jobs 跟預留資源的服務 (user-facing app) 放在一起，使用閒置時的資源跑 batch job 現代 container 的定義是 runtime-isolation 與 image Application-oriented infrastructure container 使用久了，不只滿足 utilization 的需求 資料中心從機器導向變成應用導向 Container 封裝環境，把機器與 OS 的依賴抽象化 應用不依賴 部署流程 runtime infrastrcture Container scope 在應用上，專注在應用管理而不是機器管理 Application environment cgroup, chroot, namespace 原本的目的是為了保護應用，不被其他應用影響 混合使用可以在應用與 OS 間產生抽象層，解耦 app 與 OS 提供完全相同的部署環境，避免切換環境(ex. dev, prod)時造成環境差異 進一步把 app 的依賴程式也打包 image container 對 OS 唯一的依賴只剩 Linux kernel system-call interface 大幅增加 app 調度的彈性 然而有些 interface 仍附著 OS 上，ex socket, /prod, ioctl calls 希望透過 Open Container Initiative，清楚定義 interface 與抽象 直接的好處，少數幾種 OS 與 OS Version 就可以跑所有應用，新版本也不影響 Container as the unit of management 資料中心的重心，從管理機器變成管理應用 提供彈性給 infrastructure team 提供統一的架構 收集統一的 metrics Container 統一的介面，讓 management system (ex. k8s) 可以提供 generic APIs REST API, HTTP, /healthz, exec… 統一的 health check 介面，更方便的終止與重啟 一致性 容器提供統一的資訊，ex. status, text message, … 管理平台提供統一設定 (ex. resource limits) ，並進行 logging 與 monitoring 提供更精細的功能 ex. graceful-termination cgroups 提供 app 的資源使用資訊，而不需要知道 app spec，因為 contaier 本身即是 app 提供更簡單，卻更精細且堅固的 logging 與 monitoring 應用導向的 monitoring ，而不是機器導向的 monitoring 可以收集跨 OS 的 app 狀態，進行整合分析，而不會有 OS 不同造成的雜訊 更容易對應用除錯 nested contaiers resource allocation (aka. alloc in Borg, Pod in Kubernetes) Orchestration is the beginning, not the end 原本 Borg 只是要把 workload 分配到共用的機器上，來改善 utilization 結果發現可以做更多事情，來幫助開發與部署 Naming, service discovery Application-aware load balancing Rollout tool Workflow tool Monitoring tool 成功的工具被留下 然而工具都需要各自的 API，副作用是增加部署的複雜度到 Borg 的生態系 Kubernetese 試圖降低複雜度 提供一致的 API ex. ObjectMetadata, Specification, Status Object metadata 是全域共通的 Spec 與 Status 根據 Object 有所不同，但是概念是一致的 Spec 描述 desired state of object Status 提供 read-only 的 current state of object Uniform API 有許多好處 降低學習成本 可以使用 generic 的工具讓所有 workflow 使用 統一使用者的開發流程與開發經驗 Kubernetes 本身模組化，可以使用延伸模組 ex. pod API 讓使用者使用，kubernetes 內部使用，外部自動化工具也使用 使用者可以自己增加 customized API 如何達到 Uniform API decoupling API 切分 API 關注的面向，變成不同 components API. ex. replication controller 確保 desired 數量的 Pod 存在 autoscaler 關注在需求與使用的預測，然後控制 replication controller API higher-level 服務都共用相同的 basic API 切分 API 而外的好處 有關聯但是用途不同的 API 的內容與使用方式十分相似. ex. ReplicationController: 控制長時間運行的 containers 與其複本 DeamonSet: 每個機器上都跑一個 container Job: 一次性執行完畢的 container Common design patterns ex. reconciliation controller loop 在 Borg, Omega, Kubernetes 中大量使用 需求(desired state) 觀察現況(current state) 執行動作，收斂需求與現況(reconcile) loop 由於狀態是基於實際觀測產生，reconciliation loop 非常堅固，可以承受相當的 failure Kubernetes 設計為一連串的為服務系統，以及許多小型的 control loop 對比大型的 centralized orchestration system Things to avoid Google 開發過程中，也發現許多不該做的事情\n不要使用 conainer system 來管理 port numbers Borg 會指定 unique port number 給每個 container 必須用其他方法取代 DNS port 也不易嵌入 URL 中，要另外處理轉址 需要而外的系統處理 ip:port Kubernetes 選擇指派 IP 給 Pod 可以直接使用常用 port (ex. 80,443) 可以使用內部 DNS，使用一般常用的工具 大部分公有雲都提供 networking underlays，達成 Ip-per-pod 可以使用 DNS overlay 或是 L3 routing，來控制一台機器上的多個 IPs 不要幫 container 編號，使用 label 來管理大量的 container Borg 會幫 job 從 0 開始編號 很直覺很直接，但稍後就後悔了 如果 job 死了，重啟新的 job 在機器上後，還需要去找上個死掉的 job task 中間會有很多洞 (死掉的 job) 更新版本，要更新 jobs 時會依序重啟 jobs 資料如果也是根據 index 做 sharding，重啟時要復原 index，不然會有資料遺失 Kubernetes 使用 label 可以透過 label 管理一組 container 一個 container 可使用多個 labels，更方便的調度 需要的資訊打在 label 上 (ex. role assignments, work-partitioning, sharding…)，更容易管理 注意所有權 Borg 上，tasks 都綁定在 job 上，產生 job 也產生 tasks 很直覺方便 只剩下一種 group 控制機制 Kubernetes 的 pod-lifecycle management (ex. replication controller) 使用 label selector 來控制 pod 可以彈性控制大量 pod 可能有多個上層 controller 控制同一個 pod，要盡量避免這種情況 好處是保留彈性的同時，可以很清楚界定管理的 pod，不會有 orphan / adapt pod 透過 label 進行 service load balance 如果 pod 有問題，可以變更 label，讓流量不要進來，但又保留 Pod debug 不要暴露 raw state Borgmaster 是 monolithic，可見所有的 API Operation Omega 不是 centralized，只保留被動的資訊，使用 optimistic concurrent control state 存到 client store，並基於 state 進行 operation 所有 client 需要使用一樣的 client store library Kubernetes 走中間 所有 state 存取需要透過 centralized API server client components 可以獨立運作 Some open, hard problems configuration dependency management ","date":1598421052,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"666e87031b74dc9e2f3543c53783f03a","permalink":"https://chechia.net/zh-hant/post/2020-08-26-borg-omega-and-kubernetes-tldr/","publishdate":"2020-08-26T13:50:52+08:00","relpermalink":"/zh-hant/post/2020-08-26-borg-omega-and-kubernetes-tldr/","section":"post","summary":"這是原文翻譯的太長不讀 (TL;DR) 版本。完整翻譯請見Borg Omega and Kubernetes 前世今生浩文完整翻譯\n原文：https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44843.pdf\n前言 Borg 以前就有應用管理系統，那時還沒有 Linux control group Borg 是第一套統一的 container-management system Borg 仍被大規模的使用，有許多功能而且非常堅固 Omega 繼承 Borg 上成功的設計，並希望改進 Borg 的生態系 Kubernetes 開源 透過 REST API 溝通 client 應用開發導向，著重於開發者的需求，希望能簡單的部署複雜的系統 Container Google 使用 Container 來提昇 utilization 把 batch jobs 跟預留資源的服務 (user-facing app) 放在一起，使用閒置時的資源跑 batch job 現代 container 的定義是 runtime-isolation 與 image Application-oriented infrastructure container 使用久了，不只滿足 utilization 的需求 資料中心從機器導向變成應用導向 Container 封裝環境，把機器與 OS 的依賴抽象化 應用不依賴 部署流程 runtime infrastrcture Container scope 在應用上，專注在應用管理而不是機器管理 Application environment cgroup, chroot, namespace 原本的目的是為了保護應用，不被其他應用影響 混合使用可以在應用與 OS 間產生抽象層，解耦 app 與 OS 提供完全相同的部署環境，避免切換環境(ex.","tags":["kubernetes","google","borg"],"title":"Borg Omega and Kubernetes TLDR 摘要翻譯","type":"post"},{"authors":[],"categories":["murmur"],"content":"季中回顧\n這一季開了很多新坑，卻來不及寫文章填上\n完成的有 Terraform 的基礎與實務導入經驗\n正在趕工的有 hashicorp vault，Gitops，與 tls。\n其中 GitOps 已經有強者我朋友 Hwchiu 巨巨填坑了，我這邊就會偷懶跳過。友情連結\n剩下的希望本季結束前能填完(掩面)\n已填坑 從零開始導入 Terraform DevOps Taiwan Meetup iThome Cloud Summit 新坑，碼農正在耕田，挖坑自己跳 hashicorp vault install basic operation Sign \u0026amp; manage x509 certificate with pki secret engine 填坑中，文章尚未完成 aws\npost/play-aws-eks-with-low-cost: 精算小神童，如何用最少的 credits 玩 aws eks 服務 tls\npost/openssl-self-sign-tls-with-own-ca post/k8s-manage-tls-certificate gitOps\npost/gitops-with-argo-cd terraform\npost/terraform-infrastructure-as-code-module kubernetes\nborg, omega, and kubernetes 作者外出取材中… MIT 6.824 Distributed System Learning Note ","date":1597802491,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"59a338847ee61a2c42c9383065c7b6f8","permalink":"https://chechia.net/zh-hant/post/2020-08-19-season-review/","publishdate":"2020-08-19T10:01:31+08:00","relpermalink":"/zh-hant/post/2020-08-19-season-review/","section":"post","summary":"季中回顧\n這一季開了很多新坑，卻來不及寫文章填上\n完成的有 Terraform 的基礎與實務導入經驗\n正在趕工的有 hashicorp vault，Gitops，與 tls。\n其中 GitOps 已經有強者我朋友 Hwchiu 巨巨填坑了，我這邊就會偷懶跳過。友情連結\n剩下的希望本季結束前能填完(掩面)\n已填坑 從零開始導入 Terraform DevOps Taiwan Meetup iThome Cloud Summit 新坑，碼農正在耕田，挖坑自己跳 hashicorp vault install basic operation Sign \u0026 manage x509 certificate with pki secret engine 填坑中，文章尚未完成 aws","tags":[],"title":"2020 08 Season Review","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\n各位好\nAbout this presentation 開始之前，先分享一些資源\n投影片 講稿 程式碼 SOP 範本 Facebook 粉專 都放在這裡，因為有附逐字稿，所以如果很忙的朋友，掃了 QR code 就可以回家自己看了，不用客氣。\n然後有興趣在追這系列文章的，可以幫我 facebook 粉專按個讚跟追蹤，每周新文章出來，會推播通知。\n文章在 chechia.net 上，新文章通知靠 facebook 粉專這樣。也可以只追蹤不按讚。我自己看別人技術 blog 也很常這樣(XD\n需求 好，今天來講得這個 Terraform\n我會實際分享我們公司為雲團隊導入的經驗，前半部會有點像購物頻道的廣告\n以下的對話是不是常出現在日常工作中？\n以下這段對話很耳熟？ 「是誰改了這個設定？」\nby 週一上班的 DevOps 與週六值班的維運團隊 對啟用的環境的掌握如何 環境的變更能否永久保存 「這個環境怎麼少一個設定？」\n網站噴錯後，整個團隊一步步釐清問題，不是 code，不是變數config，是 infra 有一個小地方沒設定 環境複雜，以我們的例子是複雜得多雲網路，常出這種問題 環境設定的除錯很費時 完全可以避免 「會動就好，沒事不要改環境（抖）」\nLegacy site 調整 production 環境是否有信心 我們看不慣無人整理的舊架構，導入 terraform 後，（很不要命的）把 production site 搬過一遍 我有十成的信心跟你說新舊兩個 site 完全一樣 Our stories 我們是思華科技(ＸＤ) 開發團隊大概 100+ 個左右 專案很多，而且老闆很喜歡開新專案測試商業模型 環境也越開越多，大大小小幾百台 vm，幾十個資料庫，都是不同專案在跑，規模大概這樣 每週都有新環境要交付 交接缺口 我們公司東西多，但東西多不是問題，問題是什麼呢?\n問題 手動部屬本來不是問題，但漸漸成為問題 開 infrastructure 的方法，跟著 SOP 上去 GCP GUI 介面，點一點，填一填。公有雲開機器很方便。 可是不同人開的環境漸漸出現一些不同，可能差一個設定、一個參數、或是命名規則差一點。這些細節的不同，差一個 config 有時候就會雷到人。「這機器誰建的阿，根本有問題啊」，而且這種雷很多時候都是跑下去出事了，才發現「阿靠原來設定不一樣」 命名差一點不影響功能，但看久了就很煩，「阿就對不齊阿」，有強迫症就很痛苦。然後你維運的自動化腳本就爆掉，命名差一個字，regex 就要大改。突然增加維運成本 生產環境大家都不太敢動。架構調整很沒信心 誰知道當初環境設定了那些東西，開機器的人離職了，也不知道他為啥設定，「你知道他當初為什麼要設定這個嗎?」，你問我我是要去擲茭喔。 我們這一季把所有現有環境都搬到新的架構上，因為我們對舊架構不爽很久了(XD)，這個能做到當然有作法，後面細講 有實際需求才找解決方案，沒有需求就不用衝動導入新技術，導入過程中還是蠻累的\n需求 從維運的角度，需求大概長這樣\n提升穩定度\ninfra 交付標準化 交付自動化 測試環境 infra 提交要能夠 review\n提升效率\n老闆要的。超快部屬，腳本跑下去要快，還要更快 次要目標\n成本，效能最佳化，希望能在整理過程中，找到最適合的可行架構 新人好上手，Junior 同事也能「安全」的操作，看到這個安全兩個字了嗎? 安全第一，在訓練新的 op 時要注意安全，不然他上去 GUI 點一點，一個手起刀落 DB 就不見了，整個維運團隊一周不用睡覺。安全第一吼。 權限控管，IAM 也用 terraform 管理，權限管理人多手雜越用越亂，可以考慮使用 IaC，一覽無遺 Programatical approach for infra 啊不就是 Infrastructure as code XD\n導入前，大家都有聽過，大家都覺得很想導入，但沒人有經驗，每個人都超怕，但又不知道在怕三小 這表明了一件事，大家都知道要做對的事情，但不是每個人都能改變現況，讓團隊導向對的事情 計畫\n確定需求 開始 survey 「小心」導入 有經驗領頭羊很棒，但不是必須 技術細節 先簡單講一下 IaC (Yeah 終於要講技術了)\nIaC 上面都講概念跟心法，現在實際講用到的技術。\n首先是 Infrastructure as Code，這個概念很久了，但導入的公司好像不是那麼多。所以我今天要來傳教，洗腦大家(XD，跟你推薦這個配方保證快又有效(XD)\n簡單來說就是用程式來操作 infrastructure，今天主講的 terraform 是 IaC 工具中的一個 IaC 工具可以是宣告式，或是命令式，或是兩種都支援 一個是我告訴你結果，步驟我不管，請你幫我生出這樣的結果。 一個是我告訴你步驟，你一步一步幫我做完，就會得到我要的結果 terraform 是宣告式，說明邏輯跟結果，例如我要 1 2 3 台機器，terraform 自己去幫我打 Google API 這樣，把機器生出來 ansible 是命令式，我把步驟寫成一堆命令腳本 playbook ，ansible 幫我照著跑下去，理論上跑完後我的機器也準備好 Terraform 官網在這邊，自己看 https://www.terraform.io/ 宣告式的 Iac 工具 單一語法描述各家 API 透過 provider 轉換 tf 成為 API call Terraform Core Workflow https://www.terraform.io/guides/core-workflow.html\nWrite 撰寫期待狀態 tf file plan 計畫試算結果 apply 用期待狀態去更新遠端 tf file，就是宣告式的表達 infra ，描述期待的infra長這樣，ex. tf file 裡有這些機器 1 2 3 台這樣 resource 一個一個物件描述，後面可能是對映 provider 的 API Endpoint (ex. GCP GKE API) remote resources，是真實存在遠端的機器，例如 GCP 雲端實際上只有 1 2 兩台這樣。 terraform diff tf vs remote，算出 plan，少的生出來，多的上去砍掉 Demo 1 (empty project)\nadd my-gce.tf\ngit diff\nstate\ncheck GUI remote\nexisting my-gce\nremove my-gce.tf\nplan\n這邊不 apply 我 demo 還要用\n這邊這樣有理解 terraform 的基本流程嗎？編寫，計畫，apply 三步驟 很單純 然後講一點細節\nstate\nremove (out of scope)\nplan -\u0026gt; addd\napply (deplicated ID)\nmv state (Danger)\nrename state with the same ID -\u0026gt; destroy and recreate (Danger)\nState terraform 經手(apply) 過的 resource 會納入 state (scope)\n不在 scope 裡的 resource 不會納入 plan，不會被 destroy，但可能會 create duplicated ID\nterraform 允許直接操作 state\nimport remove 但我不允許XD 注意是 diff state 喔，所以每次 plan 時候會自動 refresh state\nstate 又是什麼? remote 是一個動態環境，可能會多會少，這樣沒辦法 diff，state 是把我執行當下，遠端相關資源的狀態快照存起來，然後根據這個 snapshop 去 diff\napply 只是拿你的期待去 diff state，terraform 幫你算出來差多少，例如我們這邊就是遠端少一台。terraform 透過 provider 去知道，喔這一台要去打那些 GCP API，把這台生出來。\nState，是核心概念，我當初自己卡觀念是卡這邊，所以我特別拉出來講\n雲端空蕩蕩，refresh state 也是空的，tf file 多加一個 VM，plan 覺得要 create 雲端有東西，refresh state 未必會 refresh 到 相同 ID 的資源之前 import 在 state 中，refresh state，tf file 沒東西，plan 覺得要 destroy 相同 ID 的資源不在 state 中，這些 resource 不在當前 state 的 scopor 中，refresh state 是空的，tf file 沒東西，plan 覺得沒增沒減 相同 ID 的資源不在 state 中，這些 resource 不在當前 state 的 scopor 中，refresh state 是空的，tf file 有相同 ID 的資源，plan 覺得要 create，但實際 apply，API error 遠端已經有相同 ID 的資源存在 小結 Write -\u0026gt; Plan -\u0026gt; Apply State 大家都會 terraform 惹 初步使用感想 IaC 地端跟雲端都能做，但雲端做起來效果超級好 完全展現雲端運算的特性，迅速、彈性、隨用隨叫，調度大量的虛擬化資源 新增東西很快，不要的資源，要刪掉也很快 不小心刪錯也很快(大誤)，所以我說新人一個手起刀落公司整個雲弄不見也是有可能的，「啊我的雲勒」「被 terraform 砍了」。不要笑，那個新人就是我，我自己剛學的時候就有把整個 db 變不見過，差點一到職就引咎辭職(XD。用這些技術還是有很多安全要注意，稍後會細講注意的安全事項。 總之，Iac 就是用程式化的語法，精準的描述雲端的狀態或是步驟，完全沒有模糊的地帶。帶來的好處，降低維運的錯誤風險，加快維運效率，最佳化節省成本。\n新手 state 的雷 多人協作，同時變更 state 會造成不可預期的錯誤 避免直接操作 state state 可能有 sensitive 資料 推薦使用外部帶有 lock 的 state storage 導入 導入工具之後 新工具導入時要做好風險評估，每個人都是第一次用 terraform ，用起來很快很爽的同時也要不斷宣導安全概念，雷在哪裡坑在哪裡。\n使用 terraform 的風險\n打 DELET API 超快，砍起來很方便，但很多時候方便 = 危險。眼看小明一個手起刀落，談笑間，公有雲灰飛煙滅(XD，通通變不見。現在在講故事很開心，實際發生的話大家都笑不出來，全公司 RD 都跑來維運部門排隊盯著你看，就算修好也要懲處。壓力超大。但小明砍錯東西不是小明的錯，是大環境的錯是 SOP 的錯(XD。認真的，團隊沒有提供 SOP，新人砍錯東西當然是團隊負責。所以我們 SOP 第一行就寫得很清楚。 看見 destroy 就雙手離開鍵盤，直接求救，這樣還能出事嗎 再來，給予特殊的 IAM 權限，例如只能新增不能刪除的權限 進一步導入 git-flow，push、review、PR，讓他連犯錯的機會都沒有 根本還是要給予新人足夠的訓練，然後同時保障公司安全。 給新人過大權限砍錯東西，或是 …","date":1592189936,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"9bf6153ba216ac1387faeea40f40fa95","permalink":"https://chechia.net/zh-hant/post/2020-06-15-terraform-infrastructure-as-code-transcript/","publishdate":"2020-06-15T10:58:56+08:00","relpermalink":"/zh-hant/post/2020-06-15-terraform-infrastructure-as-code-transcript/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","devops","iac"],"title":"Terraform Infrastructure as Code Transcript","type":"post"},{"authors":[],"categories":["kubernetes","terraform"],"content":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification. Like my page if you really like it :)\nOutlline our story: issues, steps, \u0026amp; results basics IaC, terraform benefits risks and 坑 to be or not to be experience oriented\nOur stories 100+ devs, many teams 25+ projects 50+ GKEs 80+ SQLs IAMs, redis, VPCs, load-balancers, … Issues Ops manually create resources through GUI by SOP. We have many isolated, separeated resources, VPCs. It’s our culture, and we (devops) want to change. Some projects have short life-cycle. Rapid resources created \u0026amp; destroy. Our user story As a devops, I would like to introduce terraform (IaC) so that I can\nreview all existing resources minimize error from manual operation ASAP!! As a devops, I would like to fully enforce terraform (IaC) so that I can\nminimize efforts to operate infra delegate infra operations to junior team members minimize IAM privilges Introduction import existing resources review existing resources code plan best practice resource templates create new resources with templates introduce git workflow, plan, commit, PR, and review add wrapper handler automation pipeline repeat 2-4 IaC Programatic way to operate infra declarative (functional) vs. imperative (procedural) Perfect for public cloud, cloud native, virtualized resources Benefits: cost (reduction), speed (faster execution) and risk (remove errors and security violations) Terraform Terraform\nDeclarative (functional) IaC Invoke API delegation State management providers: azure / aws / gcp /alicloud / … Demo https://github.com/chechiachang/terraform-playground\nScope Compute Instances\nKubernetes\nDatabases\nIAM\nNetworking\nLoad Balancer\nExpected benefits Minimize manual operation.\nZero manual operation error\nStandarized infra. Infra as a (stable) product. fast, really fast to duplicate envs\nInfra workflow with infra review\nEasy to create identical dev, staging, prod envs Reviewed infra. Better workflow. Code needs reviews, so do infra. Fully automized infra pipeline.\nOther Benefits Don’t afraid to change prod sites anymore We made a massive infra migration in this quater!! Better readability to GUI. Allow comment everywhere. Risks Incorrect usage could cause massive destruction. 如果看見 destroy 的提示，請雙手離開鍵盤。 ~ first line in our SOP If see “destroy”, cancel operation \u0026amp; call for help. State management A little latency between infra version and terraform provider version Reduce Risks Sufficient understanding to infra \u0026amp; terraform Sufficient training to juniors Minimize IAM privilege: remove update / delete permissions Git-flow Our SOP\nedit tf push new branch commit PR, review \u0026amp; discussion merge \u0026amp; apply revert to previous tag if necessary (Utility) Provide template wrap resources for better accesibility lower operation risks uniform naming convention best practice suggested default value About introducing new tool The hardest part is always people Focus on critical issues (痛點) instead of tool itself. “We introduce tool to solve…” Put result into statistics “The outage due to misconfig is reduced by…” Overall, my IaC experience is GREAT! IaC to automation. Comment (for infra) is important. You have to write doc anyway. Why not put in IaC? Q\u0026amp;A Full transcript Presentation file Source Code on Github chechia.net \u0026lt;- full contents Follow my page to get notification Like it if you really like it :) Appendix.I more about terraform terraform validate terraform import terraform module terraform cloud \u0026amp; state management\nAppendix.I understand State conflict Shared but synced watch out for state conflicts when colaborating state diff. could cause terraform mis-plan Solution: synced state lock Colatorative edit (git branch \u0026amp; PR), synchronized terraform plan \u0026amp; apply or better: automation Appendix.II understand resources from API aspect GCP Load Balancer\nGCP Load Balancing understand resources from API aspect\nhow terraform work with GCP API internal\nregional pass-through: tcp / udp -\u0026gt; internal TCP/UDP proxy: http / https -\u0026gt; internal HTTP(S) external\nregional pass-through: tcp / udp -\u0026gt; tcp/udp network global / effective regional proxy tcp -\u0026gt; TCP Proxy ssl -\u0026gt; SSL Proxy http / https -\u0026gt; External HTTP(S) Terraform Resource forwarding_rule\nforwarding_rule: tcp \u0026amp; http global_forwarding_rule: only http backend_service\nbackend_service health_check http_health_check https_health_check region_backend_service region_health_check region_http_health_check region_https_health_check Some ways to do IaC Cloud Formation bash script with API / client 引言 Infrastructure as Code 從字面上解釋，IaC 就是用程式碼描述 infrastructure。那為何會出現這個概念？\n如果不 IaC 是什麼狀況？我們還是可以透過 GUI 或是 API 操作。隨叫隨用\n雲端運算風行，工程師可以很在 GUI 介面上，很輕易的部署資料中心的架構。輸入基本資訊，滑鼠點個一兩下，就可以在遠端啟用運算機器，啟用資料庫，設置虛擬網路與路由，幾分鐘就可以完成架設服務的基礎建設(infrastructure)，開始運行服務。\n然而隨著\n雲平台提供更多新的（複雜的）服務 服務彼此可能是有相依性（dependency），服務需要仰賴其他服務 或是動態耦合，更改服務會連動其他服務，一髮動全身 需要縝密的存取控 …","date":1592124369,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"ee558e7e37b7ee900cf55266f42c7a79","permalink":"https://chechia.net/zh-hant/post/2020-06-14-terraform-infrastructure-as-code/","publishdate":"2020-06-14T16:46:09+08:00","relpermalink":"/zh-hant/post/2020-06-14-terraform-infrastructure-as-code/","section":"post","summary":"This article is part of 從零開始的 Infrastructu as Code: Terraform\nGet-started examples / SOP on Github Introducation to Terraform Iac: Speaker transcript Presentation Check my website chechia.net for other blog. Follow my page to get notification.","tags":["kubernetes","terraform","devops","iac"],"title":"從零開始的 Infrastructure as Code: Terraform - 01","type":"post"},{"authors":[],"categories":["murmur"],"content":"今年結束了，回顧一下今年做的事情\n– 我是軟體工程師 –\n6 場公開演講，並且踏出熟悉的社群舒適圈，南下進軍高雄XD\n37 篇技術文章 其中包含 30 篇 Ithome 30天(參賽就不用睡覺)鐵人賽參賽文章 結賽撿到賀優選狂賀?\n正職工作方面，進了幣圈，切身了解敝圈真亂後，又踏出了幣圈\n開坑翻譯麻省理工學院的課程『分散式系統』，好課揪團一起修 預計會有 22 篇文章，準備在可見的未來，犧牲無數個夜晚，邁向 2020\n– 我是專業水肺潛水教練 – 也是自由潛水員\n年末的幾天，正式開始執業，帶學生下海(?) 學習教導學生，也學習對學生的安全負責\n新年復工後，正職碼農，副業潛水 有人要潛請找我，保證優惠不藏私\n– 我是數位行銷實習生 –\n跟前公司 (雖然都不是MK但卻) 超強的行銷團隊\u0026lt;3學習數位行銷 從零開始大造個人品牌，邊學邊實習 開了兩個粉絲專頁 一個是技術文章分享，一個做潛水影片分享 打造個人品牌，自己推廣行銷，學習數位行銷\n","date":1577788731,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"e5454d17dc61a9ae80f287f0871536fa","permalink":"https://chechia.net/zh-hant/post/2019-12-31-say-goodbye-2019/","publishdate":"2019-12-31T18:38:51+08:00","relpermalink":"/zh-hant/post/2019-12-31-say-goodbye-2019/","section":"post","summary":"2019 年度回顧","tags":[],"title":"Say Goodbye 2019","type":"post"},{"authors":[],"categories":["golang","distributed-system"],"content":"https://github.com/chechiachang/mit-6.824-distributed-system\n課程講義中文翻譯 個人心得 跟著 MIT 6.824 學習分散式系統\n這個專案儲存 MIT 6.824 分散式系統編程的上課內容，我將內容翻譯程中文，加上個人學習筆記\n我會在我的學習過程中，持續翻譯課程內容 一方面深入個人學習 另一方面也回饋社群 依照課程的進度進行 若有餘力，會嘗試翻譯以下內容\n課堂 Q \u0026amp; A 論文 lab 實做 ","date":1576511206,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"6f1d968a21c0e558efb6937adbf8fabb","permalink":"https://chechia.net/zh-hant/post/2019-12-16-mit-6.824-distributed-system/","publishdate":"2019-12-16T23:46:46+08:00","relpermalink":"/zh-hant/post/2019-12-16-mit-6.824-distributed-system/","section":"post","summary":"跟著 MIT 6.824 深入淺出分散式系統","tags":["mit","lecture","distributed-system","golang"],"title":"MIT 6.824 Distributed System Learning Note","type":"post"},{"authors":[],"categories":["blockchain"],"content":"https://en.bitcoin.it/wiki/Atomic_swap\nAlgorithm 2 pay txs and 2 claim tx claim txs are singed at first, locked with time 2 pay txs are encrypted by x, affects only when x is reveal on the network Initialization A: random number x\ntx1: A pay B A Pay BTC to B’s public key if x known \u0026amp; singed by B or Signed by A \u0026amp; B\ntx2: A claim tx1 pay BTC to A’s public key locked 48 hours signed by A\nA -\u0026gt; B tx2 B -\u0026gt; A tx2 signed by A \u0026amp; B\nA -\u0026gt; submit tx1 tx3: B pay A alt-coin B Pay A alt-coin if x known \u0026amp; singed by A or signed by A \u0026amp; B\ntx4: B claim tx3 pay B alt-coins locked 48 hours signed by B\nB -\u0026gt; A tx4 A -\u0026gt; B tx4 signed by A \u0026amp; B\nB submit tx3 A spends tx3, reveal x B spends tx1 using x Specialized Alt-chain ","date":1573171410,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"75133f79dfdaadb1bef40840b227839a","permalink":"https://chechia.net/zh-hant/post/2019-11-08-blockchain-atomic-swap/","publishdate":"2019-11-08T08:03:30+08:00","relpermalink":"/zh-hant/post/2019-11-08-blockchain-atomic-swap/","section":"post","summary":"https://en.bitcoin.it/wiki/Atomic_swap\nAlgorithm 2 pay txs and 2 claim tx claim txs are singed at first, locked with time 2 pay txs are encrypted by x, affects only when x is reveal on the network Initialization A: random number x","tags":["blockchain","atomic-swap"],"title":"Ablockchain Atomic Swap","type":"post"},{"authors":[],"categories":["blockchain"],"content":"BEP3 Atomic Swap Binance 在 BEP3: HTLC and Atomic Peg 提到，BEP 即將在 binance chain 上支援原生的 Hash Timer Locked Transfer (HTLT) ，這使跨鏈的原子性交換 (atomic swap) 變得可行，透過 HTLC 在兩邊的鏈上鎖住 (peg) tokens，然後只有在執行交換的時候，透過 hash 交換，一次執行雙邊的交易。\n關於 Atomic Swap 網路有非常多的訊息，有興趣的話可以看這篇\n交易只有在雙邊完成後才完成，完成之前不能動用交換的資產 在任何階段失效都可以完全 fallback，並進行 refund 交易的認證是去中心化的 這邊有個但書，Ethereum 上是透過 smart contract 實現，但 Binance chain 上還是靠 Binance 認證 XD Binance 在 BEP3 中支援 HTLC，我們這邊主要的資訊來源是 binance.org 的官方說明文件，這邊針對文章進行驗證，並且補足文件缺漏的部分，提醒過程中可能會踩到的雷。\n跨鍊(Cross Chain) 交易 在部署 asset / token 的時候，我們會選擇合適的鏈作為發布資產並運行 block chain app。常用的應用鏈如 ethereum 與 binance chain 等等。不同的主鏈上有各自的優缺點，例如使用 ethereum ，可以與許多 token 與應用互動，也是最多人使用的應用主鏈。而在 binance 鏈上執行，則能夠快速的發生 transactions，並且可以與 binance 上的資產與交易所互動。\n在某些應用場景，我們會希望兩個獨立主鏈上的資產能後互動，例如在 binance chain 上執行快速的 transaction，然而也要使用 etheruem 上既有的 ERC-20 tokens，這時便需要一個溝通兩條鏈的機制。\n文章分為三個部分 在 Binance Chain 上互換兩個 address 的 binance asset 從 ethereum token 到 binance 從 binance chain 到 ethereum Atomic Swap on Binance Chain 我們今天會實作 Atomic Peg Swap，透過 HTLT 鎖住 Binance Chain 上兩個 address 的資產，並進行原子性的一次交易，來達成鏈上的資產互換。這邊直接使用 binance 提供的 bnbcli 來執行。\n使用情境 兩個在 Binance Chain 上的 address 想交換資產\nClient: HTLT 的發起方，擁有一部分 asset，發起 HTLT 希望執行資產互換 Recipient: HTLT 的收受方，收到 HTLT，需要於時限內 deposit 指定數量的資產到 swap 中 服務元件 HTLT transactions on binance chain: 來鎖住並 claim assets Client tooling: tbnbcli 讓客戶可以操作，監測鏈上 swap 的狀況 流程 Client 使用 tbnbcli 發起 HTLT Recipient 收到發起方送來的 swap info 與 asset (frozen) Recipient Deposit 指定數量的 asset 到 swap 中 Binance Chain 自動完成 swap，完成交換，解鎖兩邊交換的資產 取得 tbnbcli tbnbcli 的說明文件\n由於 bnbcli repo 中使用 Git Large File Storage 來存放 binary，這邊要啟用 git-lfs 來下載 binary\n# Mac port sudo port install git-lfs Git clone repo\ngit clone git@github.com:binance-chain/node-binary.git cd node-binary git chechout v0.6.2 git lfs pull --include cli/testnet/0.6.2/mac/tbnbcli sudo copy cli/testnet/0.6.2/mac/tbnbcli /usr/local/bin 這邊要注意使用 v0.6.2+ 的版本，不然會沒有 HTLT 的 subcommands\n測試 tbnbcli tbnbcli status --node http://data-seed-pre-0-s3.binance.org:80 { \u0026#34;node_info\u0026#34;: { \u0026#34;protocol_version\u0026#34;: { \u0026#34;p2p\u0026#34;: \u0026#34;7\u0026#34;, \u0026#34;block\u0026#34;: \u0026#34;10\u0026#34;, \u0026#34;app\u0026#34;: \u0026#34;0\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;34ac6eb6cd914014995b5929be8d7bc9c16f724d\u0026#34;, \u0026#34;listen_addr\u0026#34;: \u0026#34;aa13359cd244f11e988520ad55ba7f5a-c3963b80c9b991b7.elb.us-east-1.amazonaws.com:27146\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;Binance-Chain-Nile\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.31.5\u0026#34;, \u0026#34;channels\u0026#34;: \u0026#34;36402021222330380041\u0026#34;, \u0026#34;moniker\u0026#34;: \u0026#34;data-seed-0\u0026#34;, \u0026#34;other\u0026#34;: { \u0026#34;tx_index\u0026#34;: \u0026#34;on\u0026#34;, \u0026#34;rpc_address\u0026#34;: \u0026#34;tcp://0.0.0.0:27147\u0026#34; } }, \u0026#34;sync_info\u0026#34;: { \u0026#34;latest_block_hash\u0026#34;: \u0026#34;359AD9BF36B7DEEB069A86D53D3B65D9F4BB77A1A65E40E1289B5798D4C1094F\u0026#34;, \u0026#34;latest_app_hash\u0026#34;: \u0026#34;E748CFA5806B587D9678F55DFDDB336E3669CDF421191CDA6D2DF8AA7A3461F3\u0026#34;, \u0026#34;latest_block_height\u0026#34;: \u0026#34;45868456\u0026#34;, \u0026#34;latest_block_time\u0026#34;: \u0026#34;2019-10-23T07:36:38.176957281Z\u0026#34;, \u0026#34;catching_up\u0026#34;: false }, \u0026#34;validator_info\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;1C360E22E04035E22A71A3765E4A8C5A6D586132\u0026#34;, \u0026#34;pub_key\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;tendermint/PubKeyEd25519\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;T56yDoH+B+OY8PP2tmeFtJtk+9ftnBUVHykKfLS45Es=\u0026#34; }, \u0026#34;voting_power\u0026#34;: \u0026#34;0\u0026#34; } } Acquire Valid Binance Testnet Account Check Testnet Doc\nGo to Binance Testnet Create a wallet Save address, mn, keystore, private key Use testnet faucet to fund testnet account Receive 200 BNB on testnet Client Create HTLT 這邊使用簡單的範例，鎖住兩個 BEP2 tokens 來進行交換，展示一下 tbnbcli 的 HTLT\n準備兩個 address，這邊是我自己的兩個 testnet address\ntbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p (179 BNB) Explorer 上查看 tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce (20 BNB) Exporler 上查看 目標：\nHTLT tbnb…j9p -\u0026gt; 0.3 BNB -\u0026gt; tbnb…7ce tbnb…j9p \u0026lt;- 0.1 BNB \u0026lt;- tbnb…7ce tbnbcli 執行 HTLT，從 from address 執行 HTLT，給 recipient-addr 0.3 BNB，並預期對方回 0.1 BNB，等待 height-span 個 block 時間(360 \u0026gt; 2 minutes)\ntbnbcli key 實際執行前，由於我們需要透過 tbnbcli 操作 from-address，要先透過 tbnbcli 把 address 的 key 加進到本地\ntbnbcli keys add tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce --recover tbnbcli keys list NAME:\tTYPE:\tADDRESS:\tPUBKEY: tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tlocal\ttbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\tbnbp1addwnpepq0pw06d3y7ykg2j33pc604j3awgqgl5vhd88wdjhjg5sptnsfpqyx2rmhl4 實際執行 參數：\nFROM ADDR: tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p\nRECIPIENT ADDR: tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce\nhightspan: 3600，height-span 是發起 HTLT，受方 deposit，發起方去 claim 的時限。\namount: asset * 10^8\ntbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p 執行 HTLT\n給 tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce 0.3 BNB\n預期對方回 0.1 BNB\n等待 3600 個 block 時間\nFROM_ADDR=tbnb1hq6v49an3wwhrd8ny7qj3exgfmvpvuelkcaj9p RECIPIENT_ADDR=tbnb1a6pv5gmnsay4a9sr7nvd0mldz29a6kdxye37ce HEIGHT_SPAN=3600 tbnbcli token HTLT \\ --recipient-addr ${RECIPIENT_ADDR} \\ --amount 30000000:BNB \\ --expected-income 10000000:BNB \\ --height-span ${HEIGHT_SPAN} \\ --from ${FROM_ADDR} \\ --chain-id Binance-Chain-Nile \\ --trust-node \\ --node http://data-seed-pre-0-s3.binance.org:80 產生 swap 的結果，可以於Testnet Explorer 上看到\nCommitted at block 47218942 (tx hash: 8F865C5C9E5CD06239DE99746BCE73AACA2F3AD881C26765FB90C9465EF06EF0, response: {Code:0 Data:[77 138 29 51 186 65 213 125 105 217 5 102 170 194 248 149 189 188 56 208 166 93 48 159 188 196 143 111 31 66 151 249] Log:Msg 0: swapID: …","date":1571711720,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"5d524ccf272a6720e6c3fb5ab8704ae7","permalink":"https://chechia.net/zh-hant/post/2019-10-22-blockchain-bep3-atomic-swap/","publishdate":"2019-10-22T10:35:20+08:00","relpermalink":"/zh-hant/post/2019-10-22-blockchain-bep3-atomic-swap/","section":"post","summary":"BEP3 Atomic Swap Binance 在 BEP3: HTLC and Atomic Peg 提到，BEP 即將在 binance chain 上支援原生的 Hash Timer Locked Transfer (HTLT) ，這使跨鏈的原子性交換 (atomic swap) 變得可行，透過 HTLC 在兩邊的鏈上鎖住 (peg) tokens，然後只有在執行交換的時候，透過 hash 交換，一次執行雙邊的交易。","tags":["blockchain","bep3","binance","ethereum","erc-20"],"title":"Blockchain Bep3 Atomic Swap","type":"post"},{"authors":[],"categories":["kubernetes"],"content":"Deploy Kafka on Kubernetes Che-Chia Chang\nQRCode\nAbout Me David (Che-Chia) Chang\nBackend / Devops @ MachiX Golang Taipei Meetup 2020 Ithelp Ironman Challenge https://t.me/chechiachang Outline Introduction to Kafka Deploy Kafka with Helm Kafka Topology Ithelp Ironman 30 days Challenge (7th-12nd day) Introduction https://kafka.apache.org/\nDistributed streaming platform Publish \u0026amp; Subscribe: r/w data like messaging system Store data in distributed, replicated, fault-tolerant cluster Scalable Realtime Concepts Kafka run as a cluster Kafka cluster stores streams of records in categories called topics record = (key, value, timestamp) Kafka Diagram Topic Partitions Topic Partitions Data categorized by topic Data replicated in partitions Durability consumer able to r/w complete data from at least 1 partition in order Distributed Data Streaming Scalible r/w bandwith\nData Durability\nConsistency\nAvailability\nPartition Tolerance\nMulti Consumer Consumer Group Consumer Group Partition deliver record to one consumer within each subscribing consumer group Deployment Helm Kafka\nDeployment https://github.com/chechiachang/kafka-on-kubernetes\ncat install.sh #!/bin/bash # https://github.com/helm/charts/tree/master/incubator/kafka HELM_NAME=kafka-1 helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator helm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml Check values-staging.yaml before deployment\nHelm Chart Values cat values-staging.yaml # ------------------------------------------------------------------------------ # Kafka: # ------------------------------------------------------------------------------ ## The StatefulSet installs 3 pods by default replicas: 3 ## The kafka image repository image: \u0026#34;confluentinc/cp-kafka\u0026#34; ## The kafka image tag imageTag: \u0026#34;5.0.1\u0026#34; # Confluent image for Kafka 2.0.0 ## Specify a imagePullPolicy ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; ## Configure resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ resources: {} # limits: # cpu: 200m # memory: 1536Mi # requests: # cpu: 100m # memory: 1024Mi kafkaHeapOptions: \u0026#34;-Xmx4G -Xms1G\u0026#34; ## The StatefulSet Update Strategy which Kafka will use when changes are applied. ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies updateStrategy: type: \u0026#34;OnDelete\u0026#34; ## Start and stop pods in Parallel or OrderedReady (one-by-one.) Note - Can not change after first release. ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy podManagementPolicy: OrderedReady ## If RBAC is enabled on the cluster, the Kafka init container needs a service account ## with permissisions sufficient to apply pod labels rbac: enabled: false affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - kafka topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - zookeeper topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet ## here in map format, as defined in the official docs. ## ref: https://kafka.apache.org/documentation/#brokerconfigs ## configurationOverrides: \u0026#34;default.replication.factor\u0026#34;: 3 \u0026#34;offsets.topic.replication.factor\u0026#34;: 1 # Increased from 1 to 2 for higher output # \u0026#34;offsets.topic.num.partitions\u0026#34;: 3 \u0026#34;confluent.support.metrics.enable\u0026#34;: false # Disables confluent metric submission # \u0026#34;auto.leader.rebalance.enable\u0026#34;: true # \u0026#34;auto.create.topics.enable\u0026#34;: true # \u0026#34;controlled.shutdown.enable\u0026#34;: true # \u0026#34;controlled.shutdown.max.retries\u0026#34;: 100 \u0026#34;message.max.bytes\u0026#34;: \u0026#34;16000000\u0026#34; # Extend global topic max message bytes to 16 Mb ## Persistence configuration. Specify if and how to persist data to a persistent volume. ## persistence: enabled: true ## Prometheus Exporters / Metrics ## prometheus: ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics jmx: enabled: true ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter kafka: enabled: true topics: [] # ------------------------------------------------------------------------------ # Zookeeper: # ------------------------------------------------------------------------------ zookeeper: ## If true, install the Zookeeper chart alongside Kafka ## ref: https://github.com/kubernetes/charts/tree/master/incubator/zookeeper enabled: true Kubernetes Configurations replicas resource pod affinity persistence Kafka Configurations zookeeper configurationOverrides \u0026#34;default.replication.factor\u0026#34;: 3 \u0026#34;offsets.topic.replication.factor\u0026#34;: 1 # Increased from 1 to 2 for higher output # \u0026#34;offsets.topic.num.partitions\u0026#34;: 3 # \u0026#34;auto.leader.rebalance.enable\u0026#34;: true # \u0026#34;auto.create.topics.enable\u0026#34;: true …","date":1571356800,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"9c9c8e25feab5bb4ca9220fa3fb2b5d7","permalink":"https://chechia.net/zh-hant/slides/2019-10-18-kafka-on-k8s/","publishdate":"2019-10-18T00:00:00Z","relpermalink":"/zh-hant/slides/2019-10-18-kafka-on-k8s/","section":"slides","summary":"Deploy Kafka on Kubernetes Che-Chia Chang\nQRCode\nAbout Me David (Che-Chia) Chang\nBackend / Devops @ MachiX Golang Taipei Meetup 2020 Ithelp Ironman Challenge https://t.me/chechiachang Outline Introduction to Kafka Deploy Kafka with Helm Kafka Topology Ithelp Ironman 30 days Challenge (7th-12nd day) Introduction https://kafka.","tags":["kafka","kubernetes"],"title":"Deploy Kafka on Kubernetes","type":"slides"},{"authors":[],"categories":[],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 超簡短推坑 oeprator-sdk 鐵人賽心得 承上 上篇介紹了 crd 與 controller，然而沒有說明 controller 的編寫與操作，因為 controller 的部分比較複雜，我們鐵人挑戰賽尾聲，篇幅說實在是不太夠。\n有興趣詳細了解的大德，請參考相同鐵人挑戰團隊的隊友文章，裏頭對 controller 有詳細介紹，這邊就不贅述。直接提供個人使用覺得最簡單上手的 operator sdk\nOperator SDK Operator SDK 是 Operator framework 中的一部分，能有效且自動化的管理 kubernetes native apps, operator 的管理工具。\n複雜的 kubernetes application 是非常難管理的，寫 operator 也是很有挑戰，不僅要處理大量 kubernetes 底層的 API，要寫很多樣版。 operator SDK 使用 controller-runtime 的 library 讓編寫 native application 變得簡單許多\n可以使用上層的 API 與抽象來編寫 operator 邏輯 快速使用 code generation 有擴充套件 Workflow 這邊以 golang 為例說明\n安裝 operator sdk 定義新的 API resource (custom resource definition) 定義 controller 來監測 custom resource 編寫 reconciling 邏輯來 sync desired state 與 current state 使用 sdk cli 進行測試 使用 sdk cli 來 build，並產生部屬用的 manifests 安裝請依照 安裝說明 操作即可。\n這邊使用 sdk cli 來增加新的 crd\n# Add a new API for the custom resource AppService $ operator-sdk add api --api-version=app.example.com/v1alpha1 --kind=AppService 產生的 go 源碼會放在 pkg 中，可以依自己需求調整 crd 的結構\n這邊使用 sdk cli 產生對應 crd 的 controller，裏頭已經寫好大部分的 code gene 與 reconcile 的樣板，直接修改就可使用，非常方便\n# Add a new controller that watches for AppService $ operator-sdk add controller --api-version=app.example.com/v1alpha1 --kind=AppService 修改完，直接使用 sdk cli build 成 image，然後推到 image hub 上\n# Build and push the app-operator image to a public registry such as quay.io $ operator-sdk build quay.io/example/app-operator $ docker push quay.io/example/app-operator 部屬前檢查一下 manefests 檔案，特別是 crd.yaml 與 operator.yaml，如果源碼有調整記得做對應的修改。\n# Setup Service Account $ kubectl create -f deploy/service_account.yaml # Setup RBAC $ kubectl create -f deploy/role.yaml $ kubectl create -f deploy/role_binding.yaml # Setup the CRD $ kubectl create -f deploy/crds/app.example.com_appservices_crd.yaml # Deploy the app-operator $ kubectl create -f deploy/operator.yaml 這樣便部屬了 operator，operator 會監看指定的 custom resource，並依照 controller 的邏輯進行 reconcile。\n這邊以增加 custom resource 為例\n# Create an AppService CR # The default controller will watch for AppService objects and create a pod for each CR $ kubectl create -f deploy/crds/app.example.com_v1alpha1_appservice_cr.yaml 增加一個 cr 到 kubernetes 上，這時 operator 會偵測到 cr 的變化，並且依照 reconcile 的邏輯 sync\n檢查一下 cr 與 operator 的狀態\n# Verify that a pod is created $ kubectl get pod -l app=example-appservice NAME READY STATUS RESTARTS AGE example-appservice-pod 1/1 Running 0 1m 詳細的操作步驟可以看 這邊\n小結 事實上，operator sdk 的功能還有非常多，細講又要花好幾篇文章講，之後有機會會放在我的個人網站上。\n另外 operator sdk 也歡迎外部的 Issue 與 PR，團隊的人非常 nice 會願意花時間跟社群朋友溝通，有興趣請來 contribute。\n這系列鐵人文章，說實在沒有什麼很深入的技術討論，多半資料都是各個項目的官方文件翻譯，加上一些個人的經驗與解讀，並不是含金量很高的文章。然而我個人在接觸這些項目時，卻往往因為找不到細節操作的步驟分享文章，在許多小細節上撞牆很久，也因此才有了這系列文章。\n這系列文就只是踩雷之旅，讓後人如果有用到這些文章，生活能過得開心一點，這 30 天的時間就有了價值。\n鐵人挑戰賽的最後一天，感謝各路大德一路相隨，讓我在假日也能心甘情願地坐下來寫文章。游於藝天一篇真的很逼人，有幾天的文章品質是有蠻多問題的，也感謝大德們協助捉錯，給予很多建議。\n謝謝各位。\n","date":1571131692,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"8c3adefbdce3373186b360c180f865b7","permalink":"https://chechia.net/zh-hant/post/2019-10-15-kubernetes-custom-resource-with-operator-sdk/","publishdate":"2019-10-15T17:28:12+08:00","relpermalink":"/zh-hant/post/2019-10-15-kubernetes-custom-resource-with-operator-sdk/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":[],"title":"Kubernetes Custom Resources with Operator SDK","type":"post"},{"authors":[],"categories":["kubernetes","crd"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 CRD 內容 Deploy CRD Use custom resource Recap 在上次的 cert-manager 內容中我們走過 cert-manager 的安裝步驟，其中有一個步驟是 apply cert-manager 的 manigests 檔案 *.yaml)\nhttps://github.com/jetstack/cert-manager/tree/release-0.11/deploy/manifests\n$ git clone https://github.com/jetstack/cert-manager $ git checkout release-0.11 $ ls deploy/manifest 00-crds.yaml 01-namespace.yaml BUILD.bazel\tREADME.md\thelm-values.yaml 我們快速看一下這個 00-crds.yaml，這個 yaml 非常長，直接跳到 certificates.certmanager.k8s.io\n希望看 golang 源碼文件的話，可以搭配godoc.org/k8s.io/apiextensions 來閱讀，更能理解 definition。\n在看之前先注意幾件事，CRD 內除了 schema 外，還定義了許多不同情境的使用資料。\nCRD 內定義了 custom resource 的資料儲存 .spec.validation.openAPIV3Schema，使用 custom resource 會透過 validator 驗證 .openAPIV3Schema 內定義了 .spec，以及 rumtime 中紀錄 .status 的資料 controller 可以把狀態 sync 到 custom resource 的 .status 中紀錄 controller 可以比對 .spec 與 .status 來決定是否要 sync 以及如何 sync CRD 內定義了與 server 以及 client 互動的方式， names 中定義各種使用情境的 custom resource 名稱 additionalPrinterColumns 中添加 kubectl 中的顯示內容 // 這邊使用的是 v1beta1 的 API (deprecated at v1.16) ，新版開發建議使用 apiextension.k8s.io/v1 的 api apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: creationTimestamp: null name: certificates.cert-manager.io spec: // 使用 kubectl 會額外顯示的資訊內容，透過 jsonpath 去 parse 顯示 additionalPrinterColumns: - JSONPath: .status.conditions[?(@.type==\u0026#34;Ready\u0026#34;)].status name: Ready type: string - JSONPath: .spec.secretName name: Secret type: string - JSONPath: .spec.issuerRef.name name: Issuer priority: 1 type: string - JSONPath: .status.conditions[?(@.type==\u0026#34;Ready\u0026#34;)].message name: Status priority: 1 type: string - JSONPath: .metadata.creationTimestamp description: CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC. name: Age type: date group: cert-manager.io // 定義 CRD 在不同情境下使用的名稱 names: kind: Certificate listKind: CertificateList plural: certificates shortNames: - cert - certs singular: certificate scope: Namespaced subresources: status: {} validation: // openAPIV3Schema 中是 custom resource 實際操作會使用的內容 // properties 使用 . .description .type ，分別定義名稱，描述，檢查型別 openAPIV3Schema: description: Certificate is a type to represent a Certificate from ACME properties: apiVersion: description: \u0026#39;APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\u0026#39; type: string kind: description: \u0026#39;Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\u0026#39; type: string // custom resource runtime 中的 metadata metadata: type: object // custom resource 使用時的 spec，定義 custom resoure 的 desired status spec: ... // custom controller 監測 custom resource 的 current status，這邊的資料完全視 controller 實作來產生，如果沒有實作 sync status，也可以沒有資料 status: ... type: object // 這個是 CRD 物件的 version，可以定義多個不同 version 的 CRD，調用時需要註明版本 version: v1alpha2 versions: - name: v1alpha2 served: true storage: true // 這個是 CRD 物件的 status，描述 CRD 部署到 API server 的狀態，例如 CRD 儲存適用 configmap 的儲存空間，這邊顯示在 API server 上的儲存狀態。不要跟 custom resource 的 status 弄混了 status: acceptedNames: kind: \u0026#34;\u0026#34; plural: \u0026#34;\u0026#34; conditions: [] storedVersions: [] helm-values.yaml 與 01-namespace.yaml 很單純，前者是使用 helm 部署的可設定參數，預設只有 kubernetes resources，後者則是為之後的 cert-manager 元件新增一個 kubernetes namespace。\n小結 CRD 內容 (apiextensions/v1beta1) CRD 顯示名稱，內容 CRD spec 驗證 custom resource custom resource schema CRD 自身部署狀態 部署 部署相較定義本身就非常簡單，直接 kubectl apply 到 kubernetes 上\n使用 custom resource 有了 CRD，我們便可以使用 CRUD API，互動模式與其他 build-in kubernetes resources 相同，只是內容會照 CRD 上的定義調整\nkubectl get certificates.certmanager.k8s.io kubectl get certificates kubectl get certs --all-namespaces kubectl get cert -n cert-manager NAMESPACE NAME READY SECRET AGE cert-manager ingress-nginx-tls True ingress-nginx-tls 221d 這邊看到的內容可能會有些落差，因為我當初用的版本比較舊，但內容大同小異。\n底下的 describe 內容已經跟上面的 CRD 版本差太多，對不起來了。但我也懶得再佈一組，還要重做 dnsName 與 authotization challenge\n直接讓大家感受一下舊版的內容XD\n$ kubectl describe cert ingress-nginx-tls Name: …","date":1570975388,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"c9449981dd25dbcce191863ae62972c7","permalink":"https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resource-deployment/","publishdate":"2019-10-13T22:03:08+08:00","relpermalink":"/zh-hant/post/2019-10-13-kubernetes-custom-resource-deployment/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","crd","gcp"],"title":"Kubernetes Custom Resource Deployment","type":"post"},{"authors":[],"categories":["kubernetes","crd"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 custom resources custom controllers 簡介 custom resources Kubernetes 預先定義許多 resource ，這些 resource 是 kubernetes API 預先設置的 API objects，例如 kubernetes pods resource 包含許多 pods 物件。\nCustom resoure 則是透過擴充 kubernetes API ，讓自定義的物件也可以在 kubernetes 上使用。上篇 cert-manager 就使用了許多 custom resource，這些 resource 在一般安裝的 kubernetes 上沒有安裝，需要安裝 custom resource difinition，向 kubernetes cluster 定義新的 custom resource。例如 certificates.certmanager.k8s.io 就是 cert-manager 自定義的資源，用來代表產生 x509 certificate 的內容。\n越來越多的 kubernetes core 方法，如今也使用 custom resources 來定義，讓 kubernetes 核心元件更加模組化。\ncustom resource 可以在運行中的 kubernetes 集群中註冊 (registration) ，也可以動態註銷，custom resource 並不會影響集群本身的運作。只要向 kubernetes 註冊完 custom resource，就可以透過 API 與 kubectl 控制 custom resource，就像操作 Pod resource 一樣。\nCustom controllers custom resource 一但註冊，就可以依據 resource 的 CRD (custom resource definition) 來操作，因次可以儲存客製化的資料內容。然而在很多情形，我們並不只要 custom resource 來讀寫，而是希望 custom resource 能執行定義的工作，如同 Pod resource 可以在 kubernetes 集群上控制 Pod，在 Pod resource 上描述的 desired state kubernetes 會透過定義在 Pod API 中的 sync 邏輯，來達到 current state 與 desired state 的平衡。\n我們希望 custom resource 也能做到上述的功能，提供 declarative API，讓使用者不需編寫完整的程式邏輯，只要透過控制 custom resource，就可以透過 controller 內定義的邏輯，來實現 desired state。使用者只需要專注在控制 custom resource 上的 desired state，讓 controller 處理細節實作。\n例如：我們在 cert-manager 中設定 certificates.certmanager.k8s.io 資源，來描述我們希望取得 x509 certificate 的 desired state，但我們在 certificates.certmanager 上面沒有寫『透過 Let’s Encrypt 取得 x509 certificate』的實現邏輯，仍然能透過 cert-manager 產生 x509 certiticate，因為 cert-manager 內部已經定義 certificates.certmanager.k8s.io 的 custom controller。\n基本的 custom resource 操作\n註冊 custom resource definition，讓 kubernetes API 看得懂 custom resource 不然 API 會回覆 error: the server doesn’t have a resource type 有 CRD 便可以 apply custom resource 到集群中 部署 custom controller，監測 custom resource 的 desired state 內容，並實現達到 desired state 的業務邏輯 沒有 custom controller，custom resource 就只是可以 apply 與 update 的資料儲存結構，沒有 cert-manager 中 controller 的邏輯，也還是生不出 x509 certificate。 kubectl get chechiachang error: the server doesn\u0026#39;t have a resource type \u0026#34;chechiachang\u0026#34; custom controller 也可以跟其他的 kubernetes resource 互動，例如 cert-manager 在產生 certificate 的時候，會把產生的 certificate 檔案放在 secret 中，cert-manager 會依據 order 中定義的 lifecycle ，持續檢查 certificate 的有效性，如果接近過期，則會觸發新的一輪 order。\n我們也可以寫一個操作 Configmap 與 Deployment Resource 的 custom controller，來進行 deploymnet 的 Image 更新。\n我需要 custom resource 嗎 kubernetes 在should I add custom resource 有列表分析該不該使用 custom resource ，將你的 API 邏輯整合到 kubernetes API 上。幾個判斷參考:\nAPI 是 declarative model，如果不是可能不適合跟 kubernetes API 整合，獨立成為一個自己運行的服務即可 需要使用 kubernetes 需要使用 kubectl 控制 API 需要使用 kubernetes 支援的功能 正哉開發全新功能，因為整合舊的服務到 kubernetes API 工程浩大 也許 configmap/secret 就可以解決 如果只是需要將資料儲存在 kubernetes 上，有一個 build-int 的 kubernetes resource 很適合，就是 configmap。可以瘀考以下條件，判斷是否 configmap 搭配能監看 configmap 的 controller 就可以達成需求。\n已經有完整的 config file，例如 mysql.cnf, nginx.conf… 主要用途是把檔案掛載到 Pod 中的 process 使用 使用時的格式，是整個檔案放在 Pod 中，或是使用環境變數塞到 Pod 裡面，而不是透過 kubernetes API 存取 (ex 使用 kubectl) 更新 configmpa 時更新 Pod，會比更新 custom resource 時更新 Pod 容易 如果使用 CRD 或 Aggregated kubernetes API，大多符合下列條件\n使用 kubernetes libraries 與客戶端 (ex kubectl) 操作 custom resource 需要 top level 的 kubernetes 支援，例如可以 kubectl get cheachiachang 自動化 kubernetes 物件 需要用到 .spec, .status, .metadata，這些比較 desired state 與 currenty state 的功能 需要抽象類別來管理一群 controlled resource Custom Resource Definition Custom Resoure Definition 讓使用者可以定義 custom resource，定義 custom resource 的格式包括名稱與 data schema，然後交給 kubernetes API 去處理 custom resource 的儲存。\n也就是說，透過 CRD 我們不用寫 custom resource 的 API，例如 cert-manager 不用寫 certificates.certmanager.k8s.io 的 API，而是向 kubernetes API server 註冊 CRD，讓 kubernetes API server 看得懂 custom source 的定義，並且直接使用 kubernetes API server，進行 custom resource 的 CRUD。\n我們可以透過 kubectl (API server) 操作 certificates.certmanager.k8s.io，這個請求也是送到 kubernetes API server。\nAPI server aggregation 能夠透過註冊 CRD ，就可以使用原來的 kubernetes API 來進行 CRUD ，是因為 kubernetes API 對於普通的 API 操作提供泛型 (generic) 介面，直接使用 CRUD 的邏輯。\n由於是 kubernetes Aggregated API ，所有 kubernetes 的 clients 都一起兼容新註冊的 custom resource，不用在 API 要定義，在冊戶端也要定義。註冊完的 custom resource definition，可以直接透過 kubectl 存取。\n小結 custom resource 簡介 custom resource 使用的情境與條件 custom resource definition 與 Aggregated API ","date":1570958892,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"fd1007cf250cbf0a57589127e2a10a31","permalink":"https://chechia.net/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/","publishdate":"2019-10-13T17:28:12+08:00","relpermalink":"/zh-hant/post/2019-10-13-kubernetes-custom-resources-basic/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","crd","gcp"],"title":"Kubernetes Custom Resources Basic","type":"post"},{"authors":[],"categories":["kubernetes","cert-manager"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\nRecap 昨天我們實際使用 cert-manager，為 nginx ingress controller 產生 certificates，過程中我們做了幾件事\n設置 Let’s Encript prod site 的 Issuer 設置 certificates.certmanager.k8s.io 資源來定義 certificate 的取得方式 或是在 ingress 中配置 tls，讓 cert-manager 自動透過 ingress-shim 產生 certifcates.cert-manager，並且產生 certificate 以上是使用 cert-manager 產生 certificate 的基本操作，剩下的是由 cert-manager 完成。實際上 cert-manager 在產生出 certificate 之前還做了很多事情，我們今天就詳細走過完整流程，藉此了解 cert-manager 配合 issuing certificate 的流程\n使用者設置 Issuer\n使用者設定 certificate -\u0026gt; cert-manager 根據 certificate -\u0026gt; 產生 certificate\nCertificateRequests certificaterequests.certmanager 是 cert-manager 產生 certificate 過程中會使用的資源，不是設計來讓人類操作的資源。\n當 cert-manager 監測到 certificate 產生後，會產生 certificaterequests.certmanager.k8s.io 資源，來向 issuer request certificate，這個過程與使用其他客戶端 (ex. certbot) 來向 3rd party CA server request certificate 時的內容相同，只是這邊我們使用 kubernetes resource 來定義。\n包含的 certificate request，會以 pem encoded 的形式，再變成 base64 encoded 存放在 resource 中。這個 pem key 也會從到遠方的 CA sercer (Let’s Encrypt prod) 來 request certificate\n如果 issuance 成功，certificaterequest 資源應該會被 cert-manager 吃掉，不會被人類看到。\n一個 certificaterequests.certmanager 大概長這樣\napiVersion: cert-manager.io/v1alpha2 kind: CertificateRequest metadata: name: my-ca-cr spec: csr: LS0tLS1CRUdJTiBDRVJUSUZJQ0FUR .................................. LQo= isCA: false duraton: 90d issuerRef: name: ca-issuer # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: Issuer group: cert-manager.io 這個 certificaterequests.certmanager 會讓 cert-manager 嘗試向 Issuer (lets-encrypt-prod) request certificate。\nOrder orders.certmanager.k8s.io 被 ACME 的 Issuer 使用，用來管理 signed TLD certificate 的 ACME order，這個 resource 也是 cert-manager 自行產生管理的 resource，不需要人類來更改。\n當一個 certificates.certmanager 產生，且需要使勇 ACME isser 時，certmanager 會產生 orders.certmanager ，來取得 certificate。\nChallenges challenges.certmanager 資源是 ACME Issuer 管理 issuing lifecycle 時，用來完成單一個 DNS name/identifier authorization 時所使用的。用來確定 issue certiticate 的客戶端真的是 DNS name 的擁有者。\n當 cert-manager 產生 order 時，order controller 接到 order ，就會為每一個需要 DNS certificate 的 DNSname ，產生 challenges.certmanager。\n這段也是 order controller 自動產生，並不需要使用者參與。\nACME certificate issuing user -\u0026gt; 設定好 issuers.certmanager\nuser -\u0026gt; 產生 certificates.certmanager -\u0026gt; 選擇 Issuer -\u0026gt;\ncert-manager -\u0026gt; 產生 certificaterequest -\u0026gt;\ncert-manager 根據 certiticfates.certmanager 產生 orders.certmanager -\u0026gt;\norder controller 根據 order ，並且跟每一個 DNS name target，產生一個 challenges.certmanager\nchallenges.certmanager 產生後，會開啟這個 DNS name challenge 的 lifecycle\nchallenges 狀態為 queued for processing，在佇列中等待， 如果沒有別的 chellenges 在進行，challenges 狀態變成 scheduled，這樣可以避免多個 DNS challenge 同時發生，或是相同名稱的 DNS challenge 重複 challenges 與遠端的 ACME server ‘synced’ 當前的狀態，是否 valid 如果 ACME 回應這個 DNS name 的 challenge 還是有效的，則直接把 challenges 的狀態改成 valid，然後移出排程佇列。 如果 challenges 狀態仍然為 pending，challenge controller 會依照設定 present 這個 challenge，使用 HTTP01 或是 DNS01，challenges 被標記為 presented challenges 先執行 self check，確定 challenge 狀態已經傳播給 dns servers，如果 self check 失敗，則會依照 interval retry ACME authorization 關聯到 challenge cert-manager 處理 ‘scheduled’ challenges.certmanager -\u0026gt; ACME challenge\n","date":1570873285,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"738d6814d1bc59ec9865db3f97f1ba8a","permalink":"https://chechia.net/zh-hant/post/2019-10-12-cert-manager-complete-workflow/","publishdate":"2019-10-12T17:41:25+08:00","relpermalink":"/zh-hant/post/2019-10-12-cert-manager-complete-workflow/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","cert-manager","devops"],"title":"Cert Manager Complete Workflow","type":"post"},{"authors":[],"categories":["kubernetes","cert-manager"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n今天我們來實際使用 cert-manager，為 nginx ingress controller 產生 certificates with ACME Issuer\nCA Terminology 先把實際執行 CA 簽發的名詞定義一下，以免跟 cert-manager 的資源搞混\nCertificate: 憑證，x509 certificate，cert-manager 自動管理的目標，透過 let’s encript 取得的 x509 certificates CA (Certificate Authority): issue signed certificate 的機構 issue: 頒發，指 CA 產生 certificate 與 key (今天的範例格式是 .crt 與 .key) Sign vs self-signed: 簽核，自己簽核，使用信任的 CA issue certificate，或是使用自己產生的 CA self-sign，然後把 CA 加到可以被信任的 CA 清單中。 Let’s Encript CA issues signed certificates\nKubernetes in-cluster CA issues self-signed certificates\ncert-manager 的 CRD 資源，使用來描述 cert-manager 如何執行上述操作，CRD 底下都會加上 ``*.certmanager.k8s.io` 方便辨識。\n設定 Issuer Issuer 要怎麼翻成中文XD，憑證頒發機構？\n總之在開始簽發 certificates 前，要先定義 issuers.certmanager.k8s.io ，代表一個能簽發 certificate CA，例如 Let’s Encript，或是 kubernetes 內部也有內部使用的憑證簽發，放在 secrets 中。\n這些 Issuer 會讓 certificates.certmanager.k8s.i8o 使用，定義如何取得 certificate 時，選擇 Issuer。\ncert-manager 上可以定義單一 namespace 的 issuers.certmanager 與集群都可使用的 clusterissuers.certmanager\ncert-manager 有支援幾種的 issuer type\nCA: 使用 x509 keypair 產生certificate，存在 kubernetes secret Self signed: 自簽 certificate ACME: 從 ACME (ex. Let’s Encrypt) server 取得 ceritificate Vault: 從 Vault PKI backend 頒發 certificate Venafi: Venafi Cloud Certificate 有了簽發憑證的單位，接下來要定義如何取得 certificate。certificates.certmanager.k8s.io 是 CRD，用來告訴 cert-manager 要如何取得 certificate\ncertifcates.certmanager.k8s.io 提供了簡單範例\napiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: acme-crt spec: secretName: acme-crt-secret duration: 90d renewBefore: 30d dnsNames: - foo.example.com - bar.example.com acme: config: - http01: ingressClass: nginx domains: - foo.example.com - bar.example.com issuerRef: name: letsencrypt-prod # We can reference ClusterIssuers by changing the kind here. # The default value is Issuer (i.e. a locally namespaced Issuer) kind: Issuer 上面這個 certificate.certmanger 告訴 cert-manager\n針對 foo.example.com 與 bar.example.com 兩個 domainsc 使用 letsencript-prd Issuer 去取得 certificate key pair 成功後把 ceritifcate 與 key 存在 secret/acme-crt-secret 中(以 tls.key, tls.crt 的形式) 與 certificate.certmanager 都放在相同 namespace 中，產生 certificate.certmanager 的時候要注意才不會找不到 secret 這邊指定了 certificate 的有效期間與 renew 時間 (預設值)，有需要可以更改 配合 Ingress 設置 tls 有上述的設定，接下來可以請求 tls certificate\n記得我們上篇 Nginx Ingress Controller 提到的 ingreess 設定嗎？這邊準備了一個適合配合 nginx ingress 使用的 tls 設定\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-nginx-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; cert-manager.io/issuer: \u0026#34;letsencrypt-prod\u0026#34; spec: tls: - hosts: - foo.example.com secretName: my-nginx-ingrss-tls rules: - host: foo.example.com http: paths: - path: / backend: serviceName: chechiachang-backend servicePort: 80 這個 ingress apply 後，就會根據 spec.tls 的 hosts 設定，自動產生一個 certificate.certmanager 資源，並在這個資源使用 letsencryp-prod。\n不用我們手動 apply 新的 ceritificate，這邊是 cert-manager 使用了 annotation 來觸發 Ingress-shim，簡單來說，當 ingress 上有使用 cert-manager.io 的 annotation 時，cert-manager 就會根據 ingress 設定內容，抽出 spec.tls 與 isuer annotation，來產生同名的 certificates.certmanager，這個 certificateas.certmanager 會觸發接下的 certificate 頒發需求。\n只要部署 Issuer 與 Ingress 就可以自動產生 certificate。當然，希望手動 apply certificates.certmanager 也是行得通。\n把產生了 certificate.certmanager 拉出來看\nkubectl describe certificate my-nginx-ingress Name: my-nginx-ingress Namespace: default API Version: cert-manager.io/v1alpha2 Kind: Certificate Metadata: Cluster Name: Creation Timestamp: 2019-10-10T17:58:37Z Generation: 0 Owner References: API Version: extensions/v1beta1 Block Owner Deletion: true Controller: true Kind: Ingress Name: my-nginx-ingress Resource Version: 9295 Spec: Dns Names: example.your-domain.com Issuer Ref: Kind: Issuer Name: letsencrypt-prod Secret Name: my-nginx-ingress-tls Status: Acme: Order: URL: https://acme-prod-v02.api.letsencrypt.org/acme/order/7374163/13665676 Conditions: Last Transition Time: 2019-10-10T18:05:57Z Message: Certificate issued successfully Reason: CertIssued Status: True Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreateOrder 1d cert-manager Created new ACME order, attempting validation... Normal DomainVerified 1d cert-manager Domain \u0026#34;foo.example.com\u0026#34; verified with \u0026#34;http-01\u0026#34; validation Normal IssueCert 1d cert-manager Issuing certificate... Normal CertObtained 1d cert-manager Obtained certificate from ACME server Normal CertIssued 1d cert-manager Certificate issued Successfully 把 certificate 從 secret 撈出來看\n$ kubectl …","date":1570764274,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"5fa9fa6dd18847dd7859d28ba06d3e16","permalink":"https://chechia.net/zh-hant/post/2019-10-11-cert-manager-how-it-work/","publishdate":"2019-10-11T11:24:34+08:00","relpermalink":"/zh-hant/post/2019-10-11-cert-manager-how-it-work/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","cert-manager","devops"],"title":"Cert Manager How It Work","type":"post"},{"authors":[],"categories":["kubernetes","cert-manager"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Cert-manager Introduction Deploy cert-manager 簡介 cert-manager TLS certificate 管理很重要，但在 kubernetes 上管理 TLS certificates 很麻煩。\n以往我們使用 Let’s Encrypt 提供的免費自動化憑證頒發，搭配 kube-lego 來自動處理 certificate issuing，然而隨著 kube-lego 已不再更新後，官方建議改使用 Cert-manager 來進行 kubernetes 上的憑證自動化管理。\ncert-manager 是 kubernetes 原生的憑證管理 controller。是的他的核心也是一個 controller，透過 kubernetes object 定義 desired state，監控集群上的實際狀態，然後根據 resource object 產生憑證。cert-manager 做幾件事情\n在 kubernetes 上 使用 CRD (Customized Resource Definition) 來定義 certificate issuing 的 desired state 向 let’s encrypt 取得公開的憑證 在 kubernetes 上自動檢查憑證的有效期限，並自動在有效時限內 renew certificate。 安裝 官方文件有提供 詳細步驟 可以直接使用 release 的 yaml 部屬，也可以透過 helm。\n使用 yaml 部屬 # Create a namespace to run cert-manager in kubectl create namespace cert-manager # Disable resource validation on the cert-manager namespace kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true 開一個獨立的 namespace 來管理 cert-manager resources\n取消 namespcae 中的 kubernetes validating webhook。由於 cert-manager 本身就會使用 ValidatingWebhookConfiguration 來為 cert-manager 定義的 Issuer, Certificate resource 做 validating。然而這會造成 cert-manager 與 webhook 的循環依賴 (circling dependency)\n# Install the CustomResourceDefinitions and cert-manager itself kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml # Install the CustomResourceDefinitions and cert-manager itself kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.10.1/cert-manager.yaml 這個 yaml 裡面還有幾個元件\nCluster Role-bindings CustomResourceDefinition certificaterequests.certmanager.k8s.io certificates.certmanager.k8s.io challenges.certmanager.k8s.io clusterissuers.certmanager.k8s.io issuers.certmanager.k8s.io orders.certmanager.k8s.io 這些元件的細節，留待運作原理分析時再詳解。\nhelm deployment 這邊也附上使用 helm 安裝的步驟\n#!/bin/bash # Install the CustomResourceDefinition resources separately kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml # Create the namespace for cert-manager kubectl create namespace cert-manager # Label the cert-manager namespace to disable resource validation kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true # Add the Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update your local Helm chart repository cache helm repo update # Install the cert-manager Helm chart helm install \\ --name cert-manager \\ --namespace cert-manager \\ --version v0.10.1 \\ jetstack/cert-managerNAMESPACE=cert-manager 部屬完檢查一下\nkubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-5c6866597-zw7kh 1/1 Running 0 2m cert-manager-cainjector-577f6d9fd7-tr77l 1/1 Running 0 2m cert-manager-webhook-787858fcdb-nlzsq 1/1 Running 0 2m 這邊部屬完，會獲得完整的 cert-manager 與 cert-manager CRD，但 certificate 的 desired state object 還沒部屬。也就是關於我們要如何 issue certificate 的相關描述，都還沒有 deploy， cert-manager 自然不會工作。關於 issuing resources configuration，我們下次再聊。\n","date":1570695130,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"4678aebc0f449b2a17c6af56d5feb0e0","permalink":"https://chechia.net/zh-hant/post/2019-10-10-cert-manager-deployment/","publishdate":"2019-10-10T16:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-10-cert-manager-deployment/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","cert-manager","devops"],"title":"Cert Manager Deployment on Kubernetes","type":"post"},{"authors":[],"categories":["kubernetes","nginx"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊該了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\nNginx Ingress Controller 簡介 nginx \u0026amp; Ingress Controller 部屬並設定 nginx ingress controller Nginx Introduction Nginx 是一款高效能、耐用、且功能強大的 load balancer 以及 web server，也是市占率最高的 web server 之一。\n高效能的 web server，遠勝傳統 apache server 的資源與效能 大量的模組與擴充功能 有充足的安全性功能與設定 輕量 容易水平擴展 Ingress \u0026amp; Ingress Controller 這邊簡單講一下 kubernetes ingress。當我們在使用 kubernetes 時需要將外部流量 route 到集群內部，這邊使用 Ingress 這個 api resource，來定義外部到內部的設定，例如:\nservice 連接 load balance 設定 SSL/TLS 終端 虛擬主機設定 一個簡單的 ingress 大概長這樣\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: test-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath backend: serviceName: test servicePort: 80 除了一般的 k8s 資源，nginx 主要的設定會落在 spec，以及依賴底下實作不同，額外設定的 annotation。\n這邊可以看到 spec.rule 定義了外部 http 流量，引導到 backend service 的路徑。\nannotations 下已經標註的 nginx.ingress 的 annotation，來快速增加額外的設定。\nIngress \u0026amp; Ingress Controller 雖然已經指定 nginx 的 annotation，但這邊要注意，ingress resource 本身是不指定底層的實現 (ingress controller)，也就是說，底下是 nginx 也好，traefik 也行，只要能夠實現 ingress 裏頭設定的 routing rules 就可以。\n只設定好 ingress，集群上是不會有任何作用的，還需要在集群上安裝 ingress controller 的實作，實作安裝完了以後，會依據 ingress 的設定，在 controller 裏頭實現，不管是 routing、ssl/tls termination、load balancing 等等功能。如同許多 Kubernetes resource 的設計理念一樣，這邊也很優雅的用 ingress 與 ingress controller，拆分的需求設定與實作實現兩邊的職責。\n例如以 nginx ingress controller，安裝完後會依據 ingress 的設定，在 nginx pod 裡設定對應的 routing rules，如果有 ssl/tls 設定，也一併載入。\nKubernetes 官方文件提供了許多不同的 controller 可以依照需求選擇。\n但如果不知道如何選擇，個人會推薦使用 nginx ingress controller，穩定、功能強大、設定又不至於太過複雜，基本的設定就能很好的支撐服務，不熟悉的大德們比較不容易被雷到。\n底下我們就要來開始使用 nginx ingress controller。\nDeployment 我們這邊使用的 ingress-nginx 是 kubernetes org 內維護的專案，專案內容主要是再 k8s 上執行 nginx，抽象與實作的整合，並透過 configmap 來設定 nginx。針對 nginx ingress kubernetes 官方有提供非常詳細的說明文件 ，剛接觸 nginx 的大德可以透過這份文件，快速的操作 nginx 的設定，而不用直接寫 nginx.conf 的設定檔案。\nrepo 版本是 nginx-0.26.1 Image 版本是 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1 Helm 我們這邊用 helm 部屬，Nginx Ingress Controller Stable Chart，讓各位大德用最簡單的步驟，獲得一個功能完整的 nginx ingress controller。\n與前面幾個 helm chart 一樣，我們可以先取得 default values.yaml 設定檔，再進行更改。\n$ wget https://raw.githubusercontent.com/helm/charts/master/stable/nginx-ingress/values.yaml $ vim values.yaml 安裝時也可以使用 –set 來變更安裝 chart 時的 parameters\n$ helm install stable/nginx-ingress \\ --set controller.metrics.enabled=true \\ -f values.yaml 安裝完後，resource 很快就起來。\nkubectl get all --selector app=nginx-ingress NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-7bbcbdcf7f-tx69n 1/1 Running 0 216d pod/nginx-ingress-default-backend-544cfb69fc-rnn6h 1/1 Running 0 216d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx-ingress-controller LoadBalancer 10.15.246.22 34.35.36.37 80:30782/TCP,443:31933/TCP 216d service/nginx-ingress-default-backend ClusterIP 10.15.243.19 \u0026lt;none\u0026gt; 80/TCP 216d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ingress-controller 1/1 1 1 216d deployment.apps/nginx-ingress-default-backend 1/1 1 1 216d NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ingress-controller-7bbcbdcf7f 1 1 1 216d replicaset.apps/nginx-ingress-default-backend-544cfb69fc 1 1 1 216d kubectl get configmap -l app=nginx-ingress NAME DATA AGE nginx-ingress-controller 2 216d kubectl get ingress NAME HOSTS ADDRESS PORTS AGE ingress-nginx api.chechiachang.com 34.35.36.37 80, 443 216d 兩個 Pods\nNginx ingress controller 是主要的 nginx pod，裡面跑的是 nginx Nginx default backend 跑的是 default backend，nginx 看不懂了 route request 都往這邊送。 Service\nnginx-ingress-contrller 是我們在 GCP 上，在集群外部的 GCP 上的對外接口。如果在不同平台上，依據預設 service load balancer 有不同實作。 在 gcp 上，會需要時間來啟動 load balancer，等 load balancer 啟動完成，service 這邊就可以取得外部的 ip，接受 load balancer 來的流量 另外一個 service 就是 default backend 的 service 踩雷 第一個雷點是 helm chart install 帶入的 parameters，有些 parameter 是直接影響 deployment 的設定，如果沒注意到，安裝完後沒辦法透過 hot reload 來處理，只能幹掉重來。建議把這份表格都看過一次，再依照環境與需求補上。\n$ helm install stable/nginx-ingress \\ --set controller.metrics.enabled=true \\ --set controller.service.externalTrafficPolicy=Local \\ -f values.yaml 這邊開了 prometheus metrics exporter，以及 source IP preservation。\nNginx Config 再安裝完後，外部的 load balancer 啟用後，就可以透過 GCP 的 external ip 連入 nginx，nginx 依照設定的 rule 向後端服務做集群內的 load balancing 與 routing。\n如果在使用過程中，有需要執行更改設定，或是 hot reload config，在 kubernetes 上要如何做呢? 我們下回分解。\n","date":1570493530,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d98ded7b05e4f7415ea2405ff8a0e2c5","permalink":"https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-controller/","publishdate":"2019-10-08T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-controller/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊該了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","nginx","ingress"],"title":"Kubernetes Nginx Ingress Controller","type":"post"},{"authors":[],"categories":["kubernetes","nginx"],"content":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Nginx Ingress Controller 運作原理 設定 Nginx Ingress Controller 運作原理 昨天講完 nginx ingress controller 部屬，今天來談談 controller 是如何運作的。\nNginx 使用 config file (nginx.conf) 做全域設定，為了讓 nginx 能隨 config file 更新，controller 要偵測 config file 變更，並且 reload nginx 針對 upstream (後端 app 的 endpoint) 變更，使用 lua-nginx-module 來更新。因為 kubernetes 上，service 後的服務常常會動態的變更，scaling，但 endpint ip list 又需要更新到 nginx，所以使用 lua 額外處理 在 kubernetes 上要如何做到上述兩件事呢?\n一般 controller 都使用同步 loop 來檢查 current state 是否與 desired state desired state 使用 k8s object 描述，例如 ingress, services, configmap 等等 object Nginx ingress controller 這邊使用的是 client-go 中的 Kubernetes Informer 的 SharedInformer，可以根據 object 的更新執行 callback 由於無法檢查每一次的 object 更動，是否對 config 產生影響，這邊直接每次更動都產生全新的 model 如果新產生的 model 與現有相同，就跳過 reload 如果 model 只影響 endpoint，使用 nginx 內部的 lua handler 產生新的 endpoint list，來避免因為 upstream 服務變更造成的頻繁 reload 如果新 Model 影響不只 endpoint，則取代現有 model，然後觸發 reload 具體會觸發 reload 的事件，請見官方文件\n除了監測 objects，build model，觸發 reload，之前 controller 還會將 ingress 送到 kubernetes validating admission webhook server 做驗證，避免描述 desired state 的 ingress 有 syntax error，導致整個 controller 爆炸。\nConfiguration 要透過 controller 更改 nginx 設定，有以下三種方式\n更改 configmap，對全域的 controller 設定 更改 ingress 上的 annotation，這些 annotation 針對獨立 ingress 生效 有更深入的客製化，是上述兩者達不到或尚未實作，可以使用 Custom Template 來做到，把 nginx.tmpl mount 進 controller Configmap 由於把全域設定放到 configmap 上，nginx ingress controller 非常好調度與擴展，controller 官方說明文件 除了列出目前已經支援的設定外，也直接附上 nginx 官方的文件說明連結，讓使用者查詢時方便比對。\n當需要更改需求，可以 google nginx 的關鍵字，找到 nginx 上設定的功能選項後，來 controller 的文件，找看看目前是否已經支援。有時候有需要對照 nginx 官方文件，來正確設定 controller。\nAnnotation 有很多 Nginx 的設定是根據 ingress 不同而有調整，例如針對這個 ingress 做白名單，設定 session，設定 ssl 等等，這些針對特定 ingress 所做的設定，可以直接寫在 ingress annotation 裡面。\n例如下面這個 Ingress\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-nginx annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \u0026#39;true\u0026#39; certmanager.k8s.io/cluster-issuer: letsencrypt-prod kubernetes.io/ingress.allow-http: \u0026#34;true\u0026#34; ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/whitelist-source-range: \u0026#34;34.35.36.37\u0026#34; nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;20m\u0026#34; ingress.kubernetes.io/proxy-body-size: \u0026#34;20m\u0026#34; # https://github.com/Shopify/ingress/blob/master/docs/user-guide/nginx-configuration/annotations.md#custom-nginx-upstream-hashing nginx.ingress.kubernetes.io/load-balance: \u0026#34;ip_hash\u0026#34; # https://kubernetes.github.io/ingress-nginx/examples/affinity/cookie/ nginx.org/server-snippets: gzip on; nginx.ingress.kubernetes.io/affinity: \u0026#34;cookie\u0026#34; nginx.ingress.kubernetes.io/session-cookie-name: \u0026#34;route\u0026#34; nginx.ingress.kubernetes.io/session-cookie-hash: \u0026#34;sha1\u0026#34; nginx.ingress.kubernetes.io/session-cookie-expires: \u0026#34;3600\u0026#34; nginx.ingress.kubernetes.io/session-cookie-max-age: \u0026#34;3600\u0026#34; nginx.ingress.kubernetes.io whitelist-source-range: 只允許白名單 ip load-balance: “ip_hash”: 更改預設 round_robin 的 load balance，為了做 session cookie affinity: “cookie”: 設定 upstream 的 session affinity session-cookie-name: “route” session-cookie-hash: “sha1” session-cookie-expires: “3600” session-cookie-max-age: “3600” 如果後端 server 有 session 需求，希望相同 source ip 來的 request 能持續到相同的 endpoint。才做了以上設定。\nhelm configuration helm 的 configuration 也是重要的設定，這裡在安裝時決定了 nginx ingress controller 的 topology、replicas、resource、k8s runtime 設定如 healthz \u0026amp; readiness、其實都會影響 nginx 具體的設定。這部分就會有很多考量。有機會我們再來分享。\n","date":1570493530,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"aa53a03752ed3f5fcae7f89c9ecffbf5","permalink":"https://chechia.net/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-config/","publishdate":"2019-10-08T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-08-kubernetes-nginx-ingress-config/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n這邊改了一些大綱，原本的內容還有一些 kubernetes 的設定，以及 GCP 相關服務的介紹。但既然我們的主題是把東西搬上 k8s 的踩雷旅程，那我們就繼續搬，繼續踩。剩下的時間大概會有四個題目。\nNginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026 Operator-sdk (3) Introduction about custom resource Deployment \u0026 Usage Deployment \u0026 Usage 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","nginx","ingress"],"title":"Kubernetes Nginx Ingress Controller Config","type":"post"},{"authors":[],"categories":["kubernetes","prometheus"],"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n如果要透過 prometheus 來監控集群的運行狀況，有兩個 exporter 是必裝的，一個是把 node 狀態 export 出來的 node exporter，一個是把 kubernetes 集群狀態 export 出來的 kube state metrics exporter。\nNode Exporter 簡介 kube metrics exporter 安裝與設定 Node Exporter Node Exporter 是 prometheus 官方維護的一個子項目，主要在把類 unix 硬體 kernel 的 metrics 送出來。官方也支援 windows node 與 nvidia gpu metrics，可以說是功能強大。\n為了能夠監測 kubernetes node 的基礎設施狀態，通常都會使用 node exporter。\nnode exporter 安裝，我們在安裝 prometheus helm chart 時就一並安裝了。這邊看一下設定與運行。\nCollectors Node exporter 把不同位置收集到的不同類型的 metrics ，做成各自獨立的 colletor，使用者可以根據求需求來啟用或是不啟用 collector，完整的 collector 目錄 在這邊。\n如果有看我們第一部份的 ELK part，應該會覺得這裡的設定，跟 metricbeat 非常像，基本上這兩者做的事情是大同小異的，收集 metrics 來源都是同樣的類 unix 系統，只是往後送的目標不一樣 (雖然現在兩者都可以兼容混搭了)。如果有接觸過其他平台的 metrics collector，也會發現其實大家做的都差不多。\nTextfile Collector Prometheus 除了有 scrape 機制，讓 prometheus 去 exporter 撈資料外，還有另外一個機制，叫做 Pushgateway，這個我們在部屬 prometheus 時也部屬了一個。這邊簡單說明一下。\n經常性執行的服務(redis, kafka,…)會一直運行，prometheus 透過這些服務的 metrics 取得 runtime metrics，作為監控資料。可是有一些 job 是暫時性的任務，例如果一個 batch job，這些服務不會有一直運行的 runtime metrics，也不會有 exporter。但這時又希望監控這些 job 的狀態，就可以使用 Pushgateway。\nPushgateway 的作用機制，就是指定收集的目標資料夾，需要監測的 batch job，只要把希望監測的資料，寫到該資料夾。Pushgateway 會依據寫入的資料，轉成 time series metrics，並且 export 出來。\n這種去 tail 指定目錄檔案，然後把 metrics 後送的機制，是否跟 filebeat 有一點類似? 只是 filebeat 一般取得資料後，會主動推送到 ELK 上，prometheus pushgateway 會暴露出 metrics 後，讓 prometheus server 來 scrape。\nPushgateway 也會在收集資料時打上需要的 label，方面後段處理資料。\nKubernetes State Metrics (Exporter) Node Exporter 將 kubernetes 集群底下的 Node 的硬體狀態，例如 cpu, memory, storage,… expose 出來，然而我們在維運 kubernetes 還需要從 api server 獲得集群內部的資料，例如說 pod state, container state, endpoints, service, …等，這邊可以使用 kube-state-metrics 來處理。\nkube-state-metrics 是 kubernetes 官方維護的專案，做的事情就是向 api server 詢問 kubernetes 的 state，例如 pod state, deployment state，然後跟 prometheus exporter 一，開放一個 http endpoint，讓需要的服務來 scrape metrics。\n工作雲裡也很單純，kubernetes api server 可以查詢 pod 當下的狀態，kube-state-metrics 則會把當下的狀態依照時間序，做成 time series 的 metrics，例如這個 pod 什麼時候是活著，什麼時候因為故障而 error。\nkube-state-metrics 預設的輸出格式是 plaintext，直接符合 Prometheus client endpoint 的格式\nDeployment 如果依照第一篇安裝 prometheus helm 的步驟，現在應該已經安裝完 kube-state-metrics 了。如果沒有安裝，也可以依照官方說明的基本範例安裝。\ngit clone git@github.com:kubernetes/kube-state-metrics.git cd kube-state-metrics kubectl apply -f examples/standard/*.yaml 安裝完可以看到\n$ kubectl get pods --selector \u0026#39;app=prometheus,component=kube-state-metrics\u0026#39; NAME READY STATUS RESTARTS AGE prometheus-kube-state-metrics-85f6d75f8b-7vlkp 1/1 Running 0 201d $ kubectl get svc --selector \u0026#39;app=prometheus,component=kube-state-metrics\u0026#39; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 201d 我們可以透過 service 打到 pod 的 /metrics 來取得 metrics。\nkubectl exec -it busybox sh curl prometheus-kube-state-metrics:8080 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Kube Metrics Server\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Kube Metrics\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#39;/metrics\u0026#39;\u0026gt;metrics\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#39;/healthz\u0026#39;\u0026gt;healthz\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; curl prometheus-kube-state-metrics:8081 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Kube-State-Metrics Metrics Server\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Kube-State-Metrics Metrics\u0026lt;/h1\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#39;/metrics\u0026#39;\u0026gt;metrics\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 這邊有兩套 metrics，一個是 kube-state-metrics 自己自我監測的 metrics，在 8081，另外一個才是 kube metrics，在 8080，兩個都要收，記得不要收錯了。\n$ curl prometheus-kube-state-metrics:8080/metrics 打下去就可以看到超多 metrics 。 Metrics 的清單與說明文件，有用到的 metrics 使用前都可以來查一下定義解釋。\n理論上不用每個 metrics 都 expose 出來，有需要可以把不會用到的 metrics 關一關，可以節省 kube-state-metrics 的 cpu 消耗。\nResource Recommendation kube-state-metrics 很貼心的還附上建議的資源分配\nAs a general rule, you should allocate 200MiB memory 0.1 cores For clusters of more than 100 nodes, allocate at least 2MiB memory per node 0.001 cores per node Scaling kube-state-metrics 還有提供 horizontal scaling 的解決方案，如果你的集群很大，node 數量已經讓 kube-state-metrics 無法負荷，也可以使用 sharding 的機制，把 metrics 的工作散布到多個 kube-state-metrics，再讓 prometheus 去收集統整。這部分我覺得很有趣，但還沒實作過，我把文件 放在這邊，有緣大德有時做過請來討論分享。\nDashboard metrics 抓出來，當然要開一下 dashboard，這邊使用的是這個kubernetes cluster，支援\nnode exporter kube state metrics nginx ingress controller 三個願望一次滿足~\n小結 跑 kubernetes 務必使用這兩個 exporter kube-state-metrics 整理得很舒服，有時間可以多看看這個專案 ","date":1570407130,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d8e25e9e10d1154a1f430d9d6d39f376","permalink":"https://chechia.net/zh-hant/post/2019-10-07-prometheus-kube-state-metrics-exporter/","publishdate":"2019-10-07T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-07-prometheus-kube-state-metrics-exporter/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026 exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","prometheus","ithome"],"title":"Prometheus \u0026 Kubernetes State Metrics Exporter","type":"post"},{"authors":[],"categories":["kubernetes","prometheus"],"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Exporter 工作原理簡介 Prometheus exporter library Exporter workflow 上次講到 exporter 可以從服務端把運行資料抽出來，並開成 http endpoint，讓 prometheus 來 scrape metrics。那 exporter 本身是如何取得服務內部的 metrics 呢? 我們今天就稍微看一下。\nRedis Exporter 我們今天以 Redis Exporter 為例，研究一下外部的 exporter 是如何取得 redis 內部的 metrcs。\nRedis exporter 是用 golang 寫的一個小程式，總共算算才 1000 行，而且很多都是對 redis 內部 metrics 的清單，以及轉化成 prometheus metrics 的 tool functions，主要的邏輯非常簡單。我們簡單看一下源碼。\nCollect 是主要的收集邏輯，就是執行 scrapeRedisHost(ch) ，然後把收集到的資訊，使用 Prometheus Go Client Library 的工具將資料註冊成 prometheus metrics\nfunc (e *Exporter) Collect(ch chan\u0026lt;- prometheus.Metric) { e.Lock() defer e.Unlock() e.totalScrapes.Inc() if e.redisAddr != \u0026#34;\u0026#34; { start := time.Now().UnixNano() var up float64 = 1 // 從 host scrape 資料，然後塞進 channel streaming 出來。 if err := e.scrapeRedisHost(ch); err != nil { up = 0 e.registerConstMetricGauge(ch, \u0026#34;exporter_last_scrape_error\u0026#34;, 1.0, fmt.Sprintf(\u0026#34;%s\u0026#34;, err)) } else { e.registerConstMetricGauge(ch, \u0026#34;exporter_last_scrape_error\u0026#34;, 0, \u0026#34;\u0026#34;) } e.registerConstMetricGauge(ch, \u0026#34;up\u0026#34;, up) e.registerConstMetricGauge(ch, \u0026#34;exporter_last_scrape_duration_seconds\u0026#34;, float64(time.Now().UnixNano()-start)/1000000000) } ch \u0026lt;- e.totalScrapes ch \u0026lt;- e.scrapeDuration ch \u0026lt;- e.targetScrapeRequestErrors } scrapeRedisHost 內部的主要邏輯，又集中在執行 Info\n// 執行 info infoAll, err := redis.String(doRedisCmd(c, \u0026#34;INFO\u0026#34;, \u0026#34;ALL\u0026#34;)) if err != nil { infoAll, err = redis.String(doRedisCmd(c, \u0026#34;INFO\u0026#34;)) if err != nil { log.Errorf(\u0026#34;Redis INFO err: %s\u0026#34;, err) return err } } 也就是說當我們在 redis-cli 連入 redis 時，可以執行 Info command，取得 redis 內部的資訊，包含節點設店與狀態，集群設定，資料的統計數據等等。然後 exporter 這邊維護持續去向 redis 更新 info ，並且把 info data 轉化成 time series 的 metrcs，再透過 Prometheus Client promhttp 提供的 http endpoint library，變成 http endpoint。\n首先看一下 redis info command 的文件，這邊有說明 info 的 option ，以及 option 各自提供的資料，包括 server 狀態，賀戶端連線狀況，系統資源，複本狀態等等。我們也可以自己透過 info 取得資料。\n$ kubectl get po | grep redis redis-2-redis-ha-server-0 3/3 Running 0 11d redis-2-redis-ha-server-1 3/3 Running 0 11d redis-2-redis-ha-server-2 3/3 Running 0 11d $ kubectl exec -it redis-2-redis-ha-server-0 sh $ redis-cli -h haproxy-service -a REDIS_PASSWORD $ haproxy-service:6379\u0026gt; $ haproxy-service:6379\u0026gt; info server # Server redis_version:5.0.5 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:4d072dc1c62d5672 redis_mode:standalone os:Linux 4.14.127+ x86_64 arch_bits:64 multiplexing_api:epoll atomicvar_api:atomic-builtin gcc_version:8.3.0 process_id:1 run_id:63a97460b7c3745577931dad406df9609c4e2464 tcp_port:6379 uptime_in_seconds:976082 uptime_in_days:11 ... $ haproxy-service:6379\u0026gt; info clients # Clients connected_clients:100 client_recent_max_input_buffer:2 client_recent_max_output_buffer:0 blocked_clients:1 Redis exporter 收集這些數據，透過 prometheus client library 把資料轉成 time series prometheus metrics。然後透過 library 放在 http enpoint 上。\n配合上次說過的 redis overview dashboard，可以直接在 Grafana 上使用\n這邊 dashboard 顯示幾個重要的 metrics\nUptime Memory Usage，要設定用量太高自動報警 Command 的執行狀況，回應時間 訊息的流量，以及超出 time-to-live 的資料清除。 都是需要好好加上 alert 的核心 metrics\n貢獻 exporter 其他服務的 exporter 工作原理也相似，如果服務本身有內部的 metrics，可以透過 client command 或是 API 取得，exporter 的工作就只是轉成 time series data。\n如果有比較特殊的 metrics 沒有匯出，例如說自家的 metrics ，但又希望能放到 prometheus 上監測，例如每秒收到多少 request count，回應速度，錯誤訊息的統計……等，這點也可以使用 client library 自幹 exporter 然後 expose http endpoint，這樣在 prometheus 上也可以看到自家產品的 metrics，非常好用。有機會我們來聊自幹 exporter。\n","date":1570320730,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"c7e19dd63fb38046554d73d7fedb225a","permalink":"https://chechia.net/zh-hant/post/2019-10-06-prometheus-exporter-library-redis-exporter/","publishdate":"2019-10-06T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-06-prometheus-exporter-library-redis-exporter/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026 exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","prometheus","ithome"],"title":"Prometheus Exporter Library \u0026 Redis Exporter","type":"post"},{"authors":[],"categories":["kubernetes","prometheus"],"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Prometheus Introduction Deploy Prometheus Prometheus Introduction 生產環境與非生產環境，其中的一指標就是有沒有足夠完整的服務監測系統，這句話可以看出服務監測對於產品化是多麼重要。而監控資料 (metrics) 的收集與可視化工具其實非常多，例如上周介紹的 ELK Stack，這次我們要來介紹另外一個很多人使用的 prometheus。\nPromethues 在官網上提到 是一個 Monitoring system and time series database\n可以收集高維度的資料 使用自己的 PromQL 做有效且精簡的資料查詢 內建資料瀏覽器，並且與 Grafana 高度整合 支援 sharding 與 federation，來達到水平擴展 有許多隨插即用的整合 exporter，例如 redis-exporter, kafka-exporter，kubernetes-exporter ，都可以直接取得資料 支援 alert，使用 PromQL 以及多功能的告警，可以設定精準的告警條件 與 ELK 做比較 基本上 Prometheus 跟 ELK 比，其實是很奇怪的一件事，但這也是最常被問的一個問題。兩者在本質上是完全不同的系統。\nPrometheus 是 based on time series database 的資料收集系統 ELK 是基於全文搜索引擎的資料查詢系統 是的，他們都能做 metrics 收集，在有限的尺度下，能達到一樣的效果。但這樣說的意思就等於是在說 mesos DC/OS 與 kubenetes 都能跑 container cluster 一樣，底下是完全不一樣的東西。\n兩者的差異使用上差非常多\nmetrics 結構: ELK 借助全文搜索引擎，基本上送什麼資料近來都可以查找。Prometheus metrics 拉進來是 time series 的 key-value pairs。 維護同樣的 metrics，prometheus 的使用的儲存空間遠小於 elasticsearch prometheus 針對 time based 的搜尋做了很多優化，效能很高 Prometheus 對於記憶體與 cpu 的消耗也少很多 Elasticsearch 資源上很貴，是因為在處理大量 text log 的時候，他能夠用後段的 pipeline 處理內容，再進行交叉比對，可以從 text 裡面提取很多未事先定義的資料 Elasticsearch 的維護工作也比較複雜困難 如果要收集服務運行資料，可以直接選 prometheus。如果有收集 log 進行交叉比對，可以考慮 elk。\nHelm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 prometheus。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nDeploy Prometheus 我把我的寶藏都放在這了https://github.com/chechiachang/prometheus-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash HELM_NAME=prometheus-1 helm upgrade --install ${HELM_NAME} stable/prometheus \\ --namespace default \\ --values values-staging.yaml Configuration Prometheus Stable Chart\nvalues.yaml 很長，但其實各個元件設定是重複的,設定好各自的 image, replicas, service, topology 等等\nalertmanager: enabled: true kubeStateMetrics: enabled: true nodeExporter: enabled: true server: enabled: true pushgateway: enabled: true 底下有更多 runtime 的設定檔\n定義好 global 的 scrape 間距，越短 metrics 維度就越精準 PersistenVolume 強謝建議開起來，維持歷史的資料 加上 storage usage 的 self monitoring（之後會講) 才不會滿出來 server 掛掉 server 的 scrapeConfigs 是 server 去收集的 job 設定。稍後再來細講。 server: global: ## How frequently to scrape targets by default ## scrape_interval: 10s ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 10s persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026#34;\u0026#34; ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 80Gi alertmanagerFiles: serverFiles: 部屬完看一下\nkubectl get pods --selector=\u0026#39;app=prometheus\u0026#39; NAME READY STATUS RESTARTS AGE prometheus-alertmanager-694d6694c6-dvkwd 2/2 Running 0 8d prometheus-kube-state-metrics-85f6d75f8b-7vlkp 1/1 Running 0 8d prometheus-node-exporter-2mpjc 1/1 Running 0 8d prometheus-node-exporter-kg7fj 1/1 Running 0 51d prometheus-node-exporter-snnn5 1/1 Running 0 8d prometheus-pushgateway-5cdfb4979c-dnmjn 1/1 Running 0 8d prometheus-server-59b8b8ccb4-bplkx 2/2 Running 0 8d kubectl get services --selector=\u0026#39;app=prometheus\u0026#39; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE prometheus-alertmanager ClusterIP 10.15.241.66 \u0026lt;none\u0026gt; 80/TCP 197d prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 197d prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 197d prometheus-pushgateway ClusterIP 10.15.254.0 \u0026lt;none\u0026gt; 9091/TCP 197d prometheus-server ClusterIP 10.15.245.10 \u0026lt;none\u0026gt; 80/TCP 197d kubectl get endpoints --selector=\u0026#39;app=prometheus\u0026#39; NAME ENDPOINTS AGE prometheus-alertmanager 10.12.6.220:9093 197d prometheus-kube-state-metrics 10.12.6.222:8080 197d prometheus-node-exporter 10.140.0.30:9100,10.140.0.9:9100,10.140.15.212:9100 197d prometheus-pushgateway 10.12.6.211:9091 197d prometheus-server 10.12.3.14:9090 197d 簡單說明一下\nprometheus-server 是主要的 api-server 以及 time series database alertmanager 負責告警工作 pushgateway 提供 client 端主動推送 metrics 給 server 的 endpoint kube-state-metrics 是開來收集 cluster wide 的 metrics, 像是 pods running counts, deployment ready count, total pods number 等等 metrics node-exporter 是 daemonsets, 把每一個 node 的 metrics, 像是 memory, cpu, disk…等資料,收集出來 主要服務存取就是透過 prometheus-server\nAccess Prometheus server …","date":1570176730,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"6a855918d449bd83b2812d9ad1b3bb83","permalink":"https://chechia.net/zh-hant/post/2019-10-04-prometheus-deployment-on-kubernetes/","publishdate":"2019-10-04T16:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-04-prometheus-deployment-on-kubernetes/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026 exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","prometheus","ithome"],"title":"Prometheus Deployment on Kubernetes","type":"post"},{"authors":[],"categories":["kubernetes","prometheus"],"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Grafana Introduction Deploy Grafana Grafana Introduction 上偏我們簡單介紹了 Prometheus，prometheus 的 Web Portol 已經附上簡單的 Query 與 Graph 工具，但一般我們在使用時，還是會搭配 Grafana 來使用。\nGrafana 在官網上提到 是一個 Analytics system，可以協助了解運行資料，建立完整的 dashboard。\n支援許多圖表，直線圖，長條圖，區域分析，基本上需要的都有 在圖表上定義 alter，並且主動告警，整合其他通訊軟體 對後端 data source 的整合，可以同時使用 ELK, prometheus, influxdb 等 30 多種的資料來源 有許多公開的 plugin 與 dashboard 可以匯入使用 總之功能強大，至於用起來的感覺，個人是非常推薦。如果有大得想要試玩看看，可以直接到 Grafana Live Demo 上面試玩\n一般使用都會圍繞 dashboard 為核心，透過單一畫面，一覽目前使用者需要讀取的資料 左上角的下拉選單，可以選擇不同的 dashboards 與 Kibana 做比較 雖然大部分使用上，我們都會使用 ELK 一套，而 Prometheus + Grafana 另一套。但其實兩邊的 data source 都可以互接。例如 grafana 可以吃 elasticsearch 的 data source，而 kibana 有 prometheus module。\n我們這邊基於兩款前端分析工具，稍微做個比較，底層的 data source 差異這邊先不提。\n都是開源: 兩者的開源社群都非常強大 兩者內建的 dashboard 都非常完整，而且不斷推出新功能 Log vs Metrics: Kibana 的 metrics 也是像 log 一樣的 key value pairs，能夠 explore 未定義的 log Grafana 的 UI 專注於呈現 time series 的 metrics，並沒有提供 data 的欄位搜尋，而是使用語法 Query 來取得數據 Data source: Grafana 可以收集各種不同的後端資料來源 ELK 主要核心還是 ELK stack，用其他 Module 輔助其他資料源 Deploy Grafana 我把我的寶藏都放在這了https://github.com/chechiachang/prometheus-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncd grafana cat install.sh #!/bin/bash HELM_NAME=grafana-1 helm upgrade --install grafana stable/grafana \\ --namespace default \\ --values values-staging.yaml Helm 我們這邊用 helm 部屬，Grafana Stable Chart\nConfiguration 簡單看一下設定檔\nvim values-staging.yaml replicas: 1 deploymentStrategy: RollingUpdate Grafana 是支援 Grafana HA ，其實也非常簡單，就是把 grafana 本身的 dashboard database 從每個 grafana 一台 SQLite，變成外部統一的 MySQL，統一讀取後端資料，前端就可水平擴展。\nreadinessProbe: httpGet: path: /api/health port: 3000 livenessProbe: httpGet: path: /api/health port: 3000 initialDelaySeconds: 60 timeoutSeconds: 30 failureThreshold: 10 image: repository: grafana/grafana tag: 6.0.0 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName 一些 Pod 的基本配置， health check 使用內建的 api，有需要也可以直接打 api\nsecurityContext: runAsUser: 472 fsGroup: 472 extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /etc/grafana/ssl/ # configMap: certs-configmap # readOnly: true 有要開外部 ingress，需要 ssl 的話可以從這邊掛進去\n## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: type: LoadBalancer port: 80 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer annotations: {} labels: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026#34;true\u0026#34; labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local 這邊可以開 service load balancer, 以及 ingress，看實際使用的需求\npersistence: enabled: true initChownData: true # storageClassName: default accessModes: - ReadWriteOnce size: 10Gi # annotations: {} # subPath: \u0026#34;\u0026#34; # existingClaim: Persistent Volume 作為本地儲存建議都開起來，\n# Administrator credentials when not using an existing secret (see below) adminUser: admin # adminPassword: strongpassword # Use an existing secret for the admin user. admin: existingSecret: \u0026#34;\u0026#34; userKey: admin-user passwordKey: admin-password 帳號密碼建議使用 secret 掛進去\ndatasources: {} # datasources.yaml: # apiVersion: 1 # datasources: # - name: Prometheus # type: prometheus # url: http://prometheus-prometheus-server # access: proxy # isDefault: true ## Configure grafana dashboard providers ## ref: http://docs.grafana.org/administration/provisioning/#dashboards ## ## `path` must be /var/lib/grafana/dashboards/\u0026lt;provider_name\u0026gt; ## dashboardProviders: {} # dashboardproviders.yaml: # apiVersion: 1 # providers: # - name: \u0026#39;default\u0026#39; # orgId: 1 # folder: \u0026#39;\u0026#39; # type: file # disableDeletion: false # editable: true # options: # path: /var/lib/grafana/dashboards/default ## Configure grafana dashboard to import ## NOTE: To use dashboards you must also enable/configure dashboardProviders ## ref: https://grafana.com/dashboards ## ## dashboards per provider, use provider name as key. ## dashboards: {} # default: # some-dashboard: # json: | # $RAW_JSON # custom-dashboard: # file: dashboards/custom-dashboard.json # prometheus-stats: # gnetId: 2 # revision: 2 # datasource: Prometheus # local-dashboard: # url: https://example.com/repository/test.json # local-dashboard-base64: # url: https://example.com/repository/test-b64.json # b64content: true Data source, Dashboard 想要直接載入，可以在這邊設定，或是 grafana 起來後，透過 Web UI 進去新 …","date":1570147930,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"893ce8522120c565ca2b72e3e5c01d89","permalink":"https://chechia.net/zh-hant/post/2019-10-04-prometheus-deploy-grafana/","publishdate":"2019-10-04T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-04-prometheus-deploy-grafana/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026 exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","prometheus","ithome"],"title":"Prometheus Deploy Grafana","type":"post"},{"authors":[],"categories":["kubernetes","prometheus"],"content":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Prometheus scrape scrape_configs Node exporter Scrape Prometheus 收集 metrics 的方式，是從被監測的目標的 http endpoints 收集 (scrape) metrics，目標服務有提供 export metrics 的 endpoint 的話，稱作 exporter。例如 kafka-exporter 就會收集 kafka 運行的 metrics，變成 http endpoint instance，prometheus 從 instance 上面收集資料。\nPromethesu 自己也是也提供 metrics endpoint，並且自己透過 scrape 自己的 metrics endpoint 來取得 self-monitoring 的 metrics。把自己當作外部服務監測。下面的設定就是直接透過 http://localhost:9090/metrics 取得。\nglobal: scrape_interval: 15s # By default, scrape targets every 15 seconds. # Attach these labels to any time series or alerts when communicating with # external systems (federation, remote storage, Alertmanager). external_labels: monitor: \u0026#39;codelab-monitor\u0026#39; # A scrape configuration containing exactly one endpoint to scrape: # Here it\u0026#39;s Prometheus itself. scrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: \u0026#39;prometheus\u0026#39; # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] 透過 Grafana -\u0026gt; explore 就可以看到 Prometheus 的 metrics\n而使用 metrics 時最好先查到說明文件，確定 metrics 的定義與計算方法，才可以有效的製圖。關於 Prometheus Exporter 的 metrics 說明 可以到這裡來找。\nDashboard 收集到 metrics 之後就可以在 prometheus 中 query，但一般使用不會一直跑進來下 query，而是會直接搭配 dashboard 製圖呈現，讓資料一覽無遺。\n例如 prometheus 自身的 metrics 也已經有搭配好的 Prometheus overview dashboard 可以使用。\n使用方法非常簡單，直接透過 Grafana import dashboard，裡面就把重要的 prometheus metrics 都放在 dashboard 上了。不能更方便了。\nExporters Prometheus 支援超級多 exporter，包含 prometheus 自身直接維護的 exporter，還有非常多外部服務友也開源的 exporter 可以使用，清單可以到這裡看\n有希望自己公司的服務，也使用 prometheus\nNode Exporter prometheus/node_exporter 是 Prometheus 直接維護的 project，主要用途就是將 node / vm 的運行 metrics export 出來。有點類似 ELK 的 metricbeat。\n我們這邊是在 kubernetes 上執行，所以直接做成 daemonsets 在 k8s 上跑，部屬方面在 deploy prometheus-server 的 helm chart 中，就已經附帶整合，部屬到每一台 node 上。\n如果是在 kubernetes 外的環境，例如說 on premise server，或是 gcp instance，希望自己部屬 node exporter 的話，可以參考這篇教學文章。\n我們這邊可以看一下 config，以及 job 定義。\nvim values-staging.yaml # Enable nodeExporter nodeExporter: create: true prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: # Add kubernetes node job - job_name: \u0026#39;kubernetes-nodes\u0026#39; # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$1/proxy/metrics kubernetes_sd_config: 可以透過 kubernetes API 來取得 scrape target，以這邊的設定，是使用 node role 去集群取得 node，並且每一台 node 都當成一個 target，這樣就不用把所有 node 都手動加到 job 的 instance list 裡面。\n從 node role 取得的 instance 會使用 ip 標註或是 hostname 標註。node role 有提供 node 範圍的 meta labels，例如 __meta_kubernetes_node_name, _meta_kubernetes_node_address 等等，方便查找整理資料。\nrelabel_configs: 針對資料做額外標記，方便之後在 grafana 上面依據需求 query。\n","date":1570147930,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"09c8d9480383bba47799390a35fc2665","permalink":"https://chechia.net/zh-hant/post/2019-10-04-prometheus-scrape/","publishdate":"2019-10-04T08:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-04-prometheus-scrape/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nPrometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026 exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","prometheus","ithome"],"title":"Prometheus Scrape","type":"post"},{"authors":[],"categories":["kubernetes","redis"],"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Failure Case Recovery Topology 上篇的例子完成應該是這樣\n+-------+ +--------+ +------------+ +---------+ |Clients|---|HAProxys|----|redis master|----|sentinels| +-------+ +--------+ +------------+ +---------+ HAproxy 作為後端 redis 的 gateway Client 透過 HAproxy 連入 redis master sentinel 負責監測 redis 狀態與 failover，只是 client 不再透過 sentinel 去取得 master，而是透過 HAProxy。 那現在就來聊聊這些服務可能怎麼死的，回復的機制又是如何\nFailure Recovery Redis master 故障 這個是目前我們這個 Redis HAProxy 配置主要想解決的問題，故障與回覆的流程大概是這樣\nRedis Master failing Sentinels detect master failure sentinel 等待 down-after-milliseconds，超過才判定 master failure sentinel 彼此取得 quorum，授權其中一台 sentinel 執行 failover sentinel 指派新的 master master 故障同時，HAProxy 也偵測 master failure HAProxy 發現沒有可用的 master tcp checklist 再次執行時，由於新的 master 尚未選出來，仍會顯示三台 server 都離線 直到 master 選出，role:master 的 tcp check 有回應後，才會將後端接到新的 master Client 由於 HAProxy 沒有可用的 master，所以連線斷掉 持續中斷到 HAProxy 回復 這邊的幾個重要的參數\nsentinel\ndown-after-milliseconds: 斷線多久才會覺得 master 死了需要 failover，可以盡量縮短，加速 failure 發生 failover 的時間 haproxy.cfg\nserver check inter 1s: 多久跑一次 tcp-check 越短，便能越早接受到 redis instance failure 的發生 從這個例子來看，這個配置的 HA 其實還是有離線時間\ndown-after-milliseconds 設定為 2s ，那從 failure 發生，到 sentinel 開始 failover 的時間就會超過 2s，這兩秒客戶端無法寫入。 事實上，這也是 redis master-slave 的模式的問題，並無法確保 zero downtime 能做到的是秒級的 auto-recovery Sentinel Failure 這個是很好解決的錯誤，如同我們在 topology 這篇提到的，原則上只要能維持 quorum 以上的 sentinel 正常運作，就可以容忍多個 sentinel 的錯誤\n例如 5 sentinel，quorum 3，就可以允許兩個 sentinel 錯誤 服務都正常 zero downtime 等待錯誤的 sentinels 復原 錯誤不一定是兩個 sentinel 死了，可能是網路斷開，把 3 sentinels 與 2 sentinels 隔開，無法溝通。 這時也不用會有複數 failover 產生，因為 quorum 只有 3 sentinels 的這端可以取得授權，正常執行 failover 2 sentinels 的這邊只會靜待網路回復。 HAProxy Failure 這個在 kubernetes 上也是很好解決\nHAProxy 不用知道彼此，只要能夠監測後端服務，並且 proxy request 即可\n我們啟動 HAProxy 時會一次啟動多個 HAProxy\nHAProxy 是無狀態的服務，可以直接水平擴展 (Horizontal Scale) 算是成本的地方，就是 HAProxy instance 會各自對後端 redis 做 tcp-check，頻繁的 check，還是會有成本，但相較於 client request 應該是比較輕 HAProxy 是高效能，而且只做 proxy，一奔來說只要維持有多餘的副本備用即可，不用開太多 Kubernetes 會自動透過 stats port，對 HAPRoxy 做 liveness check，check 失敗就不會把流量導近來\nHAproxy 前端的 kubernetes service 會自動 load balance client 到正常運作的 HAProxy 上\n例如起了 3 HAProxy\n3 HAProxy 都各自向 redis instance 做 tcp-check，每秒 3 * 3 組 check\n客戶端連入任一 HAProxy，都可以連入正確的 master\nHAProxy 只要至少有一個活著就可以，也就是可以死 2 個\nKubernetes service 會自動導向活著的 HAProxy\n2 HAPRoxy 回復的時候，就是 HAProxy 重啟後重新開始服務\n拆分 read write client 由於效能瓶頸還是在 redis master，為了能支撐夠多 client，最好把 client 需要讀寫的拆分開來\nHAProxy 的設定，就會需要\n把 frontend redis_gate 拆成 redis_slaves_gate: 接收讀取的 client redis_master_gate: 接收寫入的 client 把 redis_servers 拆成 redis_slaves: 更改 tcp-check 去找 role:slave 的 redis，應該有兩台 redis_master: 維持找尋 role:master 的 redis 這樣可以輕易地透過 scale slave 來擴大讀取的流量帶寬\nRedis Cluster Intro 的時候有提到，redis cluster 是另一個面向的 redis solution。\n使用 redis cluster 將資料做 sharding，分散到不同群組內，partitions 由複數的 master 來存取\n這部份我們下回待續\n","date":1570090330,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d1cd8f8440fcc4c1aa9d65d519bcfca8","permalink":"https://chechia.net/zh-hant/post/2019-10-03-redis-ha-failure-recovery/","publishdate":"2019-10-03T16:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-03-redis-ha-failure-recovery/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","redis","ithome"],"title":"Redis Ha Failure Recovery","type":"post"},{"authors":[],"categories":["kubernetes","redis"],"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 HAProxy Introduction Redis Sentinel with HAProxy HAProxy Intro HAproxy 全名是 High Availability Proxy，是一款開源 TCP/HTTP load balancer，他可以\n聽 tcp socket，連 server，然後把 socket 接在一起讓雙向流通 可做 Http reverse-proxy (Http gateway)，自己作為代理 server，把接受到的 connection 傳到後端的 server。 SSL 終端，可支援 client-side 與 server-side 的 ssl/tls 當 tcp/http normalizer 更改 http 的 request 與 response 當 switch，決定 request 後送的目標 做 load balancer，為後端 server 做負載均衡 調節流量，設定 rate limit，或是根據內容調整流量 HAProxy 還有其他非常多的功能，想了解細節可以來看原理解說文件\nTopology 我們今天的範例是在後端的 redis 與 clients 中間多放一層 HAProxys\n+-------+ +--------+ +------------+ +---------+ |Clients|---|HAProxys|----|redis master|----|sentinels| +-------+ +--------+ +------------+ +---------+ 可能有人會問說，那前兩天講的 redis sentinel，跑去哪裡了。\nsentinel 還在正常運作，負責監測 redis 狀態與 failover，只是 client 不再透過 sentinel 去取得 master，而是透過 HAProxy。\nDeploy HAProxy 我把我的寶藏都在這了https://github.com/chechiachang/haproxy-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash # redis-db-credentials should already exists #kubectl create secret generic redis-db-credentials \\ --from-literal=REDIS_PASSWORD=123456 # Update haproxy.cfg as configmap kubectl create configmap haproxy-config \\ --from-file=haproxy.cfg \\ --output yaml \\ --dry-run | kubectl apply -f - kubectl apply -f deployment.yaml kubectl apply -f service.yaml 這邊做的事情有幾件\n取得 redis 的 auth REDIS_PASSWORD 放在 secret 中，如果前面是照我們的範例，那都已經設定了 把 haproxy.cfg 的設定檔，使用 configmap 的方式放到 kubernetes 上 部屬 HAProxy deployment 部屬 HAProxy service 簡單看一下 deployment\napiVersion: apps/v1beta1 kind: Deployment metadata: name: haproxy spec: replicas: 3 template: metadata: labels: app: haproxy app.kubernetes.io/name: haproxy component: haproxy spec: volumes: - name: haproxy-config configMap: name: haproxy-config affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: \u0026#34;app\u0026#34; operator: \u0026#34;In\u0026#34; values: - \u0026#34;haproxy\u0026#34; topologyKey: kubernetes.io/hostname containers: - name: haproxy image: haproxy:2.0.3-alpine command: [\u0026#34;haproxy\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;/usr/local/etc/haproxy/config/haproxy.cfg\u0026#34;] readinessProbe: initialDelaySeconds: 15 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 2 failureThreshold: 2 tcpSocket: port: 26999 port: 6379 volumeMounts: - name: haproxy-config mountPath: /usr/local/etc/haproxy/config resources: requests: cpu: 10m memory: 30Mi env: - name: REDIS_PASSWORD valueFrom: secretKeyRef: name: redis-db-credentials key: REDIS_PASSWORD ports: - containerPort: 8000 name: http - containerPort: 9000 name: https - containerPort: 26999 name: stats - containerPort: 6379 name: redis Replicas: 3 ，開起來是三個 HAProxy podAntiAffinity，三個分布到不同 node 上，盡量維持 HA readinessProbe，等 tcpSocket 26999 (HAProxy Stats) 與 6370 (Redis Proxy) 通了才 READY 把 redis password 掛進去 把 haproxy.cfg 掛進去 開幾個 port 看一下 service\nkind: Service apiVersion: v1 metadata: name: haproxy-service spec: sessionAffinity: ClientIP sessionAffinityConfig: clientIP: timeoutSeconds: 10800 # 3 hr selector: app: haproxy ports: - name: http protocol: TCP port: 8000 - name: https protocol: TCP port: 9000 - name: stats protocol: TCP port: 26999 - name: redis protocol: TCP port: 6379 - name: redis-exporter protocol: TCP port: 8404 很單純，就是把幾個 port 接出來 把 sessionAffinity 開起來 這邊希望來自相同 clientIP (kubernetes 內部 app clients) 的 session 能持續走同一個 server 可以降低進到 service 往後送到一直重連浪費資源 但一直連著也不好，可能會 connection not closed 一直佔著 HAProxy1 HAProxy2 HAProxy3，上次 Client1 連 HAProxy1，service 也盡量讓你下個 request 也走 HAPRoxy1 kubectl get po | grep haproxy haproxy-56d94f857f-gmd4s 1/1 Running 0 47d haproxy-56d94f857f-p2vj6 1/1 Running 0 47d haproxy-56d94f857f-vhz8b 1/1 Running 0 47d HAProxy Config 看一下 haproxy.cfg\n# https://cbonte.github.io/haproxy-dconv/2.0/configuration.html # https://github.com/prometheus/haproxy_exporter # https://www.haproxy.com/blog/haproxy-exposes-a-prometheus-metrics-endpoint/ # curl http://localhost:8404/metrics # curl http://localhost:8404/stats frontend stats mode http timeout client 30s bind *:8404 option http-use-htx http-request use-service prometheus-exporter if { path /metrics } stats enable stats uri /stats stats refresh 10s # Redis frontend redis_gate mode tcp timeout client 7d bind 0.0.0.0:6379 name redis default_backend redis_servers backend redis_servers mode tcp timeout connect 3s timeout server 7d option tcp-check tcp-check connect tcp-check send AUTH\\ \u0026#34;${REDIS_PASSWORD}\u0026#34;\\r\\n tcp-check send PING\\r\\n tcp-check expect string PONG tcp-check send info\\ replication\\r\\n tcp-check expect string role:master tcp-check send QUIT\\r\\n tcp-check expect string +OK server R1 redis-2-redis-ha-announce-0:6379 check inter 1s server R2 …","date":1570003930,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"f57b941b25bb6869ae3ceddbc24237e0","permalink":"https://chechia.net/zh-hant/post/2019-10-02-redis-ha-on-haproxy/","publishdate":"2019-10-02T16:12:10+08:00","relpermalink":"/zh-hant/post/2019-10-02-redis-ha-on-haproxy/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","redis","ithome","haproxy"],"title":"Redis Ha HAProxy","type":"post"},{"authors":[],"categories":["kubernetes","redis"],"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Redis Sentinel Topology Topology Masters: M1, M2, M3, …, Mn. Slaves: R1, R2, R3, …, Rn (R stands for replica). Sentinels: S1, S2, S3, …, Sn. Clients: C1, C2, C3, …, Cn. 每個方格代表一台機器或是 VM 2 Sentinels DON’T DO THIS\n+----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ Configuration: quorum = 1 這個設定下，如果 M1 掛了需要 failover，很有可能 S1 跟著機器一起掛了，S2 會沒有辦法取得多數來執行 failover，整個系統掛掉\n3 VM +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2 這是最基本的蛋又兼顧安全設定的設置\n如果 M1 死了 S1 跟著機器故障，S2 與 S3 還可以取得多數，順利 failover 到 R2 或是 R3。\n寫入資料遺失 +----+ | M1 | | S1 | \u0026lt;- C1 (writes will be lost) +----+ | / / +------+ | +----+ | [M2] |----+----| R3 | | S2 | | S3 | +------+ +----+ failover 之前，M1 是 master，Client 的寫入往 M1 寫 M1 網路故障，M2 failover 後成為新的 master，可是 Client 往 M1 寫入的資料並無法 sync 回 M2 等網路修復後，M1 回覆後會變成 R1 變成 slave，由 M2 去 sync R1，變成 R1 在 master 時收到的寫入資料遺失 為了避免這種情形，做額外的設定\nmin-slaves-to-write 1 min-slaves-max-lag 10 當 master 發現自己再也無法 sync 到足夠的 slave，表示 master 可能被孤立，這時主動拒絕客戶端的寫入請求。客戶端被拒絕後，會再向 sentinel 取得有效的 master，重新執行寫入請求，確保資料寫到有效的 master 上。\nSentinel 放在 Client 端 +----+ +----+ | M1 |----+----| R1 | | | | | | +----+ | +----+ | +------------+------------+ | | | | | | +----+ +----+ +----+ | C1 | | C2 | | C3 | | S1 | | S2 | | S3 | +----+ +----+ +----+ 有些情形，redis 這端只有兩台可用機器，這種情形可以考慮把 sentinel 放在客戶端的機器上\n仍然維持了獨立的 3 sentinels 的穩定 sentinel 與 client 所觀察到的 redis 狀態是相同的 如果 M1 死了，要 failover ，客戶端的 3 sentinel 可以正確地執行 failover，不受故障影響 客戶端又不足 3 個 +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | +------+-----+ | | | | +----+ +----+ | C1 | | C2 | | S3 | | S4 | +----+ +----+ Configuration: quorum = 3 +----+ +----+ | M1 |----+----| R1 | | S1 | | | S2 | +----+ | +----+ | | | +----+ | C1 | | S3 | +----+ Configuration: quorum = 2 跟上個例子類似，但又額外確保 3 sentinels 如果 M1 死了，剩下的 sentinel 可以正確 failover ","date":1569831130,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"c9d2f08bd3e3201f8b6d1b428ffbdb59","permalink":"https://chechia.net/zh-hant/post/2019-09-30-redis-ha-topology/","publishdate":"2019-09-30T16:12:10+08:00","relpermalink":"/zh-hant/post/2019-09-30-redis-ha-topology/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","redis","ithome","haproxy"],"title":"Redis Ha Topology","type":"post"},{"authors":[],"categories":["kubernetes","redis"],"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 redis-sentinel redis sentinel 與 redis 使用相容的 api，直接使用 redis-cli 透過 26479 port 連入，可以連到 sentinel，透過 sentinel 可以取得 redis master 的狀態與連線設定。\nredis-cli -h redis-redis-ha -p 26479 上篇我們的 redis-ha 安裝完變這樣\n$ kubectl get po | grep redis NAME READY STATUS RESTARTS AGE redis-1-redis-ha-server-0 3/3 Running 0 3d4h redis-1-redis-ha-server-1 3/3 Running 0 3d5h redis-1-redis-ha-server-2 3/3 Running 0 3d4h 有三個 Pod，裡面都是一個 redis, sentinel, 跟 exporter，這篇文章會專注講 sentinel 的功能與機制\nRedis Sentinel redis-sentinel 為 Redis 提供高可用服務，實務上可以透過 sentinel 在錯誤發生時，自動進行 failover。除此之外 sentinel 也提供監測，通知，與 redis 的設定。\nMonitoring: 持續檢測 master 與 slave instances 的狀態 Notification: 有事件發生可以發出通知 Automatic failover: 如果 master 失效自動啟動 failover 程序，將一個 slave 指排為 master，並設定其他 slave 使用新的 master Configuration provider: 為客戶端提供 service discovery，客戶可以通過 sentinel 取得 master 的連線資料。 Distributed Sentinel 本身是一個分散式系統，如我們的範例所示，三個 Pod 立面個含有一個 sentinel，組成 3 個 instace 的 sentinel cluster。\n錯誤檢測是由多個 sentinel 判定，要有多個 sentinel 都接收 master 已失效的訊息，才會判定成失效。這樣可以降低 false positive 的機率。 分散讓 sentinel 本身也具備高可用性，可以承受一定程度的錯誤。用來 fail over 的系統，不能因為自身的單點錯誤(single point failure) 而倒是整個 redis 失效。 Fundamental 一個耐用的 sentinel 需要至少三個 instance 最好把 instance 分散在多個獨立的隔離區域，意思是說，三個不會放在同一台機器上，或是放在同一個區域內，因為一個區域網路故障就全死。 app 使用 sentinel 的話，客戶端要支援 有時常測試的 HA 環境，才是有效的 HA Configuration Sentinel specific configuration options 在上篇我們跳過 sentinel 的設定，這邊說明一下\nsentinel: port: 26379 quorum: 2 config: ## Additional sentinel conf options can be added below. Only options that ## are expressed in the format simialar to \u0026#39;sentinel xxx mymaster xxx\u0026#39; will ## be properly templated. ## For available options see http://download.redis.io/redis-stable/sentinel.conf down-after-milliseconds: 10000 ## Failover timeout value in milliseconds failover-timeout: 180000 parallel-syncs: 5 ## Custom sentinel.conf files used to override default settings. If this file is ## specified then the sentinel.config above will be ignored. # customConfig: |- # Define configuration here resources: {} # requests: # memory: 200Mi # cpu: 100m # limits: # memory: 200Mi Quorum quorum 是每次確定 master 失效時，需要達成共識的 sentinel 數量。 Quorum 使用在錯誤檢測，確定錯誤真的發生後，sentinel 會以多數決(majority) 的方式選出 sentinel leader，讓 leader 處理 failover。 以我們的例子為例，總共三個，確認 master 死掉只要兩個 sentinel 達成共識即可啟動 failover 程序。可以直接測試一下。\nkubectl logs -f redis-1-redis-ha-server-0 kubectl delete po redis-1-redis-ha-server-1 log 一個 Pod ，然後直接把另一個 Pod 幹掉 這樣會有 1/3 的機率砍到 master，砍中的話可以看到 redis failover ，選出新的 master 的過程。\n這邊要注意，由於我們的 sentinel 與 redis 是放在同樣一個 Pod，幹掉的同時也殺了一個 sentinel，只剩 2 個，剛好達成共識。如果 quorum 是三，就要等第三個 sentinel 回來才能取得 quorum。\nsentinel 與 redis 的配置位置，之後的 topology 會討論。\nConfigurations down-after-milliseconds: 超過多少時間沒回應 ping 或正確回應，才覺得 master 壞了 parallel-syncs: failover 時，要重新與新 master sync 的 slave 數量。數量越多 sync 時間就越久，數量少就有較多 slave 沒 sync 資料，可能會讓 client read 到舊的資料 雖然 sync 是 non-blocking ，但在 sync 大筆資料時，slave 可能會沒有回應。設定為 1 的話，最多只會有一個 slave 下線 sync。 這些參數也可以透過 redis-cli 直接連入更改，但我們是在 kubernetes 上跑，臨時的更改不易保存，所以盡可能把這些configurations 放在 configmap 裡面。\nSentinel command 6379 port 連入 redis，26379 連入 redis sentinel。都是使用 redis-cli，兩者兼容的 protocol。\n# 使用 kubectl 連入，多個 container 要明確指出連入的 container kubectl exec -it redis-1-redis-ha-server-0 --container redis sh redis-cli -h redis-redis-ha -p 26479 # 近來先 ping 一下 $ ping PONG # 列出所有 master 的資訊，以及設定資訊 sentinel master redis-2-redis-ha:26379\u0026gt; sentinel masters 1) 1) \u0026#34;name\u0026#34; 2) \u0026#34;mymaster\u0026#34; 3) \u0026#34;ip\u0026#34; 4) \u0026#34;10.15.242.245\u0026#34; 5) \u0026#34;port\u0026#34; 6) \u0026#34;6379\u0026#34; 7) \u0026#34;runid\u0026#34; 8) \u0026#34;63a97460b7c3745577931dad406df9609c4e2464\u0026#34; 9) \u0026#34;flags\u0026#34; 10) \u0026#34;master\u0026#34; 11) \u0026#34;link-pending-commands\u0026#34; 12) \u0026#34;0\u0026#34; 13) \u0026#34;link-refcount\u0026#34; 14) \u0026#34;1\u0026#34; 15) \u0026#34;last-ping-sent\u0026#34; 16) \u0026#34;0\u0026#34; 17) \u0026#34;last-ok-ping-reply\u0026#34; 18) \u0026#34;479\u0026#34; 19) \u0026#34;last-ping-reply\u0026#34; 20) \u0026#34;479\u0026#34; 21) \u0026#34;down-after-milliseconds\u0026#34; 22) \u0026#34;5000\u0026#34; 23) \u0026#34;info-refresh\u0026#34; 24) \u0026#34;5756\u0026#34; 25) \u0026#34;role-reported\u0026#34; 26) \u0026#34;master\u0026#34; 27) \u0026#34;role-reported-time\u0026#34; 28) \u0026#34;348144787\u0026#34; 29) \u0026#34;config-epoch\u0026#34; 30) \u0026#34;13\u0026#34; 31) \u0026#34;num-slaves\u0026#34; 32) \u0026#34;2\u0026#34; 33) \u0026#34;num-other-sentinels\u0026#34; 34) \u0026#34;2\u0026#34; 35) \u0026#34;quorum\u0026#34; 36) \u0026#34;2\u0026#34; 37) \u0026#34;failover-timeout\u0026#34; 38) \u0026#34;180000\u0026#34; 39) \u0026#34;parallel-syncs\u0026#34; 40) \u0026#34;5\u0026#34; # 取得集群中的 master 訊息，目前有一個 master $ sentinel master mymaster # 取得集群中的 slaves 訊息，目前有兩個 slave $ sentinel slaves mymaster # 取得集群中的 master 訊息 $ sentinel sentinels mymaster # 檢查 sentinel 的 quorum $ sentinel ckquorum mymaster OK 3 usable Sentinels. Quorum and failover authorization can be reached # 強迫觸發一次 failover sentinel failover mymaster Sentinel Connection 有支援的客戶端設定，以Golang FZambia/sentinel 為例，透過 sentinel 取得 redis-pool。\n# 使用獨立的 pod service 連入 sentinel，協助彼此識別 sntnl := \u0026amp;sentinel.Sentinel{ Addrs: []string{\u0026#34;redis-2-redis-ha-announce-0:26379\u0026#34;, \u0026#34;redis-2-redis-ha-announce-0:26379\u0026#34;, \u0026#34;redis-2-redis-ha-announce-0:26379\u0026#34;}, MasterName: \u0026#34;mymaster\u0026#34;, Dial: func(addr …","date":1569748478,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"5070378573f76232b91acd55a45f345a","permalink":"https://chechia.net/zh-hant/post/2019-09-29-redis-ha-sentinel/","publishdate":"2019-09-29T17:14:38+08:00","relpermalink":"/zh-hant/post/2019-09-29-redis-ha-sentinel/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","redis","ithome","haproxy"],"title":"Redis Ha Sentinel","type":"post"},{"authors":[],"categories":["kubernetes","redis"],"content":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n今天的文會比較短，因為我早上在綠島已經水肺潛水潛了三趟，有點累哈哈\nRedis introduction Redis 是常用的 in-memory 的資料儲存庫，可作為資料庫，快取，message broker 使用，都非常好用。Redis 官方支援 high availability，使用的是 redis-sentinel ，今天我們就來部署一個有完整 sentinel 的 redis-ha。\nRedis 另外提供了一個 solution Redis cluster (multiple writer solution)，作為增加資料輸出帶寬，與增加資料耐用度的分散式解決方案，與 redis sentinel 所處理的 ha 問題是不相同的。有機會我們也來談。\nDeploy 我把我的寶藏都在這了https://github.com/chechiachang/go-redis-ha\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash HELM_NAME=redis-1 # Stable: chart version: redis-ha-3.6.1\tapp version: 5.0.5 helm upgrade --install ${HELM_NAME} stable/redis-ha --version 3.6.1 -f values-staging.yaml Helm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 kafka。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nRedis-ha helm chart github\nInstall 這邊是用 upgrade –install，已安裝就 upgrade，沒安裝就 install，之後可以用這個指令升版\nhelm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml values-staging 完整的 values.yaml 在 helm chart github\nimage: repository: redis tag: 5.0.5-alpine pullPolicy: IfNotPresent ## replicas number for each component replicas: 3 servers: serviceType: ClusterIP # [ClusterIP|LoadBalancer] annotations: {} auth: true ## Redis password ## Defaults to a random 10-character alphanumeric string if not set and auth is true ## ref: https://github.com/kubernetes/charts/blob/master/stable/redis-ha/templates/redis-auth-secret.yaml ## #redisPassword: ## Use existing secret containing key `authKey` (ignores redisPassword) existingSecret: redis-credentials ## Defines the key holding the redis password in existing secret. authKey: auth 這邊有準備 secret/redis-credentials 裡面的 key[auth] 存放 redis 密碼，要連入的 pod 需要掛載 secret 並把 auth 匯入。\nVersion 這邊使用的版本：\nchart version: redis-ha-3.6.1 app version: 5.0.5 Redis Image: redis:5.0.5-alpine Redis exporter: oliver006/redis_exporter:v0.31.0 安裝完變這樣\n$ kubectl get po | grep redis NAME READY STATUS RESTARTS AGE redis-1-redis-ha-server-0 3/3 Running 0 3d4h redis-1-redis-ha-server-1 3/3 Running 0 3d5h redis-1-redis-ha-server-2 3/3 Running 0 3d4h describe pod 可以看到裡面有三個 container\nredis: 主要的 redis sentinel: 維護 redis 可用性的服務，會監測 redis 狀態，並把連線指派到新的 master redis-exporter: 把 redis 的運行資料(metrics) 送出到 promethues Networking Service\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR redis-redis-ha ClusterIP None \u0026lt;none\u0026gt; 6379/TCP,26379/TCP,9121/TCP 46m app=redis-ha,release=redis redis-redis-ha-announce-0 ClusterIP 10.3.243.81 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-0 redis-redis-ha-announce-1 ClusterIP 10.3.250.151 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-1 redis-redis-ha-announce-2 ClusterIP 10.3.242.59 \u0026lt;none\u0026gt; 6379/TCP,26379/TCP 46m app=redis-ha,release=redis,statefulset.kubernetes.io/pod-name=redis-redis-ha-server-2 nslookup redis-redis-ha Name: redis-redis-ha Address 1: 10.0.0.42 redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local Address 2: 10.0.1.13 redis-redis-ha-server-2.redis-redis-ha.default.svc.cluster.local Address 3: 10.0.2.8 redis-redis-ha-server-0.redis-redis-ha.default.svc.cluster.local Name: redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local Address 1: 10.0.0.43 redis-redis-ha-server-1.redis-redis-ha.default.svc.cluster.local 連線 所有連線透過 redis-redis-ha service 連入\nredis-cli -h redis-redis-ha -p 6479 -a \u0026lt;password\u0026gt; 或是直接指定 redis instance 連入。\nredis-cli -h redis-redis-ha-announce-0 -p 6479 -a \u0026lt;password\u0026gt; redis-cli -h redis-redis-ha-announce-1 -p 6479 -a \u0026lt;password\u0026gt; redis-cli -h redis-redis-ha-announce-2 -p 6479 -a \u0026lt;password\u0026gt; 但上面兩者會有問題，redis 只有 master 是 writable，連入 slave 會變成 readonly，如果沒有任何 probe 機智，那就是每次連線時有 2/3 機率會連到 readonly 的 redis slave 。所以連線前要先找到正確的 master\nSentinel Sentinel 是 redis 官方提供的 HA solution，主要負責監控 redis 的狀態，並控制 redis master 的 failover 機制，一但超過 threshold，sentinel 就會把 master failover 到其他 slave 上。並把 master 連線指向新 master。\nredis sentinel 與 redis 使用相容的 api，直接使用 redis-cli 透過 26479 port 連入，可以連到 sentinel，透過 sentinel 可以取得 redis master 的狀態與連線設定。\nredis-cli -h redis-redis-ha -p 26479 App 端支援 sentinel 需要有支援 sentinel 的 redis client library，例如: python redis-py 有支援 sentinel 的設定。\n這邊就會比較麻煩，因為不是所有的語言對 redis-sentinel 的支援性都夠好，或是沒辦法設定到妮旺使用的情境上。\n如果你找得到支援性良好的套件，恭喜你。不然就像我們公司，與我們的需求有衝突，只好自己 fork library。\n所以說直接使用有支援 redis-sentinel 可能會遇到一些問題。那也沒有更好的解決方法？我們下次說明使用 HAproxy 的高可用方案。\nBenchmark 部署完後，可以跑一下 benchmark，看看在 kubernetes 上運行的效能有沒有符合需求。\nRun a redis pod with sleep command NOTE: CPU usage (rapidly) increasing during benchmark DON’T DO THIS on PRODUCTION\nkubectl run test-redis --image redis:5.0.5-alpine …","date":1569654863,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"c2720f7d8eae8100d8dc2ce98e49999b","permalink":"https://chechia.net/zh-hant/post/2019-09-28-redis-ha-deployment/","publishdate":"2019-09-28T15:14:23+08:00","relpermalink":"/zh-hant/post/2019-09-28-redis-ha-deployment/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\n在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kubernetes","redis","ithome"],"title":"Redis Ha Deployment","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Kafka’s quorum set Kafka 的 quorum set 這篇跟上篇其實再講 quorum，應該連在一起，但礙於篇幅（以及我個人的時間ＱＱ）拆成了兩篇。各位有需要可以回顧一下。\nReplicated log commit decision 上篇提到了兩個維持 replicated log 的 model\n有更新進來，leader 等待所有 follower 都 ack，才 commit。 有更新進來，leader 取得所有 node (2n+1) 中的多數 node 回應(n+1)，就 commit。而 leader election 時，必須比對 node 上的 log，決定誰是 electable leader(有最完整 log 的 follower)，這樣稱為共識(Quorum) 前者的好處是，所有 node 都有完整的 log 後，leader 才會 commit，回覆給客戶 commit 的資訊，所以每個 follower 都是 leader electable 人人都可以當 leader，leader 一故障就選擇新的 leader 即可。壞處就是 leader 在等待所有 follow ack 的時間會非常久，而且時間複雜度可能會隨 cluster size scale，或是變成要等最慢的 node 回應(worst case)。這樣在 node 數量多的時候非常不經濟。\n後者的好處是，n+1 node ack 後就 commit，leader commit 的速度是由前段班的回應速度決定。leader 出現故障，仍能維持多數 node 的資料正確。\nLeader election decision Leader Election 的問題也是類似，如果選擇 leader 時，所有的 follower 都比對過 log，這樣花的時間會很久。要知道，這是個分散式的架構，沒有中心化的 controller，也就是 follower 彼此需要交互比對。而且時間隨 follower 數量 scale。 造成topic partition 沒有 leader 的時間(downtime)太長。\n如果使用 majority，也就是當 leader 死掉，產生新的 leader election 時，只詢問 n+1 個 follower ，然後從選出 log 最完整的人當 leader， 這樣過程中每個 follower 彼此比對，確認，然後才選出 leader，確認 leader 的結果，所花的時間會大幅縮短。\n當然，這麼做產生的 tradeoff，就是萬一取得多數決的 n+1 個 follower 裡面，沒有最完整的 log ，那從裡頭選出來的 leader 自然也沒有完整 log，選出來的 leader 就會遺失資料。\n一個完整的 Quorum 機制 這不是 kafka 的機制，但我們順帶聊聊。\ncommit decision 使用多數決(majority) leader election 也使用多數決 總共有 2n+1 replicas，leader 取得 n+1 ack 才能 commit message。然後 leader election 時，從至少 n+1 個 follower 中取得多數決才能選出 leader。有過半的完整log，加上取得過半數的人確認，兩者產生 overlap。這樣的共識就確保有完整的 log 的 follower 一定會出現在 leader election 中，確保選出來的 leader 有完整 log。\n好處如前面描述，整體效能由前段班的速度決定。\n壞處是，很容易就沒有足夠的 electable leader。要容忍 1 個錯誤，需要 3 個完整備份，要容忍 2 個錯誤需要 5 個備份。在實務上，只靠依賴夠多的 redundency 容錯非常的不實際：每一次寫入需要 5 倍寫入跟硬碟空間，但整體效能只有 1/5。資料量大就直接ＧＧ。所以 quorum 才會只存在分散式集群(ex. zookeeper)，而不會直接用在儲存系統。\nKafka’s approach Kafka 不使用 majority vote，而是去動態維護一套 in-sync replicas(ISR) ，這些 ISR 會跟上 leader 的進度，而只有這些 ISR 才能是 leader eligible。一個 update 只有在所有 ISR 都 ack 後才會 commit。\nISR 的狀態不放在 kafka 而放在 zookeeper 上，也就是目前哪些 node 是 ISR 的記錄存在 zookeeper。這件事對維持 kafka 節點上，leader 能夠分散在各個 kafka node 上(leader rebalance)是很重要的。\nkafka’s approach 與 majority vote，在等待 message commit ack 上所花的成本是一樣的。 然而在 leader election 上，kafka 的 ISR 確保了更多個 eligiable leader 的數量，持續維持在合理的數量，而不會要維持大量個 redundency。ISR 放在外部，更方便 kafka 做 leader rebalance，增加穩定度。\nUnclean leader election 如果 leaders 都死光了會怎樣？\n只要有一個 replica in-sync，Kafka 就保證資料的完整性。然而所有可用的 leaders 都死了，這個就無法保證。\n如果這個情形發生了，kafka 會做以下處理\n等 ISR 中有人完全回復過來，然後選這個 node 作為 leader(有資料遺失的風險) 直接選擇第一個回覆的 node (不一定在 ISR 中)，先回覆的就指派為 leader 前者犧牲 availability （回覆前沒有 leader 可作讀寫）來確保資料是來自 ISR，雖然錯誤中無法讀寫(downtime)，但可以確定錯誤前跟錯誤後的資料都來自 ISR\n後者犧牲 consistency （來自非 ISR 的 leader 可能導致資料不正確），然而卻能更快的從錯誤中回覆，減少 downtime\n0.11.0.0 後的 kafka 預設是選擇前者，也就是 consistency over availability，當然這可以在設定更改。\nAvailability and Durability Guarantees 近一步考慮 client 的影響。\nProducer 在寫入時可以選擇 message 需要多少 acknowledge，0, 1 or all，ack=all 指的是 message 收到所有 in-sync replicas 的 ack\n如果 2 replicas 中有 1 個故障，這時寫入只要收到 1 個 ISR 的 ack，就達成 ack=all 但如果不幸剩下一個 replicas 也死了 (0/0 ack)，寫入的資料就會遺失 有些使用情境，會希望資料的耐用度(Durability)優先於可用性(Availability)，可以透過以下兩個方式設定\n禁用 unclean leader election，效果是如果所有的 replicas 都失效，則整個 partition 都失效，直到前一個 leader 回復正常。 指定可接受的最少 ISR，如果 partition 中的 ISR 低於這個數量，就停止寫入這個 partition，直到 ISR 的數量回覆。 這樣雖然犧牲了可用性，卻可以最大程度地確保資料的可靠性。\n複本管理 上面的討論都只是再說一個 topic，實務中 kafka 中會有大量的 topic ，乘上 partition number 與 replication factor，成千上萬的複本分散在集群中，kafka 會試圖分散 replicas 到集群中，並讓 leader 的數量平均在 node 上\n","date":1569509432,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"bf29f4e12b744c76b549485f2ae693db","permalink":"https://chechia.net/zh-hant/post/2019-09-26-kafka-ha-continued/","publishdate":"2019-09-26T22:50:32+08:00","relpermalink":"/zh-hant/post/2019-09-26-kafka-ha-continued/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka HA Continued","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Zookeeper Multi-server setup Kafka Multi-broker setup Zookeeper Multi-Server 為了維持 zookeeper 有效運作，cluster 必須維持 majority (多數)，也就是至少一半的機器在線。如果總共 3 台，便可以忍受 1 台故障仍保有 majority。如果是 5 台就可以容忍 2 台故障。一般來說都建議使用基數數量。Zookeeper Multi Server Setup\n普遍情況，3 台 zookeeper 已經是 production ready 的狀態，但如果為了更高的可用性，以方便進行單節點停機維護，可以增加節點數量。\nTopology 需要將 zookeeper 放在不同的機器上，不同的網路環境，甚至是不同的雲平台區域上，以承受不同程度的故障。例如單台機器故障，或是區域性的網路故障。\n我們這邊會使用 Kubernetes PodAntiAffinity，要求 scheduler 在部屬時，必須將 zookeeper 分散到不同的機器上。設定如下：\nvim values-staging.yaml zookeeper: enabled: true resources: ~ env: ZK_HEAP_SIZE: \u0026#34;1G\u0026#34; persistence: enabled: false image: PullPolicy: \u0026#34;IfNotPresent\u0026#34; url: \u0026#34;\u0026#34; port: 2181 ## Pod scheduling preferences (by default keep pods within a release on separate nodes). ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## By default we don\u0026#39;t set affinity: affinity: # Criteria by which pod label-values influence scheduling for zookeeper pods. podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; labelSelector: matchLabels: release: zookeeper 使用 podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution，如果 topologyKey 已經有指定 label 的 pod 存在，則無法部署，需要數到其他台機器。\nkubectl get pods --output wide | grep zookeeper NAME READY STATUS RESTARTS AGE IP NODE kafka-0-zookeeper-0 1/1 Running 0 42d 10.8.12.4 gke-chechiachang-pool-1-e06e6d00-pc98 kafka-0-zookeeper-1 1/1 Running 0 42d 10.8.4.4 gke-chechiachang-pool-1-e06e6d00-c29q kafka-0-zookeeper-2 1/1 Running 0 42d 10.8.3.6 gke-chechiachang-pool-1-e06e6d00-krwc 效果是 zookeeper 都分配到不同的機器上。\nGuarantees Zookeeper 對於資料一致性，有這些保障 Consistency Guarantees\n順序一致性：資料更新的順序，與發送的順序一致 原子性：資料更新只有成功或失敗，沒有部份效果 系統一致性：可戶端連到 server 看到的東西都是一樣，無關連入哪個 server 可靠性： 客戶端的更新請求，一但收到 server 回覆更新成功，便會持續保存狀態。某些錯誤會造成客戶端收不到回覆， 可能是網路問題，或是 server 內部問題，這邊就無法確定 server 上的狀態，是否被更新了，或是請求已經遺失了。 從客戶讀取到的資料都是以確認的資料，不會因為 server 故障回滾(Roll back)而回到舊的狀態 Kafka 的設定 ## The StatefulSet installs 3 pods by default replicas: 3 resources: limits: cpu: 200m memory: 4096Mi requests: cpu: 100m memory: 1024Mi kafkaHeapOptions: \u0026#34;-Xmx4G -Xms1G\u0026#34; 設定 broker 的數量，以及 Pod 提供的 resource，並且透過 heapOption 把記憶體設定塞進 JVM\naffinity: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - kafka topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - zookeeper 這邊下了兩個 affinity\npodAntiAffinity 盡量讓 kafka-broker 分散到不同機器上 podAffinity 讓 broker prefer 跟 zookeeper 放在一起 分散的理由同上，不希望一台機器死了，就讓多個 broker 跟著死\n要和 zookeeper 放在一起，就要看需求與實際環境調整\nconfigurationOverrides: \u0026#34;default.replication.factor\u0026#34;: 3 \u0026#34;offsets.topic.replication.factor\u0026#34;: 2 # Increased from 1 to 2 for higher output \u0026#34;offsets.topic.num.partitions\u0026#34;: 3 \u0026#34;confluent.support.metrics.enable\u0026#34;: false # Disables confluent metric submission \u0026#34;auto.leader.rebalance.enable\u0026#34;: true \u0026#34;auto.create.topics.enable\u0026#34;: true \u0026#34;message.max.bytes\u0026#34;: \u0026#34;16000000\u0026#34; # Extend global topic max message bytes to 16 Mb 這邊再把 broker 運行的設定參數塞進去，參數的用途大多與複本與高可用機制有關下面都會提到。\nKafka 的複本機制 kafka 的副本機制 預設將各個 topic partition 的 log 分散到 server 上，如果其中一台 server 故障，資料仍然可用。\n兩個重要的設定\nnum.partitions=N default.replication.factor=M kafka 預設使用複本，所有機制與設計都圍繞著複本。如果（因為某些原因）不希望使用複本，可將 replication factor 設為 1。\nreplication 的單位是 topic partition，正常狀況下\n一個 partition 會有一個 leader，以及零個或以上個 follower leader + follower 總數是 replication factor 所有讀寫都是對 leader 讀寫 leader 的 log 會同步到 follower 上，leader 與 follower 狀態是一樣的 Election \u0026amp; Load balance 通常一個 topic 會有多個 partition，也就是說，每個 topic 會有多個 partition leader，分散負載\n通常 topic partition 的總數會比 broker 的數量多\n以上一篇範例，我們有三個 kafka-0-broker 各自是一個 Pod 有 topic: ticker 跟預設的 __consumer_offset__，乘上 partition number 的設定值(N)，會有 2N 個 partitions partitiion 會有各自的複本，kafka 會盡量將相同 topic 的複本分散到不同 broker 上 kafka 也會盡量維持 partition 的 leader 分散在不同的 broker 上，這個部分 kafka 會透過算法做 leader election，也可手動使用腳本做 Balancing leadership 總之，topic 的 partition 與 leader 會分散到 broker 上，維持 partition 的可用性。\nsync node 要能夠維持 zookeeper 的 session (zookeeper 有 heartbeat 機制) follower 不能落後 leader 太多 kafka 能保障資料不會遺失，只要至少一個 node 是在 sync 的狀態。例如本來有三個 partition，其中兩個 partition 不同步，只要其中一個 partition 是同步，便能作為 leader 持續提供正確的 message。\nReplicated Logs kafka 透過 replicated log 維持分散式的 partition\n複本間要維持共識(consensus)的最簡單機制，就是單一 leader 決定，其他 follower 跟隨。然而萬一 leader 死 …","date":1569423032,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"12d41ac554ed14ce53a27a90a7c4c6a3","permalink":"https://chechia.net/zh-hant/post/2019-09-25-kafka-ha-topology/","publishdate":"2019-09-25T22:50:32+08:00","relpermalink":"/zh-hant/post/2019-09-25-kafka-ha-topology/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes"],"title":"Kafka HA Topology","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 從 Zookeeper 獲取資訊 取得並處理 topic benchmark kafka zookeeper zookeeper 是 kafka 的分散式協調系統，在 kafka 上多個節點間需要協調的內容，例如：彼此節點的ID，位置與當前狀態，或是跨節點 topic 的設定與狀態。取名叫做 zookeeper 就是在協調混亂的分散式系統，,裡面各種不同種類的服務都要協調，象個動物園管理員。Zookeeper 的官方文件 有更詳細的說明。\nKafka 的節點資訊，與當前狀態，是放在 zookeeper 上，我們可以透過以下指令取得\n# 首先先取得 zkCli 的 cli，這個只有連進任何一台 zookeeper 內部都有 kubectl exec -it kafka-0-zookeeper-0 --container kafka-broker bash # 由於是在 Pod 內部，直接 localhost 詢問本地 /usr/bin/zkCli.sh -server localhost:2181 Connecting to localhost:2181 2019-09-25 15:02:36,089 [myid:] - INFO [main:Environment@100] - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT 2019-09-25 15:02:36,096 [myid:] - INFO [main:Environment@100] - Client environment:host.name=kafka-0-zookeeper-0.kafka-0-zookeeper-headless.default.svc.cluster.local 2019-09-25 15:02:36,096 [myid:] - INFO [main:Environment@100] - Client environment:java.version=1.8.0_131 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.vendor=Oracle Corporation 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.class.path=/usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.10.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.io.tmpdir=/tmp 2019-09-25 15:02:36,100 [myid:] - INFO [main:Environment@100] - Client environment:java.compiler=\u0026lt;NA\u0026gt; 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.name=Linux 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.arch=amd64 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:os.version=4.14.127+ 2019-09-25 15:02:36,101 [myid:] - INFO [main:Environment@100] - Client environment:user.name=zookeeper 2019-09-25 15:02:36,102 [myid:] - INFO [main:Environment@100] - Client environment:user.home=/home/zookeeper 2019-09-25 15:02:36,102 [myid:] - INFO [main:Environment@100] - Client environment:user.dir=/ 2019-09-25 15:02:36,105 [myid:] - INFO [main:ZooKeeper@438] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@42110406 Welcome to ZooKeeper! 2019-09-25 15:02:36,160 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1032] - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) JLine support is enabled 2019-09-25 15:02:36,374 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@876] - Socket connection established to localhost/127.0.0.1:2181, initiating session 2019-09-25 15:02:36,393 [myid:] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x16d67baf1310001, negotiated timeout = 30000 WATCHER:: WatchedEvent state:SyncConnected type:None path:null [zk: localhost:2181(CONNECTED) 0] 取得 kafka broker 資料\n# List root Nodes $ ls / [cluster, controller, controller_epoch, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config] # Brokers 的資料節點 $ ls /brokers [ids, topics, seqid] # List /brokers/ids 得到三個 kafka broker $ ls /brokers/ids [0, 1, 2] # 列出所有 topic 名稱 ls /brokers/topics [ticker] ticker 是上篇範利用到的 topic\n簡單來說，zookeeper 存放這些狀態與 topic 的 metadata\n儲存核心的狀態與資料，特別是 broker 萬一掛掉，也還需要維持的資料 協調工作，例如協助 broker 處理 quorum，紀錄 partition master 等 # 離開 zkCli quit Kafka 這邊一樣先連線進去一台 broker，取得 kafka binary\nkubectl exec -it kafka-0-0 --container kafka-broker bash ls /usr/bin/ | grep kafka kafka-acls …","date":1569423032,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"37788bcddae4226839899d43afcb10ad","permalink":"https://chechia.net/zh-hant/post/2019-09-25-kafka-operation-scripts/","publishdate":"2019-09-25T22:50:32+08:00","relpermalink":"/zh-hant/post/2019-09-25-kafka-operation-scripts/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka Operation Scripts","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 在 Kubernetes 中連線 kafka 使用 golang library 連線到 Kafka 透過 kafka script 操作 kafka kubernetes 中連線 kafka 先看一看 kafka pods\n$ kubectl get pods --selector=\u0026#39;app=kafka\u0026#39; NAME READY STATUS RESTARTS AGE kafka-1-0 1/1 Running 1 26d kafka-1-1 1/1 Running 0 26d kafka-1-2 1/1 Running 0 26d $ kubectl get pods -l \u0026#39;app=zookeeper\u0026#39; NAME READY STATUS RESTARTS AGE kafka-1-zookeeper-0 1/1 Running 0 26d kafka-1-zookeeper-1 1/1 Running 0 26d kafka-1-zookeeper-2 1/1 Running 0 26d $ kubectl get pods -l \u0026#39;app=kafka-exporter\u0026#39; NAME READY STATUS RESTARTS AGE kafka-1-exporter-88786d84b-z954z 1/1 Running 5 26d kubectl describe pods kafka-1-0 Name: kafka-1-0 Namespace: default Priority: 0 Node: gke-chechiachang-pool-1-e4622744-wcq0/10.140.15.212 Labels: app=kafka controller-revision-hash=kafka-1-69986d7477 release=kafka-1 statefulset.kubernetes.io/pod-name=kafka-1-0 Annotations: kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka-broker Status: Running IP: 10.12.6.178 Controlled By: StatefulSet/kafka-1 Containers: kafka-broker: Image: confluentinc/cp-kafka:5.0.1 Port: 9092/TCP Host Port: 0/TCP Command: sh -exc unset KAFKA_PORT \u0026amp;\u0026amp; \\ export KAFKA_BROKER_ID=${POD_NAME##*-} \u0026amp;\u0026amp; \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 \u0026amp;\u0026amp; \\ exec /etc/confluent/docker/run Requests: cpu: 100m Liveness: exec [sh -ec /usr/bin/jps | /bin/grep -q SupportedKafka] delay=30s timeout=5s period=10s #success=1 #failure=3 Readiness: tcp-socket :kafka delay=30s timeout=5s period=10s #success=1 #failure=3 Environment: POD_IP: (v1:status.podIP) POD_NAME: kafka-1-0 (v1:metadata.name) POD_NAMESPACE: default (v1:metadata.namespace) KAFKA_HEAP_OPTS: -Xmx4G -Xms1G KAFKA_ZOOKEEPER_CONNECT: kafka-1-zookeeper:2181 KAFKA_LOG_DIRS: /opt/kafka/data/logs KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: false KAFKA_DEFAULT_REPLICATION_FACTOR: 3 KAFKA_MESSAGE_MAX_BYTES: 16000000 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_JMX_PORT: 5555 Mounts: /opt/kafka/data from datadir (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2tm8c (ro) Conditions: Volumes: datadir: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: datadir-kafka-1-0 ReadOnly: false default-token-2tm8c: Type: Secret (a volume populated by a Secret) SecretName: default-token-2tm8c Optional: false 講幾個重點：\n這邊跑起來的是 kafka-broker，接收 producer 與 consumer 來的 request 這邊用的是 statefulsets，不是完全無狀態的 kafka broker，而把 message 記在 datadir 上，降低故障重啟時可能遺失資料的風險。 啟動時，把 kubernetes 指定的 pod name 塞進環境變數，然後作為當前 broker 的 ID 沒有設定 Pod antiAffinity，所以有可能會啟三個 kafka 結果三個跑在同一台 node 上，這樣 node 故障就全死，沒有HA Service \u0026amp; Endpoints 看一下 service 與 endpoints zookeeper 與 exporter 我們這邊先掠過不談，到專章講高可用性與服務監測時，再來討論。\n$ kubectl get service -l \u0026#39;app=kafka\u0026#39; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-1 ClusterIP 10.15.242.178 \u0026lt;none\u0026gt; 9092/TCP 26d kafka-1-headless ClusterIP None \u0026lt;none\u0026gt; 9092/TCP 26d 兩個 services\n一個是 cluster-ip service，有 single cluster IP 與 load-balance，DNS 會過 kube-proxy。 一個是 headless service，DNS 沒有過 kube-proxy，而是由 endpoint controller 直接 address record，指向把符合 service selector 的 pod。適合做 service discovery，不會依賴於 kubernetes 的實現。 詳細說明在官方文件\n簡單來說，kafka broker 會做 auto service discovery，我們可以使用 headless service。\n客戶端(consumer \u0026amp; producer) 連入時，則使用 cluster-ip service，做 load balancing。\n$ kubectl get endpoints -l \u0026#39;app=kafka\u0026#39; NAME ENDPOINTS AGE kafka-1 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d kafka-1-headless 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d Golang Example 附上簡單的 Golang 客戶端，完整 Github Repository 在這邊\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/segmentio/kafka-go\u0026#34; // 使用的套件 ) func main() { topic := \u0026#34;ticker\u0026#34; // 指定 message 要使用的 topic partition := 0 // 指定 partition，由於底下連線指定連線到 partition 的 leader，所以需要指定 partition kafkaURL := \u0026#34;kafka-0:9092\u0026#34; // 指定 kafkaURL，也可以透過 os.GetEnv() 從環境變數裡拿到。 // producer 對指定 topic, partition 的 leader 產生連線 producerConn, _ := kafka.DialLeader(context.Background(), \u0026#34;tcp\u0026#34;, kafkaURL, topic, partition) // 程式結束最後把 connection 關掉。不關會造成 broker 累積大量 connection，需要等待 broker 端 timeout 才會釋放。 defer producerConn.Close() //producerConn.SetWriteDeadline(time.Now().Add(10 * time.Second)) // 使用 go routine 跑一個 subprocess for loop，一直產生 message 到 kafka topic，這邊的範例是每秒推一個秒數。 go func() { for { producerConn.WriteMessages( kafka.Message{ Value: []byte(strconv.Itoa(time.Now().Second())), }, ) time.Sleep(1 * time.Second) } }() // make a new …","date":1569333589,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"51c2041a8a32cbc46e22519634c523f9","permalink":"https://chechia.net/zh-hant/post/2019-09-24-kafka-basic-usage/","publishdate":"2019-09-24T21:59:49+08:00","relpermalink":"/zh-hant/post/2019-09-24-kafka-basic-usage/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka-basic-usage","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n寫了部屬，本想談一下 kafka 的高可用性配置，看到大德的留言，才想到應該要先跟各位介紹一下 kafka，跟 kafka 的用途。也感謝大德路過發問，我也會順代調整內容。今天就說明何為 kafka，以及在什麼樣的狀況使用。\n摘要 簡介 kafka 基本元件 Kafka 的工作流程 簡介 Kafka Kafka 是分散式的 streaming platform，可以 subscribe \u0026amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。原版的說明在這邊\n這邊有幾個東西要解釋。\nMessage Queue System 當一個系統開始運作時，裡頭會有很多變數，這些變數其實就是在一定的範圍(scope）內，做訊息(message)的傳遞。例如在 app 寫了一個 function ，傳入一個變數的值給 function。\n在複雜的系統中，服務元件彼此也會有傳遞訊息的需求。例如我原本有一個 api-server，其中一段程式碼是效能瓶頸，我把它切出來獨立成一個 worker 的元件，讓它可以在更高效能地方執行，甚至 horizontal scaling。這種情境，辨可能歲需要把一部分的 message 從 api-server 傳到 worker，worker 把吃效能的工作做完，再把結果回傳給 api-server。這時就會需要一個穩定的 message queue system，來穩定，且高效能的傳遞這些 message。\nMessage Queue System 實做很多，ActiveMQ, RabbitMQ, … 等，一些 database 做 message queue 在某些應用場景下也十分適合，例如 Redis 是 in-memory key-value database，內部也實做 pubsub，能夠在某些環境穩定的傳送 message。\nRequest-Response vs Publish-Subscribe 訊息的傳送有很多方式，例如 Http request-response 很適合 server 在無狀態(stateless) 下接受來自客戶端的訊息，每次傳送都重新建立新的 http connection，這樣做有很多好處也很多壞處。其中明顯的壞處是網路資源的浪費，以及訊息的不夠即時，指定特定收件人時發件人會造成額外負擔等。\n使用 Pub-sub pattern的好處，是 publisher 不需要額外處理『這個訊息要送給誰』的工作，而是讓 subscriber 來訂閱需要的訊息類別，一有新的 event 送到該訊息類別，直接透過 broker 推播給 subscriber。不僅即時，節省效能，而且訂閱的彈性很大。\nKafka producer \u0026amp; Consumer API Kafka 作為 client 與 server 兩邊的溝通平台，提供了許多 API 葛不同角色使用。Producer 產生 message 到特定 topic 上，consumer 訂閱特定 topics，kafak 把符合條件的訊息推播給 consumer。\nProducer API: 讓 app publish 一連的訊息 Consumer API: 讓 app subscribe 許多特定 topic，並處理訊息串流(stream) Stream API: 讓 app 作為串流中介處理(stream processor) Connect API: 與 producer 與 consumer 可以對外部服務連結 Topics \u0026amp; Logs Topic 是 kafka 為訊息串流提供的抽象，topic 是訊息傳送到 kafka 時賦予的類別(category)，作為 publish 與 consume 的判斷依據。\nPartition 訊息依據 topic 分類存放，並可以依據 replication factor 設定，在 kafka 中存放多個訊息分割(partition)。partition 可以想成是 message queue 的平行化 (parallel)，併發處理訊息可以大幅提昇訊息接收與發送的速度，並且多個副本也提高資料的可用性。\n由於訊息發送跟接收過程可能因為網路與環境而不穩定，這些相同 topic 的 partition 不一定會完全一樣。但 kafka 確保了以下幾點。\nGuarantees 良好配置的 kafka 有以下保證\n訊息在系統中送出跟被收到的時間不一定，但kafak中，從相同 producer 送出的訊息，送到 topic partition 會維持送出的順序 Consumer 看見的訊息是與 kafka 中的存放順序一致 有 replication factor 為 N 的 topic ，可以容忍(fault-tolerance) N-1 個 kafka-server 壞掉，而不影響資料。 當然，這邊的前提是有良好配置。錯誤的配置可能會導致訊息不穩定，效能低落，甚至遺失。\nProducer Producer 負責把訊息推向一個 topic，並指定訊息應該放在 topic 的哪個 partition。\nConsumer Consumer 會自行標記，形成 consumer group，透過 consumer group 來保障訊息傳遞的次序，容錯，以及擴展的效率。\nConsumer 透過 consumer group 共享一個 group.id。 Consumer group 去所有 partitions 裡拿訊息，所有 partitions 的訊息分配到 consumer group 中的 consumer。 app 在接收訊息時，設置正確的化，在一個 consumer group 中，可以容忍 consumer 失效，仍能確保訊息一指定的次序送達。在需要大流量時，也可調整 consumer 的數量提高負載。\n用例 kafka 的使用例子非常的多，使用範圍非常廣泛。\n基本上是訊息傳遞的使用例子，kafka 大多能勝任。\n小結 這邊只提了 kafka 的基本概念，基本元件，以及 consumer group 機制，為我們底下要談的 configuration 與 topology 鋪路。\n","date":1569247189,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"5fa88f73dbb752d54cb84b12b6b543e2","permalink":"https://chechia.net/zh-hant/post/2019-09-23-kafka-introduction/","publishdate":"2019-09-23T21:59:49+08:00","relpermalink":"/zh-hant/post/2019-09-23-kafka-introduction/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka-introduction","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n","date":1569246929,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"a277ceed046b13f94110a7b4096388f1","permalink":"https://chechia.net/zh-hant/post/2019-09-23-kafka-helm-configuration/","publishdate":"2019-09-23T21:55:29+08:00","relpermalink":"/zh-hant/post/2019-09-23-kafka-helm-configuration/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka Helm Configuration","type":"post"},{"authors":[],"categories":["kubernetes","kafka"],"content":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n碎念 30 天每天一文真的蠻逼人的，每一篇都是新寫，還要盡可能顧及文章品質，下班趕文章，各位大德寫看看就知道\n這邊調整了當初想寫的文章，內容應該都會帶到 elk kafka-ha reids-ha prometheus kubernetes on gcp 但不會再一篇 10000 字了，逼死我吧… 寫不完的部份 30 天候會在IT邦幫忙，或是我的 Github Page https://chechia.net/補完 寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n摘要 簡介 kafka 部屬 kafka 到 kubernetes 上 簡介 kafka Kafka 是分散式的 streaming platform，可以 subscribe \u0026amp; publish 訊息，可以當作是一個功能強大的 message queue 系統，由於分散式的架構，讓 kafka 有很大程度的 fault tolerance。\n我們今天就來部屬一個 kafka。\nDeploy 我把我的寶藏都在這了https://github.com/chechiachang/kafka-on-kubernetes\n下載下來的 .sh ，跑之前養成習慣貓一下\ncat install.sh #!/bin/bash # # https://github.com/helm/charts/tree/master/incubator/kafka #HELM_NAME=kafka HELM_NAME=kafka-1 helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator # Stable: chart version: kafka-0.16.2\tapp version: 5.0.1 helm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml Helm 我們這邊用 helm 部屬，之所以用 helm ，因為這是我想到最簡單的方法，能讓輕鬆擁有一套功能完整的 kafka。所以我們先用。\n沒用過 helm 的大德可以參考 Helm Quickstart，先把 helm cli 與 kubernetes 上的 helm tiller 都設定好\nhelm init Helm Chart 一個 helm chart 可以當成一個獨立的專案，不同的 chart 可以在 kubernetes 上協助部屬不同的項目。\n這邊使用了還在 incubator 的chart，雖然是 prod ready，不過使用上還是要注意。\n使用前先把 incubator 的 helm repo 加進來\nhelm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator Install 這邊是用 upgrade –install，已安裝就 upgrade，沒安裝就 install，之後可以用這個指令升版\nhelm upgrade --install ${HELM_NAME} incubator/kafka --version 0.16.2 -f values-staging.yaml Version 這邊使用的版本：\nchart version: kafka-0.16.2 app version: 5.0.1 kafka Image: confluentinc/cp-kafka:5.0.1 zookeeper Image: gcr.io/google_samples/k8szk:v3 kafka exporter: danielqsj/kafka-exporter:v1.2.0 values-staging 透過 helm chart，把啟動參數帶進去，這邊我們看幾個比較重要的，細節之後的文章在一起討論。\nhttps://github.com/chechiachang/kafka-on-kubernetes/blob/master/values-staging.yaml\nreplicas: 3 安裝三個 kafka，topology 的東西也是敬待下篇XD\n## The kafka image repository image: \u0026#34;confluentinc/cp-kafka\u0026#34; ## The kafka image tag 底層執行的 kafka 是 conluent kafka Configure resource requests and limits ref: http://kubernetes.io/docs/user-guide/compute-resources/ resources: {}\nlimits: cpu: 200m memory: 4096Mi requests: cpu: 100m memory: 1024Mi kafkaHeapOptions: “-Xmx4G -Xms1G”\n這邊可以調整在 kubernetes 上面的 limit 跟 request * Deploy 會先去跟 node 問夠不夠，夠的話要求 node 保留這些資源給 Pod * Runtime 超過 limit，Pod 會被 kubernetes 幹掉，不過我們是 JVM，外部 resource 爆掉前，應該會先因 heap 滿而死。一個施主自盡的感覺。 * CPU 蠻省的，吃比較多是 memory。但也要看你的使用情境 prometheus\n對我們有上 promethues，基本上就是 kafka-exporter 把 kafka metrics 倒出去 prometheus，這個也是詳見下回分解。 # 跑起來了 $ kubectl get po | grep kafka\nNAME READY STATUS RESTARTS AGE kafka-1-0 1/1 Running 0 224d kafka-1-1 1/1 Running 0 224d kafka-1-2 1/1 Running 0 224d kafka-1-exporter-88786d84b-z954z 1/1 Running 0 224d kafka-1-zookeeper-0 1/1 Running 0 224d kafka-1-zookeeper-1 1/1 Running 0 224d kafka-1-zookeeper-2 1/1 Running 0 224d\n","date":1569117521,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"91c33ff59ad95668815c7c944f7eb9ef","permalink":"https://chechia.net/zh-hant/post/2019-09-22-kafka-deployment-on-kubernetes/","publishdate":"2019-09-22T09:58:41+08:00","relpermalink":"/zh-hant/post/2019-09-22-kafka-deployment-on-kubernetes/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。","tags":["鐵人賽2019","kafka","kubernetes","ithome"],"title":"Kafka Deployment on Kubernetes","type":"post"},{"authors":[],"categories":["kubernetes","elasticsearch"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n摘要 簡介 logstash 將 logstash 部屬到 kubernetes 上 設定 logstash pipeline 處理 nginx access log 介紹 Logstash Logstash 是開元的資料處理引擎，可以動態的將輸入的資料做大量的處裡。原先的目的是處理 log ，但目前以不限於處理 log ，各種 ELK beat 或是其他來源的不同監測數據，都能處理。\nLogastash 內部的功能也大多模組化，因此可以組裝不同的 plugin，來快速處理不同來源資料。\n基本上常見的資料來源，logstash 都能夠處理，並且有寫好的 plugin 可以直接使用，細節請見logstash 官方文件\n後送資料庫與最終儲存庫 在開始架設 logstash 要先考慮 pipeline 處理過後送的資料庫，可使用的資料庫非常多，這邊會展示的有：\nELK Stack 標準配備送到 Elasticsearch 存放會時常查詢的熱資料，只存放一段時間前的資料 太舊的資料自動 Rollout 最終 archieving 的資料庫，這邊使用 GCP 的 Big Query 存放查找次數少，但非常大量的歷史紀錄。 Elasticsearch 在前幾篇已經架設好，GCP Big Query 的設定也事先開好。\n部屬 Logstash kubernetes resource 的 yaml 請參考 我的 github elk-kubernetes\nkubectl apply -f config-configmap.yaml kubectl apply -f pipelines-configmap.yam kubectl apply -f deployment.yaml kubectl apply -f service.yaml 放上去的 resource\nconfig-configmap: Logstash 服務本身啟動的設定參數 pipelines-configmap: Logstash 的 pipelines 設定檔案 Lostagh Deployment Logastash 的服務 instance 可以動態 scaling，也就是會有複數 Logstash instance 做負載均衡 Logstash service 可透過 kubernetes 內部的 kube-dns 服務 集群內的 filebeat 可以直接透過 logstash.default.svc.chechiachang-cluster.local 的 dns 連線 logstash 集群內的網路，直接使用 http（當然使用 https 也是可以，相關步驟請見前幾篇文章） 簡單講一下 kubernetes service 的負載均衡，關於 kubernetes service 細節這篇附上文件\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE logstash ClusterIP 10.15.254.47 \u0026lt;none\u0026gt; 5044/TCP 182d $ kubectl get endpoints NAME ENDPOINTS AGE logstash 10.12.0.132:5044,10.12.10.162:5044,10.12.9.167:5044 + 12 more... 182d 在 Kubernetes 內部每個 Pod 都能看到 logstash, logstash.default.svc.chechiachang-cluster.local 這兩個 dns DNS 直接指向複數的 logstash endpoints， 每一個 ip 都是 kubernetes 內部配置的一個 Pod 的 IP，開啟 5044 的 logstash port Service 的 load balance 機制視 service 設定，細節可以看這邊 講到最白，就是 filebeat LOGSTASH URL 設定為 http://logstash 就會打到其中一台 logstash\n更改 filebeat configmap\n$ kubectl edit configmap filebeat-configmap # Disable output to elasticsearch output.elasticsearch: enabled: false # Output to logstash output.logstash: hosts: [\u0026#34;logstash:5044\u0026#34;] protocol: \u0026#34;http\u0026#34; username: \u0026#34;elastic\u0026#34; password: 設定 logstash 這邊要先說，logstash 也支援 centralized configuration，如果你的 logstash 不是跑在 Kubernetes 上，沒辦法配置一套 configmap 就應用到全部的 instance，記的一定要使用。\nLogastash 的運行設定 logstash.yml，這邊我們沒有做設定，都是預設值，有需求可以自行更改\n當然之後要調整 batch size 或是 queue, cache 等等效能調校，也是來這邊改，改完 configmap ，rolling update logstash 就可以。\n這邊主要是來講 pipeline 設定。\n$ kubectl describe configmap pipelines-configmap apiVersion: v1 kind: ConfigMap metadata: name: logstash-pipelines namespace: elk labels: k8s-app: logstash data: # Nginx Template # https://www.elastic.co/guide/en/logstash/7.3/logstash-config-for-filebeat-modules.html#parsing-nginx nginx.conf: | ... Configmap 裡面只有一個 pipeline，就是 nginx.conf，我們這邊就只有一條，這邊一段一段看\nInput input { beats { # The lisening port of logstash port =\u0026gt; 5044 host =\u0026gt; \u0026#34;0.0.0.0\u0026#34; } } 設定 Input 來源，是 beat 從 5044 進來\nFilter 接下來一大段是 filter，每個 filter 中間的 block 都是一個 plugin，logstash 支援非常多有趣的 plugin ，處理不同來源的工作，細節請看這篇\nfilter { # Ignore data from other source in case filebeat input is incorrectly configured. if [kubernetes][container][name] == \u0026#34;nginx-ingress-controller\u0026#34; { # Parse message with grok # Use grok debugger in kibana -\u0026gt; dev_tools -\u0026gt; grok_debugger grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IPORHOST:[nginx][access][remote_ip]} - \\[%{IPORHOST:[nginx][access][remote_ip_list]}\\] - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\u0026#34;%{WORD:[nginx][access][method]} %{DATA:[nginx][access][request_url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\u0026#34; %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\u0026#34;%{DATA:[nginx][access][referrer]}\\\u0026#34; \\\u0026#34;%{DATA:[nginx][access][agent]}\\\u0026#34; %{NUMBER:[nginx][access][request_length]} %{NUMBER:[nginx][access][request_time]} \\[%{DATA:[nginx][access][proxy_upstream_name]}\\] %{DATA:[nginx][access][upstream_addr]} %{NUMBER:[nginx][access][upstream_response_length]} %{NUMBER:[nginx][access][upstream_response_time]} %{NUMBER:[nginx][access][upstream_status]} %{DATA:[nginx][access][req_id]}\u0026#34; } } # Match url parameters if has params grok { match =\u0026gt; { \u0026#34;[nginx][access][request_url]\u0026#34; =\u0026gt; \u0026#34;%{DATA:[nginx][access][url]}\\?%{DATA:[nginx][access][url_params]}\u0026#34; } } # Remove and add fields mutate { remove_field =\u0026gt; \u0026#34;[nginx][access][request_url]\u0026#34; add_field =\u0026gt; { \u0026#34;read_timestamp\u0026#34; =\u0026gt; \u0026#34;%{@timestamp}\u0026#34; } # Add fileset.module:nginx to fit nginx dashboard add_field =\u0026gt; { \u0026#34;[fileset][module]\u0026#34; =\u0026gt; \u0026#34;nginx\u0026#34;} add_field =\u0026gt; { \u0026#34;[fileset][name]\u0026#34; =\u0026gt; \u0026#34;access\u0026#34;} } # Parse date string into timestamp date { match =\u0026gt; [ \u0026#34;[nginx][access][time]\u0026#34;, \u0026#34;dd/MMM/YYYY:H:m:s Z\u0026#34; ] remove_field =\u0026gt; \u0026#34;[nginx][access][time]\u0026#34; } # Split url_parameters with \u0026amp; # /api?uuid=123\u0026amp;query=456 # become # nginx.access.url_params.uuid=123 nginx.access.url_params.query=456 kv { source =\u0026gt; …","date":1569050543,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"cb81847665ce4fb1036e4f4760a2ff02","permalink":"https://chechia.net/zh-hant/post/2019-09-21-logstash-on-gke/","publishdate":"2019-09-21T15:22:23+08:00","relpermalink":"/zh-hant/post/2019-09-21-logstash-on-gke/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.","tags":["鐵人賽2019","elasticsearch","devops","logstash"],"title":"Logstash on GKE","type":"post"},{"authors":[],"categories":["kubernetes","logstash"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。\n官方文件 ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。\n這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：\nnode: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics) cluster: 處理 cluster 等級的 log, event 或是 metrics pod: 針對特定 pod 直接去掛一個 sidecar 上面的方法是可以混搭的，kubernetes 個個層級有log 處理流程，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。\n簡單來說，是去對的地方找對的 log。在架構上要注意 scalability 與 resource 分配，不要影響本身提供服務的 GKE ，但又能獲得盡量即時的 log。\n我們這邊直接進入 kubernetes resource 的設定，底下會附上在 GKE 找 log 的過程。\nNode level log harvest 為每一個 node 配置 filebeat，然後在 node 上面尋找 log，然後如我們上篇所敘述加到 input ，就可以把 log 倒出來。\n直覺想到就是透過 daemonsets 為每個 node 部署一個 filebeat pod，然後 mount node 的 log 資料夾，在設置 input。\nDeploy daemonsets kubernetes resource 的 yaml 請參考 我的 github elk-kubernetes\n給予足夠的 clusterrolebinding 到 elk\nkubectl apply -f filebeat/7.3.1/clusterrolebinding.yaml 先更改 filebeat 的設定，如何設定 elasticsearch 與 kibana，請參考上篇。至於 input 的部份已經配置好了。\nvim filebeat/7.3.1/daemonsets-config-configmap.yaml kubectl apply -f filebeat/7.3.1/daemonsets-config-configmap.yaml 部屬 filebeat daemonsets，會每一個 node 部屬一個 filebeat\nkubectl apply -f filebeat/7.3.1/daemonsets.yaml 取得 daemonsets 的狀態\nkubectl --namespcae elk get pods NAME READY STATUS RESTARTS AGE filebeat-bjfp9 1/1 Running 0 6m56s filebeat-fzr9n 1/1 Running 0 6m56s filebeat-vpkm7 1/1 Running 0 6m56s ... 有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\nlog havest for specific pods 由於 kubernetes 上我們可以便利的調度 filebeat 的部屬方式，這邊也可以也可以使用 deployment ，配合 pod affinity，把 filebeat 放到某個想要監測的 pod，這邊的例子是 nginx-ingress-controller。\nKubernetes 上有一個或多個 nginx ingress controller 部屬一個或多個 filebeat 到有 nginx 的 node 上 filebeat 去抓取 nginx 的 input， 並使用 filebeat 的 nginx module 做預處理 nginx module 預設路徑需要調整，這邊使用 filebeat autodiscover 來處理 一樣 apply 前記得先檢查跟設定\nvim filebeat/7.3.1/nginx-config-configmap.yaml kubectl apply -f filebeat/7.3.1/nginx-config-configmap.yaml 部屬 filebeat deployment 由於有設定 pod affinity ，這個 filebeat 只會被放到有 nginx ingress controller 的這個節點上，並且依照 autodiscover 設定的條件去蒐集 nginx 的 log\nkubectl apply -f filebeat/7.3.1/nginx-deployment.yaml 有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\n另外，由於有啟動 nginx module，logstash 收到的內容已經是處理過得內容。\nGCP fluentd 如果是使用 GKE 的朋友，可以投過開啟 stackdriver logging 的功能，把集群中服務的 log 倒到 stackdriver，基本上就是 node -\u0026gt; (daemonsets) fluentd -\u0026gt; stackdriver。\n這個 fluentd 是 GCP 如果有啟動 Stackdriver Logging 的話，自動幫你維護的 daemonsets，設定不可改，改了會被 overwrite 會去，所以不太方便從這邊動手腳。\nBtw stackdriver 最近好像改版，目前做 example 的版本已經變成 lagency （淚\n但我們先假設我們對這個 pod 的 log 很有興趣，然後把這邊的 log 透過 filebeat 送到 ELK 上XD\n因為 GKE 透過 fluentd 把 GKE 上面的 log 倒到 stackdriver，而我們是想把 log 倒到 ELK，既然這樣我們的 input 來源是相同的，而且很多處理步驟都可以在 ELK 上面互通，真的可以偷看一下 fluentd 是去哪收集 log ，怎麼處理 log pipeline，我們只要做相應設定就好。\n畢竟 google 都幫我們弄得妥妥的，不參考一下他的流程太可惜。\n偷看一下 GKE 上 fluentd 是去哪找 log ，這個是 fluentd gcp configmap，雖然看到這邊感覺扯遠了，但因為很有趣所有我就繼續看下去，各位大德可以跳過XD\nconfigmap 中的這個 input 設定檔，其中一個 source 就是一個資料來源，相當於 filebeat 的 input。這邊這個 source 就是去 /var/log/containers/*.log 收 log\n這邊還做了幾件事：\n打上 reform.* tag，讓下個 match 可以 收進去 pipeline 處理 附帶 parse 出 time containers.input.conf \u0026lt;source\u0026gt; @type tail path /var/log/containers/*.log pos_file /var/log/gcp-containers.log.pos # Tags at this point are in the format of: # reform.var.log.containers.\u0026lt;POD_NAME\u0026gt;_\u0026lt;NAMESPACE_NAME\u0026gt;_\u0026lt;CONTAINER_NAME\u0026gt;-\u0026lt;CONTAINER_ID\u0026gt;.log tag reform.* read_from_head true \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; 他這邊做一些 error handling，然後用 ruby (!) parse，這邊就真的太遠，細節大家可以 google ＸＤ。不過這邊使用的 pattern matching 我們後幾篇在 logstash pipeline 上，也會有機會提到，機制是類似的。\n\u0026lt;filter reform.**\u0026gt; @type parser format /^(?\u0026lt;severity\u0026gt;\\w)(?\u0026lt;time\u0026gt;\\d{4} [^\\s]*)\\s+(?\u0026lt;pid\u0026gt;\\d+)\\s+(?\u0026lt;source\u0026gt;[^ \\]]+)\\] (?\u0026lt;log\u0026gt;.*)/ reserve_data true suppress_parse_error_log true emit_invalid_record_to_error false key_name log \u0026lt;/filter\u0026gt; \u0026lt;match reform.**\u0026gt; @type record_reformer enable_ruby true \u0026lt;record\u0026gt; # Extract local_resource_id from tag for \u0026#39;k8s_container\u0026#39; monitored # resource. The format is: # \u0026#39;k8s_container.\u0026lt;namespace_name\u0026gt;.\u0026lt;pod_name\u0026gt;.\u0026lt;container_name\u0026gt;\u0026#39;. \u0026#34;logging.googleapis.com/local_resource_id\u0026#34; ${\u0026#34;k8s_container.#{tag_suffix[4].rpartition(\u0026#39;.\u0026#39;)[0].split(\u0026#39;_\u0026#39;)[1]}.#{tag_suffix[4].rpartition(\u0026#39;.\u0026#39;)[0].split(\u0026#39;_\u0026#39;)[0]}.#{tag_suffix[4].rpartition(\u0026#39;.\u0026#39;)[0].split(\u0026#39;_\u0026#39;)[2].rpartition(\u0026#39;-\u0026#39;)[0]}\u0026#34;} # Rename the field \u0026#39;log\u0026#39; to a more generic field \u0026#39;message\u0026#39;. This way the # fluent-plugin-google-cloud knows to flatten the field as textPayload # instead of jsonPayload after extracting \u0026#39;time\u0026#39;, \u0026#39;severity\u0026#39; and # \u0026#39;stream\u0026#39; from the record. message ${record[\u0026#39;log\u0026#39;]} # If \u0026#39;severity\u0026#39; is not set, assume stderr is ERROR and stdout is INFO. severity ${record[\u0026#39;severity\u0026#39;] …","date":1568883989,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"c82138a78b70f3c89d82c9eb81002374","permalink":"https://chechia.net/zh-hant/post/2019-09-19-monitoring-gke-with-elk/","publishdate":"2019-09-19T17:06:29+08:00","relpermalink":"/zh-hant/post/2019-09-19-monitoring-gke-with-elk/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.","tags":["鐵人賽2019","kubernetes","logstash","ithome","filebeat","fluentd"],"title":"Monitoring GKE With Elk","type":"post"},{"authors":[],"categories":["kubernetes","logstash"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\nELK 的 beats 是輕量級的系統監測收集器，beats 收集到的 data 經過 mapping 可以送到 Elasticsearch 後，進行彈性的搜尋比對。\nbeat 有許多種類，依據收集的 data 區別：\nAuditbeat: Audit data Filebeat: Log files Functionbeat: Cloud data Heartbeat: Availability Journalbeat: Systemd journals Metricbeat: Metrics Packetbeat: Network traffic Winlogbeat: Windows event logs 這邊先以 filebeat 為例，在 GCE 上收集圓端服務節點上的服務日誌與系統日誌，並在 ELK 中呈現。\nInstallation 安裝及 filebeat 安全性設定的步驟，在這篇Secure ELK Stack 中已經說明。這邊指附上連結，以及官方文件 提供參考。\nConfiguration 這邊談幾個使用方面的設定。\n首先，apt 安裝的 filebeat 預設的 /etc/filebeat/filebeat.yml 不夠完整，我們先到 github 把對應版本的完整載下來。\nwget https://raw.githubusercontent.com/elastic/beats/master/filebeat/filebeat.reference.yml sudo mv filebeat.reference.yml /etc/filebeat/filebeat.yml Beats central management beats 透過手動更改 config 都可以直接設定，但這邊不推薦在此設定，理由是\n系統中通常會有大量的 filebeat，每個都要設定，數量多時根本不可能 更改設定時，如果不一起更改，會造成資料格式不統一，之後清理也很麻煩 推薦的方式是透過 Kibana 對所有 filebeat 做集中式的的管理配置，只要初始設定連上 kibana，剩下的都透過 kibana 設定。文件在此，我們有空有可以分篇談這個主題。\n不過這邊還是待大家過一下幾個重要的設定。畢竟要在 kibana 上配置，filebeat 的設定概念還是要有。\nmodules filebeat 有許多模組，裡面已經包含許多預設的 template ，可以直接使用 default 的設定去系統預設的路徑抓取檔案，並且先進一步處理，減少我們輸出到 logstash 還要再做 pipeline 預處理，非常方便。\n例如這個 system module 會處理系統預設的 log 路徑，只要開啟 module ，就會自動處理對應的 input。\n- module: system syslog: enabled: true 剩下的就是照需求啟用 module ，並且給予對應的 input。\nELK 為自己的服務設定了不少 module ，直接啟用就可以獲取這協服務元件運行的 log 與監測數值。這也是 self-monitoring 監測數據的主要來源。\n- module: kibana - module: elasticsearch - module: logstash ... input filebeat 支援複數 inputs，每個 input 會啟動一個收集器，而 filebeat 收集目標是 log 檔案。基本上可以簡單理解為 filebeat 去讀取這些 log 檔案，並且在系統上紀錄讀取的進度，偵測到 log 有增加，變繼續讀取新的 log。\nfilebeat 具體的工作機制，可以看這篇How Filebeat works?\n這篇文件也提到 filebeat 是確保至少一次(at-least-once delivery)的數據讀取，使用時要特別注意重複獲取的可能。\n首先把 input 加上 ubuntu 預設的 log 路徑\nfilebeat.inputs: - type: log enabled: true paths: - /var/log/*.log 這邊注意 input 支援多種 type，參照完整設定檔案的說明配合自己的需求使用。\nProcessor 在 filebeat 端先進行資料的第一層處理，可以大幅講少不必要的資料，降低檔案傳輸，以及對 elasticsearch server 的負擔。\noutput output 也是 filebeat 十分重要的一環，好的 filebeat output 設定，可以大幅降低整體 ELK stack 的負擔。壞的設定也會直接塞爆 ELK stask。\noutput.elasticsearch: 直接向後送進 elasticsearch output.logstash: 先向後送到 logstash\n這邊非常推薦大家，所有的 beat 往後送進 elasticsearch 之前都先過一層 logstash，就算你的 logstash 內部完全不更改 data，沒有 pipeline mutation，還是不要省這一層。\nbeat 的數量會隨應用愈來越多而線性增加，elasticsearch 很難線性 scale，或是 scale 成本很大 filebeat 沒有好好調校的話，對於輸出端的網路負擔很大，不僅佔用大量連線，傳輸檔案的大小也很大。 logstash 的 queue 與後送的 batch 機制比 filebeat 好使用 filebeat 是收 log 的，通常 log 爆炸的時候，是應用出問題的時候，這時候需要 log 交叉比對，發現 elasticsearch 流量也爆衝，反應很應用 logstash 透過一些方法，可以很輕易的 scale，由於 pipeline 本身可以分散是平行處理，scale logstash 並不會影響資料最終狀態。 load balance 有網友留言詢問 logstash 前面的 load balance 如何處理比較好，我這邊也順便附上。不只是 logstash ，所有自身無狀態(stateless) 的服務都可以照這樣去 scale。\n在 kubernetes 上很好處理，使用 k8s 預設的 service 就輕易作到簡易的 load balance\n設置複數 logstash instances 使用 kubernetes 內部網路 service 實現 load balancing。 在 GCE 上實現的話，我說實話沒實作過，所以以下是鍵盤實現XD。\n官方文件 建議使用 beats 端設定多個 logstash url 來做 load balancing。\n但我不是很喜歡 beat 去配置多個 logstash url 的作法：beat 要感知 logstash 數量跟 url ，增加減少 logstash instance 還要更改 beats 配置，產生配置的依賴跟耦合。\n最好是在 logstash 前過一層 HAproxy 或是雲端服務的 Load balancer（ex. GCP https/tcp load balancer），beat 直接送進 load balance 的端點。\nautodiscover 如果有使用 container ，例如 docker 或 kubernetes，由於 container 內的 log 在主機上的位置是動態路徑，這邊可以使用 autodiscover 去尋找。\n在 kubernetes 上面的設定，之後會另開一天討論。\ndashboard kibana 預設是空的，沒有預先載入 dashboard，但我們會希望資料送進去，就有設定好的 dashboard ，圖像化把資料呈現出來。這部份需要從 beat 這邊向 kibana 寫入。\n在上面的部份設定好 kibana 的連線資料，沒有設定的話 beat 啟動會警告。\nsetup.dashboards.enabled: true 一起中就會檢查 kibana 是否有匯入 dashboard，沒有的話就匯入。\n也會一併匯入 modules 的 dashboard，例如如果有啟用 nginx module 處理 nginx 的 access log，nginx module 會處理 request source ip ，並透過 geoip database, 將 ip 轉會成經緯度座標。這時如果在 kibana 上有匯入 nginx dashboard，就可以看到圖像化的全球 request 分佈圖。\n小結 取得完整 filebeat 設定檔案並設定 filebeat 盡量透過 beat central management 來管理 beat 的設定檔 啟用對應 module 來更優雅的處理 log 後送到 elasticsearch 前的資料都必須經過精細的處理，送進去後就不好刪改了 ","date":1568805050,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"a17489478b4b0757bc9a9a46d28cf313","permalink":"https://chechia.net/zh-hant/post/2019-09-18-monitoring-gce-with-elk/","publishdate":"2019-09-18T19:10:50+08:00","relpermalink":"/zh-hant/post/2019-09-18-monitoring-gce-with-elk/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.","tags":["鐵人賽2019","elasticsearch","devops","logstash","ithome"],"title":"Monitoring GCE With ELK","type":"post"},{"authors":[],"categories":["kubernetes","elasticsearch"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n有板友問到，要如何選擇要不要用 ELK，其實也這是整篇 ELK 的初衷。這邊分享一下 ELK 與其他選擇，以及選擇解決方案應該考慮的事情。\n其他常用的服務 Prometheus: 開源的 time series metrics 收集系統\nStackdriver: GCP 的 log 與 metrics 平台\nElastic Cloud: ELK 的 Sass\nSelf-hosted ELK\n或是依照需求混搭，各個服務使用的各層套件是可以相容，例如\n在 GKE 上不用 beat 可以用 fluentd\nPrometheus -\u0026gt; Stackdriver\nELK -\u0026gt; Stackdriver\nFluentd -\u0026gt; Prometheus …\nSass vs cloud self-hosted vs on-premised\nMetrics: ELK vs Prometheus vs Stackdriver\nLogging: ELK vs Stackdriver\n取捨原則 各個方法都各有利弊，完全取決於需求\n已知條件限制，例如安全性考量就是要放在私有網路防火牆內，或是預算 資料讀取方式，有沒有要交叉比對收集的資料，還是單純依照時間序查詢 或是資料量非常大，應用數量非常多 維護的團隊，有沒有想，或有沒有能力自己養 self-host 服務 Sass vs Self-hosted vs On-premised Sass: 指的是直接用 Elasitc Cloud，或是直接使用公有雲的服務(ex. 在 GCP 上使用 stackdriver)\nCloud Self-hosted: 在公有雲上使用 ELK\nOn-Premised: 自己在機房搭設\n安全性 看公司的安全政策，允許將日誌及監控數據，送到私有網路以外的地方嗎？如果在防火牆內，搞不好 port 根本就不開給你，根本不用考慮使用外部服務。\n要知道服務的 log 其實可以看出很多東西．如果有特別做資料分析，敏感的資料，金流相關數據，通常不會想要倒到第三方服務平台。\n可能有做金流的，光是安全性這點，就必須選擇自架。\n成本 金錢成本 + 維護成本\n金錢成本就看各個服務的計費方式\nElastic Cloud Pricing Self-hosted ELK \u0026amp; Prometheus：機器成本 公有雲服務(ex. GCP Stackdriver): 用量計費 維護成本: 工程師的月薪 * 每個月要花在維護服務的工時比例\n一般 Sass 代管的服務，會降低維護成本，基本上就是做到網頁點一點就可以用。\n如果公司有完整的維護團隊，有機房，服務的使用量也很大，當然 self-hosted 是比較省。 中小型企業以及新創，服務在公有雲上的，直接使用Sass 服務往往比較節省成本，服務直接由 Sass 維護，節省很多機器上管理跟日常維護。\n避免迷思，買外部服務的帳單是顯性的，報帳時看得到，而工程師維護的時間成本是隱性的。self-host 可能省下 Sass 費用，但工程因為分了時間去維護，而影響進度。這部分就看團隊如何取捨。\n易用性 如果應用都跑在公有雲上，可以考慮使用雲平台提供的監測服務，使用便利，而且整合度高。ex GCP 上，要啟用 Stackdriver 是非常輕鬆的事情，只是改一兩個選項，就可以開啟 / 關閉 logging 與 metrics\n如果是 On-premised 自家機房，也許 self-hosted 會更為適合。\n客製化程度 在大多數時候，沒有需要更改到服務的核心設定，都可以不可律客製化程度，直接使用 Sass 的設定，就能滿足大部分需求。可以等有有明確需求後再考慮這一點。短期內沒有特殊需求就可以從簡使用。\n使用GKE 到 Stackdriver 的話，對主機本身的機器是沒有控制權的，執行的 pipeline 也不太能更改 Elastic Cloud 有提供上傳 elasticsearch config 檔案的介面，也就是可以更改 server 運行的參數設定 Self-Hosted 除了上述的設定，還可以依照需求更改 ELK / prometheus 服務，在實體機器上的 topology，cpu 記憶體的資源配置，儲存空間配置等，可以最大化機器的效能。\nScalability 資料流量大，儲存空間消耗多，服務負擔大，可能就會需要擴展。\n一個是資料量的擴展。一個是為了應付服務的負擔，對 ELK 服務元件做水平擴展。\n除了 elasticsearch 以爲的元件，例如 kibana，apm-server, beats 都可以透過 kubernetes 輕易的擴展，唯有 elasticsearch ，由於又牽扯上述資料量的擴展，以及分佈，還有副本管理，index 本身的 lifecycle 管理。Elasticsearch 的 scaling 設定上是蠻複雜的，也有很多工要做。index 的 shards / replicas 設定都要注意到。否則一路 scale 上去，集群大的時候彼此 sharding sync 的效能消耗是否會太重。\nStackdriver 從使用者的角度，是不存在服務節點的擴展問題，節點的維護全都給 Sass 管理。資料量的擴展問題也不大，只要整理資料 pipeline，讓最後儲存的資料容易被查找。\nTimeseries vs non-timeseriese Prometheus 是自帶 time series database，stackdriver 也是 time series 的儲存。ELK 的 elasticsearch 是全文搜索引擎，用了 timestamp 做分析所以可以做到 time series 的資料紀錄與分析。這點在本質上是完全不同的。\n光只處理 time series data，Prometheus 的 query 效能是比 elasticsearch 好很多 Elasticsearch 有大量的 index 維護，需要較多系統資源處理，在沒有 query 壓力的情形下會有系統自動維護的效能消耗 ELK 的資料不需要預先建模，就可以做到非常彈性的搜尋查找。Stackdriver 的話，無法用未建模的資料欄位交叉查找。 Log 收集方面 Elasticsearch 中的資料欄位透過 tempalte 匯入後，都是有做 index ，所以交叉查找，例如可以從 log text 中包含特定字串的紀錄，在做 aggregate 算出其他欄位的資料分佈。會比較慢，但是是做得到的全文搜索 Stackdriver 可以做基本的 filter ，例如 filter 某個欄位，但不能做太複雜的交叉比對，也不能針對 text 內容作交互查找，需要換出來另外處理。 Metrics 收集方面 (同上) Elasticsearch 可以用全文搜索，做到很複雜的交叉比對，例如：從 metrics 數值，計算在時間範圍的分佈情形(cpu 超過 50% 落在一天 24 小時，各個小時的次數) Stackdriver 只能做基本的 time series 查找，然後透過預先定義好的 field filter 資料，再各自圖像化。 Prometheus 也是必須依照 time series 查找，語法上彈性比 stackdriver 多很多，但依樣不能搜尋沒有 index 的欄位 這邊要替別提，雖然 Elasticsearch 能用全文搜索輕易地做到複雜的查詢語法，但以 metrics 來說，其實沒有太多跳脫 time series 查找的需求。能做到，但有沒有必要這樣做，可以打個問號。 個人心得，如果驗證全新的 business model，或是還不確定的需求，可以使用 ELK 做各種複雜的查詢\n如果需求明確，收進來的 log 處理流程都很明確，也許不用使用 ELK。\n論系統資源 CP 值以及效能，time series 的 db 都會比 Elasticsearch 好上不少。 Elasticsearch 中也不太適合一直存放大量的資料在 hot 可寫可讀狀態，繪希好很多系統資源。 其他服務 Elastic 有出許多不同的增值服務\nApplication Performance Monitoring(APM) Realtime User Monitoring(RUM) Machine Learning(ELK ML) 而 ELK 以外也都有不同的解決方案，例如\nGCP 也出了自己的 APM Sass Google Analytics(GA) 不僅能做多樣的前端使用者行為分析，還能整合 Google 收集到的使用者行為，做更多維度的分析 相較之下 ELK 在這塊其實沒有特別優勢。\nElastic Cloud 我這邊要特別說 Elastic Cloud vs ELK\nElatic Cloud 的運行方式，是代為向公與恩平台(aaws, gcp,…)，帶客戶向平台租用機器，然後把 ELK 服務部署到租用的機器上。用戶這邊無法直接存取機器，只能透過 ELK 介面或是 Kibana , API 進入 ELK。Elastic Cloud 會監控無誤節點的狀況，並做到一定程度的代管。\n這邊指的一定程度的代管，是 Elastic Cloud 只是代為部署服務，監控。有故障時並不負責排除，如果 ELK 故障，簡單的問題（ex. 記憶體資源不足）會代為重開機器，但如果是複雜的問題，還是要用戶自己處理．但是用戶又沒有主機節點的直接存取權限，所以可能會造成服務卡住無法啟動，只能透過 Elastic Cloud 的管理介面嘗試修復。\n使用服務除了把服務都架設完以外，還是需要定期要花時間處理 performance tuning，設定定期清理跟維護。包括 kafka, redis, mongoDB, cassandra, SQLs…都是一樣，架構越複雜，效能要求越高，這部分的工都會更多。如果公司有 DBA，或是專職維護工程師，那恭喜就不用煩惱。\nElasticsearch server 目前用起來，算是是數服務中，維護上會花比較多時間的服務。\n因為引擎本身設計的架構，並不是很多人都熟悉。在使用ELK同時，對ELK底層引擎的運作流程有多熟悉，會直接影響穩定性跟跑出來的效能。 需要好好處理設計資料的儲存，如果使用上沒處理好，會直接讓整個ELK 掛掉。 然後產品本身的維護介面，目前只是在堪用，許多重要的功能也還在開發中。 如果公司有人會管 ELK，個人建議是可以 self-host\n小結 弄清楚需求，如果沒有特殊需求可以走 general solution Sass vs Self-hosted vs On-premised Time series vs non time series ","date":1568803900,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"87215dda18fd365acbb8f996f725c08c","permalink":"https://chechia.net/zh-hant/post/2019-09-18-elastic-or-not-elastic/","publishdate":"2019-09-18T18:51:40+08:00","relpermalink":"/zh-hant/post/2019-09-18-elastic-or-not-elastic/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.","tags":["鐵人賽2019","elasticsearch","devops"],"title":"ELK or Not ELK","type":"post"},{"authors":[],"categories":["kubernetes","certificate"],"content":"簡單講一下 certificate X.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)\ncertificate 包含公鑰及識別資訊(hostname, organization, …等資訊)\ncertificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)\n使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site\n在 elastic stack 上，如果有多個 elasticsearch server node 彼此連線，由於 node 彼此是 client 也是 server\n使用 self-signed CA 產出來的 certificate，連入時會檢查使用的 certificate 是否由同一組 CA 簽署 server 使用 certificate，確定連入 server 的 client 都帶有正確的私鑰與 public certificate，是 authenticated user 附帶說明，X.509 有多種檔案格式\n.pem .cer, .crt, .der .p12 .p7b, .p7c … 另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證\nCA $ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys MAC: sha1, Iteration 100000 MAC length: 20, salt length: 20 PKCS7 Data Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000 PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000 Certificate bag Bag Attributes friendlyName: ca localKeyID: subject=CN = Elastic Certificate Tool Autogenerated CA issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- issuer command name 為 Elastic autogen CA subject command name 為 Elastic autogen CA\nhttps://shazi.info/openssl-%E6%AA%A2%E6%B8%AC-ssl-%E7%9A%84%E6%86%91%E8%AD%89%E4%B8%B2%E9%8D%8A-certificate-chain/\nopenssl s_client -connect google.com https://medium.com/@superseb/get-your-certificate-chain-right-4b117a9c0fce\nopenssl verify -CAfile client-ca.cer client.cer openssl verify -show_chain -CAfile client-ca.cer client.cer Certificate 用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-certificates.p12 -info -nokeys MAC: sha1, Iteration 100000 MAC length: 20, salt length: 20 PKCS7 Data Shrouded Keybag: pbeWithSHA1And3-KeyTripleDES-CBC, Iteration 50000 PKCS7 Encrypted data: pbeWithSHA1And40BitRC2-CBC, Iteration 50000 Certificate bag Bag Attributes friendlyName: elk.asia-east1-b.c.machi-x.internal localKeyID: subject=CN = elk.asia-east1-b.c.machi-x.internal issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- Certificate bag Bag Attributes friendlyName: ca 2.16.840.1.113894.746875.1.1: \u0026lt;Unsupported tag 6\u0026gt; subject=CN = Elastic Certificate Tool Autogenerated CA issuer=CN = Elastic Certificate Tool Autogenerated CA -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- ","date":1568686536,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"6404fe1bfa57bcb3873536bc50dedd6a","permalink":"https://chechia.net/zh-hant/post/2020-09-17-x.509-certificate/","publishdate":"2019-09-17T10:15:36+08:00","relpermalink":"/zh-hant/post/2020-09-17-x.509-certificate/","section":"post","summary":"簡單講一下 certificate X.509 是公鑰憑證(public key certificate) 的一套標準，用在很多網路通訊協定 (包含 TLS/SSL)\ncertificate 包含公鑰及識別資訊(hostname, organization, …等資訊)\ncertificate 是由 certificate authority(CA) 簽署，或是自簽(Self-signed)\n使用 browser 連入 https server時，會檢查 server 的 certificate 是否有效，確定這個 server 真的是合法的 site","tags":["kubernetes","tls","certificate"],"title":"X.509 certificate","type":"post"},{"authors":[],"categories":["kubernetes","elasticsearch"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n–\n上篇Self-host ELK stack on GCP 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。\n官方的文件在這裡，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的Tutorial: Getting started with security 開始，過程比較不會這麼血尿。\n總之為了啟用 authentication \u0026amp; https，這篇要做的事情：\nenable x-pack \u0026amp; activate basic license Generate self-signed ca, server certificate, client certificate Configure Elasticsearch, Kibana, \u0026amp; other components to use server certificate when act as server use client certificate when connect to an ELK server 啟用 X-pack Elasticsearch 的安全性模組由 x-pack extension 提供，在 6.3.0 之後的版本，安裝 elasticsearch 的過程中就預設安裝 x-pack。\n附上啟用的官方文件\n然而，由於舊版的 x-pack 是付費內容，目前的 elasticsearch 安裝完後，elasticsearch.yml 設定預設不啟用 x-pack，也就是說沒看到這篇官方文件的話，很容易就獲得沒有任何 security 功能的 ELK。\n雖然目前已經可以使用免費的 basic license 使用 security 功能，還是希望官方可以 default 啟用 security。\n$ sudo vim /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: true xpack.license.self_generated.type: basic discovery.type: single-node 我們這邊啟用 xpack.security，同時將 self-generated license 生出來，我們這邊只使用基本的 basic subscription。若希望啟用更多功能，可以看官方subcription 方案介紹\n另外，如果不同時設定為 single-node 的話，預設會尋找其他elasticsearch node 來組成 cluster，而我們就必須要在所有 node 上啟用 security，這篇只帶大家做一個 single node cluster，簡化步驟。\n重啟 elasticsearch ，檢查 log，看啟動時有沒有載入 x-pack\nsudo systemctl restart elasticsearch $ tail -f /var/log/elasticsearch/elasticsearch.log [2019-09-16T07:39:49,467][INFO ][o.e.e.NodeEnvironment ] [elk] using [1] data paths, mounts [[/mnt/disks/elk (/dev/sdb)]], net usable_space [423.6gb], net total_space [491.1gb], types [ext4] [2019-09-16T07:39:49,474][INFO ][o.e.e.NodeEnvironment ] [elk] heap size [3.9gb], compressed ordinary object pointers [true] [2019-09-16T07:39:50,858][INFO ][o.e.n.Node ] [elk] node name [elk], node ID [pC22j9D4R6uiCM7oTc1Fiw], cluster name [elasticsearch] [2019-09-16T07:39:50,866][INFO ][o.e.n.Node ] [elk] version[7.3.1], pid[17189], build[default/deb/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/4.15.0-1040-gcp/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/12.0.2/12.0.2+10] [2019-09-16T07:39:50,878][INFO ][o.e.n.Node ] [elk] JVM home [/usr/share/elasticsearch/jdk] ... [2019-09-16T07:39:59,108][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-ccr] [2019-09-16T07:39:59,109][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-core] ... [2019-09-16T07:39:59,111][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-logstash] [2019-09-16T07:39:59,113][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-voting-only-node] [2019-09-16T07:39:59,114][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-watcher] [2019-09-16T07:39:59,115][INFO ][o.e.p.PluginsService ] [elk] no plugins loaded [2019-09-16T07:40:07,964][INFO ][o.e.x.s.a.s.FileRolesStore] [elk] parsed [0] roles from file [/etc/elasticsearch/roles.yml] [2019-09-16T07:40:10,369][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elk] [controller/17314] [Main.cc@110] controller (64 bit): Version 7.3.1 (Build 1d93901e09ef43) Copyright (c) 2019 Elasticsearch BV [2019-09-16T07:40:11,776][DEBUG][o.e.a.ActionModule ] [elk] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2019-09-16T07:40:14,396][INFO ][o.e.d.DiscoveryModule ] [elk] using discovery type [single-node] and seed hosts providers [settings] [2019-09-16T07:40:16,222][INFO ][o.e.n.Node ] [elk] initialized [2019-09-16T07:40:16,224][INFO ][o.e.n.Node ] [elk] starting ... [2019-09-16T07:40:16,821][INFO ][o.e.t.TransportService ] [elk] publish_address {10.140.0.10:9300}, bound_addresses {[::]:9300} [2019-09-16T07:40:16,872][INFO ][o.e.c.c.Coordinator ] [elk] cluster UUID [1CB6_Lt-TUWEmRoN9SE49w] [2019-09-16T07:40:17,088][INFO ][o.e.c.s.MasterService ] [elk] elected-as-master ([1] nodes joined)[{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 921, reason: master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]} [2019-09-16T07:40:17,819][INFO ][o.e.c.s.ClusterApplierService] [elk] master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}, term: 9, version: 921, reason: Publication{term=9, version=921} [2019-09-16T07:40:17,974][INFO ][o.e.h.AbstractHttpServerTransport] [elk] publish_address {10.140.0.10:9200}, bound_addresses {[::]:9200} …","date":1568559633,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"251492a4ac45b0e56225488343f29cc8","permalink":"https://chechia.net/zh-hant/post/2019-09-15-secure-elk-stack/","publishdate":"2019-09-15T23:00:33+08:00","relpermalink":"/zh-hant/post/2019-09-15-secure-elk-stack/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 對我的文章有興趣，歡迎到我的網站上 https://chechia.","tags":["鐵人賽2019","tls","elasticsearch","devops","logstash"],"title":"Secure Elk Stack","type":"post"},{"authors":[],"categories":["kubernetes","elasticsearch"],"content":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n–\n簡介 ELK stack 官方說明文件\nELK 的元件 Elasticsearch: 基於 Lucene 的分散式全文搜索引擎 Logstash: 數據處理 pipeline Kibana: ELK stack 的管理後台與數據視覺化工具 Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics) ELK 的工作流程 beats -\u0026gt; (logstash) -\u0026gt; elasticsearch -\u0026gt; kibana\n將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起 設定 beats 從指定的路徑收集 log 與 metrics 設定 beats 向後輸出的遠端目標 (Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據 透過 Kibana，將 elasticsearch 上的 log 顯示 官方不是有出文件嗎 Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。\n說穿了本篇的技術含量不高，就只是一個踩雷過程。\nLets get our hands dirty.\nWARNING 這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在v6.3.0 以前的版本是不包含安全性模組的，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。\n我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為\n另起一篇對安全性設定多加說明 在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯 雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。\n如果希望第一次安裝就有完整的 security 設定，請等待下篇 Secure ELK Stask\n討論需求與規格 這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。\nelk-node-standalone 10.140.0.10 app-node-1 10.140.0.11 ... ... 本機的 ELK stack 元件，彼此透過 localhost 連線\nElasticsearch: localhost:9200 Kibana: localhost:5601 Apm-server: localhost:8200 Self Monitoring Services 私有網路中的外部服務透過 10.140.0.10\nbeats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200 beats 從其他 node 輸出到 Apm-server: 10.140.0.10:8200 在內部網路中 透過 browser 存取 Kibana: 10.140.0.10:5601 standalone 的好處:\n方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰) 最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分 Standalone可能造成的問題:\nNo High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛 外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。 如果要有 production ready 的 ELK\nHA 開起來 把服務分散到不同 node 上, 方便之後 scale out 多開幾台 elasticsearch-1, elasticsearch-2, elasticsearch-3… kibana-1 apm-server-1, apm-server-2, … 如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇 主機設定 Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。\nGCP 上使用 Google Compote Engine 的朋友，可以照 Google 官方操作步驟操作\n完成後接近這樣\n$ df -h $ df --human-readable Filesystem Size Used Avail Use% Mounted on /dev/sda1 9.6G 8.9G 682M 93% / /dev/sdb 492G 63G 429G 13% /mnt/disks/elk $ ls /mnt/disks/elk /mnt/disks/elk/elasticsearch /mnt/disks/elk/apm-server /mnt/disks/elk/kibana 至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。\n開防火牆 需要開放 10.140.0.10 這台機器的幾個 port\nelasticsearch :9200 來源只開放私有網路其他 ip 10.140.0.0/9 apm-server :8200 (同上) kibana :5601 (同上)，如果想從外部透過 browser開，需要 whitelist ip GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線\ndefault-allow-internal: :all :10.140.0.0/9 tcp:0-65535 Install Elasticsearch Install Elasticsearch 官方文件 7.3\n我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝\nsudo apt-get install apt-transport-https wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - add-apt-repository \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; sudo apt-get update sudo apt-get install elasticsearch 安裝完後路徑長這樣\n/etc/elasticsearch /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/jvm.options # Utility /usr/share/elasticsearch/bin/ # Log /var/log/elasticsearch/elasticsearch.log 有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs\n服務控制 透過 systemd 管理，我們可以用 systemctl 控制， 用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。\n但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。\nmkdir -p /mnt/disks/elk/elasticsearch chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch 設定檔案 ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案\nsudo vim /etc/elasticsearch/elasticsearch.yml # Change Network network.host: 0.0.0.0 # Change data path path.data: /mnt/disks/elk/elasticsearch vim /etc/elasticsearch/jvm-options # Adjust heap to 4G -Xms4g -Xmx4g # Enable xpack.security discovery.seed_hosts: [\u0026#34;10.140.0.10\u0026#34;] discovery.type: \u0026#34;single-node\u0026#34; xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.license.self_generated.type: basic 6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。\n有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。\n/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto # Keep your passwords safe 然後把啟動 Elasticsearch\nsudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service 看一下 log，確定服務有在正常工作\ntail -f /var/log/elasticsearch/elasticsearch.log 在 node 上試打 Elasticsearch API\n$ curl localhost:9200 { \u0026#34;name\u0026#34; : \u0026#34;elk\u0026#34;, \u0026#34;cluster_name\u0026#34; : \u0026#34;elasticsearch\u0026#34;, \u0026#34;cluster_uuid\u0026#34; : \u0026#34;uiMZe7VETo-H6JLFLF4SZg\u0026#34;, \u0026#34;version\u0026#34; : { …","date":1568518983,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"89b94a9f78ad2ba6feb1029b40b9f2b2","permalink":"https://chechia.net/zh-hant/post/2019-09-15-self-host-elk-stack-on-gcp/","publishdate":"2019-09-15T11:43:03+08:00","relpermalink":"/zh-hant/post/2019-09-15-self-host-elk-stack-on-gcp/","section":"post","summary":"2020 It邦幫忙鐵人賽 系列文章\nSelf-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP 作為範例的 ELK 的版本是當前的 stable release 7.","tags":["鐵人賽2019","gcp","tls","elasticsearch","devops","logstash"],"title":"Self-host ELK stack - Installation","type":"post"},{"authors":[],"categories":["kubernetes"],"content":"各位好，我是Che-Chia Chang，社群上常用的名子是 David Chang。是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 Golang Taiwan Meetup 的 organizer。\n受到友人們邀請（推坑）參加了2020 It邦幫忙鐵人賽，挑戰在30天內，每天發一篇技術分享文章。一方面將工作上遇到的問題與解法分享給社群，另一方面也是給自己一點成長的壓力，把這段時間的心得沈澱下來，因此也了這系列文章。\n本系列文章重點有三：\n提供的解決方案，附上一步步的操作步驟。希望讓讀者可以重現完整操作步驟，直接使用，或是加以修改\n著重 Google Cloud Platform，特別是Google Compute Engine (GCE) 與Google Kubernetes Engine (GKE) 兩大服務。這也是我最熟悉的平台，順便推廣，並分享一些雷點。\n從維運的角度除錯，分析問題，提升穩定性。\n預定的主題如下（可能會依照實際撰寫狀況微調）\nELK Stask on GCP (8) Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 在 GKE 上部署 Redis HA (5) 使用 helm 部署 redis-ha Redis HA with sentinel Redis sentinel topology Redis HA with HAproxy Redis HA Failure Recovery Prometheus Metrics Exporter Prometheus / Grafana (5) GKE 上自架 Prometheus GKE 上自架 Grafana scrape config \u0026amp; exporter Dive into Redis Exporter 輸出 kube-state 的監測數據 Nginx Ingress (3) Deploy Nginx Ingress Controller Configure Nginx Ingress Cert-manager (3) Deploy cert-manager How cert-manager work Cert-manager complete workflow Kubernetes CRD \u0026amp; Operator-sdk (3) Introduction about custom resource Deployment \u0026amp; Usage Deployment \u0026amp; Usage 文章發表於鐵人挑戰頁面，同時發布與本站備份。有任何謬誤，還煩請各方大德\u0026lt;3透過底下的聯絡方式聯絡我，感激不盡。\nFeatures\nstep-by-step guide for deployment: guarentee a running deployment on GCP basic configuration, usage, monitoring, networking on GKE debugging, stability analysis in an aspect of devop Topics\nELK stack(8) Deploy self-hosted ELK stack on GCE instance Secure ELK stack with SSL and role-based authentication Monitoring services on Kubernetes with ELK beats Monitoring services on GCE instances Logstash pipelines and debugging walk through Elasticsearch operations: house-cleaning, tuning, pernament storage Elasticsearch maitainence, trouble shooting Get-Started with Elastic Cloud SASS General operations on Kubernetes(4) Kubernetes Debug SOP Kubectl cheat sheet Secure services with SSL by cert-manager Speed up container updating with operator My operator example Deploy Kafka HA on Kubernetes(4) deploy kafka-ha on Kubernertes with helm in-cluster networking configuration for high availability basic app-side usage, performance tuning Operate Kafka: update config, upgrade version, migrate data Promethus / grafana(5) Deploy Prometheus / Grafana stack on GCE instance Monitoring services on Kubernetes with exporters Export Kubernetes metrics to Prometheus Export Redis-ha metrics to Prometheus Export Kafka metrics to Prometheus GCP networking(4) Firewall basic concept for private network with GCE instances \u0026amp; Kubernetes Load balancer for Kubernetes service \u0026amp; ingress DNS on GCP from Kube-dns to GCP DNS service GCP log management(3) Basic usage about GCP logging \u0026amp; GCP Error Report Stackdriver, metrics, alerts Logging on GKE from gcp-fluentd to stackdriver ","date":1568019363,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"66dad33329f6f9234dead62f90905865","permalink":"https://chechia.net/zh-hant/post/2019-09-09-ithome-ironman-challenge/","publishdate":"2019-09-09T16:56:03+08:00","relpermalink":"/zh-hant/post/2019-09-09-ithome-ironman-challenge/","section":"post","summary":"2019 IT邦幫忙鐵人賽","tags":["鐵人2019","kubernetes","elk","kafka","redis","nginx","cert-manager","crd"],"title":"2019 IT邦幫忙鐵人賽","type":"post"},{"authors":[],"categories":null,"content":"各位好，我是Che-Chia Chang，專長 DevOps \u0026amp; Kubernetes\n今年不保證不爛尾的 2021 鐵人挑戰頁面 第一手消息發布，免費線上諮詢，請見 FB 往年回顧 4 萬餘字的血淚 30 日 - 2020 鐵人挑戰 優等 回到 2021 年，本系列會寫得很隨興，讀者有幫助為目的撰寫，\n如果有興趣的題目，或是好奇想問問題，歡迎底下留言，或到我的 FB 私訊，我都會一對一回復。基於「取之社群回饋社群」的精神，諮詢聊天都是永遠免費。\n基於明確需求，討論解決方案 跟 Kubernetes 有關 提供手把手 SOP 喜歡也請幫我點個讚，或是留言讓我知道，讓我有點動力繼續寫，大幅降低我偷懶爛尾的機率(XD)\n預定的主題（可能會微調或大改XD）\n好文翻譯分享 Borg, Omega, Kubernetes - Google 容器化開發十年經驗，Kubernetes 的前日今生 公有雲省錢大作戰 - 我這邊有一批便宜的好 VM 打三折賣你，Preemptible / Spot Instance 先占節點實戰分享 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，原理簡介與規格 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，適用案例實戰分析 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，使用範例 公有雲省錢大作戰 - Preemptible / Spot Instance 先占節點，經驗分享，雷點 公有雲省錢大作戰 - 我跑了一個 bot，24 小時上線， GCP 一天收你 4 元台幣 大家都來用 Terraform，Infrastructure as Code 演講全文分享 iThome Cloud Summit 講到時間超過，一對東西沒講，所以來這邊發文 真的好用，實戰經驗分享 解答粉專私訊問題與觀眾發問 分散式工具實驗室 - Scalable Database on Kubernetes Thanos - Scalable HA Prometheus 簡介 需求分析，Unlimited Retention 鐵一般的需求 一步步帶你架 Scalable DB 實驗- Cockroach DB - 耐用打不死又高效能的小強資料庫 前言，分散式系統下的困境，資料庫瓶頸 Cockroach DB 基本原理簡介 Cockroach DB Free Trial 玩起來 Cassandra - 支撐百萬級寫入的分散式資料庫 Cassandra 簡介 Cassandra 細部原理，分散式的資料庫設計超好玩 Cassandra 細部原理，Consistency Hashing Cassandra 細部原理，Data modeling Netflix case study - 偷學 Netflix 能學到幾成功力呢 有任何謬誤，還煩請各方大德\u0026lt;3聯絡我，感激不盡。\n","date":1568019363,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"9831314c60d0890d9f8d13ad808717b1","permalink":"https://chechia.net/zh-hant/post/2020-09-09-ithome-ironman-challenge/","publishdate":"2019-09-09T16:56:03+08:00","relpermalink":"/zh-hant/post/2020-09-09-ithome-ironman-challenge/","section":"post","summary":"2020 IT邦幫忙鐵人賽","tags":["鐵人賽2020","kubernetes","borg"],"title":"2020 IT邦幫忙鐵人賽","type":"post"},{"authors":[],"categories":["quantum-computing"],"content":"This post is about my learning steps for quantum-computing.\nFor a quick-start tutorial, check my workshop project throught the project link above.\nResources Courses\nCoursera\non MIT x pro\nQuantum Information Processing from UW Madison\nQuantum Computation by John Preskill\nIBM Q Experience\nhttps://github.com/Qiskit/openqasm\nhttps://github.com/Qiskit/qiskit-tutorials\nIBM Q Experience Day 1 Getting Started with Circuit Composer\nHello Quantum World circuit transformed two qubits, from $ \\vert0\\rangle $ to $ \\frac{\\vert00\\rangle + \\vert11\\rangle}{\\sqrt{2}} $\nQuestions\n[] Hadamard Gate [] Bell states [] Annotations ","date":1559442097,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"d21581c66d8270f0b5118fcec7e7d711","permalink":"https://chechia.net/zh-hant/post/2019-06-02-journey-to-quantum-computing/","publishdate":"2019-06-02T10:21:37+08:00","relpermalink":"/zh-hant/post/2019-06-02-journey-to-quantum-computing/","section":"post","summary":"This post is about my learning steps for quantum-computing.\nFor a quick-start tutorial, check my workshop project throught the project link above.\nResources Courses\nCoursera\non MIT x pro\nQuantum Information Processing from UW Madison","tags":["quantum-computing","ibm-q-experience","tutorial"],"title":"Journey to Quantum Computing","type":"post"},{"authors":[],"categories":["kubernetes","istio"],"content":"Create GKE gcloud beta container --project \u0026#34;istio-playground-239810\u0026#34; clusters create \u0026#34;istio-playground\u0026#34; \\ --zone \u0026#34;asia-east1-b\u0026#34; \\ --username \u0026#34;admin\u0026#34; \\ --cluster-version \u0026#34;1.11.8-gke.6\u0026#34; \\ --machine-type \u0026#34;n1-standard-2\u0026#34; \\ --image-type \u0026#34;COS\u0026#34; \\ --disk-type \u0026#34;pd-standard\u0026#34; \\ --disk-size \u0026#34;100\u0026#34; \\ --preemptible \\ --num-nodes \u0026#34;1\u0026#34; \\ --enable-cloud-logging \\ --enable-cloud-monitoring \\ --no-enable-ip-alias \\ --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard,Istio \\ --istio-config auth=MTLS_PERMISSIVE \\ --no-enable-autoupgrade \\ --enable-autorepair Take a Peek $ kubectl get namespaces NAME STATUS AGE default Active 2m istio-system Active 1m kube-public Active 2m kube-system Active 2m $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7f6f77cd7b-nxfbf 1/1 Running 0 3m istio-cleanup-secrets-h454m 0/1 Completed 0 3m istio-egressgateway-7c56db84cc-nlrwq 1/1 Running 0 3m istio-galley-6c747bdb4f-45jrp 1/1 Running 0 3m istio-ingressgateway-6ff68cf95d-tlkq4 1/1 Running 0 3m istio-pilot-8ff66f8c4-q9chz 2/2 Running 0 3m istio-policy-69b78b7d6-c8pld 2/2 Running 0 3m istio-sidecar-injector-558996c897-hr6q4 1/1 Running 0 3m istio-telemetry-f96459fb-5cbpg 2/2 Running 0 3m promsd-ff878d44b-hv8nh 2/2 Running 1 3m Deploy app kubectl label namespace default istio-injection=enabled Bookinfo Application\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods kubectl get services Gateway\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml kubectl get gateways kubectl get svc istio-ingressgateway -n istio-system Go to ingress public ip\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].port}\u0026#39;) export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;https\u0026#34;)].port}\u0026#39;) curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage 404 Not Found Apply destination rules\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/destination-rule-all.yaml curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage Brief review kubectl get virtualservices kubectl get destinationrules kubectl get gateways Istio Tasks https://istio.io/docs/tasks/traffic-management/\n","date":1557137535,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"1693002988c5b0fb96505223edb59a21","permalink":"https://chechia.net/zh-hant/post/2019-05-06-service-mesh-for-microservice-on-kubernetes/","publishdate":"2019-05-06T18:12:15+08:00","relpermalink":"/zh-hant/post/2019-05-06-service-mesh-for-microservice-on-kubernetes/","section":"post","summary":"基於 Kubernetes 平台上的 Istio ，實際部署，並一步一步操作Istio 的功能。","tags":["kubernetes","istio"],"title":"Istio 三分鐘就入坑 佈署篇","type":"post"},{"authors":[],"categories":["kubernetes","devops","jenkins"],"content":"Jenkins is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.\nJenkins X, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.\nHere we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.\nArchitecture of Jenkins X Install Jenkins with jx Create a Pipeline with jx Develope with jx client For more information about jx itself, check Jenkins-X Github Repo\nArchitecture Check this beautiful diagram.\nhttps://jenkins-x.io/architecture/diagram/ Install Create GKE cluster \u0026amp; Get Credentials gcloud init gcloud components update CLUSTER_NAME=jenkins-server #CLUSTER_NAME=jenkins-serverless gcloud container clusters create ${CLUSTER_NAME} \\ --num-nodes 1 \\ --machine-type n1-standard-4 \\ --enable-autoscaling \\ --min-nodes 1 \\ --max-nodes 2 \\ --zone asia-east1-b \\ --preemptible # After cluster initialization, get credentials to access cluster with kubectl gcloud container clusters get-credentials ${CLUSTER_NAME} # Check cluster stats. kubectl get nodes Install jx on Local Machine [Jenkins X Release](https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases)\nJX_VERSION=v2.0.2 OS_ARCH=darwin-amd64 #OS_ARCH=linux-amd64 curl -L https://github.com/jenkins-x/jx/releases/download/\u0026#34;${JX_VERSION}\u0026#34;/jx-\u0026#34;${OS_ARCH}\u0026#34;.tar.gz | tar xzv sudo mv jx /usr/local/bin jx version NAME VERSION jx 2.0.2 Kubernetes cluster v1.11.7-gke.12 kubectl v1.11.9-dispatcher helm client v2.11.0+g2e55dbe helm server v2.11.0+g2e55dbe git git version 2.20.1 Operating System Mac OS X 10.14.4 build 18E226 (Option 1) Install Serverless Jenkins Pipeline DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider=\u0026#39;gke\u0026#39; Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD Select the organization where you want to create the environment repository: chechiachang Your Kubernetes context is now set to the namespace: jx INFO[0231] To switch back to your original namespace use: jx namespace jx INFO[0231] Or to use this context/namespace in just one terminal use: jx shell INFO[0231] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0231] To import existing projects into Jenkins: jx import INFO[0231] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0231] To create a new microservice from a quickstart: jx create quickstart (Option 2) Install Static Jenkins Server DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider=\u0026#39;gke\u0026#39; Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD Select the organization where you want to create the environment repository: chechiachang INFO[0465]Your Kubernetes context is now set to the namespace: jx INFO[0465] To switch back to your original namespace use: jx namespace default INFO[0465] Or to use this context/namespace in just one terminal use: jx shell INFO[0465] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0465] To import existing projects into Jenkins: jx import INFO[0465] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0465] To create a new microservice from a quickstart: jx create quickstart Access Static Jenkins Server through Domain with username and password Domain http://jenkins.jx.11.22.33.44.nip.io/\nUninstall jx uninstall # rm -rf ~/.jx Setup CI/CD Pipeline Create Quickstart Repository kubectl get pods --namespace jx --watch # cd workspace jx create quickstart Options:\nWhich organisation do you want to use? chechiachang Enter the new repository name: serverless-jenkins-quickstart select the quickstart you wish to create [Use arrows to move, type to filter] angular-io-quickstart aspnet-app dlang-http golang-http jenkins-cwp-quickstart jenkins-quickstart node-http\nINFO[0121] Watch pipeline activity via: jx get activity -f serverless-jenkins-quickstart -w INFO[0121] Browse the pipeline log via: jx get build logs chechiachang/serverless-jenkins-quickstart/master INFO[0121] Open the Jenkins console via jx console INFO[0121] You can list the pipelines via: jx get pipelines INFO[0121] Open the Jenkins console via jx …","date":1555647341,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"e6fd32ec536b4d36f5f015b1d2b76cc0","permalink":"https://chechia.net/zh-hant/post/2019-04-19-jenkins-x-on-kubernetes/","publishdate":"2019-04-19T12:15:41+08:00","relpermalink":"/zh-hant/post/2019-04-19-jenkins-x-on-kubernetes/","section":"post","summary":"Jenkins is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.","tags":["jenkins","ci","cd","kubernetes"],"title":"Jenkins X on Kubernetes","type":"post"},{"authors":null,"categories":["kubernetes"],"content":"","date":1538798820,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"751c6ae47d075143b976bf2c3cd443d5","permalink":"https://chechia.net/zh-hant/post/2018-10-06-kubernetes-container-runtime-interface/","publishdate":"2018-10-06T12:07:00+08:00","relpermalink":"/zh-hant/post/2018-10-06-kubernetes-container-runtime-interface/","section":"post","summary":"","tags":["kubernetes","container","docker","cri"],"title":"Kubernetes Container Runtime Interface","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"zh-hant","lastmod":1694442834,"objectID":"fcd4fd6219c1229c77d4228beb208235","permalink":"https://chechia.net/zh-hant/project/terraform-30-days/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/zh-hant/project/terraform-30-days/","section":"project","summary":"所有關於公有雲 (Public Cloud) 的問題，我一率建議 Terraform。Hashicorp Terraform 是開源的宣告式 IaC 管理工具，對公有雲支援度高。本課程如何使用 Terraform 管理公有雲端資源，導入 PR review / git-flow 等軟體開發流程，大幅提升團隊效率，降低人為錯誤。是公有雲管理必學工具。","tags":["鐵人賽2021","iac","azure","terraform"],"title":"Terraform 30 days Workshop - IaC for Public Cloud 疫情警戒陪你度過 30 天","type":"project"}]