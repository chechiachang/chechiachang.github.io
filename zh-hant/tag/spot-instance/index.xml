<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spot-instance | Che-Chia Chang</title>
    <link>https://chechia.net/zh-hant/tag/spot-instance/</link>
      <atom:link href="https://chechia.net/zh-hant/tag/spot-instance/index.xml" rel="self" type="application/rss+xml" />
    <description>spot-instance</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>zh-Hant</language><lastBuildDate>Sat, 26 Sep 2020 17:24:20 +0800</lastBuildDate>
    <image>
      <url>https://chechia.net/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>spot-instance</title>
      <link>https://chechia.net/zh-hant/tag/spot-instance/</link>
    </image>
    
    <item>
      <title>Gcp Preemptible Instance Kubernetes</title>
      <link>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/</link>
      <pubDate>Sat, 26 Sep 2020 17:24:20 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-kubernetes/</guid>
      <description>&lt;h3 id=&#34;先占虛擬機與-kubernetes&#34;&gt;先占虛擬機與 Kubernetes&lt;/h3&gt;
&lt;p&gt;在 GCP 使用先占虛擬機，會需要面對先占虛擬機的額外限制&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;資料中心會 (可預期或不可預期地) 終止先占虛擬機&lt;/li&gt;
&lt;li&gt;先占虛擬機不能自動重啟，而是會被資料中心終止後回收&lt;/li&gt;
&lt;li&gt;GCP 不保證有足夠的先占虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;節點的終止會造成額外的維運成本，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;管理多個節點，容忍先占虛擬機的移除，自動補充新的先占虛擬機&lt;/li&gt;
&lt;li&gt;管理多個應用複本，節點終止時，維護整體應用的可用性&lt;/li&gt;
&lt;li&gt;將移除節點上的應用，重新排程到其他可用節點&lt;/li&gt;
&lt;li&gt;動態維護應用複本的服務發現 (Service Discovery) 與服務端點 (Endpoints)
&lt;ul&gt;
&lt;li&gt;意思是應用關閉重啟後，換了一個新 IP，還要能持續存取應用。舊的 IP 要主動失效&lt;/li&gt;
&lt;li&gt;配合應用的健康檢查 (Health Check) 與可用檢查 (Readiness Check)，再分配網路流量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些需求，必須要有自動化的管理工具，是不可能人工管理的，想像你手上使用 100 個先占節點，平均每天會有 10% - 15% 的先占節點被資料中心回收，維運需要&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;補足被移除的 15 個節點&lt;/li&gt;
&lt;li&gt;計算被移除的應用，補足移除的應用數量&lt;/li&gt;
&lt;li&gt;移除失效的應用端點，補上新的應用端點&lt;/li&gt;
&lt;li&gt;持續監控應用狀態&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;沒有自動化管理工具，看了心已累 (貓爪掩面)&lt;/p&gt;
&lt;p&gt;我們使用 Kubernetes 協助維運自動化，在 GCP 上我們使用 GKE，除了上述提到的容器應用管理自動化外，GKE 還額外整合先占虛擬機的使用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;啟用先占虛擬機的節點池 (node-pool)，設定節點池的自動拓展，自動補足先占節點的數量&lt;/li&gt;
&lt;li&gt;GKE 自動維護先占虛擬機的 labels&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;關於 GKE 的先占虛擬機的完整細節，請見&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 官方文件&lt;/a&gt;。這份文件底下也提供了 GCP 官方建議的先占虛擬機最佳實踐&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;架構設計需要假設，部分或是全部的先占虛擬機都不可用的情形&lt;/li&gt;
&lt;li&gt;Pod 不一定有時間能優雅終止 (graceful shutdown)&lt;/li&gt;
&lt;li&gt;同時使用隨選虛擬機與先占虛擬機，以維持先占虛擬機不可用時，服務依然可用&lt;/li&gt;
&lt;li&gt;注意節點替換時的 IP 變更&lt;/li&gt;
&lt;li&gt;避免使用有狀態的 Pod 在先占虛擬機上 (這點稍後的文章，我們會試圖超越)&lt;/li&gt;
&lt;li&gt;使用 node taint 來協助排程到先占虛擬機，與非先占虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;總之，由於有容器自動化管理，我們才能輕易的使用先占虛擬機。&lt;/p&gt;
&lt;h3 id=&#34;gke&#34;&gt;GKE&lt;/h3&gt;
&lt;p&gt;然而，決定使用 GKE 後，就有許多關於成本的事情需要討論&lt;/p&gt;
&lt;p&gt;先看 &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/pricing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GKE 的計費方式 pricing&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每個 GKE 集群管理費用 $0.1/hr = $72/hr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這個費用是固定收費，只要開一個集群，不論集群的節點數量。所以在節點多、算力大的集群裡，這個費用會被稀釋，但在節點少的集群裡比例會被放大。&lt;/p&gt;
&lt;p&gt;然後 GKE 還是會有一些自己的毛，俗話說有一好沒兩好，我們使用它的好處同時，也要注意許多眉眉角角。再來爬&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;文件&lt;/a&gt;。如同最前面宣導，用產品就要乖乖把文件看完，不過這裡先針對與先占虛擬機相關的議題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#memory_cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Allocatable Resource&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/regional-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Regional Cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cluster autoscaler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;allocatable-resource&#34;&gt;Allocatable Resource&lt;/h3&gt;
&lt;p&gt;在網路上看到這篇好文 &lt;a href=&#34;https://learnk8s.io/allocatable-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GKE 上的可使用的資源 Allocatable Resource&lt;/a&gt;。啥意思呢？難道還有不能使用的資源嗎？&lt;/p&gt;
&lt;p&gt;沒錯，GKE 會保留一定的機器資源 (e.g. cpu, memory, disk)，來維持節點的管理元件，例如 container runtime (e.g. Docker)、kubelet、cAdvisor。&lt;/p&gt;
&lt;p&gt;也就是說，就算我們跟 GCP 購買了算力，有一個比例的資源我們是使用不到的。細節請見 &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#memory_cpu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;理解 GKE 集群架構&lt;/a&gt;。這會影響我們單一節點的規格，我們也需要一並計算，能實際使用的資源 (allocatable resource)。&lt;/p&gt;
&lt;p&gt;Allocatable = Capacity - Reserved - Eviction Threshold&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Capacity，是機器上實際裝載的資源，例如 n1-standard-4 提供 4 cpu 15 Gb memory&lt;/li&gt;
&lt;li&gt;Reserved，公有雲代管集群，預保留的資源&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/#eviction-thresholds&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eviction Threshold&lt;/a&gt;：Kubernetes 設定的 kubelet 驅逐門檻&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;驅逐門檻-eviction-threshold&#34;&gt;驅逐門檻 (Eviction threshold)&lt;/h3&gt;
&lt;p&gt;Kubelet 會主動監測節點上的資源使用狀況，當節點發生資源不足的狀況時，kubelet 會主動終止某些 Pod 的運行，並回收節點的資源，來避免整個節點資源不足導致的系統不穩定。被終止的 Pod 可以再次排程到其他資源足夠的節點上。細節請見 &lt;a href=&#34;https://kubernetes.io/docs/concepts/scheduling-eviction/eviction-policy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文件 Scheduling and Eviction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在 Kubernetes 上，我們可以進一步設定驅逐門檻，當節點的可用資源低於驅逐的門檻，kubelet 會觸發 Pod 驅逐機制&lt;/p&gt;
&lt;p&gt;GKE 上每個節點會額外保留 100 MiB 的記憶體，作為驅逐門檻，意思是當節點耗盡資源，導致剩餘記憶體低於 100 MiB 的時候，會直接觸發 GKE 的 Pod Eviction，終止並回收部分的 Pod。換句話說，這 100 MiB 是不能被使用的資源。細節請見&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-architecture#eviction_threshold&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;集群保留資源精算&#34;&gt;集群保留資源精算&lt;/h3&gt;
&lt;p&gt;資源的定義，使用雲平台的一般費用大多來自此&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cpu&lt;/li&gt;
&lt;li&gt;memory&lt;/li&gt;
&lt;li&gt;storage&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然後是這個表，注意保留的資源是累進級距&lt;/p&gt;
&lt;p&gt;255 MiB of memory for machines with less than 1 GB of memory
25% of the first 4GB of memory
20% of the next 4GB of memory (up to 8GB)
10% of the next 8GB of memory (up to 16GB)
6% of the next 112GB of memory (up to 128GB)
2% of any memory above 128GB&lt;/p&gt;
&lt;p&gt;值計上能夠用到的資源，底下 GCP 也整理好了，例如 n1-standard-4 實際使用的是 memory 12.3/15，cpu 3.92/4。&lt;/p&gt;
&lt;p&gt;在維持合理的使用率下，開啟大的機器，可以降低被保留的資源比例，依照筆者公司過去經驗，GKE 起跳就是 n1-standard-4 或是以上規格，如果低於這個規格，可調度的資源比例真的太低，應該重新考慮一下這個解決方案是否合乎成本。&lt;/p&gt;
&lt;p&gt;但究竟什麼規格的機器適合我們的需求，說實在完全要看執行的應用而定。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Introduction</title>
      <link>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/</link>
      <pubDate>Sat, 26 Sep 2020 11:03:40 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-26-gcp-preemptible-instance-introduction/</guid>
      <description>&lt;h1 id=&#34;先占虛擬機技術文件二三事&#34;&gt;先占虛擬機，技術文件二三事&lt;/h1&gt;
&lt;p&gt;第一篇的內容大部份還是翻譯跟講解官方文件。後面幾篇才會有實際的需求與解決方案分析。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google 先占虛擬機官方文件&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用不熟悉的產品前一定要好好看文件，才不會踩到雷的時候，發現人家就是這樣設計的，而且文件上寫得清清楚楚。以為是 bug 結果真的是 feature，雷到自己。先占虛擬機是用起來跟普通虛擬機沒什麼兩樣，但實際上超級多細節要注意，毛很多的產品，請務必要小心使用。&lt;/p&gt;
&lt;p&gt;以下文章是筆者工作經驗，覺得好用、確實有幫助公司，來跟大家分享。礙於篇幅，這裡只能非常粗略地描述我們團隊思考過的問題，實際上的問題會複雜非常多。文章只是作個發想，並不足以支撐實際的業務，所以如果要考慮導入，還是要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多作功課，仔細查閱官方文件，理解服務的規格&lt;/li&gt;
&lt;li&gt;深入分析自身的需求&lt;/li&gt;
&lt;li&gt;基於上面兩者，量化分析&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;什麼是先占虛擬機器preemptible-instance&#34;&gt;什麼是先占虛擬機器(Preemptible Instance)&lt;/h1&gt;
&lt;p&gt;先占虛擬機器，是資料中心的多餘算力，讀者可以想像是目前賣剩的機器，會依據資料中心的需求動態調整，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目前資料中心的算力需求低，可使用的先占虛擬機釋出量多，可能可以用更便宜的價格使用&lt;/li&gt;
&lt;li&gt;目前資料中心算力需求高，資料中心會收回部分先占虛擬機的額度，轉化成隨選付費的虛擬機 (pay-as-you-go)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占虛擬機會不定時（但可預期）地被資料中心收回，因此上頭執行的應用，需要可以承受機器的終止，適合有容錯機制 (fault-tolerant) 的應用，或是批次執行的工作也很適合。&lt;/p&gt;
&lt;h1 id=&#34;先占機器的優缺點&#34;&gt;先占機器的優缺點&lt;/h1&gt;
&lt;p&gt;除了有一般隨選虛擬機的特性，先占虛擬機還有以下特點&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比一般的虛擬機器便宜非常多，這也是我們選用先占虛擬機優於一般虛擬機的唯一理由&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先占虛擬機有以下限制，以維運的角度，這些都是需要考量的點。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP 不保證會有足夠的先占虛擬機&lt;/li&gt;
&lt;li&gt;先占虛擬機不能直接轉換成普通虛擬機&lt;/li&gt;
&lt;li&gt;資料中心觸發維護事件時(ex. 回收先占虛擬機)，先占虛擬機不能設定自動重啟，而是會直接關閉&lt;/li&gt;
&lt;li&gt;先占機器排除在 &lt;a href=&#34;https://cloud.google.com/compute/sla&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 的服務等級協議 (SLA)&lt;/a&gt;之外&lt;/li&gt;
&lt;li&gt;先占虛擬機不適用&lt;a href=&#34;https://cloud.google.com/free&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GCP 免費額度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;費用粗估試算&#34;&gt;費用粗估試算&lt;/h1&gt;
&lt;p&gt;至於便宜是多便宜呢？這邊先開幾個例子給各位一些概念。&lt;/p&gt;
&lt;p&gt;以常用的 N1 standard 虛擬機：https://cloud.google.com/compute/vm-instance-pricing#n1_standard_machine_types&lt;/p&gt;
&lt;p&gt;Hourly
Machine type	CPUs	Memory	Price (USD)	Preemptible price (USD)
n1-standard-1	1	3.75GB	$0.0550		$0.0110
n1-standard-2	2	7.5GB	$0.1100		$0.0220
n1-standard-4	4	15GB	$0.2200		$0.0440
n1-standard-8	8	30GB	$0.4400		$0.0880
n1-standard-16	16	60GB	$0.8800		$0.1760
n1-standard-32	32	120GB	$1.7600		$0.3520
n1-standard-64	64	240GB	$3.5200		$0.7040&lt;/p&gt;
&lt;p&gt;如果是用 GPU 運算：https://cloud.google.com/compute/gpus-pricing&lt;/p&gt;
&lt;p&gt;Model			GPUs	GPU memory	GPU price (USD)	Preemptible GPU price (USD)
NVIDIA® Tesla® T4	1 GPU	16 GB GDDR6	$0.35 per GPU	$0.11 per GPU
NVIDIA® Tesla® V100	1 GPU	16 GB HBM2	$2.48 per GPU	$0.74 per GPU&lt;/p&gt;
&lt;p&gt;依據虛擬機規格的不同，先占虛擬機大約是隨選虛擬機價格的 2 到 3 折。在 AWS 與 Azure，由於計費方式不同，有可能拿到 1 折左右的浮動價格。從各種角度來說，都是非常高的折數。&lt;/p&gt;
&lt;p&gt;不妨說，這整系列文章，都是衝這著個折數來的 XD。畢竟成本是實實在在的花費，工作負載 (workload) 合適的話，應該盡量嘗試導入。&lt;/p&gt;
&lt;p&gt;這個折數還有另外一個效果是，可以在相同成本下，添增更多資源算力，作為解決方案。什麼意思呢？就是如果工作負載合適的話，可以使用更高規格的先占節點，整體成本反而會下降。&lt;/p&gt;
&lt;p&gt;至於究竟差多少，需要依據規格與定價詳細試算才知道。底下我們就來算算看。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Resource Calculation</title>
      <link>https://chechia.net/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/</link>
      <pubDate>Fri, 25 Sep 2020 12:22:02 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-25-gcp-preemptible-instance-resource-calculation/</guid>
      <description>&lt;h3 id=&#34;關於資源評估&#34;&gt;關於資源評估&lt;/h3&gt;
&lt;p&gt;架構團隊提供虛擬機給應用，有個問題時常出現：應該分配多少資源給應用？例如：後端準備一個 API server，SRE 這邊要準備多少什麼規格的機器？&lt;/p&gt;
&lt;p&gt;以往使用虛擬機直接部署應用時，會需要明確規劃各群虛擬機，各自需要執行的應用，如果沒有做資源的事前評估，有可能放上機器運行後就發生資源不足。&lt;/p&gt;
&lt;p&gt;導入 Kubernetes 後，透過節點池 (Node Pool) 形成一個大型資源池，設定部署的政策後，讓 Kubernetes 自動調度應用：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一個節點的資源夠大，使得應用虛擬機器上所佔的比例相對較小，也就是單一應用的調度不會影響節點的整體負載
&lt;ul&gt;
&lt;li&gt;如果節點太小，調度應用就會有些侷促，例如：一個 API server 均載時消耗 1 cpu 滿載時消耗 2 cpu。準備 3 cpu 的虛擬機，調度應用時幾乎是遷移整台虛擬機的負載&lt;/li&gt;
&lt;li&gt;此外還有機會因為&lt;a href=&#34;&#34;&gt;上篇&lt;/a&gt;提到的資源保留，造成調度失敗。如果準備 24 cpu 的機器，調度起來彈性就很大，對節點的性能衝擊也比較低&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;只需要估計整體的資源消耗率計算需求，配合自動擴展，動態器補足不足的資源
&lt;ul&gt;
&lt;li&gt;例如：估計總共需要 32 cpu ，準備 36 cpu 的虛擬機，當滿載時依據 cpu 壓力自動擴容到 48 cpu&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;希望整體資源的使用率夠高，當然預留太多的資源會造成浪費&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;要控管 Kubernetes 的資源使用量也可設定&lt;a href=&#34;https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;資源需求與資源限制&lt;/a&gt;，延伸閱讀。&lt;/p&gt;
&lt;p&gt;估計得越準確，當然實際部署的資源掌握度就越高，然而筆者過去的經驗，團隊在交付源碼時未必就能夠做出有效的資源消耗評估，那有沒有什麼辦法可以幫助我們？&lt;/p&gt;
&lt;h3 id=&#34;資源需求估估看&#34;&gt;資源需求估估看&lt;/h3&gt;
&lt;p&gt;如果應用開發團隊，有先作應用的 profiling，然後 release candidate 版本有在 staging 上作壓力測試的話，維運團隊這邊應該就取得的數據，做部署前的資源評估。&lt;/p&gt;
&lt;p&gt;應用在不同狀態或是工作階段，會消耗不同的資源&lt;/p&gt;
&lt;p&gt;例如：運算密集的 batch job 可能會有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;控制節點 (master node) 啟動後會佔有一定的資源，一般來說不會消耗太多，只是需要為控制節點優先保留資源&lt;/li&gt;
&lt;li&gt;工作節點 (worker node) 啟動時會需要預留足夠的資源，接收工作後會逐漸增加資源使用，拉到滿載&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：面向用戶的服務，可能會有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;啟動應用所需的資源&lt;/li&gt;
&lt;li&gt;沒有大量請求，只維持基本應用運行所使用的資源&lt;/li&gt;
&lt;li&gt;負載壓力灌進來時，消耗資源隨用戶請求數量的成長曲線，設定的安全上界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果沒有這些數據，其實維運很難事前估計資源，變成要實際推上線後見招拆招，基於實際的資源消耗去做自動擴容，其實有可能會造成資源的浪費，因此我建議如果開發團隊沒有作 profiling，維運團隊可以在工作流程內簡單加一步 profiling，目前主流語言都有提供相關工具，簡單的執行就可以獲得很多資訊。&lt;/p&gt;
&lt;p&gt;至於壓力測試，也是可以使用基本的工具(例如 &lt;a href=&#34;https://artillery.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artilery&lt;/a&gt;)簡單整到工作流程。特別是面對客戶的應用，務必要進行壓力測試。&lt;/p&gt;
&lt;p&gt;有了上述的資源需求數據，才能事先安排機器的規格。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用是面對客戶的 API server&lt;/li&gt;
&lt;li&gt;基本資源是 200m cpu 1Gi memory，這部分直接寫進 Kubernetes resource request，在排程時就先結點上預留。&lt;/li&gt;
&lt;li&gt;負載拉到 1,000 RPS，latency 95% 20ms  99% 30ms，這時的資源需求的上界大約 2 cpu 4 Gi memory&lt;/li&gt;
&lt;li&gt;超過 1,000 RPS 就應該要透過水平擴展去增加更多 instamce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果單跑這個 API server，就可以安排 memory 8，cpu 4 的 GKE node-pool，讓負載落在可用資源的 60%-70%，這樣還有餘裕可以承受大流量，給自動水平擴展做動的時間。&lt;/p&gt;
&lt;h3 id=&#34;資源調整參考&#34;&gt;資源調整參考&lt;/h3&gt;
&lt;p&gt;當然這些數據都可以依照時實際需求調整，資源要壓縮得更緊或更鬆都是可行的。&lt;/p&gt;
&lt;p&gt;如果應用有整合分析後台，例如 Real-time Uer Monitoring、或是基本的 Google Analytics，都可以觀察這些調整實際對用戶帶來的影響，用戶行為改變對公司營收的影響，全都可以量化。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;機器負載拉到 80%，cpu 的壓力，導致 API latency 增加到 95% 50ms 99% 100ms&lt;/li&gt;
&lt;li&gt;此時用戶已經很有感了，會導致 0.1% 用戶跳轉離開&lt;/li&gt;
&lt;li&gt;而這 0.1% 的用戶，以往的平均消費，換算成為公司營收，是 $1,000/month&lt;/li&gt;
&lt;li&gt;把機器負載壓到 60%，只計算 cpu 的數量的話，需要多開 3 台 n1-standard-4 機器，共計 $337.26/month&lt;/li&gt;
&lt;li&gt;提供老闆做參考，老闆可能會趨向加開機器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;當然上面的例子都非常簡化，變成國中數學問題，這邊只是提供一個估算的例子。現實中的問題都會複雜百倍，例如機器規格拉上去出現新的瓶頸、例如依賴的服務，message queue、database 壓力上升，或是公司內部問題，就拿不到預算(血淚 SRE)。如果要減少機器，也可以參考，一般來說聽到關機器省錢的話，老闆都會接受的 XD。&lt;/p&gt;
&lt;h3 id=&#34;回到先占機器&#34;&gt;回到先占機器&lt;/h3&gt;
&lt;p&gt;根據上面的國中數學，把應用一個一個都計算清楚，需求逐漸明確了。假設，架構團隊拿到開機器的工單，掐指一算，決定&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 GKE cluster&lt;/li&gt;
&lt;li&gt;6 n1-standard-4&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些都是隨選虛擬機，價格大約是 $747/month (含集群管理費 $73/month)。今天有人腦動大開，那如果全部換成先占節點呢？變成 $265/month，虛擬機費用 $192 / $674 = 0.28 直接打超過三折。&lt;/p&gt;
&lt;p&gt;有人就擔心，這樣真的可以嗎？真的沒問題嗎？會不會影響用戶阿。&lt;/p&gt;
&lt;p&gt;答案是會，就是會影響用戶 XD&lt;/p&gt;
&lt;p&gt;聽到這邊很多人就怕了。但是怎麼個影響法呢，還需要看底下幾個段落，如果換成先占虛擬機，用戶會怎麼受到影響。我們也要試圖量化這個影響，當作要不要導入的判斷依據。&lt;/p&gt;
&lt;p&gt;句個反例，「我覺得可以」「你覺得不行」，或是「某某公司的某某團隊可以啊」「我們公司也來做吧」，這些都是很糟糕的理由。除了對內部毫無說服力之外，也沒有辦法作為導入成效的指標，會讓團隊陷入「導入了也不知道有比導入前好？」或是「具體導入後成效量化」，會影響團隊做出真正有效的判斷，應極力避免。&lt;/p&gt;
&lt;p&gt;此外，問行不行之前，其實需要知道團隊願意為了三折機器，付出多少成本。如果只是每月省個台幣 15,000，工程師薪水都超過了。但如果手下有三十台或三百台以上，也許就非常值得投資。成本不是絕對的，很多時候要與其他成本 (e.g. 開發人力時間成本) 一起考量。&lt;/p&gt;
&lt;p&gt;以上都是說明導入的動機，以下說明先占虛擬機的各種機制，以及對應用的實際影響。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Requirement</title>
      <link>https://chechia.net/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/</link>
      <pubDate>Thu, 24 Sep 2020 13:39:12 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-24-gcp-preemptible-instance-requirement/</guid>
      <description>&lt;h1 id=&#34;需求規劃&#34;&gt;需求規劃&lt;/h1&gt;
&lt;p&gt;使用先占節點比起使用一般隨選虛擬機，會多出許多技術困難需要克服，只有節省下的成本大於整體技術成本時，我們才會選用先占節點。因此這邊要進行成本精算，重新調整的架構下，實際到底能省多少錢。務必使用 &lt;a href=&#34;https://cloud.google.com/products/calculator?hl=zh-tw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cloud Pricing Calculator&lt;/a&gt; 精算成本。&lt;/p&gt;
&lt;p&gt;另外，雖然先占虛擬機會有很多額外的限制與技術困難，但實務上還是要對比實際的需求，有些限制與需求是衝突的，有些限制則完全不會影響我們的需求。前者當然會帶給我們較高的導入難度，後者可能會非常輕鬆。&lt;/p&gt;
&lt;p&gt;這邊想給大家的概念是，務必先明確需求，再討論技術。這點很重要，技術的適用與否，不是由個人的喜好決定，唯一的判斷標準，是能不能有效率的滿足需求。&lt;/p&gt;
&lt;p&gt;所以這邊先定義我們以下幾個需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;執行短期的 batch job&lt;/li&gt;
&lt;li&gt;執行長期的 user-facing API server&lt;/li&gt;
&lt;li&gt;執行長期的 stateful 資料庫、儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;batch-job&#34;&gt;Batch Job&lt;/h3&gt;
&lt;p&gt;常見的範例，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用網路爬蟲 (crawler) 去抓取許多網站的所有內容&lt;/li&gt;
&lt;li&gt;使用 GPU 進行機器學習的 Model Training&lt;/li&gt;
&lt;li&gt;大數據計算&lt;/li&gt;
&lt;li&gt;MapReduce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些任務的核心需求，很簡單直接&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;盡快完成整體工作&lt;/li&gt;
&lt;li&gt;盡可能節省大量算力成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如：我手上的機器學習 Model 粗略估計 10000 小時*GPU 的算力需求，才能產出一個有效的Model。由於大量的算力需求，一般來說都會選擇分散式的運算框架 (ex. MapReduce) ，將真正消耗算力的工作，使用分而化之 (divide and conquer) 的架構設計，將分配任務的控制節點 (master)，與實際進行運算的工作節點(worker) 拆分。基於原本的分散式架構，幾乎可以無痛地將工作節點轉移到先占虛擬機上。&lt;/p&gt;
&lt;p&gt;根據上述的需求，這類的工作特性可能有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU / GPU 算力需求高的運算節點 (Worker)&lt;/li&gt;
&lt;li&gt;Worker 本身是無狀態的 Stateless&lt;/li&gt;
&lt;li&gt;可控的即時負載&lt;/li&gt;
&lt;li&gt;將整體工作切分成任務單元 (task)，分配給工作節點&lt;/li&gt;
&lt;li&gt;任務單元的狀態外部保留，工作節點可容錯 (fault-tolerent)，任務單元可復原&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占虛擬機可能是浮動價格，這類工作可以根據優先程度，調整合適的工作時間，例如在資料中心算力需求低，先占虛擬機的費用低廉時，啟用較多的工作節點加快運算，如果費用高時，可以降低先占虛擬機的使用，延後工作，甚至是調用不同區域，費用低的工作節點，來降低整體的成本。&lt;/p&gt;
&lt;p&gt;執得注意的是，這類任務的控制節點 (master)，也許是集中式的，也許是分散式的，需要根據性質考量，是否適合放在先占虛擬機上。有些架構控制節點可以容錯，然而錯誤發生後會需要復原狀態，這時會消耗額外的算力，可能會拖緩整體進度，造成算力的消耗。也許就可以考量使用隨選虛擬機配合使用。&lt;/p&gt;
&lt;h3 id=&#34;user-facing-services&#34;&gt;User-facing services&lt;/h3&gt;
&lt;p&gt;常見的範例，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Restful API server&lt;/li&gt;
&lt;li&gt;Websocket Server&lt;/li&gt;
&lt;li&gt;TCP/HTTP reverse proxy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些工作的核心需求如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;整題服務的高可用性 (high availability)&lt;/li&gt;
&lt;li&gt;承受不可預期的負載高峰 (load spikes)&lt;/li&gt;
&lt;li&gt;整體表現需要低延遲 (low latency)&lt;/li&gt;
&lt;li&gt;可以水平擴展 (horizontal scaling)，支撐用戶的成長&lt;/li&gt;
&lt;li&gt;最終平衡效能呈現與成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於會面對使用者，需要能支持使用者的壓力，又同時需要有一定的服務效能，來維持使用者體驗。實務上設計可能採用無狀態應用 (stateless)，多副本 (replica) 部署到集群中。需要儲存的狀態（如用戶狀態），使用外部的共用儲存 (例如：Redis，RDBMS，或是 Non-SQL DB)。請求的流量，透過上游的負載均衡器 (Load Balancer)，送進多個後端，處理完成請求後，再返回給使用者。&lt;/p&gt;
&lt;p&gt;這樣的設計，使用先占虛擬機也不會有太多的問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;現代的附載均衡器多半都能像後端做可用性檢測 (health check)，可以把流量導向工作正常的節點，如果後端的虛擬機被資料中心收回了，流量也會移轉到其他節點上，不會遺失用戶請求&lt;/li&gt;
&lt;li&gt;配合自動水平拓展工具 (Auto horizontal scaler)，可以設定期望的服務節點數量，如果資料中心回收先占節點，拓展工具可以同時去取得新的先占節點，或是取得隨選隨用的虛擬機&lt;/li&gt;
&lt;li&gt;配合流量監測，也可以動態調整期望的服務節點數量，例如：偵測到大量用戶連線數時，增加更多服務節點，待流量下降後，再降低服務節點數量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這樣的設計實務上有幾點注意&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;雖然說應用後端本身是無狀態的，但面對用戶也許還是會有部分狀態存在應用外部，例如：User session，或是 websocket 的長連線。特別注意這些服務斷線的時候，對於使用者的影響，配合前端增強使用者體驗&lt;/li&gt;
&lt;li&gt;後端水平拓展後，瓶頸會轉移到其他地方，例如 Database 成為效能瓶頸，應用這邊需要做一定程度的自律( ex. connection limit，rate limit)，避免不斷增長的應用壓垮依賴的服務，如 MessageQueue 或是 Database&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分散式的儲存中心 (distributed DB)，如：cassandra 或是小強 DB。而這樣類型的服務，是否適合放在先占節點上？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Speficication</title>
      <link>https://chechia.net/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/</link>
      <pubDate>Wed, 23 Sep 2020 16:23:14 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-23-gcp-preemptible-instance-speficication/</guid>
      <description>&lt;h1 id=&#34;先占虛擬機終止流程-preemption-process&#34;&gt;先占虛擬機終止流程 (Preemption process)&lt;/h1&gt;
&lt;p&gt;子曰：未知生焉知死。但做工程師要反過來，考量最差情形，也就是要知道應用可能如何死去。不知道應用可能怎麼死，別說你知道應用活得好好的，大概想表達這麼意思。&lt;/p&gt;
&lt;p&gt;這對先占虛擬機來說特別重要，一般應用面對的機器故障或是機器終止，在使用先占西你幾的狀況下，變成每日的必然，因此，需要對應用的終止情境，與終止流程有更精細的掌控。如同前幾篇所說的，先占虛擬機會被公有雲收回，但收回的時候不會突然機器就 ben 不見，會有一個固定的流程。&lt;/p&gt;
&lt;p&gt;如果你的應用已經帶有可容錯的機制，能夠承受機器突然變不見，服務還好好的，仍然要花時間理解這邊的流程，藉此精算每天虛擬機的終止與替換：應用會有什麼反應，會產生多少衝擊，稍後可以量化服務的影響。例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用重啟初始化時 cpu memory 突然拉高&lt;/li&gt;
&lt;li&gt;承受節點錯誤後的復原流程，需要消耗額外算力。例如需要從上個 checkpoint 接續做，需要去讀取資料造成 IO，或是資料需要做 rebalance &amp;hellip;等等&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你的應用需要有 graceful shutdown 的機制，那你務必要細心理解這邊的步驟。並仔細安排安全下樁的步驟。又或是無法保證在先占虛擬機回收的作業時限內，完成優雅終止，需要考慮其他可能的實作解法。&lt;/p&gt;
&lt;p&gt;這邊有幾個面向要注意&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GCP 如何終止先占節點&lt;/li&gt;
&lt;li&gt;GCP 移除節點對 GKE 、以及執行中應用的影響&lt;/li&gt;
&lt;li&gt;GKE 集群如何應對的節點失效&lt;/li&gt;
&lt;li&gt;GCP 自動調度補足新的先占節點&lt;/li&gt;
&lt;li&gt;GKE 集群如何應對節點補足&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;三個重點&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機終止對集群的影響&lt;/li&gt;
&lt;li&gt;Pod 隨之終止對應用的影響，是否能夠優雅終止&lt;/li&gt;
&lt;li&gt;有沒有方法可以避免上面兩者的影響&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;劇透一下：有的，有一些招式可以處理。讓我們繼續看下去。&lt;/p&gt;
&lt;h3 id=&#34;gcp-如何終止虛擬機&#34;&gt;GCP 如何終止虛擬機&lt;/h3&gt;
&lt;p&gt;先占虛擬機的硬體終止步驟與一般隨選虛擬機相同，所以我們要先理解&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/stop-start-instance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;虛擬機的停止流程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;這裡指的終止 (Stop) 是&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/instance-life-cycle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;虛擬機生命週期&lt;/a&gt; 的 RUNNING -&amp;gt; instances.stop() -&amp;gt; STOPPING -&amp;gt; TERMINATED 的步驟。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;instances.stop()&lt;/li&gt;
&lt;li&gt;ACPI shutdown&lt;/li&gt;
&lt;li&gt;OS 會進行 shutdown 流程，並嘗試執行各個服務的終止流程，以安全的終止服務。如果虛擬機有設定&lt;a href=&#34;https://cloud.google.com/compute/docs/shutdownscript&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Shtudown Script&lt;/a&gt; 會在這步驟處理&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/deleting-instance&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;等待至少 90 秒&lt;/a&gt;，讓 OS 完成終止的流程
&lt;ul&gt;
&lt;li&gt;逾時的終止流程，GCP 會直接強制終止，就算 shutdown script 還沒跑完&lt;/li&gt;
&lt;li&gt;GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;虛擬機變成 TERMINATED 狀態&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gcp-如何終止先占虛擬機&#34;&gt;GCP 如何終止先占虛擬機&lt;/h3&gt;
&lt;p&gt;與隨選虛擬機不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機的時間 30 秒&lt;/li&gt;
&lt;li&gt;搭配 GKE 使用 Managed Instance Group，終止的虛擬機會被刪除，Autoscaler 會啟動新的虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一樣先看&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#preemption-process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;先占虛擬機的說明文件：終止流程&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;資料中心開始回收先占虛擬機，選中我們專案其中的一台先占虛擬機&lt;/li&gt;
&lt;li&gt;Compute Engine 傳送 ACPI G2 Soft Off，這裡 OS 會試圖安全關變服務，也會執行 &lt;a href=&#34;https://cloud.google.com/compute/docs/instances/create-start-preemptible-instance?hl=zh-tw#handle_preemption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shutdown script&lt;/a&gt;，可以做簡短的優雅終止&lt;/li&gt;
&lt;li&gt;30秒後，ACPI G3 Mechanical off&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但 30 秒能做什麼？只能快速的交代當前進度。如果應用需要花時間收尾，保存工作進度，可能會產生許多問題&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GCP 不保證終止時限的時間，官方建議不要寫重要的依賴腳本在終止時限內&lt;/li&gt;
&lt;li&gt;在面對大量 IO 的工作，可能會導致者台虛擬機的大量應用一起進入優雅終止，先占虛擬機最後耗盡資源，來不及做完&lt;/li&gt;
&lt;li&gt;如果可能會超時，或是沒完成會有資料遺失風險，就不能在這個階段處理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;依賴 shutdown script 做收尾是危險的，我們之後要想辦法處理這個不保證做完的優雅終止。&lt;/p&gt;
&lt;p&gt;如果應用本身有容錯的框架，或是有容錯機制，我們這邊要額外做的工作就會少很多。例如許多程式框架提供自動重啟的功能，在外部保存 checkpoint，worker 只負責運算，終止信號一進來，也不用保存，直接拋棄未完成的工作進度，留待繼起的 worker 從 checkpoint 接手。&lt;/p&gt;
&lt;h3 id=&#34;preemption-selection&#34;&gt;Preemption selection&lt;/h3&gt;
&lt;p&gt;除了 24 小時的壽命限制會終止虛擬機，資料中心的事件也會觸發主動的虛擬機回收，&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#limitations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;由 GCP 主動觸發的回收機制機率很低&lt;/a&gt;，會根據每日每個區域 (zone) 的狀態而定。這裡描述資料中心啟動的臨時回收。&lt;/p&gt;
&lt;p&gt;GCP 不會把所你手上的 preemptible 機器都收走，而是依照&lt;a href=&#34;https://cloud.google.com/compute/docs/instances/preemptible?hl=zh-tw#preemption-process&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;一定的規則&lt;/a&gt;，選擇一個比例撤換的機器。&lt;/p&gt;
&lt;p&gt;先看文件敘述&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute Engine 會避免從單一客戶移除太多先占虛擬機&lt;/li&gt;
&lt;li&gt;優先移除新的虛擬機，偏向保留舊的虛擬機 (但最多仍活不過 24hr)&lt;/li&gt;
&lt;li&gt;開啟後馬上被移除的先占虛擬機不計費&lt;/li&gt;
&lt;li&gt;機器尺寸較小的機器，可用性較高。例如：16 cpu 的先占虛擬機，比 128 cpu 的先占虛擬機容易取得&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GCP 每天平均會移除一個專案中 5% - 15% 的虛擬機，從用戶的角度我們需要預期至少這個程度的回收。不過這個比例，GCP 也不給予任何保證。以筆者經驗，只能說絕大部分的時候，都不會擔心超過這個比例的回收，但是還是要做好最壞的打算，如果臨時無法取得足夠的先占虛擬機，要有方法暫時補足隨選虛擬機。&lt;/p&gt;
&lt;h3 id=&#34;gke&#34;&gt;GKE&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#kubernetes_constraint_violations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;使用先占虛擬機會違反 Kubernetes 的設計&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod grace period 會被忽略&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/workloads/pods/disruptions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pod disreuption budget&lt;/a&gt;，不會被遵守 (可能會超過)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;對應用的影響&#34;&gt;對應用的影響&lt;/h3&gt;
&lt;p&gt;GCP 觸發的 Preemptible process，對應用的影響&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;involuntary disruptions，GCP 送 ACPI G2 Soft Off&lt;/li&gt;
&lt;li&gt;OS 終止服務，包含執行 Kubernetes 的 container runtime&lt;/li&gt;
&lt;li&gt;容器內的應用會收到 SIGTERM，啟動 graceful shutdown
&lt;ul&gt;
&lt;li&gt;Kubernetes 提供的 Graceful-shutdown 可能會跑不完&lt;/li&gt;
&lt;li&gt;實務上只是中斷當前工作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cloud.google.com/solutions/scope-and-size-kubernetes-engine-clusters&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;大量節點同時回收&#34;&gt;大量節點同時回收&lt;/h3&gt;
&lt;p&gt;由於 GCP 並不保證收回的機器的數量，同時回收的機器量大，還是會衝擊到服務。例如：一次收回 15% 的算力，當然服務還是會受到衝擊。當然這樣事件的機率並不高，但我們仍是需要為此打算。這邊有幾個做法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;預留更多的算力&lt;/li&gt;
&lt;li&gt;使用 regional cluster，在多個 zone 上分配先占虛擬機&lt;/li&gt;
&lt;li&gt;我們自行控制，提前主動回收虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;預留更多資源&#34;&gt;預留更多資源&lt;/h3&gt;
&lt;p&gt;這點很直觀，由於使用了更加便宜的機器，我們可以用同樣的成本，開更多的機器。&lt;/p&gt;
&lt;p&gt;退一步說，使用打三折的先占虛擬機，然後開原本兩倍的機器數量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;總成本是 0.3 * 2 = 0.6 倍&lt;/li&gt;
&lt;li&gt;同時間可用資源是 2 倍&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由於先占節點回收的單位是一個一個虛擬機&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安排合適的機器尺寸&lt;/li&gt;
&lt;li&gt;尺寸較小的先占虛擬機，可用性較高。意思是零碎的先占虛擬機容易取得&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但當然也不能都開太小的機器，這會嚴重影響應用的分配。至於具體需要開多大，可以根據預計在機器上運行的應用，做綜合考量，例如有以下影用需要執行：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;app A: 1 cpu 5 replica&lt;/li&gt;
&lt;li&gt;app B: 3 cpu 5 replica&lt;/li&gt;
&lt;li&gt;app C: 5 cpu 5 replica&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;總共至少 45 cpu ，預期機器負載8 成的話，需要總共 45 / 0.8 = 56 cpu。也許可以考慮&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;8 cpu * 7 先占虛擬機&lt;/li&gt;
&lt;li&gt;4 cpu * 14&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也舉幾個極端不可行的例子&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;56 cpu * 1
&lt;ul&gt;
&lt;li&gt;這樣的虛擬機回收時的影響範圍 (blast radius) 就是 100% 服務&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1 cpu * 56
&lt;ul&gt;
&lt;li&gt;機器太瑣碎，可能超出 Qouta (節點數量，IP 數量&amp;hellip;)&lt;/li&gt;
&lt;li&gt;應用會更分散，節點間的內部網路流量會增加&lt;/li&gt;
&lt;li&gt;前幾篇提到的 reserved resource 比例高，會影響應用的部署&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果希望更保險，可以再補上隨選虛擬機混合搭配，例如&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;8 cpu * 7
&lt;ul&gt;
&lt;li&gt;5 先占虛擬機 2 隨選虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;4 cpu * 14
&lt;ul&gt;
&lt;li&gt;10 先占虛擬機 4 隨選虛擬機&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;虛擬機區域&#34;&gt;虛擬機區域&lt;/h3&gt;
&lt;p&gt;虛擬機的回收觸發，也是會依據服務的區域 (zone) 回收。意思是節點回收不會同時觸發 asia-east1 中所有 zone 的節點回收，一般來說時間是錯開的 (不過GCP 也不保證這點 XD)。為了維持 GKE 的可用性，我們都會開多個 node-pool 在多個區域下。&lt;/p&gt;
&lt;p&gt;總之避免把機器都放在同個區域中。&lt;/p&gt;
&lt;h3 id=&#34;自行控制的虛擬機汰換&#34;&gt;自行控制的虛擬機汰換&lt;/h3&gt;
&lt;p&gt;簡單來說我們在 24 hr 期限之前，先分批自盡 XD，打散個各個虛擬機的 24 小時限制。&lt;/p&gt;
&lt;p&gt;使用這個有趣的工具 &lt;a href=&#34;https://github.com/estafette/estafette-gke-preemptible-killer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estafette-gke-preemptible-killer&lt;/a&gt;，自動汰換先占虛擬機，讓整個繼起虛擬機都分散在 24 小時間。&lt;/p&gt;
&lt;p&gt;estafette-gke-preemptible-killer ，使用上簡單，大家自己看著辦 XD。如果大家有興趣，留言的人多的話，我再另外開一篇細講。&lt;/p&gt;
&lt;h3 id=&#34;小結&#34;&gt;小結&lt;/h3&gt;
&lt;p&gt;為了使用先占虛擬機，我們要多做以下幾件事&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;為應用設計可容錯分散式架構，例如應用可以同時執行一樣的 API server 3 個 replica&lt;/li&gt;
&lt;li&gt;分散 Pod 到合適的機器上，例如設置 PodAntiAffinity&lt;/li&gt;
&lt;li&gt;設定合適的虛擬機大小，合適的分散應用&lt;/li&gt;
&lt;li&gt;使用 &lt;a href=&#34;https://github.com/estafette/estafette-gke-preemptible-killer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;estafette-gke-preemptible-killer&lt;/a&gt;，自動汰換先占虛擬機&lt;/li&gt;
&lt;li&gt;不依賴應用的 Graceful-shutdown 流程&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance Requirement Distributed</title>
      <link>https://chechia.net/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/</link>
      <pubDate>Tue, 22 Sep 2020 14:39:00 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-22-gcp-preemptible-instance-requirement-distributed/</guid>
      <description>&lt;p&gt;我們以下幾個需求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;執行短期的 batch job&lt;/li&gt;
&lt;li&gt;執行長期的 user-facing API server&lt;/li&gt;
&lt;li&gt;執行長期的 stateful 資料庫、儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;該不該在 Kubernetes 上面跑 database？&lt;/p&gt;
&lt;p&gt;TL;DR ，如果你剛開始考慮這件事，通常的答案都是否定的&lt;/p&gt;
&lt;p&gt;等等，我們這邊不是討論該不該上 Kuberentes ，而是該不該使用先占虛擬機吧。然而由於先占虛擬機節點的諸多限制，光憑先占虛擬機並不適合跑任何持久性的儲存庫。我們這邊仰賴 Kubernetes 的網路功能 (e.g. 服務發現)，與自動管理 (e.g. health check，HPA，auto-scaler)，基於先占虛擬機，建構高可用性的服務架構，來支撐高可用，且有狀態的的儲存庫。&lt;/p&gt;
&lt;p&gt;應用是否適合部署到 Kubernetes 上，可以看這篇 &lt;a href=&#34;https://cloud.google.com/blog/products/databases/to-run-or-not-to-run-a-database-on-kubernetes-what-to-consider&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Blog: To run or not to run a database on Kubernetes: What to consider&lt;/a&gt;，如果大家有興趣，再留言告訴我，我再進行中文翻譯。&lt;/p&gt;
&lt;p&gt;文中針對三個可能的方案做分析，以 MySQL 為例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sass，GCP 的 Cloud SQL
&lt;ul&gt;
&lt;li&gt;最低的管理維運成本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自架 MySQL 在 GCP 的 VM 上，自行管理
&lt;ul&gt;
&lt;li&gt;自負完全的管理責任，包含可用性，備份 (backup)，以及容錯移轉 (failover)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自架 MySQL 在 Kubernetes 上
&lt;ul&gt;
&lt;li&gt;自負完全的管理責任&lt;/li&gt;
&lt;li&gt;Kubernetes 的複雜抽象層，會加重維運工作的複雜程度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而 RDBMS 的提供商，自家也提供 Operator&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/oracle/mysql-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Oracle 自家提供的 MySQL Operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/CrunchyData/postgres-operator&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CrunchyData 也有提供的 Postgres Operator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你就想，所以這些人是想怎樣，RDBMS 放 Kubernetes 上到底是行不行 XD。Google 的文章說明：如果應用本身並不符合 Kubernetes 的工作流程 (Pod life-cycle)，可以透過上述的 Operator 來自動化許多維運的作業，降低維運的困難。&lt;/p&gt;
&lt;p&gt;然而 DB 有千百種，除了 RDBMS 以外，還有另外一批 Database 天生就具有分散式的架構，這些儲存庫部署到 Kubernetes 上，並不會太痛苦 (還是要付出一定的成本XD)，但是卻可以得益於 Kubernetes 的諸多功能。&lt;/p&gt;
&lt;p&gt;底下我們先根據分散式的儲存庫做概觀描述，本系列文的最後，會根據時間狀況，做實例分享：Cassandra 或是 CockroachDB。提供各位一點發想，並根據需求去選擇需要的儲存庫&lt;/p&gt;
&lt;p&gt;「行不行要問你自己了施主，技術上都可以，維運上要看看你的團隊有沒有那個屁股吃這份藥 XD」&lt;/p&gt;
&lt;h3 id=&#34;distributed-database&#34;&gt;Distributed Database&lt;/h3&gt;
&lt;p&gt;底下非常粗淺的簡介分散式儲存庫的概念，提供一個基準點，幫助接下來討論是否可以使用先占虛擬機。這邊要強調，儲存庫的類型千百種，底層的各種實作差異都非常大，底下的模型是基於 cassandra 但不會走太多細節。 cassandra 的規格有機會再細聊。&lt;/p&gt;
&lt;p&gt;當後端應用已經順利水平拓展之後，整體服務的效能瓶頸往往都壓在後端 DB 上。這些不同的 DB 面向不同的需求，當需求符合時，可以考慮使用這些解決方法。&lt;/p&gt;
&lt;p&gt;這邊要強調，不是放棄現有的 RDBMS ，完全移轉到新的資料庫，這樣的成本太高，也沒有必要性。更好的做法，是搭配既有的關聯性資料庫，將不是核心業務的資料處理抽出，移轉到合適的資料庫上。讓不同需求的資料儲存到更合適的儲存庫，是這段話要強調的重點，關連式資料庫也不是唯一選擇。&lt;/p&gt;
&lt;p&gt;分散式的資料庫有以下特徵&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分散式節點集群 (Cluster)：資料庫是多個節點共存，而非 single master, multiple slaves 的架構
&lt;ul&gt;
&lt;li&gt;配合共識算法 (consensus algorithm) 溝通節點之間的資訊&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;無單點錯誤 (Single-point failure)：e.g. 不會因為 master 錯誤導致整個服務失效&lt;/li&gt;
&lt;li&gt;高可用(High Availitility：可以承受集群中一定數量虛擬機故障，服務仍然可用&lt;/li&gt;
&lt;li&gt;資料 sharding 到不同節點上&lt;/li&gt;
&lt;li&gt;複本 (replica)
&lt;ul&gt;
&lt;li&gt;節點複本 (node replicas)：多個節點提供服務，提供流量的帶寬與可用性&lt;/li&gt;
&lt;li&gt;資料複本 (data replicas)：在多個節點上儲存資料，提供資料的備份，同時也提供讀取帶寬與可用性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;從以上特徵來說，使用此架構的服務可以承受先占虛擬機的不定時終止，或許可以使用。&lt;/p&gt;
&lt;p&gt;實務上有非常多需要注意，需要依據各自服務的性質，各自處理。常見的問題舉例如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;應用可以容錯 (fault-tolerent)，然而錯誤發生後，會需要消耗復原成本，例如重啟後需要花時間初始化，或是在多節點上進行 data rebalance。&lt;/li&gt;
&lt;li&gt;可以承受突然的錯誤，使用先占虛擬機，變成每日固定會承受必然的錯誤。這裡犧牲了部分算力，甚至造成隱性的維護成本，最後是否符合節省成本的需求。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;都是需要仔細了解解決方案，並且分析需求，來評估是否有合乎成本。&lt;/p&gt;
&lt;h1 id=&#34;gke&#34;&gt;GKE&lt;/h1&gt;
&lt;p&gt;以上分析了三種常見需求例子：從 batch job，user-facing service，與 distributed database。明天會實際搬出 GKE 與 GCP Preemptible Instance 的技術規格，與大家實際討論。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gcp Preemptible Instance</title>
      <link>https://chechia.net/zh-hant/post/2020-09-21-gcp-preemptible-instance/</link>
      <pubDate>Mon, 21 Sep 2020 09:22:17 +0800</pubDate>
      <guid>https://chechia.net/zh-hant/post/2020-09-21-gcp-preemptible-instance/</guid>
      <description>&lt;h1 id=&#34;前言&#34;&gt;前言&lt;/h1&gt;
&lt;p&gt;鐵人賽的第二部分，要帶來公有雲省錢系列文章。&lt;/p&gt;
&lt;p&gt;架構的成本，很多時候會影響架構的設計與需求。公司的營運都需要在成本與需求之前平衡，成本其實是影響公司決策的重要因素。身為架構管理員，應該要試著量化並且進行成本管理，提出解決方案時，也需要思考如何幫公司開源節流。&lt;/p&gt;
&lt;p&gt;一昧消減架構的成本也未必是最佳方案，帳面上消減的成本有時也會反映在其他地方，例如：使用比較便宜的解決方案，或是較低的算力，但卻造成維運需要花更多時間維護，造成隱性的人力成本消耗。用什麼替代方案 (trade-off) 省了這些錢。&lt;/p&gt;
&lt;p&gt;Kubernetes 是一個很好的例子：例如：有人說「Kubernetes 可以省錢」，但也有人說「Kubernetes 產生的 Overhead 太重會虧錢」。&lt;/p&gt;
&lt;p&gt;「要不要導入 Kubernetes 是一個好問題」。應該回歸基本的需求，了解需求是什麼。例如：Google 當初開發容器管理平台，是面對什麼樣的使用需求，最終開發出 Kubernetes，各位可以回顧前篇文章「Borg Omega and Kubernete，Kubernetes 的前日今生，與 Google 十餘年的容器化技術」，從 Google 的角度理解容器管理平台，反思自身團隊的實際需求。&lt;/p&gt;
&lt;p&gt;這套解決方案是否真的適合團隊，解決方案帶來的效果到底是怎樣呢？希望看完這系列文章後，能幫助各位，從成本面思考這些重要的問題。&lt;/p&gt;
&lt;p&gt;這篇使用 GCP 的原因，除了是我最熟悉的公有雲外，也是因為 GCP 提供的免費額度，讓我可以很輕鬆地作為社群文章的 Demo，如果有別家雲平台有提供相同方案，請留言告訴我，我可能就會多開幾家不同的範例。&lt;/p&gt;
&lt;h1 id=&#34;先占虛擬機-tldr&#34;&gt;先占虛擬機 TL;DR&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;先占虛擬機為隨選虛擬機定價的 2-3 折，使用先占虛擬機可能可以節省 7 成的雲平台支出&lt;/li&gt;
&lt;li&gt;先占虛擬機比起隨選虛擬機，外加有諸多限制，e.g. 最長壽命 24 hr、雲平台會主動終止先占虛擬機&amp;hellip;等&lt;/li&gt;
&lt;li&gt;配合使用自動水平擴展 (auto-scaler)，讓舊的先占虛擬機回收的同時，去購買新的先占虛擬機&lt;/li&gt;
&lt;li&gt;配合可容錯 (fault-tolerent) 的分散式應用，讓應用可以無痛在虛擬機切換轉移，不影響服務&lt;/li&gt;
&lt;li&gt;要讓應用可以容錯，需要做非常多事情&lt;/li&gt;
&lt;li&gt;搭配 kubernetes ，自動化管理來簡化工作&lt;/li&gt;
&lt;li&gt;配合正確的設定，可以穩定的執行有狀態的分散式資料庫或儲存庫&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或是看 Google 官方 Blog：&lt;a href=&#34;https://cloud.google.com/blog/products/containers-kubernetes/cutting-costs-with-google-kubernetes-engine-using-the-cluster-autoscaler-and-preemptible-vms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cutting costs with Google Kubernetes Engine: using the cluster autoscaler and Preemptible VMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;預計內容&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需求假設、釐清需求，並且精準計價&lt;/li&gt;
&lt;li&gt;精準計價使用先占虛擬機的節省成本&lt;/li&gt;
&lt;li&gt;先占虛擬機的規格、額外限制&lt;/li&gt;
&lt;li&gt;額外限制，造成技術要多做很多額外的事情&lt;/li&gt;
&lt;li&gt;實務經驗分享：API server&lt;/li&gt;
&lt;li&gt;實務經驗：從使用隨選虛擬機，移轉到先占虛擬機，公司實際導入經驗&lt;/li&gt;
&lt;li&gt;實務經驗：Elasticsearch&lt;/li&gt;
&lt;li&gt;實務經驗分享：其他分散式資料庫，也許是 Cassandra 或是 cockroachDB&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的內容不曉得會寫幾篇看感覺 XD&lt;/p&gt;
&lt;p&gt;有寫過鐵人賽的都知道 30 篇真的很漫長，一篇文章幾千字，都要花好幾個小時。我去年後半，真的都會看讀者的留言跟按讚，取暖一波，才有動力繼續寫。留言的人多就會多寫，留言的人少就會少寫，各位覺得文章還看得下去，請務必來我粉專按讚留個言，不管是推推、鞭鞭、或是有想看的文章來許願，都十分歡迎。有你們的支持，我才有動力繼續寫。&lt;/p&gt;
&lt;p&gt;請大家務必以實際行動支持好文章，不要讓劣幣驅逐良幣。不然 iThome 上面之後只剩洗觀看數的熱門文章了 XD&lt;/p&gt;
&lt;p&gt;當然，沒人留言我就會當作自己才是垃圾文 (自知之明XD)，就會收一收回家嚕貓睡覺，掰掰~&lt;/p&gt;
&lt;p&gt;-&amp;gt;我的粉專，等你來留言&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
