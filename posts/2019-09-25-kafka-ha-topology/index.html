<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kafka HA Topology | Che-Chia Chang</title><meta name=keywords content="鐵人賽2019,kafka,kubernetes"><meta name=description content="2020 It邦幫忙鐵人賽 系列文章

ELK Stack

Self-host ELK stack on GCP
Secure ELK Stask
監測 Google Compute Engine 上服務的各項數據
監測 Google Kubernetes Engine 的各項數據
是否選擇 ELK 作為解決方案
使用 logstash pipeline 做數據前處理
Elasticsearch 日常維護：數據清理，效能調校，永久儲存
Debug ELK stack on GCP


Kafka HA on Kubernetes(6)

Deploy kafka-ha
Kafka Introduction
kafka 基本使用
kafka operation scripts
集群內部的 HA topology
集群內部的 HA 細節
Prometheus Metrics Exporter 很重要
效能調校



由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><meta name=author content="chechiachang"><link rel=canonical href=https://chechia.net/posts/2019-09-25-kafka-ha-topology/><meta name=google-site-verification content="G-QYR8JCDGM9"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://chechia.net/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://chechia.net/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://chechia.net/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://chechia.net/favicon/favicon-32x32.png><link rel=mask-icon href=https://chechia.net/favicon/favicon-32x32.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://chechia.net/posts/2019-09-25-kafka-ha-topology/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://chechia.net/posts/2019-09-25-kafka-ha-topology/"><meta property="og:site_name" content="Che-Chia Chang"><meta property="og:title" content="Kafka HA Topology"><meta property="og:description" content="2020 It邦幫忙鐵人賽 系列文章
ELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-25T22:50:32+08:00"><meta property="article:modified_time" content="2019-09-25T22:50:32+08:00"><meta property="article:tag" content="鐵人賽2019"><meta property="article:tag" content="Kafka"><meta property="article:tag" content="Kubernetes"><meta property="og:image" content="https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Kafka HA Topology"><meta name=twitter:description content="2020 It邦幫忙鐵人賽 系列文章

ELK Stack

Self-host ELK stack on GCP
Secure ELK Stask
監測 Google Compute Engine 上服務的各項數據
監測 Google Kubernetes Engine 的各項數據
是否選擇 ELK 作為解決方案
使用 logstash pipeline 做數據前處理
Elasticsearch 日常維護：數據清理，效能調校，永久儲存
Debug ELK stack on GCP


Kafka HA on Kubernetes(6)

Deploy kafka-ha
Kafka Introduction
kafka 基本使用
kafka operation scripts
集群內部的 HA topology
集群內部的 HA 細節
Prometheus Metrics Exporter 很重要
效能調校



由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://chechia.net/posts/"},{"@type":"ListItem","position":2,"name":"Kafka HA Topology","item":"https://chechia.net/posts/2019-09-25-kafka-ha-topology/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kafka HA Topology","name":"Kafka HA Topology","description":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n","keywords":["鐵人賽2019","kafka","kubernetes"],"articleBody":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 Zookeeper Multi-server setup Kafka Multi-broker setup Zookeeper Multi-Server 為了維持 zookeeper 有效運作，cluster 必須維持 majority (多數)，也就是至少一半的機器在線。如果總共 3 台，便可以忍受 1 台故障仍保有 majority。如果是 5 台就可以容忍 2 台故障。一般來說都建議使用基數數量。Zookeeper Multi Server Setup\n普遍情況，3 台 zookeeper 已經是 production ready 的狀態，但如果為了更高的可用性，以方便進行單節點停機維護，可以增加節點數量。\nTopology 需要將 zookeeper 放在不同的機器上，不同的網路環境，甚至是不同的雲平台區域上，以承受不同程度的故障。例如單台機器故障，或是區域性的網路故障。\n我們這邊會使用 Kubernetes PodAntiAffinity，要求 scheduler 在部屬時，必須將 zookeeper 分散到不同的機器上。設定如下：\nvim values-staging.yaml zookeeper: enabled: true resources: ~ env: ZK_HEAP_SIZE: \"1G\" persistence: enabled: false image: PullPolicy: \"IfNotPresent\" url: \"\" port: 2181 ## Pod scheduling preferences (by default keep pods within a release on separate nodes). ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## By default we don't set affinity: affinity: # Criteria by which pod label-values influence scheduling for zookeeper pods. podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: \"kubernetes.io/hostname\" labelSelector: matchLabels: release: zookeeper 使用 podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution，如果 topologyKey 已經有指定 label 的 pod 存在，則無法部署，需要數到其他台機器。\nkubectl get pods --output wide | grep zookeeper NAME READY STATUS RESTARTS AGE IP NODE kafka-0-zookeeper-0 1/1 Running 0 42d 10.8.12.4 gke-chechiachang-pool-1-e06e6d00-pc98 kafka-0-zookeeper-1 1/1 Running 0 42d 10.8.4.4 gke-chechiachang-pool-1-e06e6d00-c29q kafka-0-zookeeper-2 1/1 Running 0 42d 10.8.3.6 gke-chechiachang-pool-1-e06e6d00-krwc 效果是 zookeeper 都分配到不同的機器上。\nGuarantees Zookeeper 對於資料一致性，有這些保障 Consistency Guarantees\n順序一致性：資料更新的順序，與發送的順序一致 原子性：資料更新只有成功或失敗，沒有部份效果 系統一致性：可戶端連到 server 看到的東西都是一樣，無關連入哪個 server 可靠性： 客戶端的更新請求，一但收到 server 回覆更新成功，便會持續保存狀態。某些錯誤會造成客戶端收不到回覆， 可能是網路問題，或是 server 內部問題，這邊就無法確定 server 上的狀態，是否被更新了，或是請求已經遺失了。 從客戶讀取到的資料都是以確認的資料，不會因為 server 故障回滾(Roll back)而回到舊的狀態 Kafka 的設定 ## The StatefulSet installs 3 pods by default replicas: 3 resources: limits: cpu: 200m memory: 4096Mi requests: cpu: 100m memory: 1024Mi kafkaHeapOptions: \"-Xmx4G -Xms1G\" 設定 broker 的數量，以及 Pod 提供的 resource，並且透過 heapOption 把記憶體設定塞進 JVM\naffinity: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - kafka topologyKey: \"kubernetes.io/hostname\" podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - zookeeper 這邊下了兩個 affinity\npodAntiAffinity 盡量讓 kafka-broker 分散到不同機器上 podAffinity 讓 broker prefer 跟 zookeeper 放在一起 分散的理由同上，不希望一台機器死了，就讓多個 broker 跟著死\n要和 zookeeper 放在一起，就要看需求與實際環境調整\nconfigurationOverrides: \"default.replication.factor\": 3 \"offsets.topic.replication.factor\": 2 # Increased from 1 to 2 for higher output \"offsets.topic.num.partitions\": 3 \"confluent.support.metrics.enable\": false # Disables confluent metric submission \"auto.leader.rebalance.enable\": true \"auto.create.topics.enable\": true \"message.max.bytes\": \"16000000\" # Extend global topic max message bytes to 16 Mb 這邊再把 broker 運行的設定參數塞進去，參數的用途大多與複本與高可用機制有關下面都會提到。\nKafka 的複本機制 kafka 的副本機制 預設將各個 topic partition 的 log 分散到 server 上，如果其中一台 server 故障，資料仍然可用。\n兩個重要的設定\nnum.partitions=N default.replication.factor=M kafka 預設使用複本，所有機制與設計都圍繞著複本。如果（因為某些原因）不希望使用複本，可將 replication factor 設為 1。\nreplication 的單位是 topic partition，正常狀況下\n一個 partition 會有一個 leader，以及零個或以上個 follower leader + follower 總數是 replication factor 所有讀寫都是對 leader 讀寫 leader 的 log 會同步到 follower 上，leader 與 follower 狀態是一樣的 Election \u0026 Load balance 通常一個 topic 會有多個 partition，也就是說，每個 topic 會有多個 partition leader，分散負載\n通常 topic partition 的總數會比 broker 的數量多\n以上一篇範例，我們有三個 kafka-0-broker 各自是一個 Pod 有 topic: ticker 跟預設的 __consumer_offset__，乘上 partition number 的設定值(N)，會有 2N 個 partitions partitiion 會有各自的複本，kafka 會盡量將相同 topic 的複本分散到不同 broker 上 kafka 也會盡量維持 partition 的 leader 分散在不同的 broker 上，這個部分 kafka 會透過算法做 leader election，也可手動使用腳本做 Balancing leadership 總之，topic 的 partition 與 leader 會分散到 broker 上，維持 partition 的可用性。\nsync node 要能夠維持 zookeeper 的 session (zookeeper 有 heartbeat 機制) follower 不能落後 leader 太多 kafka 能保障資料不會遺失，只要至少一個 node 是在 sync 的狀態。例如本來有三個 partition，其中兩個 partition 不同步，只要其中一個 partition 是同步，便能作為 leader 持續提供正確的 message。\nReplicated Logs kafka 透過 replicated log 維持分散式的 partition\n複本間要維持共識(consensus)的最簡單機制，就是單一 leader 決定，其他 follower 跟隨。然而萬一 leader 死了，選出的新 leader 卻還沒跟上原先 leader 的資料。這時便使用 replicated log，來確保新的 leader 就算原先沒跟上，也能透過 replicated log 隨後跟上且不遺失資料。維持 log 一直都同步的前提，就是 leader 要一直確認 followers 的 log 都有跟上，這個其實就是變相的多 leader，效能消耗較大。\n另一個維持 log 機制，如果希望 follower 彼此的 log 應該先進行比對，讓資料交接過程有 overlap，這個過程稱為 Quorum。一個常用的方式是多數決(majority)\n如果總共有 2n+1 的 node，leader 要向 n+1 個 follower 取得共識，才確定這個 log 已經 commit 了 leader 總是維持 n+1 follower 的 log 有跟上，因此可以容忍最多 n 個 node 死了，集群整體能有 n+1 的 node 維持著正確的 commited log 不用向所有 node 確認才 commit ，節省了一半的 ack majarity 的另一個好處是，n+1 共識的速度是由前 1/2 快的 node 決定的。由於只要先取得 n+1 就可以 commit，速度快的 node 會先回應，讓整體速度提升。 ","wordCount":"616","inLanguage":"en","image":"https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2019-09-25T22:50:32+08:00","dateModified":"2019-09-25T22:50:32+08:00","author":{"@type":"Person","name":"chechiachang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://chechia.net/posts/2019-09-25-kafka-ha-topology/"},"publisher":{"@type":"Organization","name":"Che-Chia Chang","logo":{"@type":"ImageObject","url":"https://chechia.net/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chechia.net/ accesskey=h title="Home (Alt + H)"><img src=https://chechia.net/favicon/favicon-32x32.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://chechia.net/#posts title=Posts><span>Posts</span></a></li><li><a href=https://www.instagram.com/c.c.leather/ title=Leather><span>Leather</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.instagram.com/dive.with.cat.coach/ title=Scuba><span>Scuba</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://mvp.microsoft.com/zh-TW/mvp/profile/e407d0b9-5c01-eb11-a815-000d3a8ccaf5 title=MVP><span>MVP</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://chechia.net/categories/ title=categories><span>categories</span></a></li><li><a href=https://chechia.net/tags/ title=tags><span>tags</span></a></li><li><a href=https://chechia.net/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://chechia.net/>Home</a>&nbsp;»&nbsp;<a href=https://chechia.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Kafka HA Topology</h1><div class=post-meta><span title='2019-09-25 22:50:32 +0800 CST'>September 25, 2019</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;616 words&nbsp;·&nbsp;chechiachang&nbsp;|&nbsp;<a href=https://github.com/chechiachang.github.io-src/content/posts/2019-09-25-kafka-ha-topology.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#topology>Topology</a></li><li><a href=#guarantees>Guarantees</a></li></ul></li></ul><ul><li><ul><li><a href=#election--load-balance>Election & Load balance</a></li><li><a href=#sync>sync</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><a href=https://ithelp.ithome.com.tw/2020ironman>2020 It邦幫忙鐵人賽</a> 系列文章</p><ul><li>ELK Stack<ul><li><a href=https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/>Self-host ELK stack on GCP</a></li><li><a href=https://chechia.net/posts/2019-09-15-secure-elk-stack/>Secure ELK Stask</a></li><li><a href=https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/>監測 Google Compute Engine 上服務的各項數據</a></li><li><a href=https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/>監測 Google Kubernetes Engine 的各項數據</a></li><li><a href=https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/>是否選擇 ELK 作為解決方案</a></li><li><a href=https://chechia.net/posts/2019-09-21-logstash-on-gke/>使用 logstash pipeline 做數據前處理</a></li><li>Elasticsearch 日常維護：數據清理，效能調校，永久儲存</li><li>Debug ELK stack on GCP</li></ul></li><li>Kafka HA on Kubernetes(6)<ul><li><a href=https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/>Deploy kafka-ha</a></li><li><a href=https://chechia.net/posts/2019-09-23-kafka-introduction/>Kafka Introduction</a></li><li><a href=https://chechia.net/posts/2019-09-24-kafka-basic-usage/>kafka 基本使用</a></li><li><a href=https://chechia.net/posts/2019-09-25-kafka-operation-scripts/>kafka operation scripts</a></li><li><a href=https://chechia.net/posts/2019-09-25-kafka-ha-topology/>集群內部的 HA topology</a></li><li><a href=https://chechia.net/posts/2019-09-26-kafka-ha-continued/>集群內部的 HA 細節</a></li><li>Prometheus Metrics Exporter 很重要</li><li>效能調校</li></ul></li></ul><p>由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。</p><p>寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。</p><p>對我的文章有興趣，歡迎到我的網站上 <a href=https://chechia.net>https://chechia.net</a> 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。</p><p><img alt="Exausted Cat Face" loading=lazy src=https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg></p><hr><h1 id=摘要>摘要<a hidden class=anchor aria-hidden=true href=#摘要>#</a></h1><ul><li>Zookeeper Multi-server setup</li><li>Kafka Multi-broker setup</li></ul><h1 id=zookeeper-multi-server>Zookeeper Multi-Server<a hidden class=anchor aria-hidden=true href=#zookeeper-multi-server>#</a></h1><p>為了維持 zookeeper 有效運作，cluster 必須維持 majority (多數)，也就是至少一半的機器在線。如果總共 3 台，便可以忍受 1 台故障仍保有 majority。如果是 5 台就可以容忍 2 台故障。一般來說都建議使用基數數量。<a href=https://zookeeper.apache.org/doc/r3.4.12/zookeeperAdmin.html#sc_zkMulitServerSetup>Zookeeper Multi Server Setup</a></p><p>普遍情況，3 台 zookeeper 已經是 production ready 的狀態，但如果為了更高的可用性，以方便進行單節點停機維護，可以增加節點數量。</p><h3 id=topology>Topology<a hidden class=anchor aria-hidden=true href=#topology>#</a></h3><p>需要將 zookeeper 放在不同的機器上，不同的網路環境，甚至是不同的雲平台區域上，以承受不同程度的故障。例如單台機器故障，或是區域性的網路故障。</p><p>我們這邊會使用 Kubernetes PodAntiAffinity，要求 scheduler 在部屬時，必須將 zookeeper 分散到不同的機器上。設定如下：</p><pre><code>vim values-staging.yaml

zookeeper:
  enabled: true

  resources: ~

  env:
    ZK_HEAP_SIZE: &quot;1G&quot;

  persistence:
    enabled: false

  image:
    PullPolicy: &quot;IfNotPresent&quot;

  url: &quot;&quot;
  port: 2181

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we don't set affinity:
  affinity: # Criteria by which pod label-values influence scheduling for zookeeper pods.
   podAntiAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
       - topologyKey: &quot;kubernetes.io/hostname&quot;
         labelSelector:
           matchLabels:
             release: zookeeper
</code></pre><p>使用 podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution，如果 topologyKey 已經有指定 label 的 pod 存在，則無法部署，需要數到其他台機器。</p><pre><code>kubectl get pods --output wide | grep zookeeper

NAME                  READY   STATUS      RESTARTS   AGE     IP              NODE                                    
kafka-0-zookeeper-0   1/1     Running     0          42d     10.8.12.4       gke-chechiachang-pool-1-e06e6d00-pc98   
kafka-0-zookeeper-1   1/1     Running     0          42d     10.8.4.4        gke-chechiachang-pool-1-e06e6d00-c29q   
kafka-0-zookeeper-2   1/1     Running     0          42d     10.8.3.6        gke-chechiachang-pool-1-e06e6d00-krwc   
</code></pre><p>效果是 zookeeper 都分配到不同的機器上。</p><h3 id=guarantees>Guarantees<a hidden class=anchor aria-hidden=true href=#guarantees>#</a></h3><p>Zookeeper 對於資料一致性，有這些保障 <a href=https://zookeeper.apache.org/doc/r3.5.5/zookeeperProgrammers.html#ch_zkGuarantees>Consistency Guarantees</a></p><ul><li>順序一致性：資料更新的順序，與發送的順序一致</li><li>原子性：資料更新只有成功或失敗，沒有部份效果</li><li>系統一致性：可戶端連到 server 看到的東西都是一樣，無關連入哪個 server</li><li>可靠性：<ul><li>客戶端的更新請求，一但收到 server 回覆更新成功，便會持續保存狀態。某些錯誤會造成客戶端收不到回覆， 可能是網路問題，或是 server 內部問題，這邊就無法確定 server 上的狀態，是否被更新了，或是請求已經遺失了。</li><li>從客戶讀取到的資料都是以確認的資料，不會因為 server 故障回滾(Roll back)而回到舊的狀態</li></ul></li></ul><h1 id=kafka-的設定>Kafka 的設定<a hidden class=anchor aria-hidden=true href=#kafka-的設定>#</a></h1><pre><code>## The StatefulSet installs 3 pods by default
replicas: 3

resources:
   limits:
     cpu: 200m
     memory: 4096Mi
   requests:
     cpu: 100m
     memory: 1024Mi
kafkaHeapOptions: &quot;-Xmx4G -Xms1G&quot;
</code></pre><p>設定 broker 的數量，以及 Pod 提供的 resource，並且透過 heapOption 把記憶體設定塞進 JVM</p><pre><code>affinity:
 affinity:
   podAntiAffinity:
     requiredDuringSchedulingIgnoredDuringExecution:
     - labelSelector:
         matchExpressions:
         - key: app
           operator: In
           values:
           - kafka
       topologyKey: &quot;kubernetes.io/hostname&quot;
   podAffinity:
     preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
                - zookeeper
</code></pre><p>這邊下了兩個 affinity</p><ul><li>podAntiAffinity 盡量讓 kafka-broker 分散到不同機器上</li><li>podAffinity 讓 broker prefer 跟 zookeeper 放在一起</li></ul><p>分散的理由同上，不希望一台機器死了，就讓多個 broker 跟著死</p><p>要和 zookeeper 放在一起，就要看需求與實際環境調整</p><pre><code>configurationOverrides:
  &quot;default.replication.factor&quot;: 3
  &quot;offsets.topic.replication.factor&quot;: 2 # Increased from 1 to 2 for higher output
  &quot;offsets.topic.num.partitions&quot;: 3
  &quot;confluent.support.metrics.enable&quot;: false  # Disables confluent metric submission
  &quot;auto.leader.rebalance.enable&quot;: true
  &quot;auto.create.topics.enable&quot;: true
  &quot;message.max.bytes&quot;: &quot;16000000&quot; # Extend global topic max message bytes to 16 Mb
</code></pre><p>這邊再把 broker 運行的設定參數塞進去，參數的用途大多與複本與高可用機制有關下面都會提到。</p><h1 id=kafka-的複本機制>Kafka 的複本機制<a hidden class=anchor aria-hidden=true href=#kafka-的複本機制>#</a></h1><p><a href=https://kafka.apache.org/documentation/#replication>kafka 的副本機制</a> 預設將各個 topic partition 的 log 分散到 server 上，如果其中一台 server 故障，資料仍然可用。</p><p>兩個重要的設定</p><ul><li>num.partitions=N</li><li>default.replication.factor=M</li></ul><p>kafka 預設使用複本，所有機制與設計都圍繞著複本。如果（因為某些原因）不希望使用複本，可將 replication factor 設為 1。</p><p>replication 的單位是 topic partition，正常狀況下</p><ul><li>一個 partition 會有一個 leader，以及零個或以上個 follower</li><li>leader + follower 總數是 replication factor</li><li>所有讀寫都是對 leader 讀寫</li><li>leader 的 log 會同步到 follower 上，leader 與 follower 狀態是一樣的</li></ul><h3 id=election--load-balance>Election & Load balance<a hidden class=anchor aria-hidden=true href=#election--load-balance>#</a></h3><p>通常一個 topic 會有多個 partition，也就是說，每個 topic 會有多個 partition leader，分散負載</p><p>通常 topic partition 的總數會比 broker 的數量多</p><ul><li>以上一篇範例，我們有三個 kafka-0-broker 各自是一個 Pod</li><li>有 topic: ticker 跟預設的 <code>__consumer_offset__</code>，乘上 partition number 的設定值(N)，會有 2N 個 partitions</li><li>partitiion 會有各自的複本，kafka 會盡量將相同 topic 的複本分散到不同 broker 上</li><li>kafka 也會盡量維持 partition 的 leader 分散在不同的 broker 上，這個部分 kafka 會透過算法做 leader election，也可手動使用腳本做 <a href=https://kafka.apache.org/documentation/#basic_ops_leader_balancing>Balancing leadership</a></li></ul><p>總之，topic 的 partition 與 leader 會分散到 broker 上，維持 partition 的可用性。</p><h3 id=sync>sync<a hidden class=anchor aria-hidden=true href=#sync>#</a></h3><ul><li>node 要能夠維持 zookeeper 的 session (zookeeper 有 heartbeat 機制)</li><li>follower 不能落後 leader 太多</li></ul><p>kafka 能保障資料不會遺失，只要至少一個 node 是在 sync 的狀態。例如本來有三個 partition，其中兩個 partition 不同步，只要其中一個 partition 是同步，便能作為 leader 持續提供正確的 message。</p><h1 id=replicated-logs>Replicated Logs<a hidden class=anchor aria-hidden=true href=#replicated-logs>#</a></h1><p>kafka 透過 <a href=https://kafka.apache.org/documentation/#design_replicatedlog>replicated log</a> 維持分散式的 partition</p><p>複本間要維持共識(consensus)的最簡單機制，就是單一 leader 決定，其他 follower 跟隨。然而萬一 leader 死了，選出的新 leader 卻還沒跟上原先 leader 的資料。這時便使用 replicated log，來確保新的 leader 就算原先沒跟上，也能透過 replicated log 隨後跟上且不遺失資料。維持 log 一直都同步的前提，就是 leader 要一直確認 followers 的 log 都有跟上，這個其實就是變相的多 leader，效能消耗較大。</p><p>另一個維持 log 機制，如果希望 follower 彼此的 log 應該先進行比對，讓資料交接過程有 overlap，這個過程稱為 Quorum。一個常用的方式是多數決(majority)</p><ul><li>如果總共有 2n+1 的 node，leader 要向 n+1 個 follower 取得共識，才確定這個 log 已經 commit 了</li><li>leader 總是維持 n+1 follower 的 log 有跟上，因此可以容忍最多 n 個 node 死了，集群整體能有 n+1 的 node 維持著正確的 commited log</li><li>不用向所有 node 確認才 commit ，節省了一半的 ack</li><li>majarity 的另一個好處是，n+1 共識的速度是由前 1/2 快的 node 決定的。由於只要先取得 n+1 就可以 commit，速度快的 node 會先回應，讓整體速度提升。</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://chechia.net/tags/%E9%90%B5%E4%BA%BA%E8%B3%BD2019/>鐵人賽2019</a></li><li><a href=https://chechia.net/tags/kafka/>Kafka</a></li><li><a href=https://chechia.net/tags/kubernetes/>Kubernetes</a></li></ul><nav class=paginav><a class=prev href=https://chechia.net/posts/2019-09-26-kafka-ha-continued/><span class=title>« Prev</span><br><span>Kafka HA Continued</span>
</a><a class=next href=https://chechia.net/posts/2019-09-25-kafka-operation-scripts/><span class=title>Next »</span><br><span>Kafka Operation Scripts</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on x" href="https://x.com/intent/tweet/?text=Kafka%20HA%20Topology&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f&amp;hashtags=%e9%90%b5%e4%ba%ba%e8%b3%bd2019%2ckafka%2ckubernetes"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f&amp;title=Kafka%20HA%20Topology&amp;summary=Kafka%20HA%20Topology&amp;source=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f&title=Kafka%20HA%20Topology"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on whatsapp" href="https://api.whatsapp.com/send?text=Kafka%20HA%20Topology%20-%20https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on telegram" href="https://telegram.me/share/url?text=Kafka%20HA%20Topology&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka HA Topology on ycombinator" href="https://news.ycombinator.com/submitlink?t=Kafka%20HA%20Topology&u=https%3a%2f%2fchechia.net%2fposts%2f2019-09-25-kafka-ha-topology%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://chechia.net/>Che-Chia Chang</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>