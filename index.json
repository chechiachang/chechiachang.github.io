[{"authors":["admin"],"categories":null,"content":"我是Che-Chia Chang，社群上常用的名子是 David Chang. 我是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 Golang Taiwan Meetup 的 organizer。對 Golang 有興趣的朋友請務必加入。\nHi, I\u0026rsquo;m Che-Chia Chang. Also known as David Chang in communities in Taiwan. I\u0026rsquo;m a software engineer specilized in Back-End development, DevOps, containerization and Kubernetes administration. I\u0026rsquo;m hosting a developer community, Golang Taiwan Meetup. Join us if you are interested in Golang!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1568171821,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://chechiachang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"我是Che-Chia Chang，社群上常用的名子是 David Chang. 我是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 Golang Taiwan Meetup 的 organizer。對 Golang 有興趣的朋友請務必加入。\nHi, I\u0026rsquo;m Che-Chia Chang. Also known as David Chang in communities in Taiwan. I\u0026rsquo;m a software engineer specilized in Back-End development, DevOps, containerization and Kubernetes administration. I\u0026rsquo;m hosting a developer community, Golang Taiwan Meetup. Join us if you are interested in Golang!","tags":null,"title":"Che-Chia Chang","type":"authors"},{"authors":[],"categories":[],"content":" 2020 It邦幫忙鐵人賽 系列文章\n Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP  作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n這篇來要 Kubernetes 環境(GKE)裡面的 log 抓出來，送到 ELK 上。\n官方文件 ，寫得很簡易，如果已經很熟 kubernetes 的人可以直接腦補其他的部屬設定。\n這邊有幾個做法，依照 filebeat 部署的位置與收集目標簡單分為：\n node: 處理每一台 node 的 log ，包含 system log 與 node 監測資料(metrics) cluster: 處理 cluster 等級的 log, event 或是 metrics pod: 針對特定 pod 直接去掛一個 sidecar  上面的方法是可以混搭的，kubernetes 個個層級有log 處理流程，我們這邊把 log 送往第三方平台，也是需要依照原本的 log 流程，去收取我們想收集的 log。\n簡單來說，是去對的地方找對的 log。在架構上要注意 scalability 與 resource 分配，不要影響本身提供服務的 GKE ，但又能獲得盡量即時的 log。\n我們這邊直接進入 kubernetes resource 的設定，底下會附上在 GKE 找 log 的過程。\nNode level log harvest 為每一個 node 配置 filebeat，然後在 node 上面尋找 log，然後如我們上篇所敘述加到 input ，就可以把 log 倒出來。\n直覺想到就是透過 daemonsets 為每個 node 部署一個 filebeat pod，然後 mount node 的 log 資料夾，在設置 input。\nDeploy daemonsets kubernetes resource 的 yaml 請參考 我的 github elk-kubernetes\n給予足夠的 clusterrolebinding 到 elk\nkubectl apply -f filebeat/7.3.1/clusterrolebinding.yaml  先更改 filebeat 的設定，如何設定 elasticsearch 與 kibana，請參考上篇。至於 input 的部份已經配置好了。\nvim filebeat/7.3.1/daemonsets-config-configmap.yaml kubectl apply -f filebeat/7.3.1/daemonsets-config-configmap.yaml  部屬 filebeat daemonsets，會每一個 node 部屬一個 filebeat\nkubectl apply -f filebeat/7.3.1/daemonsets.yaml  取得 daemonsets 的狀態\nkubectl --namespcae elk get pods NAME READY STATUS RESTARTS AGE filebeat-bjfp9 1/1 Running 0 6m56s filebeat-fzr9n 1/1 Running 0 6m56s filebeat-vpkm7 1/1 Running 0 6m56s ...  有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\nlog havest for specific pods 由於 kubernetes 上我們可以便利的調度 filebeat 的部屬方式，這邊也可以也可以使用 deployment ，配合 pod affinity，把 filebeat 放到某個想要監測的 pod，這邊的例子是 nginx-ingress-controller。\n Kubernetes 上有一個或多個 nginx ingress controller 部屬一個或多個 filebeat 到有 nginx 的 node 上 filebeat 去抓取 nginx 的 input， 並使用 filebeat 的 nginx module 做預處理  nginx module 預設路徑需要調整，這邊使用 filebeat autodiscover 來處理   一樣 apply 前記得先檢查跟設定\nvim filebeat/7.3.1/nginx-config-configmap.yaml kubectl apply -f filebeat/7.3.1/nginx-config-configmap.yaml  部屬 filebeat deployment 由於有設定 pod affinity ，這個 filebeat 只會被放到有 nginx ingress controller 的這個節點上，並且依照 autodiscover 設定的條件去蒐集 nginx 的 log\nkubectl apply -f filebeat/7.3.1/nginx-deployment.yaml  有設定成功的話，kibana 這邊就會收到 kubernetes 上面 pod 的 log\n另外，由於有啟動 nginx module，logstash 收到的內容已經是處理過得內容。\nGCP fluentd 如果是使用 GKE 的朋友，可以投過開啟 stackdriver logging 的功能，把集群中服務的 log 倒到 stackdriver，基本上就是 node -\u0026gt; (daemonsets) fluentd -\u0026gt; stackdriver。\n這個 fluentd 是 GCP 如果有啟動 Stackdriver Logging 的話，自動幫你維護的 daemonsets，設定不可改，改了會被 overwrite 會去，所以不太方便從這邊動手腳。\nBtw stackdriver 最近好像改版，目前做 example 的版本已經變成 lagency （淚\n但我們先假設我們對這個 pod 的 log 很有興趣，然後把這邊的 log 透過 filebeat 送到 ELK 上XD\n因為 GKE 透過 fluentd 把 GKE 上面的 log 倒到 stackdriver，而我們是想把 log 倒到 ELK，既然這樣我們的 input 來源是相同的，而且很多處理步驟都可以在 ELK 上面互通，真的可以偷看一下 fluentd 是去哪收集 log ，怎麼處理 log pipeline，我們只要做相應設定就好。\n畢竟 google 都幫我們弄得妥妥的，不參考一下他的流程太可惜。\n偷看一下 GKE 上 fluentd 是去哪找 log ，這個是 fluentd gcp configmap，雖然看到這邊感覺扯遠了，但因為很有趣所有我就繼續看下去，各位大德可以跳過XD\nconfigmap 中的這個 input 設定檔，其中一個 source 就是一個資料來源，相當於 filebeat 的 input。這邊這個 source 就是去 /var/log/containers/*.log 收 log\n這邊還做了幾件事：\n 打上 reform.* tag，讓下個 match 可以 收進去 pipeline 處理 附帶 parse 出 time\ncontainers.input.conf \u0026lt;source\u0026gt; @type tail path /var/log/containers/*.log pos_file /var/log/gcp-containers.log.pos # Tags at this point are in the format of: # reform.var.log.containers.\u0026lt;POD_NAME\u0026gt;_\u0026lt;NAMESPACE_NAME\u0026gt;_\u0026lt;CONTAINER_NAME\u0026gt;-\u0026lt;CONTAINER_ID\u0026gt;.log tag reform.* read_from_head true \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json time_key time time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ time_format %Y-%m-%dT%H:%M:%S.%N%:z \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt;   他這邊做一些 error handling，然後用 ruby (!) parse，這邊就真的太遠，細節大家可以 google ＸＤ。不過這邊使用的 pattern matching 我們後幾篇在 logstash pipeline 上，也會有機會提到，機制是類似的。\n\u0026lt;filter reform.**\u0026gt; @type parser format /^(?\u0026lt;severity\u0026gt;\\w)(?\u0026lt;time\u0026gt;\\d{4} [^\\s]*)\\s+(?\u0026lt;pid\u0026gt;\\d+)\\s+(?\u0026lt;source\u0026gt;[^ \\]]+)\\] (?\u0026lt;log\u0026gt;.*)/ reserve_data true suppress_parse_error_log true emit_invalid_record_to_error false key_name log \u0026lt;/filter\u0026gt; \u0026lt;match reform.**\u0026gt; @type record_reformer enable_ruby true \u0026lt;record\u0026gt; # Extract local_resource_id from tag for 'k8s_container' monitored # resource. The format is: # 'k8s_container.\u0026lt;namespace_name\u0026gt;.\u0026lt;pod_name\u0026gt;.\u0026lt;container_name\u0026gt;'. \u0026quot;logging.googleapis.com/local_resource_id\u0026quot; ${\u0026quot;k8s_container.#{tag_suffix[4].rpartition('.')[0].split('_')[1]}.#{tag_suffix[4].rpartition('.')[0].split('_')[0]}.#{tag_suffix[4].rpartition('.')[0].split('_')[2].rpartition('-')[0]}\u0026quot;} # Rename the field 'log' to a more generic field 'message'. This way the # fluent-plugin-google-cloud knows to flatten the field as textPayload # instead of jsonPayload after extracting 'time', 'severity' and # 'stream' from the record. message ${record['log']} # If 'severity' is not set, assume stderr is ERROR and stdout is INFO. severity ${record['severity'] || if record['stream'] == 'stderr' then 'ERROR' else 'INFO' end} \u0026lt;/record\u0026gt; tag ${if record['stream'] == 'stderr' then 'raw.stderr' else 'raw.stdout' end} remove_keys stream,log \u0026lt;/match\u0026gt;  ssh 進去逛 想看機器上實際的 log 狀況，我們也可以直接 ssh 進去\n先透過 kubectl 看一下 pod\n$ kubectl get daemonsets --namespace kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluentd-gcp-v3.2.0 7 7 7 7 7 beta.kubernetes.io/fluentd-ds-ready=true 196d $ kubectl get pods --output wide --namespace kube-system NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES fluentd-gcp-scaler-1234567890-vfbhc 1/1 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-5gqn \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-44tl7 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-wcq0 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-5vc6l 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-tp05 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-6rqvc 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-5gqn \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluentd-gcp-v3.2.0-mmwk4 2/2 Running 0 37d 10.140.0. gke-chechiachang-pool-1-123456789-vxld \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  先透過 kubectl 看一下 node\n$ kubectl get node NAME STATUS ROLES AGE VERSION gke-chechaichang-pool-1-123456789-3bzp Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 gke-chechaichang-pool-1-123456789-5gqn Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 gke-chechaichang-pool-1-123456789-8n8z Ready \u0026lt;none\u0026gt; 37d v1.13.7-gke.8 ... gcloud compute ssh gke-chechaichang-pool-1-123456789-3bzp  如使用其他雲平台的 kubernetes service，或是 bare metal 的集群，請依照各自系統的方式連進去看看。\nssh node 找 log ssh 進去後就可以到處來探險，順便看看 GKE 跑在機器上到底做了什麼事情。\n如果官方有出文件，可能可以不用進來看。各位大德有發現文件請留言跟我說。我個人很喜歡自己架集群起來連就去看，面對照官方文件上寫的東西，當然大部份時候都是文件沒有帶到，有很多發現。\n$ ls /var/log gcp-*-log.pos kube-proxy.log containers/ metrics/ pods/ ...  /var/log/containers 看一下，格式是 pod_namespace_container 這邊是 link 到 /var/log/pods/\n$ ls -al /var/log/containers lrwxrwxrwx 1 root root 105 Aug 12 07:42 fluentd-gcp-v3.2.0-st6cl_kube-system_fluentd-gcp-5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac.log -\u0026gt; /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/0.log  看到 pods 就覺得是你了，裡面有 pod 資料夾，格式是 namespace_pod_uuid\nls /var/log/pods default_pod-1-1234567890-fxxhp_uuid kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008 kube-system_heapster-v1.6.0-beta.1- kube-system_kube-proxy-gke- kube-system_l7-default-backend- kube-system_prometheus-to-sd-  再進去有 container log，格式是 pod_namespace_container.log，也是 link\nls -al /var/log/pods/kube-system_fluentd-gcp-v3.2.0-st6cl_b76bed0b-bcd4-11e9-a55c-42010a8c0008/fluentd-gcp/ lrwxrwxrwx 1 root root 165 Aug 12 07:42 0.log -\u0026gt; /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log  最終 link 到\nsudo su $ ls -alh /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/ total 3.9M drwx------ 4 root root 4.0K Aug 12 07:42 . drwx------ 92 root root 20K Sep 18 11:28 .. -rw-r----- 1 root root 3.8M Sep 18 11:29 5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log drwx------ 2 root root 4.0K Aug 12 07:42 checkpoints -rw------- 1 root root 7.8K Aug 12 07:42 config.v2.json -rw-r--r-- 1 root root 2.3K Aug 12 07:42 hostconfig.json drwx------ 2 root root 4.0K Aug 12 07:42 mounts  頭尾偷喵一下，確定是我們在找的東西\nhead /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log tail /var/lib/docker/containers/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac/5e38c9b63c8d767091b122a9aa48c576a88cc20b4470d9ca18a820afa5c168ac-json.log  這樣就找到我們的 log 了\n小節  使用 filebeat 去查找 透過 kubernetes daemonsets 可以快速佈置一份 filebeat 到所有 node，且設定都是一起更新 透過 kubernetes deployment 可以指定 filebeat 的位置，去跟隨想要監測的服務 如果不熟 log 處理流程，可以直接看偷看大廠的服務，會有很多靈感 沒事可以多跑進 Kubernetes 服務節點逛逛，有很多有趣的東西  ","date":1568883989,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568904882,"objectID":"fa0cedc5d49a28d395b9eeb517eccdc4","permalink":"https://chechiachang.github.io/post/monitoring-gke-with-elk/","publishdate":"2019-09-19T17:06:29+08:00","relpermalink":"/post/monitoring-gke-with-elk/","section":"post","summary":"Monitoring GKE With Elk","tags":["kubernetes","elk","filebeat","fluentd","log","daemonsets","pod-affinity"],"title":"Monitoring GKE With Elk","type":"post"},{"authors":[],"categories":[],"content":" 2020 It邦幫忙鐵人賽 系列文章\n Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP  作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\nELK 的 beats 是輕量級的系統監測收集器，beats 收集到的 data 經過 mapping 可以送到 Elasticsearch 後，進行彈性的搜尋比對。\nbeat 有許多種類，依據收集的 data 區別：\n Auditbeat: Audit data Filebeat: Log files Functionbeat: Cloud data Heartbeat: Availability Journalbeat: Systemd journals Metricbeat: Metrics Packetbeat: Network traffic Winlogbeat: Windows event logs  這邊先以 filebeat 為例，在 GCE 上收集圓端服務節點上的服務日誌與系統日誌，並在 ELK 中呈現。\nInstallation 安裝及 filebeat 安全性設定的步驟，在這篇[Secure ELK Stack]() 中已經說明。這邊指附上連結，以及官方文件 提供參考。\nConfiguration 這邊談幾個使用方面的設定。\n首先，apt 安裝的 filebeat 預設的 /etc/filebeat/filebeat.yml 不夠完整，我們先到 github 把對應版本的完整載下來。\nwget https://raw.githubusercontent.com/elastic/beats/master/filebeat/filebeat.reference.yml sudo mv filebeat.reference.yml /etc/filebeat/filebeat.yml  Beats central management beats 透過手動更改 config 都可以直接設定，但這邊不推薦在此設定，理由是\n 系統中通常會有大量的 filebeat，每個都要設定，數量多時根本不可能 更改設定時，如果不一起更改，會造成資料格式不統一，之後清理也很麻煩  推薦的方式是透過 Kibana 對所有 filebeat 做集中式的的管理配置，只要初始設定連上 kibana，剩下的都透過 kibana 設定。文件在此，我們有空有可以分篇談這個主題。\n不過這邊還是待大家過一下幾個重要的設定。畢竟要在 kibana 上配置，filebeat 的設定概念還是要有。\nmodules filebeat 有許多模組，裡面已經包含許多預設的 template ，可以直接使用 default 的設定去系統預設的路徑抓取檔案，並且先進一步處理，減少我們輸出到 logstash 還要再做 pipeline 預處理，非常方便。\n例如這個 system module 會處理系統預設的 log 路徑，只要開啟 module ，就會自動處理對應的 input。\n- module: system syslog: enabled: true  剩下的就是照需求啟用 module ，並且給予對應的 input。\nELK 為自己的服務設定了不少 module ，直接啟用就可以獲取這協服務元件運行的 log 與監測數值。這也是 self-monitoring 監測數據的主要來源。\n- module: kibana - module: elasticsearch - module: logstash ...  input filebeat 支援複數 inputs，每個 input 會啟動一個收集器，而 filebeat 收集目標是 log 檔案。基本上可以簡單理解為 filebeat 去讀取這些 log 檔案，並且在系統上紀錄讀取的進度，偵測到 log 有增加，變繼續讀取新的 log。\nfilebeat 具體的工作機制，可以看這篇How Filebeat works?\n這篇文件也提到 filebeat 是確保至少一次(at-least-once delivery)的數據讀取，使用時要特別注意重複獲取的可能。\n首先把 input 加上 ubuntu 預設的 log 路徑\nfilebeat.inputs: - type: log enabled: true paths: - /var/log/*.log  這邊注意 input 支援多種 type，參照完整設定檔案的說明配合自己的需求使用。\nProcessor 在 filebeat 端先進行資料的第一層處理，可以大幅講少不必要的資料，降低檔案傳輸，以及對 elasticsearch server 的負擔。\noutput output 也是 filebeat 十分重要的一環，好的 filebeat output 設定，可以大幅降低整體 ELK stack 的負擔。壞的設定也會直接塞爆 ELK stask。\noutput.elasticsearch: 直接向後送進 elasticsearch output.logstash: 先向後送到 logstash\n這邊非常推薦大家，所有的 beat 往後送進 elasticsearch 之前都先過一層 logstash，就算你的 logstash 內部完全不更改 data，沒有 pipeline mutation，還是不要省這一層。\n beat 的數量會隨應用愈來越多而線性增加，elasticsearch 很難線性 scale，或是 scale 成本很大 filebeat 沒有好好調校的話，對於輸出端的網路負擔很大，不僅佔用大量連線，傳輸檔案的大小也很大。 logstash 的 queue 與後送的 batch 機制比 filebeat 好使用 filebeat 是收 log 的，通常 log 爆炸的時候，是應用出問題的時候，這時候需要 log 交叉比對，發現 elasticsearch 流量也爆衝，反應很應用 logstash 透過一些方法，可以很輕易的 scale，由於 pipeline 本身可以分散是平行處理，scale logstash 並不會影響資料最終狀態。  load balance 有網友留言詢問 logstash 前面的 load balance 如何處理比較好，我這邊也順便附上。不只是 logstash ，所有自身無狀態(stateless) 的服務都可以照這樣去 scale。\n在 kubernetes 上很好處理，使用 k8s 預設的 service 就輕易作到簡易的 load balance * 設置複數 logstash instances * 使用 kubernetes 內部網路 service 實現 load balancing。\n在 GCE 上實現的話，我說實話沒實作過，所以以下是鍵盤實現XD。\n官方文件 建議使用 beats 端設定多個 logstash url 來做 load balancing。\n但我不是很喜歡 beat 去配置多個 logstash url 的作法：beat 要感知 logstash 數量跟 url ，增加減少 logstash instance 還要更改 beats 配置，產生配置的依賴跟耦合。\n最好是在 logstash 前過一層 HAproxy 或是雲端服務的 Load balancer（ex. GCP https/tcp load balancer），beat 直接送進 load balance 的端點。\nautodiscover 如果有使用 container ，例如 docker 或 kubernetes，由於 container 內的 log 在主機上的位置是動態路徑，這邊可以使用 autodiscover 去尋找。\n在 kubernetes 上面的設定，之後會另開一天討論。\ndashboard kibana 預設是空的，沒有預先載入 dashboard，但我們會希望資料送進去，就有設定好的 dashboard ，圖像化把資料呈現出來。這部份需要從 beat 這邊向 kibana 寫入。\n在上面的部份設定好 kibana 的連線資料，沒有設定的話 beat 啟動會警告。\nsetup.dashboards.enabled: true  一起中就會檢查 kibana 是否有匯入 dashboard，沒有的話就匯入。\n也會一併匯入 modules 的 dashboard，例如如果有啟用 nginx module 處理 nginx 的 access log，nginx module 會處理 request source ip ，並透過 geoip database, 將 ip 轉會成經緯度座標。這時如果在 kibana 上有匯入 nginx dashboard，就可以看到圖像化的全球 request 分佈圖。\n小結  取得完整 filebeat 設定檔案並設定 filebeat 盡量透過 beat central management 來管理 beat 的設定檔 啟用對應 module 來更優雅的處理 log 後送到 elasticsearch 前的資料都必須經過精細的處理，送進去後就不好刪改了  ","date":1568805050,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568904882,"objectID":"88ab2f49d8b5530b7adb867cb3b71627","permalink":"https://chechiachang.github.io/post/monitoring-gce-with-elk/","publishdate":"2019-09-18T19:10:50+08:00","relpermalink":"/post/monitoring-gce-with-elk/","section":"post","summary":"Monitoring GCE with ELK","tags":[],"title":"Monitoring GCE With ELK","type":"post"},{"authors":[],"categories":[],"content":" 2020 It邦幫忙鐵人賽 系列文章\n Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP  對我的文章有興趣，歡迎到我的網站上 https://chechiachang.github.io 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n\u0026ndash;\n上篇Self-host ELK stack on GCP 介紹了，elk stack 基本的安裝，安裝完獲得一個只支援 http (裸奔)的 elk stack，沒有 https 在公開網路上使用是非常危險的。這篇要來介紹如何做安全性設定。\n官方的文件在這裡，碎念一下，除非對 ELK 的功能有一定了解，不然這份真的不是很友善。建議從官方文件底下的Tutorial: Getting started with security 開始，過程比較不會這麼血尿。\n總之為了啟用 authentication \u0026amp; https，這篇要做的事情：\n enable x-pack \u0026amp; activate basic license Generate self-signed ca, server certificate, client certificate Configure Elasticsearch, Kibana, \u0026amp; other components to  use server certificate when act as server use client certificate when connect to an ELK server   啟用 X-pack Elasticsearch 的安全性模組由 x-pack extension 提供，在 6.3.0 之後的版本，安裝 elasticsearch 的過程中就預設安裝 x-pack。\n附上啟用的官方文件\n然而，由於舊版的 x-pack 是付費內容，目前的 elasticsearch 安裝完後，elasticsearch.yml 設定預設不啟用 x-pack，也就是說沒看到這篇官方文件的話，很容易就獲得沒有任何 security 功能的 ELK。\n雖然目前已經可以使用免費的 basic license 使用 security 功能，還是希望官方可以 default 啟用 security。\n$ sudo vim /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: true xpack.license.self_generated.type: basic discovery.type: single-node  我們這邊啟用 xpack.security，同時將 self-generated license 生出來，我們這邊只使用基本的 basic subscription。若希望啟用更多功能，可以看官方subcription 方案介紹\n另外，如果不同時設定為 single-node 的話，預設會尋找其他elasticsearch node 來組成 cluster，而我們就必須要在所有 node 上啟用 security，這篇只帶大家做一個 single node cluster，簡化步驟。\n重啟 elasticsearch ，檢查 log，看啟動時有沒有載入 x-pack\nsudo systemctl restart elasticsearch $ tail -f /var/log/elasticsearch/elasticsearch.log [2019-09-16T07:39:49,467][INFO ][o.e.e.NodeEnvironment ] [elk] using [1] data paths, mounts [[/mnt/disks/elk (/dev/sdb)]], net usable_space [423.6gb], net total_space [491.1gb], types [ext4] [2019-09-16T07:39:49,474][INFO ][o.e.e.NodeEnvironment ] [elk] heap size [3.9gb], compressed ordinary object pointers [true] [2019-09-16T07:39:50,858][INFO ][o.e.n.Node ] [elk] node name [elk], node ID [pC22j9D4R6uiCM7oTc1Fiw], cluster name [elasticsearch] [2019-09-16T07:39:50,866][INFO ][o.e.n.Node ] [elk] version[7.3.1], pid[17189], build[default/deb/4749ba6/2019-08-19T20:19:25.651794Z], OS[Linux/4.15.0-1040-gcp/amd64], JVM[Oracle Corporation/OpenJDK 64-Bit Server VM/12.0.2/12.0.2+10] [2019-09-16T07:39:50,878][INFO ][o.e.n.Node ] [elk] JVM home [/usr/share/elasticsearch/jdk] ... [2019-09-16T07:39:59,108][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-ccr] [2019-09-16T07:39:59,109][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-core] ... [2019-09-16T07:39:59,111][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-logstash] [2019-09-16T07:39:59,113][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-voting-only-node] [2019-09-16T07:39:59,114][INFO ][o.e.p.PluginsService ] [elk] loaded module [x-pack-watcher] [2019-09-16T07:39:59,115][INFO ][o.e.p.PluginsService ] [elk] no plugins loaded [2019-09-16T07:40:07,964][INFO ][o.e.x.s.a.s.FileRolesStore] [elk] parsed [0] roles from file [/etc/elasticsearch/roles.yml] [2019-09-16T07:40:10,369][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elk] [controller/17314] [Main.cc@110] controller (64 bit): Version 7.3.1 (Build 1d93901e09ef43) Copyright (c) 2019 Elasticsearch BV [2019-09-16T07:40:11,776][DEBUG][o.e.a.ActionModule ] [elk] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2019-09-16T07:40:14,396][INFO ][o.e.d.DiscoveryModule ] [elk] using discovery type [single-node] and seed hosts providers [settings] [2019-09-16T07:40:16,222][INFO ][o.e.n.Node ] [elk] initialized [2019-09-16T07:40:16,224][INFO ][o.e.n.Node ] [elk] starting ... [2019-09-16T07:40:16,821][INFO ][o.e.t.TransportService ] [elk] publish_address {10.140.0.10:9300}, bound_addresses {[::]:9300} [2019-09-16T07:40:16,872][INFO ][o.e.c.c.Coordinator ] [elk] cluster UUID [1CB6_Lt-TUWEmRoN9SE49w] [2019-09-16T07:40:17,088][INFO ][o.e.c.s.MasterService ] [elk] elected-as-master ([1] nodes joined)[{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 9, version: 921, reason: master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]} [2019-09-16T07:40:17,819][INFO ][o.e.c.s.ClusterApplierService] [elk] master node changed {previous [], current [{elk}{pC22j9D4R6uiCM7oTc1Fiw}{Os-2FBjgSTOd1G_I3QYwVQ}{10.140.0.10}{10.140.0.10:9300}{dim}{ml.machine_memory=7836028928, xpack.installed=true, ml.max_open_jobs=20}]}, term: 9, version: 921, reason: Publication{term=9, version=921} [2019-09-16T07:40:17,974][INFO ][o.e.h.AbstractHttpServerTransport] [elk] publish_address {10.140.0.10:9200}, bound_addresses {[::]:9200} [2019-09-16T07:40:17,975][INFO ][o.e.n.Node ] [elk] started [2019-09-16T07:40:18,455][INFO ][o.e.c.s.ClusterSettings ] [elk] updating [xpack.monitoring.collection.enabled] from [false] to [true] [2019-09-16T07:40:22,555][INFO ][o.e.l.LicenseService ] [elk] license [************************************] mode [basic] - valid [2019-09-16T07:40:22,557][INFO ][o.e.x.s.s.SecurityStatusChangeListener] [elk] Active license is now [BASIC]; Security is enabled  Enable user authentication 啟用 security 之前，我們直接連入 Kibana http://10.140.0.10:5601 ，不用任何使用者登入，便可以完整使用 Kibana 功能（包含 admin 管理介面）。\n啟用 security 後，便需要使用帳號密碼登入。在這邊先用工具把使用者密碼產生出來。\n# 互動式 /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive # 自動產生 /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto  密碼生出來後，就把帳號密碼收好，等等會用到。之後初次登入也是使用這些密碼。\nConfigure passwords on client-side 由於已經啟用 authentication，其他 ELK 元件 (Kibana, logstash, filebeat, apm-server,\u0026hellip;) 連入 Elasticsearch 也都會需要各自的帳號密碼驗證。\n以 Kibana 為例，可以直接在 kibana.yml 中直接設定帳號密碼\n$ sudo vim /etc/kibana/kibana.yml elasticsearch.hosts: [\u0026quot;http://localhost:9200\u0026quot;] xpack.security.enabled: true elasticsearch.username: \u0026quot;kibana\u0026quot; elasticsearch.password: \u0026quot;***********\u0026quot;  當然，這邊就是明碼的，看了不太安全。\n或是使用 keystore 把 built-in user 的密碼加密，存在 kibana 的 keystore 裡面，重啟 kibana 時便會載入。\n/usr/share/kibana/bin/kibana-keystore create /usr/share/kibana/bin/kibana-keystore add elasticsearch.username /usr/share/kibana/bin/kibana-keystore add elasticsearch.password  如果有啟用 Filebeat 功能，beat 元件連入 elasticsearch 一樣需要設定\n/usr/share/apm-server/bin/filebeat keystore create /usr/share/apm-server/bin/filebeat add elasticsearch.username /usr/share/apm-server/bin/filebeat add elasticsearch.password  如果有啟用 application performance monitoring(APM) 功能，apm-server 元件連入 elasticsearch 一樣需要設定\n/usr/share/apm-server/bin/apm-server keystore create /usr/share/apm-server/bin/apm-server add elasticsearch.username /usr/share/apm-server/bin/apm-server add elasticsearch.password  Encrypting Communications 上面加了 username/password authentication，但如果沒 https/tls 基本上還是裸奔。接下來要處理連線加密。\n官方 tutorial\n一堆官方文件，我們先跳過XD\n elasticsearch security elastic stack ssl tls elasticsearch configuring tls certutil  分析一下需求跟規格 我們需要為每一個 node 生一組 node certificate，使用 node certificate 產生 client certificates 提供給其他 client，連入時會驗證 client 是否為 authenticated user。\n針對目前這個 single-node ELK stack，我們可能有幾種選擇\n 簽署一個 localhost，當然這個只能在 localhost 上的客戶端元件使用，別的 node 無法用這個連入 簽署一個 public DNS elk.chechiachang.com，可以在公開網路上使用，別人也可以使用這個DNS嘗試連入 簽署一個私有網域的 DNS，例如在 GCP 上可以使用內部dns服務  長這樣 elk.asia-east1-b.c.chechiachang.internal [INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal  有需要也可以一份 server certificate 中簽署複數個 site  我們這邊選擇使用內部 dns，elk.asia-east1-b-c-chechaichang.internal，讓這個 single-node elk 只能透過內部網路存取。\n elasticsearch: elk.asia-east1-b.c.chechaichang.internal:9200 kibana: elk.asia-east1-b.c.chechaichang.internal:5601 外部要連近來 kibana，我們使用 vpn 服務連進私有網路  如果想使用外部 dns，讓 elk stack 在公開網路可以使用，ex. elk.chechiachang.com，可以\n GCP 的 load balancer掛進來，用 GCP 的 certificate manager 自動管理 certificate 或是在 node 上開一個 nginx server，再把 certificate 用 certbot 生出來  Generate certificates 先把 X.509 digital certificate 的 certificate authority(CA) 生出來。我們可以設定密碼保護這個檔案\nmkdir -p /etc/elasticsearch/config # CA generated with Elastic tool /usr/share/elasticsearch/bin/elasticsearch-certutil ca \\ -out /etc/elasticsearch/config/elastic-stack-ca.p12  生出來是 PKCS#12 格式的 keystore，包含：\n CA 的 public certificate CA 的基本資訊 簽署其他 node certificates 使用的私鑰(private key)  用 openssl 工具看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/elastic-stack-ca.p12 -info -nokeys  附帶說明，X.509 有多種檔案格式\n .pem .cer, .crt, .der .p12 .p7b, .p7c \u0026hellip;  另外檔案格式可以有其他用途，也就是說裡面裝的不一定是 X.509 憑證。裡面的內容也不同。\nELK 設定的過程中，由於不是所有的 ELK component 都支援使用 .p12 檔案，我們在設定過程中會互相專換，或是混用多種檔案格式。\nGenerate certificate \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD\n我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有\n node certificate node key CA certificate  這邊只產生一組 server certificate 給 single-node cluster 的 node-1\n=======\n我們用 elastic-stack-ca.p12 這組 keystore裡面的 CA 與 private key，為 elk.asia-east1-b.c.chechiachang.internal 簽一個 p12 keystore，裡面有\n node certificate node key CA certificate  這邊只產生一組 server certificate 給 single-node cluster 的 node-1\n如果 cluster 中有多個 elasticsearch，為每個 node 產生 certificate 時都要使用同樣 CA 來簽署，讓 server 信任這組 CA。\n使用 elasticsearch-certutil 簡化簽署過程，從產生 CA ，到使用 CA 簽署 certificate。另外，再產生 certificate 中使用 Subject Alternative Name(SAN)，並輸入 ip 與 dns。\n# certificate for site: private dns with Elastic CA /usr/share/elasticsearch/bin/elasticsearch-certutil cert \\ --ca /etc/elasticsearch/config/elastic-stack-ca.p12 \\ --name elk.asia-east1-b.c.chechaichang.internal \\ --dns elk.asia-east1-b.c.chechaichang.internal \\ --ip 10.140.0.10 \\ -out /etc/elasticsearch/config/node-1.p12  用 openssl 看一下內容，如果有密碼這邊要用密碼解鎖\n$ openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -info -nokeys  server 用這個 certificate ，啟用 ssl。\nclient 使用這個 certificate 產生出來的 client.cer 與 client.key 與 server 連線，server 才接受客戶端是安全的。\n       25f5ab795b9e698333a36fde7ecf23a8ba9d4595 記得把所有權還給 elasticsearch 的使用者，避免 permission denied\n       # Change owner to fix read permission chown -R elasticsearch:elasticsearch /etc/elasticsearch/config  有密碼記得也要用 keystore 把密碼加密後喂給 elasticsearch\n/usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password  關於 X.509 Certifcate 之後有空我們來聊一下\n更新 elasticsearch 設定 Certificates 都生完了，接下來更改 elasticsearch 的參數，在 transport layer 啟用 ssl。啟用 security 後，在 transport layer 啟動 ssl 是必須的。\n$ sudo vim /etc/elasticsearch/elasticsearch.yml xpack.security.enabled: true xpack.security.transport.ssl.enabled: true # use certificate. full will verify dns and ip xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12 xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12  啟用 security 與 transport layer 的 ssl，然後指定 keystore路徑，讓 server 執行 client authentication 由於這筆 p12 帶有 CA certificate 作為 trusted certificate entry，所以也可以順便當作 trustore，讓 client 信任這個 CA\nsecurity 這邊提供了 server side (elasticsearch) 在檢查客戶端連線時的檢查模式(vertification mode)，文件有說明，可以設定\n certificate: 檢查 certificate 加密是否有效 full: 簽 node certificate 時可以指定 ip dns，啟用會檢查來源 node ip dns 是否也正確  (Optional) HTTP layer 啟動 ssl\nvim /etc/elasticsearch/elasticsearch.yml xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: /etc/elasticsearch/config/node-1.p12 xpack.security.http.ssl.truststore.path: /etc/elasticsearch/config/node-1.p12 /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password /usr/share/elasticsearch/bin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password  重啟 elasticsearch，看一下 log\nsudo systemctl restart elasticsearch tail -f /var/log/elasticsearch/elasticsearch.log  然後你就發現，原來 kibana 連入 的 http 連線，不斷被 server 這端拒絕。所以以下要來設定 kibana\nKibana  using kibana with security kibana configuring tls  使用剛剛簽的 server certificate，從裡面 parse 出 client-ca.cer，還有 client.cer 與 client.key\nmkdir -p /etc/kibana/config $ openssl pkcs12 --help Usage: pkcs12 [options] Valid options are: -chain Add certificate chain -nokeys Don't output private keys -nocerts Don't output certificates -clcerts Only output client certificates -cacerts Only output CA certificates -info Print info about PKCS#12 structure -nodes Don't encrypt private keys -in infile Input filename # no certs, no descript openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/kibana/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/kibana/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/kibana/config/client-ca.cer sudo chown -R kibana:kibana /etc/kibana/config/  更改 kibana 連入 elasticsearch 的連線設定\nsudo vim /etc/kigana/kibana.yml elasticsearch.hosts: [\u0026quot;https://elk.asia-east1-b.c.chechaichang.internal:9200\u0026quot;] xpack.security.enabled: true elasticsearch.ssl.certificate: /etc/kibana/config/client.cer elasticsearch.ssl.key: /etc/kibana/config/client.key elasticsearch.ssl.certificateAuthorities: [ \u0026quot;/etc/kibana/config/client-ca.cer\u0026quot; ] elasticsearch.ssl.verificationMode: full   指定 ssl.certificate, ssl.key 做連線 elasticsearch server 時的 user authentication 由於我們是 self-signed CA，所以需要讓客戶端信任這個我們自簽的 CA  注意這邊 elasticsearch.hosts 我們已經從 http://localhost 換成 https 的內部 dns，原有的 localhost 已經無法使用（如果 elasicsearch 有 enforce https 的話）\n重啟 Kibana，看一下 log\nsudo systemctl restart kibana journalctl -fu kibana  如果沒有一直噴 ssl certificate error 的話，恭喜你成功了\n然而，除了 kibana 以外，我們還有其他的 client 需要連入 elasticsearch\n 把上述步驟在 apm-server, filebeat, 其他的 beat 上也設定 如果在 k8s 上，要把 cer, key 等檔案用 volume 掛進去 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD  Kibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定\n kibana server certificate filebeat client to kibana server  =======\nKibana 本身也有 server 的功能，讓其他 client 連入。例如讓 filebeat 自動將 document tempalte 匯入 kibana，我們也需要設定\n kibana server certificate filebeat client to kibana server  就是他們彼此互打，都要有 ca, key, cert\n但基本上的設定都一樣，下面可以不用看下去了XD 如果有用到再查文件就好，這邊直接小結\n 設定 security 前要先想號自己的需求，如何連入，安全性設定到哪邊 使用 utility 自簽 CA，然後產生 server certificate 使用 server certificate 再 parse 出 ca-certificate, client cers, key  kibana 作為 server 工作路徑可能是這樣： app(apm-client library) -\u0026gt; apm-server -\u0026gt; kibana -\u0026gt; elasticsearch\n kibana 連入 elasticsearch時， kibana 是 client 吃 elasticsearch 的憑證 apm-server 連入 kibana時，kibana 是 server，apm-server 吃 kibana 的憑證  首先更改 kibana 設定\n$ sudo vim /etc/kibana/kibana.yml server.ssl.enabled: true server.ssl.certificate: /etc/kibana/config/client.cer server.ssl.key: /etc/kibana/config/client.key  重啟 kibana\njournalctl -fu kibana  Apm-server https://www.elastic.co/guide/en/apm/server/7.3/securing-apm-server.html\n應用端的 apm-client (ex. apm-python-client)，連入 apm-server\n 在 http 的狀況下，雖然有使用 secret-token，但還是裸奔 在 https 的狀況下，要把 certificates，然後餵給應用端的client library  更改 apm-server 的設定\nsudo vim /etc/apm-server/apm-server.yml host: \u0026quot;0.0.0.0:8200\u0026quot; secret_token: \u0026lt;設定一組夠安全的 token\u0026gt; rum: enabled: true kibana: protocol: \u0026quot;https\u0026quot; ssl.enabled: true output.kibana: enable: false # can only have 1 output output.elasticsearch: monitoring.elasticsearch: protocol: \u0026quot;https\u0026quot; username: \u0026quot;elastic\u0026quot; password: \u0026quot;*******************\u0026quot; hosts: [\u0026quot;elk.asia-east1-b.c.checahichang.internal:9200\u0026quot;] ssl.enabled: true ssl.verification_mode: full ssl.certificate_authorities: [\u0026quot;/etc/apm-server/config/client-ca.cer\u0026quot;] ssl.certificate: \u0026quot;/etc/apm-server/config/client.cer\u0026quot; ssl.key: \u0026quot;/etc/apm-server/config/client.key\u0026quot;  重啟 apm-server\nsystemctl restart apm-server journalctl -fu apm-server  APM library 應用端的設定就需要依據 library 的實做設定，例如 flask-apmagent-python\nELASTIC_APM_SERVER_CERT=/etc/elk/certificates/client.cer  apm agent python config server cert\nfilebeat 記得我們在 node 上有安裝 Self-monitoring filebeat，elasticsearch 改成 ssl 這邊當然也連不盡去了，再做同樣操作\nhttps://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html\nsudo apt-get install filebeat mkdir -p /etc/filebeat/config openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/filebeat/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/filebeat/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/filebeat/config/client-ca.cer  Restart filebeat\nsystemctl restart filebeat journalctl -fu filebeat  如果你的應用在 kubernetes 上 可以使用下面方法拿到 client.cer ，然後用 secret 塞進 k8s，在用 volume from secrets，掛給監測應用的 filebeat\n mkdir -p /etc/beats/config openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -nocerts -nodes \u0026gt; /etc/beats/config/client.key openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -clcerts -nokeys \u0026gt; /etc/beats/config/client.cer openssl pkcs12 -in /etc/elasticsearch/config/node-1.p12 -cacerts -nokeys -chain \u0026gt; /etc/beats/config/client-ca.cer gcloud compute scp elk:/etc/beats/config/* . client-ca.cer client.cer client.key kubectl -n elk create secret generic elk-client-certificates \\ --from-file=client-ca.cer=client-ca.cer \\ --from-file=client.cer=client.cer \\ --from-file=client.key=client.key kubectl apply -f elk/gke/filebeat/  ","date":1568559633,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568703098,"objectID":"e3558f49cd97b73e5548fe3ea2d48b53","permalink":"https://chechiachang.github.io/post/secure-elk-stack/","publishdate":"2019-09-15T23:00:33+08:00","relpermalink":"/post/secure-elk-stack/","section":"post","summary":"Secure ELK stack","tags":["elk","tls","xpack","kubernetes"],"title":"Secure Elk Stack","type":"post"},{"authors":[],"categories":[],"content":" 2020 It邦幫忙鐵人賽 系列文章\n Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP  作為範例的 ELK 的版本是當前的 stable release 7.3.1。\n由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n對我的文章有興趣，歡迎到我的網站上 https://chechiachang.github.io 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n\u0026ndash;\n簡介 ELK stack 官方說明文件\nELK 的元件  Elasticsearch: 基於 Lucene 的分散式全文搜索引擎 Logstash: 數據處理 pipeline Kibana: ELK stack 的管理後台與數據視覺化工具 Beats: 輕量級的應用端數據收集器，會從被監控端收集 log 與監控數據(metrics)  ELK 的工作流程 beats -\u0026gt; (logstash) -\u0026gt; elasticsearch -\u0026gt; kibana\n 將 beats 放在應用端的主機上，或是在容器化環境種作為 sidecar，跟應用放在一起 設定 beats 從指定的路徑收集 log 與 metrics 設定 beats 向後輸出的遠端目標 (Optional) beats 輸出到 logstash ，先進行數據的變更、格式整理，在後送到 elasticsearch beats 向後輸出到 elasticsearch，儲存數據文件(document)，並依照樣式(template)與索引(index)儲存，便可在 elasticsearch 上全文搜索數據 透過 Kibana，將 elasticsearch 上的 log 顯示  官方不是有出文件嗎 Elastic 官方準備了大量的文件，理論上要跟著文件一步一步架設這整套工具應該是十分容易。然而實際照著做卻遇上很多困難。由於缺乏 get-started 的範例文件，不熟悉 ELK 設定的使用者，常常需要停下來除錯，甚至因為漏掉某個步驟，而需要回頭重做一遍。\n說穿了本篇的技術含量不高，就只是一個踩雷過程。\nLets get our hands dirty.\nWARNING 這篇安裝過程沒有做安全性設定，由於 ELK stack 的安全性功能模組，在v6.3.0 以前的版本是不包含安全性模組的，官方的安裝說明文件將安全性設定另成一篇。我第一次安裝，全部安裝完後，才發現裏頭沒有任何安全性設定，包含帳號密碼登入、api secret token、https/tls 通通沒有，整組 elk 裸奔。\n我這邊分開的目的，不是讓大家都跟我一樣被雷(XD)，而是因為 - 另起一篇對安全性設定多加說明 - 在安全的內網中，沒有安全性設定，可以大幅加速開發與除錯\n雖然沒有安全性設定，但仍然有完整的功能，如果只是在測試環境，或是想要評估試用 self-hosted ELK，這篇的說明已足夠。但千萬不要用這篇上 public network 或是用在 production 環境喔。\n如果希望第一次安裝就有完整的 security 設定，請等待下篇 Secure ELK Stask\n討論需求與規格 這邊只是帶大家過一下基礎安裝流程，我們在私有網路中搭建一台 standalone 的 ELK stack，通通放在一台節點(node)上。\nelk-node-standalone 10.140.0.10 app-node-1 10.140.0.11 ... ...  本機的 ELK stack 元件，彼此透過 localhost 連線\n Elasticsearch: localhost:9200 Kibana: localhost:5601 Apm-server: localhost:8200 Self Monitoring Services  私有網路中的外部服務透過 10.140.0.10\n beats 從其他 node 輸出到 Elasticsearch: 10.140.0.10:9200 beats 從其他 node 輸出到 Apm-server: 10.140.0.10:8200 在內部網路中 透過 browser 存取 Kibana: 10.140.0.10:5601  standalone 的好處:\n 方便 (再次強調這篇只是示範，實務上不要貪一時方便，維運崩潰) 最簡化設定，ELK 有非常大量的設定可以調整，這篇簡化了大部分  Standalone可能造成的問題:\n No High Availablity: 沒有任何容錯備援可以 failover，這台掛就全掛 外部服務多的話，很容易就超過 node 上對於網路存取的限制，造成 tcp drop 或 delay。需要調整 ulimit 來增加網路，當然這在雲端上會給維運帶來更多麻煩，不是一個好解法。  如果要有 production ready 的 ELK\n HA 開起來 把服務分散到不同 node 上, 方便之後 scale out 多開幾台  elasticsearch-1, elasticsearch-2, elasticsearch-3\u0026hellip; kibana-1 apm-server-1, apm-server-2, \u0026hellip;  如果應用在已經容器化, 這些服務元件也可以上 Kubernetes 做容器自動化，這個部份蠻好玩，如果有時間我們來聊這篇  主機設定 Elasticsearch 儲存數據會佔用不少硬碟空間，我個人的習慣是只要有額外占用儲存空間，都要另外掛載硬碟，不要占用 root，所以這邊會需要另外掛載硬碟。\nGCP 上使用 Google Compote Engine 的朋友，可以照 Google 官方操作步驟操作\n完成後接近這樣\n$ df -h $ df --human-readable Filesystem Size Used Avail Use% Mounted on /dev/sda1 9.6G 8.9G 682M 93% / /dev/sdb 492G 63G 429G 13% /mnt/disks/elk $ ls /mnt/disks/elk /mnt/disks/elk/elasticsearch /mnt/disks/elk/apm-server /mnt/disks/elk/kibana  至於需要多少容量，取決收集數據的數量，落差非常大，可以先上個 100Gb ，試跑一段時間，再視情況 scale storage disk。\n開防火牆 需要開放 10.140.0.10 這台機器的幾個 port\n elasticsearch :9200 來源只開放私有網路其他 ip 10.140.0.0/9 apm-server :8200 (同上) kibana :5601 (同上)，如果想從外部透過 browser開，需要 whitelist ip  GCP 上有 default 的防火牆允許規則，私有網路可以彼此連線 - default-allow-internal: :all :10.140.0.0/9 tcp:0-65535\nInstall Elasticsearch Install Elasticsearch 官方文件 7.3\n我們這邊直接在 ubuntu 18.04 上使用 apt 作為安裝\nsudo apt-get install apt-transport-https wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - add-apt-repository \u0026quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026quot; sudo apt-get update sudo apt-get install elasticsearch  安裝完後路徑長這樣\n/etc/elasticsearch /etc/elasticsearch/elasticsearch.yml /etc/elasticsearch/jvm.options # Utility /usr/share/elasticsearch/bin/ # Log /var/log/elasticsearch/elasticsearch.log  有需要也可以複寫設定檔，把 log 也移到 /mnt/disks/elk/elasticsearch/logs\n服務控制 透過 systemd 管理，我們可以用 systemctl 控制， 用戶 elasticsearch:elasticsearch，操作時會需要 sudo 權限。\n但在啟動前要先調整數據儲存路徑，並把權限移轉給使用者。\nmkdir -p /mnt/disks/elk/elasticsearch chown elasticsearch:elasticsearch /mnt/disks/elk/elasticsearch  設定檔案 ELK 提供了許多可設定調整的設定,但龐大的設定檔案也十分難上手。我們這邊先簡單更改以下設定檔案\nsudo vim /etc/elasticsearch/elasticsearch.yml # Change Network network.host: 0.0.0.0 # Change data path path.data: /mnt/disks/elk/elasticsearch vim /etc/elasticsearch/jvm-options # Adjust heap to 4G -Xms4g -Xmx4g # Enable xpack.security discovery.seed_hosts: [\u0026quot;10.140.0.10\u0026quot;] discovery.type: \u0026quot;single-node\u0026quot; xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.license.self_generated.type: basic  6.3.0 後的版本已經附上安全性模組 xpack，這邊順便開起來。關於 xpack 的安全性設定，這邊先略過不提。\n有啟用 xpack ，可以讓我們透過 elasticsearch 附帶的工具，產生使用者與帳號密碼。\n/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto # Keep your passwords safe  然後把啟動 Elasticsearch\nsudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service  看一下 log，確定服務有在正常工作\ntail -f /var/log/elasticsearch/elasticsearch.log  在 node 上試打 Elasticsearch API\n$ curl localhost:9200 { \u0026quot;name\u0026quot; : \u0026quot;elk\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;uiMZe7VETo-H6JLFLF4SZg\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;7.3.1\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;deb\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;4749ba6\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2019-08-19T20:19:25.651794Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.1.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;6.8.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;6.0.0-beta1\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; }  Kibana 有了正常工作的 Elasticsearch，接下來要安裝 kibana，由於 apt repository 已經匯入，這邊直接\nsudo apt-get update sudo apt-get install kibana  一樣快速設定一下\n$ vim /etc/kibana/kinana.yml # change server.host from localhost to 0.0.0.0 to allow outside requests server.host: \u0026quot;0.0.0.0\u0026quot; # Add elasticsearch password elasticsearch.username: \u0026quot;kibana\u0026quot; elasticsearch.password: sudo systemctl enable kibana.service sudo systemctl start kibana.service sudo systemctl status kibana.service  檢查 log 並試打一下\nsudo systemctl status kibana $ curl localhost:5601  透過內網 ip 也可以用 browser 存取 使用 elastic 這組帳號密碼登入，可以有管理員權限 可以檢視一下 kibana 的頁面，看一下是否系統功能都上常上線 http://10.140.0.10/app/monitoring#\nFilebeat 以上是 ELK 最基本架構: elasticsearch 引擎與前端視覺化管理工具 Kibana。當然現在進去 kibana 是沒有數據的，所以我們現在來安裝第一個 beat，收集第一筆數據。\n你可能會覺得奇怪: 我現在沒有任何需要監控的應用，去哪收集數據?\nELK 提供的自我監測 (self-monitoring) 的功能，也就是在 node 上部屬 filebeat 並啟用 modules，便可以把這台 node 上的 elasticsearch 運行的狀況，包含cpu 狀況、記憶體用量、儲存空間用量、安全性告警、\u0026hellip;都做為數據，傳到 elasticsearch 中，並在 Kibana monitoring 頁面製圖顯示。\n這邊也剛好做為我們 ELK stack 的第一筆數據收集。\nWARNING: 這邊一樣要提醒， production 環境多半會使用另外一組的 elasticsearch 來監控主要的這組 elastic stack，以維持 elk stack 的穩定性，才不會自己 monitoring 自己，結果 elastic 掛了，metrics 跟錯誤訊息都看不到。\n官方安裝文件\nsudo apt-get update sudo apt-get install filebeat  預設的 filebeat.yml 設定檔案不是完整的，請到官網下載完整版，但官網沒給檔案連結(慘)，只有網頁版 https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-reference-yml.html\n我們上 github 把她載下來\n$ wget https://raw.githubusercontent.com/elastic/beats/v7.3.1/filebeat/filebeat.reference.yml $ sudo mv filebeat-reference-y $ sudo vim /etc/filebeat/filebeat.yml # Enable elasticsearch module and kibana module to process metrics of localhost elasticsearch \u0026amp; kibana filebeat.modules: - module: elasticsearch # Server log server: enabled: true - module: kibana # All logs log: enabled: true # The name will be added to metadata name: filebeat-elk fields: env: elk # Add additional cloud_metadata since we're on GCP processors: - add_cloud_metadata: ~ # Output to elasticsearch output.elasticsearch: enabled: true hosts: [\u0026quot;localhost:9200\u0026quot;] protocol: \u0026quot;http\u0026quot; username: \u0026quot;elastic\u0026quot; password: # Configure kibana with filebeat: add template, dashboards, etc... setup.kibana: host: \u0026quot;localhost:5601\u0026quot; protocol: \u0026quot;http\u0026quot; username: \u0026quot;elastic\u0026quot; password:  啟動 filebeat\nsudo systemctl start filebeat  看一下 log，filebeat 會開始收集 elasticsearch 的 log 與 metrics，可以在 log 上看到收集的狀況。\n$ sudo journalctl -fu filebeat Sep 15 06:28:50 elk filebeat[9143]: 2019-09-15T06:28:50.176Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\u0026quot;monitoring\u0026quot;: {\u0026quot;metrics\u0026quot;: {\u0026quot;beat\u0026quot;:{\u0026quot;cpu\u0026quot;:{\u0026quot;system\u0026quot;:{\u0026quot;ticks\u0026quot;:1670860,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:66}},\u0026quot;total\u0026quot;:{\u0026quot;ticks\u0026quot;:6964660,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:336},\u0026quot;value\u0026quot;:6964660},\u0026quot;user\u0026quot;:{\u0026quot;ticks\u0026quot;:5293800,\u0026quot;time\u0026quot;:{\u0026quot;ms\u0026quot;:270}}},\u0026quot;handles\u0026quot;:{\u0026quot;limit\u0026quot;:{\u0026quot;hard\u0026quot;:4096,\u0026quot;soft\u0026quot;:1024},\u0026quot;open\u0026quot;:11},\u0026quot;info\u0026quot;:{\u0026quot;ephemeral_id\u0026quot;:\u0026quot;62fd4bfa-1949-4356-9615-338ca6a95075\u0026quot;,\u0026quot;uptime\u0026quot;:{\u0026quot;ms\u0026quot;:786150373}},\u0026quot;memstats\u0026quot;:{\u0026quot;gc_next\u0026quot;:7681520,\u0026quot;memory_alloc\u0026quot;:4672576,\u0026quot;memory_total\u0026quot;:457564560376,\u0026quot;rss\u0026quot;:-32768},\u0026quot;runtime\u0026quot;:{\u0026quot;goroutines\u0026quot;:98}},\u0026quot;filebeat\u0026quot;:{\u0026quot;events\u0026quot;:{\u0026quot;active\u0026quot;:-29,\u0026quot;added\u0026quot;:1026,\u0026quot;done\u0026quot;:1055},\u0026quot;harvester\u0026quot;:{\u0026quot;open_files\u0026quot;:4,\u0026quot;running\u0026quot;:4}},\u0026quot;libbeat\u0026quot;:{\u0026quot;config\u0026quot;:{\u0026quot;module\u0026quot;:{\u0026quot;running\u0026quot;:0}},\u0026quot;output\u0026quot;:{\u0026quot;events\u0026quot;:{\u0026quot;acked\u0026quot;:1055,\u0026quot;active\u0026quot;:-50,\u0026quot;batches\u0026quot;:34,\u0026quot;total\u0026quot;:1005},\u0026quot;read\u0026quot;:{\u0026quot;bytes\u0026quot;:248606},\u0026quot;write\u0026quot;:{\u0026quot;bytes\u0026quot;:945393}},\u0026quot;pipeline\u0026quot;:{\u0026quot;clients\u0026quot;:9,\u0026quot;events\u0026quot;:{\u0026quot;active\u0026quot;:32,\u0026quot;published\u0026quot;:1026,\u0026quot;total\u0026quot;:1026},\u0026quot;queue\u0026quot;:{\u0026quot;acked\u0026quot;:1055}}},\u0026quot;registrar\u0026quot;:{\u0026quot;states\u0026quot;:{\u0026quot;current\u0026quot;:34,\u0026quot;update\u0026quot;:1055},\u0026quot;writes\u0026quot;:{\u0026quot;success\u0026quot;:35,\u0026quot;total\u0026quot;:35}},\u0026quot;system\u0026quot;:{\u0026quot;load\u0026quot;:{\u0026quot;1\u0026quot;:1.49,\u0026quot;15\u0026quot;:0.94,\u0026quot;5\u0026quot;:1.15,\u0026quot;norm\u0026quot;:{\u0026quot;1\u0026quot;:0.745,\u0026quot;15\u0026quot;:0.47,\u0026quot;5\u0026quot;:0.575}}}}}}  如果數據都有送出，就可以回到 kibana 的頁面，看一下目前這個 elasticsearch 集群，有開啟 monitoring 功能的元件們，是否都有正常工作。\nhttp://10.140.0.10/app/monitoring#\n頁面長得像這樣\n  Standalone cluster 中的 filebeat，是還未跟 elasticsearch 配對完成的數據，會顯示在另外一個集群中，配對完後會歸到 elk cluster 中，就是我們的主要 cluster。\n點進去可以看各個元件的服務情形。\n小結  簡單思考 self-host ELK stack 搭建的架構 在單一 node 上安裝最簡易的 elastic stack 設定元件的 output 位置 設定 self-monitoring  恭喜各位獲得一個裸奔但是功能完整的 ELK, 我們下篇再向安全性邁進。\n","date":1568518983,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568904882,"objectID":"7c82de693b6423525d081c7ff85aa7b3","permalink":"https://chechiachang.github.io/post/self-host-elk-stack-on-gcp/","publishdate":"2019-09-15T11:43:03+08:00","relpermalink":"/post/self-host-elk-stack-on-gcp/","section":"post","summary":"Self-host ELK stack on GCP - Installation","tags":["gcp","elk","kubernetes","elasticsearch"],"title":"Self-host ELK stack - Installation","type":"post"},{"authors":[],"categories":[],"content":"各位好，我是Che-Chia Chang，社群上常用的名子是 David Chang。是個軟體工程師，專長的領域是後端開發，開發維運，容器化應用，以及Kubernetes開發管理。目前為 Golang Taiwan Meetup 的 organizer。\n受到友人們邀請（推坑）參加了2020 It邦幫忙鐵人賽，挑戰在30天內，每天發一篇技術分享文章。一方面將工作上遇到的問題與解法分享給社群，另一方面也是給自己一點成長的壓力，把這段時間的心得沈澱下來，因此也了這系列文章。\n本系列文章重點有三：\n 提供的解決方案，附上一步步的操作步驟。希望讓讀者可以重現完整操作步驟，直接使用，或是加以修改\n 著重 Google Cloud Platform，特別是Google Compute Engine (GCE) 與Google Kubernetes Engine (GKE) 兩大服務。這也是我最熟悉的平台，順便推廣，並分享一些雷點。\n 從維運的角度除錯，分析問題，提升穩定性。\n  預定的主題如下（可能會依照實際撰寫狀況微調）\n ELK Stask on GCP (8)  Self-host ELK stack on GCP Secure ELK Stask ELK Stask 的安全性連線(TLS/HTTP2) 設定 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 於 GKE 上為 ELK stack 除錯 我們為何不使用 Elastic Cloud Sass 方案  GKE 維運心得 (5)  我的 Kubernetes 除錯流程 Kubectl cheat sheet 使用 cert-manager 維護 TLS/HTTPS 使用 redhat operator-sdk 初探 CRD 與 operator 我的 operator 範例分享  在 GKE 上部署 Kafka HA (4)  使用 helm 部署 kafka-ha 集群內部的 HA 設定，網路設定 應用端的基本範例，效能調校 在 GKE 上維運 kafka  在 GKE 上部署 Redis HA (4)  使用 helm 部署 redis-ha 集群內部的 HA 設定，網路設定 應用端的基本範例，效能調校 在 GKE 上維運 redis  Prometheus / Grafana (5)  GKE 上自架 Prometheus / Grafana 使用 exporter 監測 GKE 上的各項服務 輸出 kubernetes 的監測數據 輸出 redis-ha 的監測數據 輸出 kafka 的監測數據  GCP 網路設定 (3)  防火牆的私有網路基本設定 配合 GKE 實現負載均衡 DNS 基本觀念，從 kube-dns 到 GCP DNS service  GCP 日誌管理 (2)  基本 GCP 日誌管理與錯誤回報 Stackdriver 服務的日誌管理，監測數據，告警   文章發表於鐵人挑戰頁面，同時發布與本站備份。有任何謬誤，還煩請各方大德\u0026lt;3透過底下的聯絡方式聯絡我，感激不盡。\nFeatures\n step-by-step guide for deployment: guarentee a running deployment on GCP basic configuration, usage, monitoring, networking on GKE debugging, stability analysis in an aspect of devop  Topics\nELK stack(8)  Deploy self-hosted ELK stack on GCE instance Secure ELK stack with SSL and role-based authentication Monitoring services on Kubernetes with ELK beats Monitoring services on GCE instances Logstash pipelines and debugging walk through Elasticsearch operations: house-cleaning, tuning, pernament storage Elasticsearch maitainence, trouble shooting Get-Started with Elastic Cloud SASS  General operations on Kubernetes(4)  Kubernetes Debug SOP Kubectl cheat sheet Secure services with SSL by cert-manager Speed up container updating with operator My operator example  Deploy Kafka HA on Kubernetes(4)  deploy kafka-ha on Kubernertes with helm in-cluster networking configuration for high availability basic app-side usage, performance tuning Operate Kafka: update config, upgrade version, migrate data  Promethus / grafana(5)  Deploy Prometheus / Grafana stack on GCE instance Monitoring services on Kubernetes with exporters Export Kubernetes metrics to Prometheus Export Redis-ha metrics to Prometheus Export Kafka metrics to Prometheus  GCP networking(4)  Firewall basic concept for private network with GCE instances \u0026amp; Kubernetes Load balancer for Kubernetes service \u0026amp; ingress DNS on GCP from Kube-dns to GCP DNS service  GCP log management(3) Basic usage about GCP logging \u0026amp; GCP Error Report Stackdriver, metrics, alerts Logging on GKE from gcp-fluentd to stackdriver   ","date":1568019363,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568561639,"objectID":"71ec55f87de7a48a942c10d2ee651db0","permalink":"https://chechiachang.github.io/post/2020-ithome-ironman-challenge/","publishdate":"2019-09-09T16:56:03+08:00","relpermalink":"/post/2020-ithome-ironman-challenge/","section":"post","summary":"2020 IT邦幫忙鐵人賽","tags":[],"title":"2020 IT邦幫忙鐵人賽","type":"post"},{"authors":[],"categories":[],"content":" API resources kubectl api-resources  ","date":1561094273,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566547740,"objectID":"cb4f5e8cea40783770693b4e6baf0d66","permalink":"https://chechiachang.github.io/post/kuberentes-source-code/","publishdate":"2019-06-21T13:17:53+08:00","relpermalink":"/post/kuberentes-source-code/","section":"post","summary":" API resources kubectl api-resources  ","tags":[],"title":"Kuberentes Source Code","type":"post"},{"authors":null,"categories":null,"content":"","date":1559454714,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559455067,"objectID":"39a764db7449bf949f1eb4b54becb2be","permalink":"https://chechiachang.github.io/project/quantum-computing-journey/","publishdate":"2019-06-02T13:51:54+08:00","relpermalink":"/project/quantum-computing-journey/","section":"project","summary":"","tags":["quantum-computing","ibm-q-experience","tutorial"],"title":"Quantum Computing Journey","type":"project"},{"authors":[],"categories":[],"content":" This post is about my learning steps for quantum-computing.\nFor a quick-start tutorial, check my workshop project throught the project link above.\nResources Courses\nCoursera\non MIT x pro\nQuantum Information Processing from UW Madison\nQuantum Computation by John Preskill\nIBM Q Experience\nhttps://github.com/Qiskit/openqasm\nhttps://github.com/Qiskit/qiskit-tutorials\nIBM Q Experience Day 1 Getting Started with Circuit Composer\nHello Quantum World circuit transformed two qubits, from $ \\vert0\\rangle $ to $ \\frac{\\vert00\\rangle + \\vert11\\rangle}{\\sqrt{2}} $\nQuestions\n [] Hadamard Gate [] Bell states  [] Annotations   ","date":1559442097,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559531117,"objectID":"291a457ec654f749d1a75bb365b864ef","permalink":"https://chechiachang.github.io/post/journey-to-quantum-computing/","publishdate":"2019-06-02T10:21:37+08:00","relpermalink":"/post/journey-to-quantum-computing/","section":"post","summary":"This post is about my learning steps for quantum-computing.\nFor a quick-start tutorial, check my workshop project throught the project link above.\nResources Courses\nCoursera\non MIT x pro\nQuantum Information Processing from UW Madison\nQuantum Computation by John Preskill\nIBM Q Experience\nhttps://github.com/Qiskit/openqasm\nhttps://github.com/Qiskit/qiskit-tutorials\nIBM Q Experience Day 1 Getting Started with Circuit Composer\nHello Quantum World circuit transformed two qubits, from $ \\vert0\\rangle $ to $ \\frac{\\vert00\\rangle + \\vert11\\rangle}{\\sqrt{2}} $","tags":["quantum-computing","ibm-q-experience","tutorial"],"title":"Journey to Quantum Computing","type":"post"},{"authors":[],"categories":null,"content":" Outlines 傳統的 Monolith被分解為分散的微服務，以取得更高的效能與更彈性的管理。當眾多的為服務同時運作，產生複雜的依賴與交流，網路層不再只是有『有通就好』，而是需要精細且彈性的流量管理與監控，來提供穩定的效能。本次主題將基於 Kubernetes 平台上的 Istio ，探討 Service Mesh 的概念與相關應用。\n 何為 Service Mesh ？為何需要 Service Mesh ？ Service Mesh 基本概念 如何Service-to-Service的網路層管理監控 導入 Istio 到 Kubernetes  目標聽眾  微運大量微服務，希望導入Service Mesh 的Operator 想了解微服務生態中竄紅的 Service Mesh  收穫  了解為服務的優勢與Cloud Native應用發展趨勢 了解 Service Mesh 與 Istio 觀念 能使用 Istio 於 Kubernetes，進行服務網路的管理。  你有聽過 Microservice / Istio有聽過嗎？ 今天來介紹一款好藥：Istio。如果你有以下問題：\n 維運大量(成千上百)微服務 需要服務對服務的流量控制，監控，管理  談 Service Mesh 之前，不免的要先談一下 Microservice，這個目前好像很夯的一個技術名詞。\n如果手上有一個 App，會希望依照 Monolith 的架構，或是 Microservices？ Microservices 聽起來又新又潮。相對於 Monolith有許多明顯的好處：\n Decoupling Scalability Performance  也有明顯的壞處：\n Development Complexity Operation Cost   沒事別挖坑跳\n 何為 Service Mesh？\n Service Mesh: Model / Pattern Implementations: linkerd, istio, \u0026hellip; 基於底層的網路服務，在複雜的 topology 中可靠的傳遞  使用Microservie 可能會遇到的問題：\n Traffic control Monitoring A/B Testing  ","date":1557892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558582214,"objectID":"ab0c03aeb8a862eb9ba49b1163a53255","permalink":"https://chechiachang.github.io/talk/service-mesh-for-microservices-on-kubernetes/","publishdate":"2019-04-02T17:10:57+08:00","relpermalink":"/talk/service-mesh-for-microservices-on-kubernetes/","section":"talk","summary":"基於 Kubernetes 平台上的 Istio ，探討 Service Mesh 的概念與相關應用。","tags":["kubernetes","istio","service-mesh"],"title":"Service Mesh for Microservices on Kubernetes","type":"talk"},{"authors":[],"categories":[],"content":" Create GKE gcloud beta container --project \u0026quot;istio-playground-239810\u0026quot; clusters create \u0026quot;istio-playground\u0026quot; \\ --zone \u0026quot;asia-east1-b\u0026quot; \\ --username \u0026quot;admin\u0026quot; \\ --cluster-version \u0026quot;1.11.8-gke.6\u0026quot; \\ --machine-type \u0026quot;n1-standard-2\u0026quot; \\ --image-type \u0026quot;COS\u0026quot; \\ --disk-type \u0026quot;pd-standard\u0026quot; \\ --disk-size \u0026quot;100\u0026quot; \\ --preemptible \\ --num-nodes \u0026quot;1\u0026quot; \\ --enable-cloud-logging \\ --enable-cloud-monitoring \\ --no-enable-ip-alias \\ --addons HorizontalPodAutoscaling,HttpLoadBalancing,KubernetesDashboard,Istio \\ --istio-config auth=MTLS_PERMISSIVE \\ --no-enable-autoupgrade \\ --enable-autorepair  Take a Peek $ kubectl get namespaces NAME STATUS AGE default Active 2m istio-system Active 1m kube-public Active 2m kube-system Active 2m $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-7f6f77cd7b-nxfbf 1/1 Running 0 3m istio-cleanup-secrets-h454m 0/1 Completed 0 3m istio-egressgateway-7c56db84cc-nlrwq 1/1 Running 0 3m istio-galley-6c747bdb4f-45jrp 1/1 Running 0 3m istio-ingressgateway-6ff68cf95d-tlkq4 1/1 Running 0 3m istio-pilot-8ff66f8c4-q9chz 2/2 Running 0 3m istio-policy-69b78b7d6-c8pld 2/2 Running 0 3m istio-sidecar-injector-558996c897-hr6q4 1/1 Running 0 3m istio-telemetry-f96459fb-5cbpg 2/2 Running 0 3m promsd-ff878d44b-hv8nh 2/2 Running 1 3m  Deploy app kubectl label namespace default istio-injection=enabled  Bookinfo Application\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml kubectl get pods kubectl get services  Gateway\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml kubectl get gateways kubectl get svc istio-ingressgateway -n istio-system  Go to ingress public ip\nexport INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\u0026quot;http2\u0026quot;)].port}') export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name==\u0026quot;https\u0026quot;)].port}') curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage 404 Not Found  Apply destination rules\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/destination-rule-all.yaml curl -v ${INGRESS_HOST}:{$INGRESS_PORT}/productpage  Brief review kubectl get virtualservices kubectl get destinationrules kubectl get gateways  Istio Tasks https://istio.io/docs/tasks/traffic-management/\n","date":1557137535,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557885090,"objectID":"262caf52e34b5cf4614d25adf2d45618","permalink":"https://chechiachang.github.io/post/service-mesh-for-microservice-on-kubernetes/","publishdate":"2019-05-06T18:12:15+08:00","relpermalink":"/post/service-mesh-for-microservice-on-kubernetes/","section":"post","summary":"基於 Kubernetes 平台上的 Istio ，實際部署，並一步一步操作Istio 的功能。","tags":["kubernetes","istio","service-mesh"],"title":"Istio 三分鐘就入坑 佈署篇","type":"post"},{"authors":[],"categories":null,"content":" How to deploy a cloud-native Jenkins with Jenkins X. A pipeline with Kubernetes based dynamics worker sclaing (jenkins-kubernetes). Give it a try. (Defered) Customized test reports for multiple language (ex. go-junit-report)  ","date":1555736400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555683962,"objectID":"3212c13fe0ec93769c195996be6fa2a5","permalink":"https://chechiachang.github.io/talk/jenkins-on-kubernetes/","publishdate":"2019-04-09T15:29:21+08:00","relpermalink":"/talk/jenkins-on-kubernetes/","section":"talk","summary":"Working pipeline with jenkins-x on Kubernetes.","tags":["kubernetes","jenkins","devops"],"title":"Jenkins on Kubernetes","type":"talk"},{"authors":null,"categories":null,"content":" Jenkins  One of the earliest open source antomation server Most common option in use Flexible and customizable. Hundreds of plugins Support automation for any project  Jenkins X  CI/CD platform (Jenkins Platform) Cloud native serverless For modern cloud applications on Kubernetes.  Outlines  Install Jenkins with jx Create a Pipeline with jx Develope with jx client  check Jenkins-X Github Repo\n\nInstall Create GKE cluster \u0026amp; Get Credentials\ngcloud init gcloud components update  CLUSTER_NAME=jenkins-server #CLUSTER_NAME=jenkins-serverless gcloud container clusters create ${CLUSTER_NAME} \\ --num-nodes 1 \\ --machine-type n1-standard-4 \\ --enable-autoscaling \\ --min-nodes 1 \\ --max-nodes 2 \\ --zone asia-east1-b \\ --preemptible  Create GKE cluster \u0026amp; Get Credentials\n# Get credentials to access cluster with kubectl gcloud container clusters get-credentials ${CLUSTER_NAME} # Check cluster stats. kubectl get nodes  Download Jenkins X Release \u0026amp; install jx on Local Machine\nJX_VERSION=v2.0.2 OS_ARCH=darwin-amd64 #OS_ARCH=linux-amd64 curl -L https://github.com/jenkins-x/jx/releases/download/\u0026quot;${JX_VERSION}\u0026quot;/jx-\u0026quot;${OS_ARCH}\u0026quot;.tar.gz | tar xzv sudo mv jx /usr/local/bin jx version NAME VERSION jx 2.0.2 Kubernetes cluster v1.11.7-gke.12 kubectl v1.11.9-dispatcher helm client v2.11.0+g2e55dbe helm server v2.11.0+g2e55dbe git git version 2.20.1 Operating System Mac OS X 10.14.4 build 18E226  (Install Option 1) Serverless Jenkins Pipeline\nDEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke'  Install options:\nSelect Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins  Pick default workload build pack\nKubernetes Workloads: Automated CI+CD with GitOps Promotion [ ] Library Workloads: CI+Release but no CD\nYour Kubernetes context is now set to the namespace: jx INFO[0231] To switch back to your original namespace use: jx namespace jx ...    (Install Option 2) Static Jenkins Server\nDEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke'  Options:\nSelect Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins  Pick default workload build pack\nKubernetes Workloads: Automated CI+CD with GitOps Promotion [ ] Library Workloads: CI+Release but no CD\nINFO[0465]Your Kubernetes context is now set to the namespace: jx INFO[0465] To switch back to your original namespace use: jx namespace default Access Static Jenkins Server through Domain with username and password Domain http://jenkins.jx.11.22.33.44.nip.io/    Uninstall jx uninstall # rm -rf ~/.jx  Setup CI/CD Pipeline Create Quickstart Repository\nkubectl get pods --namespace jx --watch  # cd workspace jx create quickstart  Options:\n$ select the quickstart you wish to create [Use arrows to move, type to filter] aspnet-app dlang-http \u0026gt; golang-http jenkins-cwp-quickstart jenkins-quickstart node-http INFO[0121] Watch pipeline activity via: jx get activity -f serverless-jenkins-quickstart -w  Check log of the first run\njx logs pipeline  Add a setup step for pullrequest\ncd serverless-jenkins-quickstart jx create step --pipeline pullrequest \\ --lifecycle setup \\ --mode replace \\ --sh \u0026quot;echo hello world\u0026quot;  Validate pipeline step for each modification\njx step validate  A build-pack pod started after git push. Watch pod status with kubectl.\nkubectl get pods --namespace jx --watch  Check Build Status on Prow (Serverless)\nhttp://deck.jx.130.211.245.13.nip.io/ Login with username and password\nImport Existing Repository\n# In source code repository # Import jx to remote jenkins-server. This will apply a Jenkinsfile to repository by default jx import \\ --url git@github.com:chechiachang/serverless-jenkins-quickstart.git  Update jenkins-x.yml\njx create step git commit git push  Trouble Shooting: Failed to get jx resources\njx get pipelines  Make sure your jx (or kubectl) context is with the correct GKE and namespace\nkc config set-context gke_my-project_asia-east1-b_jenkins \\ --namespace=jx  Helm vs Jenkins X  Jenkins Helm Chart\n create Jenkins master and slave cluster on Kubernetes utilizing the Jenkins Kubernetes plugin.  Jenkin Platform with jx\n Jenkins Platform native to Kubernetes Powerful cloud native components: Prow, Nexus, Docker Registry, Tekton Pipeline, \u0026hellip;   Check jenkins-x examples https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs\njx Client jx get urls Name URL jenkins http://jenkins.jx.11.22.33.44.nip.io jenkins-x-chartmuseum http://chartmuseum.jx.11.22.33.44.nip.io jenkins-x-docker-registry http://docker-registry.jx.11.22.33.44.nip.io jenkins-x-monocular-api http://monocular.jx.11.22.33.44.nip.io jenkins-x-monocular-ui http://monocular.jx.11.22.33.44.nip.io nexus http://nexus.jx.11.22.33.44.nip.io  Get Cluster Status\njx diagnose  Get Applications \u0026amp; Pipelines\njx get applications jx get pipelines  Get CI Activities \u0026amp; build log\njx get activities jx get activities --filter='jenkins-x-on-kubernetes' jx get build log INFO[0003] view the log at: http://jenkins.jx.11.22.33.44.nip.io/job/chechiachang/job/jenkins-x-on-kubernetes/job/feature-add-test/3/console ...  Trigger Build \u0026amp; Check Activity\njx start pipeline jx start pipeline --filter='jenkins-x-on-kubernetes/feature-add-test' jx get activities --filter='jenkins-x-on-kubernetes'  Create Pull Request\njx create pullrequest  Summary  Demonstrate a Jenkins pipeline Jenkins plugin  master slave cluster kubernetes plugin lovely GUI  jx on k8s jx cli   Jenkins 簡單用\n 設定與維護人力會比其他工具稍微多  Jenkins 複雜用\n Deep Customization: 希望花時間打造最符合自己需求的工具 預期有特殊需求  Jenkins X\n 應用依賴 Kubernetes 開發，測試，部屬 (ex. kubernetes client-go) 使用 jx 一站式服務   The End\n  ","date":1555681010,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568171821,"objectID":"f6ec8f2d00ca9f7da384d7b19d802ba9","permalink":"https://chechiachang.github.io/slides/jenkins-x-on-kubernetes/","publishdate":"2019-04-19T21:36:50+08:00","relpermalink":"/slides/jenkins-x-on-kubernetes/","section":"slides","summary":"Jenkins  One of the earliest open source antomation server Most common option in use Flexible and customizable. Hundreds of plugins Support automation for any project  Jenkins X  CI/CD platform (Jenkins Platform) Cloud native serverless For modern cloud applications on Kubernetes.  Outlines  Install Jenkins with jx Create a Pipeline with jx Develope with jx client  check Jenkins-X Github Repo\n\nInstall Create GKE cluster \u0026amp; Get Credentials","tags":null,"title":"Jenkins X on Kubernetes","type":"slides"},{"authors":[],"categories":null,"content":"是的，我們做了一款七龍珠中的戰鬥力探測器，透過人臉辨識，探測工程師的開源貢獻力。\n本次演講內容有:\n 初心者的人臉辨識技術 ，Face Recognition API 使用 Golang 在 Github 上做 Data Mining 從零開始的 side project，開發心路歷程與收穫分享  ","date":1555666318,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679944,"objectID":"ba5cd94fc710d297f573c13cc92496c4","permalink":"https://chechiachang.github.io/talk/gdg-devfest-2018-scouter/","publishdate":"2019-04-19T17:31:58+08:00","relpermalink":"/talk/gdg-devfest-2018-scouter/","section":"talk","summary":"是的，我們做了一款七龍珠中的戰鬥力探測器，透過人臉辨識技術探測工程師在Github上的的開源貢獻力","tags":["machine-learning","face-detection","face-recognition","unity","ios","golang","github-api","ar"],"title":"從零開始的人臉辨識，七龍珠戰鬥力探測器","type":"talk"},{"authors":[],"categories":[],"content":" Jenkins is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.\nJenkins X, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.\nHere we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.\n Architecture of Jenkins X Install Jenkins with jx Create a Pipeline with jx Develope with jx client  For more information about jx itself, check Jenkins-X Github Repo\n\nArchitecture Check this beautiful diagram.\n https://jenkins-x.io/architecture/diagram/   \nInstall Create GKE cluster \u0026amp; Get Credentials gcloud init gcloud components update  CLUSTER_NAME=jenkins-server #CLUSTER_NAME=jenkins-serverless gcloud container clusters create ${CLUSTER_NAME} \\ --num-nodes 1 \\ --machine-type n1-standard-4 \\ --enable-autoscaling \\ --min-nodes 1 \\ --max-nodes 2 \\ --zone asia-east1-b \\ --preemptible # After cluster initialization, get credentials to access cluster with kubectl gcloud container clusters get-credentials ${CLUSTER_NAME} # Check cluster stats. kubectl get nodes  Install jx on Local Machine [Jenkins X Release](https://github.com/jenkins-x/jx/releases](https://github.com/jenkins-x/jx/releases)\nJX_VERSION=v2.0.2 OS_ARCH=darwin-amd64 #OS_ARCH=linux-amd64 curl -L https://github.com/jenkins-x/jx/releases/download/\u0026quot;${JX_VERSION}\u0026quot;/jx-\u0026quot;${OS_ARCH}\u0026quot;.tar.gz | tar xzv sudo mv jx /usr/local/bin jx version NAME VERSION jx 2.0.2 Kubernetes cluster v1.11.7-gke.12 kubectl v1.11.9-dispatcher helm client v2.11.0+g2e55dbe helm server v2.11.0+g2e55dbe git git version 2.20.1 Operating System Mac OS X 10.14.4 build 18E226  (Option 1) Install Serverless Jenkins Pipeline DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke'  Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins  Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD  Select the organization where you want to create the environment repository:\n chechiachang\nYour Kubernetes context is now set to the namespace: jx INFO[0231] To switch back to your original namespace use: jx namespace jx INFO[0231] Or to use this context/namespace in just one terminal use: jx shell INFO[0231] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0231] To import existing projects into Jenkins: jx import INFO[0231] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0231] To create a new microservice from a quickstart: jx create quickstart    (Option 2) Install Static Jenkins Server DEFAULT_PASSWORD=mySecretPassWord123 jx install \\ --default-admin-password=${DEFAULT_PASSWORD} \\ --provider='gke'  Options:\nEnter Github user name Enter Github personal api token for CI/CD Enable Github as Git pipeline server Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekon Static Master Jenkins  Pick default workload build pack Kubernetes Workloads: Automated CI+CD with GitOps Promotion Library Workloads: CI+Release but no CD  Select the organization where you want to create the environment repository:\n chechiachang\nINFO[0465]Your Kubernetes context is now set to the namespace: jx INFO[0465] To switch back to your original namespace use: jx namespace default INFO[0465] Or to use this context/namespace in just one terminal use: jx shell INFO[0465] For help on switching contexts see: https://jenkins-x.io/developing/kube-context/ INFO[0465] To import existing projects into Jenkins: jx import INFO[0465] To create a new Spring Boot microservice: jx create spring -d web -d actuator INFO[0465] To create a new microservice from a quickstart: jx create quickstart    Access Static Jenkins Server through Domain with username and password Domain http://jenkins.jx.11.22.33.44.nip.io/\nUninstall jx uninstall # rm -rf ~/.jx  \nSetup CI/CD Pipeline Create Quickstart Repository kubectl get pods --namespace jx --watch  # cd workspace jx create quickstart  Options:\n Which organisation do you want to use? chechiachang Enter the new repository name: serverless-jenkins-quickstart select the quickstart you wish to create [Use arrows to move, type to filter] angular-io-quickstart aspnet-app dlang-http \u0026gt; golang-http jenkins-cwp-quickstart jenkins-quickstart node-http\nINFO[0121] Watch pipeline activity via: jx get activity -f serverless-jenkins-quickstart -w INFO[0121] Browse the pipeline log via: jx get build logs chechiachang/serverless-jenkins-quickstart/master INFO[0121] Open the Jenkins console via jx console INFO[0121] You can list the pipelines via: jx get pipelines INFO[0121] Open the Jenkins console via jx console INFO[0121] You can list the pipelines via: jx get pipelines INFO[0121] When the pipeline is complete: jx get applications   Check log of the first run jx logs pipeline  Add Step to Pipeline Add a setup step for pullrequest\ncd serverless-jenkins-quickstart jx create step --pipeline pullrequest \\ --lifecycle setup \\ --mode replace \\ --sh \u0026quot;echo hello world\u0026quot;  Validate pipeline step for each modification\njx step validate  A build-pack pod started after git push. Watch pod status with kubectl.\nkubectl get pods --namespace jx --watch  Check Build Status on Prow (Serverless) http://deck.jx.130.211.245.13.nip.io/ Login with username and password\nImport Existing Repository In source code repository:\nImport jx to remote jenkins-server. This will apply a Jenkinsfile to repository by default\njx import --url git@github.com:chechiachang/serverless-jenkins-quickstart.git  Update jenkins-x.yml\njx create step  git commit \u0026amp; push\nTrouble Shooting Failed to get jx resources\njx get pipelines  Make sure your jx (or kubectl) context is with the correct GKE and namespace\nkc config set-context gke_my-project_asia-east1-b_jenkins \\ --namespace=jx  Why not use helm chart? It\u0026rsquo;s readlly depend on what we need in CI/CD automation.\nJenkins Helm Chart create Jenkins master and slave cluster on Kubernetes utilizing the Jenkins Kubernetes plugin. Jenkin Platform with jx is Jenkins Platform native to Kubernetes. It comes with powerful cloud native components like Prow automation, Nexus, Docker Registry, Tekton Pipeline, \u0026hellip;\nCheck jenkins-x examples https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs\n\nClient jx get urls Name URL jenkins http://jenkins.jx.11.22.33.44.nip.io jenkins-x-chartmuseum http://chartmuseum.jx.11.22.33.44.nip.io jenkins-x-docker-registry http://docker-registry.jx.11.22.33.44.nip.io jenkins-x-monocular-api http://monocular.jx.11.22.33.44.nip.io jenkins-x-monocular-ui http://monocular.jx.11.22.33.44.nip.io nexus http://nexus.jx.11.22.33.44.nip.io  Get Cluster Status jx diagnose  Get Applications \u0026amp; Pipelines jx get applications jx get pipelines  Get CI Activities \u0026amp; build log jx get activities jx get activities --filter='jenkins-x-on-kubernetes' jx get build log INFO[0003] view the log at: http://jenkins.jx.11.22.33.44.nip.io/job/chechiachang/job/jenkins-x-on-kubernetes/job/feature-add-test/3/console ...  Trigger Build \u0026amp; Check Activity jx start pipeline jx start pipeline --filter='jenkins-x-on-kubernetes/feature-add-test' jx get activities --filter='jenkins-x-on-kubernetes'  Create Pull Request jx create pullrequest  ","date":1555647341,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555655323,"objectID":"cbbd48a2d957ed817017c3a954636f11","permalink":"https://chechiachang.github.io/post/jenkins-x-on-kubernetes/","publishdate":"2019-04-19T12:15:41+08:00","relpermalink":"/post/jenkins-x-on-kubernetes/","section":"post","summary":"Jenkins is one of the earliest open source antomation server and remains the most common option in use today. Over the years, Jenkins has evolved into a powerful and flexible framework with hundreds of plugins to support automation for any project.\nJenkins X, on the other hand, is a CI/CD platform (Jenkins Platform) for modern cloud applications on Kubernetes.\nHere we talk about some basic concepts about Jenkins X and provide a hand-to-hand guide to deploy jenkins-x on Kubernetes.","tags":["jenkins","ci","cd","kubernetes"],"title":"Jenkins X on Kubernetes","type":"post"},{"authors":null,"categories":null,"content":"An example project to demonstrate a working pipeline with jenkins-x on Kubernetes.\n","date":1555643519,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679944,"objectID":"286a27ce77e0cb13184316712581d55a","permalink":"https://chechiachang.github.io/project/jenkins-x-on-kubernetes/","publishdate":"2019-04-19T11:11:59+08:00","relpermalink":"/project/jenkins-x-on-kubernetes/","section":"project","summary":"An example project to demonstrate a working pipeline with jenkins-x on Kubernetes.","tags":["jenkins","ci","cd","kubernetes"],"title":"Jenkins X on Kubernetes","type":"project"},{"authors":[],"categories":null,"content":"","date":1548154800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679045,"objectID":"97cd09c1f8cbf6f2e86caf33ab5b942d","permalink":"https://chechiachang.github.io/talk/elk-on-kubernetes/","publishdate":"2019-01-19T20:56:45+08:00","relpermalink":"/talk/elk-on-kubernetes/","section":"talk","summary":"手把手教你部屬 ELK 監測 Kubernetes 上的應用","tags":["kubernetes","elasticsearch","elk","monitoring","devops"],"title":"ELK for Applications on Kubernetes","type":"talk"},{"authors":[],"categories":null,"content":"","date":1545130800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555675029,"objectID":"8c6feda28d4a3d60d85157945f8067c0","permalink":"https://chechiachang.github.io/talk/go-webassembly-intro/","publishdate":"2018-12-18T19:52:31+08:00","relpermalink":"/talk/go-webassembly-intro/","section":"talk","summary":"A quick introduction about WebAssembly and Go-WebAssembly","tags":["golang","webassembly"],"title":"Go Webassembly Intro","type":"talk"},{"authors":null,"categories":["kubernetes"],"content":"-\u0026gt; Slides here \u0026lt;-\n","date":1538798820,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568171821,"objectID":"010124f9801011d6b344103b2d14a2d8","permalink":"https://chechiachang.github.io/post/kubernetes-container-runtime-interface/","publishdate":"2018-10-06T12:07:00+08:00","relpermalink":"/post/kubernetes-container-runtime-interface/","section":"post","summary":"-\u0026gt; Slides here \u0026lt;-","tags":["kubernetes","container","docker","cri"],"title":"Kubernetes Container Runtime Interface","type":"post"},{"authors":[],"categories":null,"content":"","date":1535454000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679944,"objectID":"d7400a9a14ccbb3fceede5ff7c8f1e82","permalink":"https://chechiachang.github.io/talk/go-github/","publishdate":"2018-08-28T19:00:00+08:00","relpermalink":"/talk/go-github/","section":"talk","summary":"使用 go-github 接 api 的一些眉角","tags":["golang","github"],"title":"使用 go-github 接 github api","type":"talk"},{"authors":null,"categories":null,"content":" footer: Che-Chia David Chang, 2018, https://github.com/chechiachang slidenumbers: true\nCRI, OCI, CRI-O David Chang DevOps @ Mithril Back-End Developer, Kuberentes admin, DevOps\n     Outline  Container Runtime Interface (CRI) Open Container Initiative (OCI) CRI-O Kubernetes on CRI-O       Trend Kubernetes  Kubernetes 1.3 introduced rktnetes Kubernetes 1.5 introduced CRI Kubernetes 1.7 removed pre-CRI Docker / rkt integration Currently works Kubelet to use CRI\n CRI-O: released 1.0.x to match Kubernetes 1.7\n  Nomination CRI-O - OCI-based implementation of Kubernetes Container Runtime Interface\nCRI - Kubernetes Container Runtime Interface\nOCI - Open Container initiative\nProjects with Container Runtime docker, rkt, LXC/LXD, runC, containerd, OpenVZ, systemd-nspawn, machinectl, qemu-kvm, lkvm\u0026hellip;\nKubernetes (before 1.6) native supports - Docker - rkt\n     Container Runtime Interface(CRI)  Enable Kubernetes to support more runtimes Free kubernetes to focus on orchestration from runtime integration Consists  a protocol buffers and gRPC API libraries, additional specifications and tools   Container Runtime Interface(CRI)   CRI api in kubernetes https://github.com/kubernetes/kubernetes/ blob/master/pkg/kubelet/apis/ cri/runtime/v1alpha2/api.proto\nCRI runtimes  Docker CRI shim (cri-containerd) CoreOS rktlet frakti: hypervisor-based container runtimes Intel Clear container OpenStack kata runtime cri-o  Open Container Inititive (OCI)  open governance structure container industry standards\n runtime spec defines configuration, execution environment, and lifecycle of a container\n image spec spec on archetecture and OS, filesystem layers and configuration\n  OCI from aspect of user  Use all OCI-conplimant container runtime Use all OCI-complimant images registries Similar UX  https://www.opencontainers.org/blog/2018/06/20/cri-o-how-standards-power-a-container-runtime\nCRI-O  OCI-based implementation of Kubernetes Container Runtime Interface Kubernetes incubator project also part of the CNCF Dedicated for Kubernetes Enable CRI-O plugin to other runtimes Available on RHEL, Fedora, Centos, Ubuntu\u0026hellip;  http://cri-o.io/\nCRI-O vs Docker (containerd) kubelet -\u0026gt; cri-containerd (shim) -\u0026gt; containerd -\u0026gt; runC -\u0026gt; container kubelet -\u0026gt; cri-o -\u0026gt; runC -\u0026gt; container\n Lightweight Stability  built for Kubernetes No cli, image utilities, \u0026hellip; No swarm, mesosphere integration, \u0026hellip;     Let\u0026rsquo;s use CRI-O  Install cri-o and dependencies, runC and CNI Install Podman\n Podman to cri-o as Docker-cli to Docker daemon\nsudo podman run --name my-golang golang:alpine bash    Minikube\nminikube start \\ --network-plugin=cni \\ --container-runtime=cri-o minikube start \\ --network-plugin=cni \\ --extra-config=kubelet.container-runtime=remote \\ --extra-config=kubelet.container-runtime-endpoint=/var/run/crio/crio.sock \\ --extra-config=kubelet.image-service-endpoint=/var/run/crio/crio.sock  Run Kubernetes on CRI-O Kubespray\nkubeadm_enabled: true ... container_manager: crio  Full cluster\nkubelet --container-runtime-endpoint=unix:///var/run/crio/crio.sock ...  References https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/ https://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/ Rttps://kubernetes.io/blog/2017/11/containerd-container-runtime-options-kubernetes/ https://xuxinkun.github.io/2017/12/12/docker-oci-runc-and-kubernetes/ https://www.kubernetes.org.cn/1079.html\n","date":1534672835,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568171821,"objectID":"2b56757a962b2aa7289451db7e5eff3e","permalink":"https://chechiachang.github.io/slides/container-runtime-interface/","publishdate":"2018-08-19T18:00:35+08:00","relpermalink":"/slides/container-runtime-interface/","section":"slides","summary":"footer: Che-Chia David Chang, 2018, https://github.com/chechiachang slidenumbers: true\nCRI, OCI, CRI-O David Chang DevOps @ Mithril Back-End Developer, Kuberentes admin, DevOps\n     Outline  Container Runtime Interface (CRI) Open Container Initiative (OCI) CRI-O Kubernetes on CRI-O       Trend Kubernetes  Kubernetes 1.3 introduced rktnetes Kubernetes 1.5 introduced CRI Kubernetes 1.7 removed pre-CRI Docker / rkt integration Currently works Kubelet to use CRI","tags":null,"title":"CRI, OCI, CRI-O","type":"slides"},{"authors":[],"categories":null,"content":"是的，我們做了一款七龍珠中的戰鬥力探測器，透過人臉辨識，探測工程師的開源貢獻力。\n本次演講內容有:\n 初心者的人臉辨識技術 ，Face Recognition API 使用 Golang 在 Github 上做 Data Mining 從零開始的 side project，開發心路歷程與收穫分享  Live Stream on Youtube\n  ","date":1533949200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679944,"objectID":"be139de05e17ea7c0c08fcb523e4acd2","permalink":"https://chechiachang.github.io/talk/coscup-2018-scouter/","publishdate":"2018-08-11T17:31:58+08:00","relpermalink":"/talk/coscup-2018-scouter/","section":"talk","summary":"是的，我們做了一款七龍珠中的戰鬥力探測器，透過人臉辨識技術探測工程師在Github上的的開源貢獻力","tags":["machine-learning","face-detection","face-recognition","unity","ios","golang","github-api","ar"],"title":"從零開始的人臉辨識，七龍珠戰鬥力探測器","type":"talk"},{"authors":[],"categories":null,"content":"從系統管理層面看Kubernetes的網路架構\n網路實作為Kubernetes架構，也是開發過程中容易出錯的部分。本次演講將從群集管理員的角度，說明Kubernetes 中網路的實作。\n大綱:\n Docker 與 Kubernetes 的網路架構 不同層級的網路溝通實作  容器對容器 Pod對Pod 集群內部與Service 集群外部對Service  以flannel為例講解網路實作 開發過程中常遇到的網路問題  希望聽眾對Kubernetes的網路架構能有基礎的概念，並在開發過程中遇到問題時，有明確的除錯步驟來判定網路是否有問題。遇到網路的問題，也能明確的知道問題的核心，並找到解法。\n","date":1528938000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555670692,"objectID":"1a116ed5492a1d3ee42d40ec7b5667b8","permalink":"https://chechiachang.github.io/talk/kubernetes-networking/","publishdate":"2018-06-10T18:35:07+08:00","relpermalink":"/talk/kubernetes-networking/","section":"talk","summary":"從系統管理層面看Kubernetes的網路架構","tags":["kubernetes","networking","docker","flannel"],"title":"Kubernetes Networking","type":"talk"},{"authors":[],"categories":null,"content":"Manage and Schedule GPU Computing Tasks on Kubernetes\n使用Kubernets管理集群GPU機器，靈活的分配調度GPU資源，並自動排程GPU運算工作。 使用者如資料科學家，只需將運算工作實施到Kubernetes上，Kubernetes便會檢視機器上可用的GPU資源，將運算工作分配到合適的機器 上，並監控工作的狀況。如資源不足Kubernetes會自動將工作加入排程，當前面的工作完成，GPU資源釋放後，Kubernetes會自動將運算 工作，配置到合適的機器上。管理者如系統工程師，只需透過Kubernetes，將機器上的GPU資源加入到Kubernetes。\n Why we need Kubernetes for GPUs computing? Pros \u0026amp; Cons How to deploy a GPU-enabled Kubernetes cluster Run GPU computing on Kubernetes cluster  ","date":1526439600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555671770,"objectID":"484a47a6b60f09b4da23188574382a0b","permalink":"https://chechiachang.github.io/talk/gpu-computing-on-kubernetes/","publishdate":"2018-05-10T18:46:23+08:00","relpermalink":"/talk/gpu-computing-on-kubernetes/","section":"talk","summary":"Manage and Schedule GPU Computing Tasks on Kubernetes\n使用Kubernets管理集群GPU機器，靈活的分配調度GPU資源，並自動排程GPU運算工作。 使用者如資料科學家，只需將運算工作實施到Kubernetes上，Kubernetes便會檢視機器上可用的GPU資源，將運算工作分配到合適的機器 上，並監控工作的狀況。如資源不足Kubernetes會自動將工作加入排程，當前面的工作完成，GPU資源釋放後，Kubernetes會自動將運算 工作，配置到合適的機器上。管理者如系統工程師，只需透過Kubernetes，將機器上的GPU資源加入到Kubernetes。\n Why we need Kubernetes for GPUs computing? Pros \u0026amp; Cons How to deploy a GPU-enabled Kubernetes cluster Run GPU computing on Kubernetes cluster  ","tags":["kubernetes","gpu-computing","cloud-computing","container","automation"],"title":"Manage and Schedule GPU Computing Tasks on Kubernetes","type":"talk"},{"authors":null,"categories":null,"content":" footer: Che-Chia David Chang, 2018, https://github.com/chechiachang/my-speeches/tree/master/go-github slidenumbers: true\nGithub API with Go-Github David Chang Back-End Developer, Kuberentes admin, DevOps\n  Outline  Let\u0026rsquo;s try Github API Use github API with go-github Work with limitation of github API An application of github user data    Let\u0026rsquo;s Try Github API Get data of a user with username\nWeb page\nhttps://github.com/chechiachang\nApi\nhttps://api.github.com/users/chechiachang\nhttps://github.com/search?q=location:Taiwan+type:user\n  /users?q=location:Taiwan\u0026amp;sort=followers\u0026amp;order=desc\n  Github Search User API Docs\n  Github API Authentication  Most API requires authentication https://developer.github.com/v3/#authentication Let\u0026rsquo;s generate api access token from web page  Github -\u0026gt; User -\u0026gt; settings -\u0026gt; Developer settings -\u0026gt; Personal access tokens\n  Go-github  Provide programmatic way to access APIs A client library for accessing github API in golang  https://github.com/google/go-github\n  https://github.com/chechiachang/scouter/blob/master/github.go#L44\n  Limitation of Github API  API paging limit: Search API only return first 1000 users API request limit Search API 30 query / min User API 50000 query / hour  /users?q=location:taiwan+created:2008-01-01..2008-02-01\u0026amp;sort=joined\u0026amp;order=asc\nWork with limitation  Paging limit: narrow down search query with time interval Control your requests rate One of the easist ways is time.Sleep()  https://github.com/chechiachang/scouter/blob/master/cmd/user_fetcher/main.go#L68\n  Application: User Data Miners  Use access token Add Query, SearchOption, Sort, Order, ListOption \u0026hellip; fetch user data with search API  https://github.com/chechiachang/scouter/blob/master/cmd/user_fetcher/main.go\nAn Application Using Github Data Scouter https://github.com/chechiachang/scouter\nLet\u0026rsquo;s Live Demo\nAn Application Using Github Data  User Data Miners to fetch user details, avatars, and contribution Face detection \u0026amp; Face recognition Api server  Video Stream -\u0026gt; Face Image -\u0026gt; Identify User -\u0026gt; User Data -\u0026gt; Show data\nVideo Stream on COSCUP 2018\n  The end ","date":1524132035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568171821,"objectID":"730af9b2ad0060e2ad4f95d20df19f9a","permalink":"https://chechiachang.github.io/slides/go-github/","publishdate":"2018-04-19T18:00:35+08:00","relpermalink":"/slides/go-github/","section":"slides","summary":"footer: Che-Chia David Chang, 2018, https://github.com/chechiachang/my-speeches/tree/master/go-github slidenumbers: true\nGithub API with Go-Github David Chang Back-End Developer, Kuberentes admin, DevOps\n  Outline  Let\u0026rsquo;s try Github API Use github API with go-github Work with limitation of github API An application of github user data    Let\u0026rsquo;s Try Github API Get data of a user with username\nWeb page\nhttps://github.com/chechiachang\nApi\nhttps://api.github.com/users/chechiachang\nhttps://github.com/search?q=location:Taiwan+type:user\n  /users?q=location:Taiwan\u0026amp;sort=followers\u0026amp;order=desc\n  Github Search User API Docs","tags":null,"title":"Github API with go-github","type":"slides"},{"authors":null,"categories":null,"content":" footer: Che-Chia David Chang, 2018, https://github.com/chechiachang/scouter slidenumbers: true\nScouter: Face recognition contribution detector David Chang Back-End Developer, Kuberentes admin, DevOps\nScouter: 3 reasons why Garbage Talks with Linkers Drangon Ball! For COSCUP!\nLet\u0026rsquo;s Live Demo! Outline Data mining Face detection \u0026amp; recognition\nFeature \u0026amp; Architecture  Face + Contributino \u0026mdash;-\u0026gt; Github api + go-github (Golang) Face Recognitiion \u0026mdash;-\u0026gt; Face detection api (Python) API server + Database \u0026mdash;-\u0026gt; Flask + PyMongodb (Python) Webcam + AR + Face detection \u0026mdash;-\u0026gt; Unity + face tracker (C#)  Face and User Data Mining  Download user data and avatar Fetch contribution statics  Github API go-github (Api library in Golang)\n4 Data Miners (Golang)  User fetcher \u0026ndash; fetch user data with search API User detail fetcher \u0026ndash; fetch user detail with user API Avatar downloader \u0026ndash; fetch user\u0026rsquo;s avatar by user data Contribution fetcher \u0026ndash; parse github contribution HTML  Notes about Github API  API paging limit Search API only return first 1000 users API request limit Search API 30 query / min User API 50000 query / hour Parallel request with Wait Group (Optional)  https://github.com/kubernetes/kubernetes/pull/66403\nFace detection \u0026amp; Face recognition The world\u0026rsquo;s simplest facial recognition api Data pre-processing -\u0026gt; Face encoding -\u0026gt; Face recognition\nFace detection \u0026amp; Face recognition  Detect face from avatars Detect identities face image Store identities and userID in a \u0026lsquo;big\u0026rsquo; matrix Detect face from a unknown image Compare unknown face with matrix find the distances between all face identities  Api server  Consume face image from App Detect face from image and recogniize user by face identity Get user data from DB and return to App  App Workflow               Camera APP API server Face recognition DB API server App   Video Stream Face Image  Identify User User Data  Show data    App and AR unity Unity : build app and AR UI\nOpenCV : image processing library\ndlib : face recognition tools, models and algorithms\nUnity App  Control camera Detect face on App-side with face tracker Cut and Send face to API server and get user data back Display user data to view  Issues  Github data source Nobody use their won face! 3000 human faces / 14000 avatars Github avatar has very low resolution Face recognition API tuning required I\u0026rsquo;m a Unity and C# newbie ;)   『不是不準，只是正確機率不夠高。』 \u0026ndash;XD\n Review || | |\u0026mdash;|\u0026mdash;|\u0026mdash;| |Golang crawler \u0026amp; html parser|Golang| |Github API| | |Python Flask|| |Face Recognition API| | |Unity|| |OpenCvForUnity|| |dlib shape predictor|| |C#||\nHow to learn anything in one Month 我想分享的是一個越級打怪，一邊快速成長的捷徑\n#『去報 COSCUP ，講一個 session』\n 『因為我自己想做，還有當初推坑我的人太厲害。』\n Reviews Scouter is relatively simple project. Do try this at home!\nThe end 投影片及講稿 https://github.com/chechiachang/my-speeches/tree/master/fr-ar-open-source-power-detector\nScouter 原始碼 https://github.com/chechiachang/scouter\n","date":1524132035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568171821,"objectID":"d8f12de29d1e74cc8ca050aed0f74d5f","permalink":"https://chechiachang.github.io/slides/coscup-2018-scouter/","publishdate":"2018-04-19T18:00:35+08:00","relpermalink":"/slides/coscup-2018-scouter/","section":"slides","summary":"footer: Che-Chia David Chang, 2018, https://github.com/chechiachang/scouter slidenumbers: true\nScouter: Face recognition contribution detector David Chang Back-End Developer, Kuberentes admin, DevOps\nScouter: 3 reasons why Garbage Talks with Linkers Drangon Ball! For COSCUP!\nLet\u0026rsquo;s Live Demo! Outline Data mining Face detection \u0026amp; recognition\nFeature \u0026amp; Architecture  Face + Contributino \u0026mdash;-\u0026gt; Github api + go-github (Golang) Face Recognitiion \u0026mdash;-\u0026gt; Face detection api (Python) API server + Database \u0026mdash;-\u0026gt; Flask + PyMongodb (Python) Webcam + AR + Face detection \u0026mdash;-\u0026gt; Unity + face tracker (C#)  Face and User Data Mining  Download user data and avatar Fetch contribution statics  Github API go-github (Api library in Golang)","tags":null,"title":"Scouter","type":"slides"},{"authors":[],"categories":null,"content":" Prerequisites If you\u0026rsquo;re interested in building your own Kubernetes. Install the following tools we use.\nvirtualbox 5.1+ to create VMs, on which we deploy our Kubernetes.\nvagrant 2.0.x+ to control virtualbox to build and manage vms.\nansible-playbook to run Kubespray playbook to deploy Kuberentes\nkubectl to control Kubernetes cluster\n# Ubuntu apt-add-repository ppa:ansible/ansible \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get install -y python3 ansible \u0026amp;\u0026amp; pip install netaddr # Mac pip install ansible port install py27-netaddr # netaddr is required by Kubespray  Let\u0026rsquo;s get started clone https://github.com/kubernetes-incubator/kubespray.git cd kubespray vagrant up  That\u0026rsquo;s it!\nThis gonna take a while. Let\u0026rsquo;s get to some details.\nVirtualbox Install virtualbox 5.1+.\nDisadvantage about vbox GUI:\n Clicking is time-consuming and engineers are lazy. Bad for automation. Lack of Scalibility Manual operation could cause mistakes.  A good practice is to Write shell script with VBoxManage, the client of virtualbox\nOr even better, use Vagrant\nVagrant vagrant 2.0.x+\nCreate you VMs with (ruby based) script.\nBring VMs up \u0026amp; down within only one command\nCheck the Vagrantfile\nAnsible playbook Ansible is a IT automation tools\nBasically, ansible playbook ssh and execute bash command on servers.\n Reduce manual efforts. Deliver and deploy faster Install K8s components to each servers and check components status on each step Come with lots of handy tools (like native array supports) Automation is everything  Kubespay Deploy k8s with ansible-playbook\nAvailable on AWS, GCE, or baremetal\nHigh Available cluster\nGenerate inventory file with inventory.py\ncp -rfp inventory/sample inventory/mycluster declare -a IPS=(10.10.1.3 10.10.1.4 10.10.1.5) CONFIG_FILE=inventory/mycluster/hosts.ini python3 contrib/inventory_builder/inventory.py ${IPS[@]}  (Optional) Change parameters\ndeploy\nansible-playbook -i inventory/myCluster/hosts.ini cluster.yml  Kubectl kubectl config use-context kubectl get po  Destroy Remember to suspend / destroy VMs\nvagrant suspend vagrant destroy  More about Kubernetes Why k8s\nUse case 1: when data scientist wants GPU Workflow dispatching and resouce management\nUse case 2: when your site grows bigger Scalibility\nFYI\n","date":1522234800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555672661,"objectID":"6c5ec292fa3a055366c6dd3109f1ee27","permalink":"https://chechiachang.github.io/talk/deploy-kubernetes-with-kubespray/","publishdate":"2018-03-27T19:08:35+08:00","relpermalink":"/talk/deploy-kubernetes-with-kubespray/","section":"talk","summary":"Build your own Kubernetes cluster with ease","tags":["kubernetes","kubespray"],"title":"Deploy Kubernetes With Kubespray","type":"talk"},{"authors":[],"categories":null,"content":" Outlines  Docker Storage Kubernetes Storage\n GlusterFS for K8s\n  Docker Storage Doc\n within container: inside writable layer of a container\n deleted with container couple with host machine require storage driver\ndocker ps -s docker inspect ubuntu dd if=/dev/zero of=1Mfile bs=1k count=1000   Docker volume\n a directory on host prepare: provision on host usage: set volume on docker run   Kubernetes Storage https://kubernetes.io/docs/concepts/storage/volumes/\n On-disk files:\n Deleted on container restart File sharing in Pod  Kubernetes Volume:\n a directory Coexist with Pod Data preserved across container restarts Pod can use many volumes of different types   (Some of) Types of volumes : * emptyDir - first created volume - prepare: none - usage: always\n gcePersistentDisk\n independent to pod prepare: gcp usage: claim by name\ngcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk   PersistentVolumeClaim\n prepare: provision by admin usage: add PVC request   Example\nPersistentVolume Doc * Persistent Volume - a piece of provisioned storage - Independent lifecycle - abstract with k8s object API - many implementations: ex. GCEPersistentDisk, NFS, GlusterFS\u0026hellip;\n Why PersistentVolume\n one APIs, many PV implementations Separates providers (admin) and consumers (users) PV subsystem API handles details of implementation Handle different need like size, access mode, performance\u0026hellip;  PersistentVolumeClaim\n PV: a resource PVC: a request for storage Pods consume Node resources and PVCs consume PV resources  PVC lifecycle\n Povisioning Binding Using Reclaiming Deleting  PV Access Modes\n ReadWriteOnce: 1 node R/W ReadOnlyMany: n node R, 1 node W ReadWriteMany: n node R/W  StorageClass\n usage:PV.storageClassName   Doc\nGlusterFS Doc\n Why glusterFS\n Network FS Distributed FS  High Availability Scalability  High performance  Architecture: Types of Volumes\n Distributed Replicated Distributed Replicated Striped: file Distributed Striped  note: glusterFS Volume vs Kubernetes PV\n  GlusterFS for k8s  Heketi\n REST storage management API Receive requests from k8s storage driver use secret to control glusterFS  Usage\n has a glusterFS apply storage class and secret to k8s Create PV Request PVC with Pods   Demo  Env  Kubernetes 1.9.2   ","date":1518228000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679957,"objectID":"3c8d261f61b72c4d3057b4f2e5eb5b6f","permalink":"https://chechiachang.github.io/talk/kubernetes-storage-and-glusterfs/","publishdate":"2018-02-10T20:49:32+08:00","relpermalink":"/talk/kubernetes-storage-and-glusterfs/","section":"talk","summary":"Kubernetes storage system: use GlusterFS as an example","tags":["kubernetes","storage","filesystem","glusterfs"],"title":"Kubernetes Storage and Glusterfs","type":"talk"},{"authors":null,"categories":null,"content":"","date":1492595116,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555679944,"objectID":"a474a93641fbd62e9acd012c967e21db","permalink":"https://chechiachang.github.io/project/scouter/","publishdate":"2017-04-19T17:45:16+08:00","relpermalink":"/project/scouter/","section":"project","summary":"A face recognizer app which retrieves your Github contribution by your face.","tags":["machine-learning","face-detection","face-recognition","unity","ios","golang","github-api","ar"],"title":"Scouter: face recognition.","type":"project"}]