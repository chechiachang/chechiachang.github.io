<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kafka-basic-usage | Che-Chia Chang</title><meta name=keywords content="鐵人賽2019,kafka,kubernetes,ithome"><meta name=description content="2020 It邦幫忙鐵人賽 系列文章

ELK Stack

Self-host ELK stack on GCP
Secure ELK Stask
監測 Google Compute Engine 上服務的各項數據
監測 Google Kubernetes Engine 的各項數據
是否選擇 ELK 作為解決方案
使用 logstash pipeline 做數據前處理
Elasticsearch 日常維護：數據清理，效能調校，永久儲存
Debug ELK stack on GCP


Kafka HA on Kubernetes(6)

Deploy kafka-ha
Kafka Introduction
kafka 基本使用
kafka operation scripts
集群內部的 HA topology
集群內部的 HA 細節
Prometheus Metrics Exporter 很重要
效能調校



由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><meta name=author content="chechiachang"><link rel=canonical href=https://chechia.net/posts/2019-09-24-kafka-basic-usage/><meta name=google-site-verification content="G-QYR8JCDGM9"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://chechia.net/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://chechia.net/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://chechia.net/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://chechia.net/favicon/favicon-32x32.png><link rel=mask-icon href=https://chechia.net/favicon/favicon-32x32.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://chechia.net/posts/2019-09-24-kafka-basic-usage/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-QYR8JCDGM9"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-QYR8JCDGM9")}</script><meta property="og:url" content="https://chechia.net/posts/2019-09-24-kafka-basic-usage/"><meta property="og:site_name" content="Che-Chia Chang"><meta property="og:title" content="Kafka-basic-usage"><meta property="og:description" content="2020 It邦幫忙鐵人賽 系列文章
ELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-09-24T21:59:49+08:00"><meta property="article:modified_time" content="2019-09-24T21:59:49+08:00"><meta property="article:tag" content="鐵人賽2019"><meta property="article:tag" content="Kafka"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Ithome"><meta property="og:image" content="https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Kafka-basic-usage"><meta name=twitter:description content="2020 It邦幫忙鐵人賽 系列文章

ELK Stack

Self-host ELK stack on GCP
Secure ELK Stask
監測 Google Compute Engine 上服務的各項數據
監測 Google Kubernetes Engine 的各項數據
是否選擇 ELK 作為解決方案
使用 logstash pipeline 做數據前處理
Elasticsearch 日常維護：數據清理，效能調校，永久儲存
Debug ELK stack on GCP


Kafka HA on Kubernetes(6)

Deploy kafka-ha
Kafka Introduction
kafka 基本使用
kafka operation scripts
集群內部的 HA topology
集群內部的 HA 細節
Prometheus Metrics Exporter 很重要
效能調校



由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。
寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。
對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://chechia.net/posts/"},{"@type":"ListItem","position":2,"name":"Kafka-basic-usage","item":"https://chechia.net/posts/2019-09-24-kafka-basic-usage/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kafka-basic-usage","name":"Kafka-basic-usage","description":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n","keywords":["鐵人賽2019","kafka","kubernetes","ithome"],"articleBody":"2020 It邦幫忙鐵人賽 系列文章\nELK Stack Self-host ELK stack on GCP Secure ELK Stask 監測 Google Compute Engine 上服務的各項數據 監測 Google Kubernetes Engine 的各項數據 是否選擇 ELK 作為解決方案 使用 logstash pipeline 做數據前處理 Elasticsearch 日常維護：數據清理，效能調校，永久儲存 Debug ELK stack on GCP Kafka HA on Kubernetes(6) Deploy kafka-ha Kafka Introduction kafka 基本使用 kafka operation scripts 集群內部的 HA topology 集群內部的 HA 細節 Prometheus Metrics Exporter 很重要 效能調校 由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。\n寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。\n對我的文章有興趣，歡迎到我的網站上 https://chechia.net 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。\n摘要 在 Kubernetes 中連線 kafka 使用 golang library 連線到 Kafka 透過 kafka script 操作 kafka kubernetes 中連線 kafka 先看一看 kafka pods\n$ kubectl get pods --selector='app=kafka' NAME READY STATUS RESTARTS AGE kafka-1-0 1/1 Running 1 26d kafka-1-1 1/1 Running 0 26d kafka-1-2 1/1 Running 0 26d $ kubectl get pods -l 'app=zookeeper' NAME READY STATUS RESTARTS AGE kafka-1-zookeeper-0 1/1 Running 0 26d kafka-1-zookeeper-1 1/1 Running 0 26d kafka-1-zookeeper-2 1/1 Running 0 26d $ kubectl get pods -l 'app=kafka-exporter' NAME READY STATUS RESTARTS AGE kafka-1-exporter-88786d84b-z954z 1/1 Running 5 26d kubectl describe pods kafka-1-0 Name: kafka-1-0 Namespace: default Priority: 0 Node: gke-chechiachang-pool-1-e4622744-wcq0/10.140.15.212 Labels: app=kafka controller-revision-hash=kafka-1-69986d7477 release=kafka-1 statefulset.kubernetes.io/pod-name=kafka-1-0 Annotations: kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka-broker Status: Running IP: 10.12.6.178 Controlled By: StatefulSet/kafka-1 Containers: kafka-broker: Image: confluentinc/cp-kafka:5.0.1 Port: 9092/TCP Host Port: 0/TCP Command: sh -exc unset KAFKA_PORT \u0026\u0026 \\ export KAFKA_BROKER_ID=${POD_NAME##*-} \u0026\u0026 \\ export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 \u0026\u0026 \\ exec /etc/confluent/docker/run Requests: cpu: 100m Liveness: exec [sh -ec /usr/bin/jps | /bin/grep -q SupportedKafka] delay=30s timeout=5s period=10s #success=1 #failure=3 Readiness: tcp-socket :kafka delay=30s timeout=5s period=10s #success=1 #failure=3 Environment: POD_IP: (v1:status.podIP) POD_NAME: kafka-1-0 (v1:metadata.name) POD_NAMESPACE: default (v1:metadata.namespace) KAFKA_HEAP_OPTS: -Xmx4G -Xms1G KAFKA_ZOOKEEPER_CONNECT: kafka-1-zookeeper:2181 KAFKA_LOG_DIRS: /opt/kafka/data/logs KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: false KAFKA_DEFAULT_REPLICATION_FACTOR: 3 KAFKA_MESSAGE_MAX_BYTES: 16000000 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_JMX_PORT: 5555 Mounts: /opt/kafka/data from datadir (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-2tm8c (ro) Conditions: Volumes: datadir: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: datadir-kafka-1-0 ReadOnly: false default-token-2tm8c: Type: Secret (a volume populated by a Secret) SecretName: default-token-2tm8c Optional: false 講幾個重點：\n這邊跑起來的是 kafka-broker，接收 producer 與 consumer 來的 request 這邊用的是 statefulsets，不是完全無狀態的 kafka broker，而把 message 記在 datadir 上，降低故障重啟時可能遺失資料的風險。 啟動時，把 kubernetes 指定的 pod name 塞進環境變數，然後作為當前 broker 的 ID 沒有設定 Pod antiAffinity，所以有可能會啟三個 kafka 結果三個跑在同一台 node 上，這樣 node 故障就全死，沒有HA Service \u0026 Endpoints 看一下 service 與 endpoints zookeeper 與 exporter 我們這邊先掠過不談，到專章講高可用性與服務監測時，再來討論。\n$ kubectl get service -l 'app=kafka' NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kafka-1 ClusterIP 10.15.242.178 9092/TCP 26d kafka-1-headless ClusterIP None 9092/TCP 26d 兩個 services\n一個是 cluster-ip service，有 single cluster IP 與 load-balance，DNS 會過 kube-proxy。 一個是 headless service，DNS 沒有過 kube-proxy，而是由 endpoint controller 直接 address record，指向把符合 service selector 的 pod。適合做 service discovery，不會依賴於 kubernetes 的實現。 詳細說明在官方文件\n簡單來說，kafka broker 會做 auto service discovery，我們可以使用 headless service。\n客戶端(consumer \u0026 producer) 連入時，則使用 cluster-ip service，做 load balancing。\n$ kubectl get endpoints -l 'app=kafka' NAME ENDPOINTS AGE kafka-1 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d kafka-1-headless 10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092 26d Golang Example 附上簡單的 Golang 客戶端，完整 Github Repository 在這邊\npackage main import ( \"context\" \"fmt\" \"strconv\" \"time\" \"github.com/segmentio/kafka-go\" // 使用的套件 ) func main() { topic := \"ticker\" // 指定 message 要使用的 topic partition := 0 // 指定 partition，由於底下連線指定連線到 partition 的 leader，所以需要指定 partition kafkaURL := \"kafka-0:9092\" // 指定 kafkaURL，也可以透過 os.GetEnv() 從環境變數裡拿到。 // producer 對指定 topic, partition 的 leader 產生連線 producerConn, _ := kafka.DialLeader(context.Background(), \"tcp\", kafkaURL, topic, partition) // 程式結束最後把 connection 關掉。不關會造成 broker 累積大量 connection，需要等待 broker 端 timeout 才會釋放。 defer producerConn.Close() //producerConn.SetWriteDeadline(time.Now().Add(10 * time.Second)) // 使用 go routine 跑一個 subprocess for loop，一直產生 message 到 kafka topic，這邊的範例是每秒推一個秒數。 go func() { for { producerConn.WriteMessages( kafka.Message{ Value: []byte(strconv.Itoa(time.Now().Second())), }, ) time.Sleep(1 * time.Second) } }() // make a new reader that consumes from topic-A, partition 0 r := kafka.NewReader(kafka.ReaderConfig{ Brokers: []string{kafkaURL}, Topic: topic, Partition: 0, MinBytes: 10e2, // 1KB MaxBytes: 10e3, // 10KB }) defer r.Close() //r.SetOffset(42) // 印出 reader 收到的 message for { m, err := r.ReadMessage(context.Background()) if err != nil { break } fmt.Printf(\"%v message at offset %d: %s = %s\\n\", time.Now(), m.Offset, string(m.Key), string(m.Value)) } } 這邊可以使用 Dockerfile 包成一個 container image，然後丟上 kubernetes\n我稍晚補一下 docker image 跟 deployment 方便大家操作好了。\n或是攋人測試，直接 kubectl run 一個 golang base image 讓它 sleep，然後在連進去\nkubectl run DEPLOYMENT_NAME --image=golang:1.13.0-alpine3.10 sleep 3600 kubectl exec -it POD_NAME sh # 裡面沒有 Git 跟 vim 裝一下 apk add git vim go get github.com/chechiachang/kafka-on-kubernetes cd src/github.com/chechiachang/kafka-on-kubernetes/ vim main.go go build . ./kafka-on-kubernetes 2019-09-24 14:20:46.872554693 +0000 UTC m=+9.154112787 message at offset 1: = 46 2019-09-24 14:20:47.872563087 +0000 UTC m=+9.154121166 message at offset 2: = 47 2019-09-24 14:20:48.872568848 +0000 UTC m=+9.154126926 message at offset 3: = 48 2019-09-24 14:20:49.872574499 +0000 UTC m=+9.154132576 message at offset 4: = 49 2019-09-24 14:20:50.872579957 +0000 UTC m=+9.154138032 message at offset 5: = 50 2019-09-24 14:20:51.872588823 +0000 UTC m=+9.154146892 message at offset 6: = 51 2019-09-24 14:20:52.872594672 +0000 UTC m=+9.154152748 message at offset 7: = 52 2019-09-24 14:20:53.872599986 +0000 UTC m=+9.154158060 message at offset 8: = 53 這樣就連上了，完成一個最簡單的使用範例。\n這個例子太過簡單，上一篇講的 consumer group, partitions, offset 什麼設定全都沒用上。實務上這些都需要好好思考，並且依據需求做調整設定。\nClean up 把測試用的 deployment 幹掉\nkubectl delete deployment DEPLOYMENT_NAME 小結 簡述 kafka 在 kubernetes 上運行的狀況，連線方法 Demo 一個小程式 ","wordCount":"780","inLanguage":"en","image":"https://chechia.net/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2019-09-24T21:59:49+08:00","dateModified":"2019-09-24T21:59:49+08:00","author":{"@type":"Person","name":"chechiachang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://chechia.net/posts/2019-09-24-kafka-basic-usage/"},"publisher":{"@type":"Organization","name":"Che-Chia Chang","logo":{"@type":"ImageObject","url":"https://chechia.net/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://chechia.net/ accesskey=h title="Home (Alt + H)"><img src=https://chechia.net/favicon/favicon-32x32.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://chechia.net/#posts title=Posts><span>Posts</span></a></li><li><a href=https://mvp.microsoft.com/zh-TW/mvp/profile/e407d0b9-5c01-eb11-a815-000d3a8ccaf5 title=MVP><span>MVP</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://chechia.net/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://chechia.net/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://chechia.net/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://chechia.net/>Home</a>&nbsp;»&nbsp;<a href=https://chechia.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Kafka-basic-usage</h1><div class=post-meta><span title='2019-09-24 21:59:49 +0800 CST'>September 24, 2019</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;780 words&nbsp;·&nbsp;chechiachang&nbsp;|&nbsp;<a href=https://github.com/chechiachang.github.io-src/content/posts/2019-09-24-kafka-basic-usage.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#service--endpoints>Service & Endpoints</a></li></ul></li></ul><ul><li><ul><li><a href=#clean-up>Clean up</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><a href=https://ithelp.ithome.com.tw/2020ironman>2020 It邦幫忙鐵人賽</a> 系列文章</p><ul><li>ELK Stack<ul><li><a href=https://chechia.net/posts/2019-09-15-self-host-elk-stack-on-gcp/>Self-host ELK stack on GCP</a></li><li><a href=https://chechia.net/posts/2019-09-15-secure-elk-stack/>Secure ELK Stask</a></li><li><a href=https://chechia.net/posts/2019-09-18-monitoring-gce-with-elk/>監測 Google Compute Engine 上服務的各項數據</a></li><li><a href=https://chechia.net/posts/2019-09-19-monitoring-gke-with-elk/>監測 Google Kubernetes Engine 的各項數據</a></li><li><a href=https://chechia.net/posts/2019-09-18-elastic-or-not-elastic/>是否選擇 ELK 作為解決方案</a></li><li><a href=https://chechia.net/posts/2019-09-21-logstash-on-gke/>使用 logstash pipeline 做數據前處理</a></li><li>Elasticsearch 日常維護：數據清理，效能調校，永久儲存</li><li>Debug ELK stack on GCP</li></ul></li><li>Kafka HA on Kubernetes(6)<ul><li><a href=https://chechia.net/posts/2019-09-22-kafka-deployment-on-kubernetes/>Deploy kafka-ha</a></li><li><a href=https://chechia.net/posts/2019-09-23-kafka-introduction/>Kafka Introduction</a></li><li><a href=https://chechia.net/posts/2019-09-24-kafka-basic-usage/>kafka 基本使用</a></li><li><a href=https://chechia.net/posts/2019-09-25-kafka-operation-scripts/>kafka operation scripts</a></li><li><a href=https://chechia.net/posts/2019-09-25-kafka-ha-topology/>集群內部的 HA topology</a></li><li><a href=https://chechia.net/posts/2019-09-26-kafka-ha-continued/>集群內部的 HA 細節</a></li><li>Prometheus Metrics Exporter 很重要</li><li>效能調校</li></ul></li></ul><p>由於我比較熟悉 GCP / GKE 的服務，這篇的操作過程都會以 GCP 平台作為範例，不過操作過程大體上是跨平台通用的。</p><p>寫文章真的是體力活，覺得我的文章還有參考價值，請左邊幫我點讚按個喜歡，右上角幫我按個追縱，底下歡迎留言討論。給我一點繼續走下去的動力。</p><p>對我的文章有興趣，歡迎到我的網站上 <a href=https://chechia.net>https://chechia.net</a> 閱讀其他技術文章，有任何謬誤也請各方大德直接聯繫我，感激不盡。</p><p><img alt="Exausted Cat Face" loading=lazy src=https://d32l83enj9u8rg.cloudfront.net/wp-content/uploads/iStock-966846550-cat-overheating-simonkr-1-940x470.jpg></p><hr><h1 id=摘要>摘要<a hidden class=anchor aria-hidden=true href=#摘要>#</a></h1><ul><li>在 Kubernetes 中連線 kafka</li><li>使用 golang library 連線到 Kafka</li><li>透過 kafka script 操作 kafka</li></ul><h1 id=kubernetes-中連線-kafka>kubernetes 中連線 kafka<a hidden class=anchor aria-hidden=true href=#kubernetes-中連線-kafka>#</a></h1><p>先看一看 kafka pods</p><pre><code>$ kubectl get pods --selector='app=kafka'

NAME        READY   STATUS    RESTARTS   AGE
kafka-1-0   1/1     Running   1          26d
kafka-1-1   1/1     Running   0          26d
kafka-1-2   1/1     Running   0          26d

$ kubectl get pods -l 'app=zookeeper'

NAME                  READY   STATUS    RESTARTS   AGE
kafka-1-zookeeper-0   1/1     Running   0          26d
kafka-1-zookeeper-1   1/1     Running   0          26d
kafka-1-zookeeper-2   1/1     Running   0          26d

$ kubectl get pods -l 'app=kafka-exporter'

NAME                               READY   STATUS    RESTARTS   AGE
kafka-1-exporter-88786d84b-z954z   1/1     Running   5          26d
</code></pre><pre><code>kubectl describe pods kafka-1-0

Name:           kafka-1-0
Namespace:      default
Priority:       0
Node:           gke-chechiachang-pool-1-e4622744-wcq0/10.140.15.212
Labels:         app=kafka
                controller-revision-hash=kafka-1-69986d7477
                release=kafka-1
                statefulset.kubernetes.io/pod-name=kafka-1-0
Annotations:    kubernetes.io/limit-ranger: LimitRanger plugin set: cpu request for container kafka-broker
Status:         Running
IP:             10.12.6.178
Controlled By:  StatefulSet/kafka-1
Containers:
  kafka-broker:
    Image:         confluentinc/cp-kafka:5.0.1
    Port:          9092/TCP
    Host Port:     0/TCP
    Command:
      sh
      -exc
      unset KAFKA_PORT &amp;&amp; \
      export KAFKA_BROKER_ID=${POD_NAME##*-} &amp;&amp; \
      export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_IP}:9092 &amp;&amp; \
      exec /etc/confluent/docker/run

    Requests:
      cpu:      100m
    Liveness:   exec [sh -ec /usr/bin/jps | /bin/grep -q SupportedKafka] delay=30s timeout=5s period=10s #success=1 #failure=3
    Readiness:  tcp-socket :kafka delay=30s timeout=5s period=10s #success=1 #failure=3
    Environment:
      POD_IP:                                   (v1:status.podIP)
      POD_NAME:                                kafka-1-0 (v1:metadata.name)
      POD_NAMESPACE:                           default (v1:metadata.namespace)
      KAFKA_HEAP_OPTS:                         -Xmx4G -Xms1G
      KAFKA_ZOOKEEPER_CONNECT:                 kafka-1-zookeeper:2181
      KAFKA_LOG_DIRS:                          /opt/kafka/data/logs
      KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE:  false
      KAFKA_DEFAULT_REPLICATION_FACTOR:        3
      KAFKA_MESSAGE_MAX_BYTES:                 16000000
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR:  1
      KAFKA_JMX_PORT:                          5555
    Mounts:
      /opt/kafka/data from datadir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-2tm8c (ro)
Conditions:
Volumes:
  datadir:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  datadir-kafka-1-0
    ReadOnly:   false
  default-token-2tm8c:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-2tm8c
    Optional:    false
</code></pre><p>講幾個重點：</p><ul><li>這邊跑起來的是 kafka-broker，接收 producer 與 consumer 來的 request</li><li>這邊用的是 statefulsets，不是完全無狀態的 kafka broker，而把 message 記在 datadir 上，降低故障重啟時可能遺失資料的風險。</li><li>啟動時，把 kubernetes 指定的 pod name 塞進環境變數，然後作為當前 broker 的 ID</li><li>沒有設定 Pod antiAffinity，所以有可能會啟三個 kafka 結果三個跑在同一台 node 上，這樣 node 故障就全死，沒有HA</li></ul><h3 id=service--endpoints>Service & Endpoints<a hidden class=anchor aria-hidden=true href=#service--endpoints>#</a></h3><p>看一下 service 與 endpoints
zookeeper 與 exporter 我們這邊先掠過不談，到專章講高可用性與服務監測時，再來討論。</p><pre><code>$ kubectl get service -l 'app=kafka'

NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
kafka-1            ClusterIP   10.15.242.178   &lt;none&gt;        9092/TCP   26d
kafka-1-headless   ClusterIP   None            &lt;none&gt;        9092/TCP   26d

</code></pre><p>兩個 services</p><ul><li>一個是 cluster-ip service，有 single cluster IP 與 load-balance，DNS 會過 kube-proxy。</li><li>一個是 headless service，DNS 沒有過 kube-proxy，而是由 endpoint controller 直接 address record，指向把符合 service selector 的 pod。適合做 service discovery，不會依賴於 kubernetes 的實現。</li></ul><p><a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services>詳細說明在官方文件</a></p><p>簡單來說，kafka broker 會做 auto service discovery，我們可以使用 headless service。</p><p>客戶端(consumer & producer) 連入時，則使用 cluster-ip service，做 load balancing。</p><pre><code>$ kubectl get endpoints -l 'app=kafka'

NAME                            ENDPOINTS                                                          AGE
kafka-1                         10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092                  26d
kafka-1-headless                10.12.1.14:9092,10.12.5.133:9092,10.12.6.178:9092                  26d
</code></pre><h1 id=golang-example>Golang Example<a hidden class=anchor aria-hidden=true href=#golang-example>#</a></h1><p>附上簡單的 Golang 客戶端，<a href=https://github.com/chechiachang/kafka-on-kubernetes>完整 Github Repository 在這邊</a></p><pre><code>package main

import (
	&quot;context&quot;
	&quot;fmt&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;github.com/segmentio/kafka-go&quot; // 使用的套件
)

func main() {
	topic := &quot;ticker&quot; // 指定 message 要使用的 topic
	partition := 0 // 指定 partition，由於底下連線指定連線到 partition 的 leader，所以需要指定 partition
	kafkaURL := &quot;kafka-0:9092&quot; // 指定 kafkaURL，也可以透過 os.GetEnv() 從環境變數裡拿到。

  // producer 對指定 topic, partition 的 leader 產生連線
	producerConn, _ := kafka.DialLeader(context.Background(), &quot;tcp&quot;, kafkaURL, topic, partition)
  // 程式結束最後把 connection 關掉。不關會造成 broker 累積大量 connection，需要等待 broker 端 timeout 才會釋放。
	defer producerConn.Close()

	//producerConn.SetWriteDeadline(time.Now().Add(10 * time.Second))
  // 使用 go routine 跑一個 subprocess for loop，一直產生 message 到 kafka topic，這邊的範例是每秒推一個秒數。
	go func() {
		for {
			producerConn.WriteMessages(
				kafka.Message{
					Value: []byte(strconv.Itoa(time.Now().Second())),
				},
			)
			time.Sleep(1 * time.Second)
		}
	}()

	// make a new reader that consumes from topic-A, partition 0
	r := kafka.NewReader(kafka.ReaderConfig{
		Brokers:   []string{kafkaURL},
		Topic:     topic,
		Partition: 0,
		MinBytes:  10e2, // 1KB
		MaxBytes:  10e3, // 10KB
	})
	defer r.Close()
	//r.SetOffset(42)

  // 印出 reader 收到的 message
	for {
		m, err := r.ReadMessage(context.Background())
		if err != nil {
			break
		}
		fmt.Printf(&quot;%v message at offset %d: %s = %s\n&quot;, time.Now(), m.Offset, string(m.Key), string(m.Value))
	}

}
</code></pre><p>這邊可以使用 Dockerfile 包成一個 container image，然後丟上 kubernetes</p><p>我稍晚補一下 docker image 跟 deployment 方便大家操作好了。</p><p>或是攋人測試，直接 kubectl run 一個 golang base image 讓它 sleep，然後在連進去</p><pre><code>kubectl run DEPLOYMENT_NAME --image=golang:1.13.0-alpine3.10 sleep 3600

kubectl exec -it POD_NAME sh
</code></pre><pre><code># 裡面沒有 Git 跟 vim 裝一下
apk add git vim

go get github.com/chechiachang/kafka-on-kubernetes

cd src/github.com/chechiachang/kafka-on-kubernetes/
vim main.go

go build .
./kafka-on-kubernetes

2019-09-24 14:20:46.872554693 +0000 UTC m=+9.154112787 message at offset 1:  = 46
2019-09-24 14:20:47.872563087 +0000 UTC m=+9.154121166 message at offset 2:  = 47
2019-09-24 14:20:48.872568848 +0000 UTC m=+9.154126926 message at offset 3:  = 48
2019-09-24 14:20:49.872574499 +0000 UTC m=+9.154132576 message at offset 4:  = 49
2019-09-24 14:20:50.872579957 +0000 UTC m=+9.154138032 message at offset 5:  = 50
2019-09-24 14:20:51.872588823 +0000 UTC m=+9.154146892 message at offset 6:  = 51
2019-09-24 14:20:52.872594672 +0000 UTC m=+9.154152748 message at offset 7:  = 52
2019-09-24 14:20:53.872599986 +0000 UTC m=+9.154158060 message at offset 8:  = 53
</code></pre><p>這樣就連上了，完成一個最簡單的使用範例。</p><p>這個例子太過簡單，上一篇講的 consumer group, partitions, offset 什麼設定全都沒用上。實務上這些都需要好好思考，並且依據需求做調整設定。</p><h3 id=clean-up>Clean up<a hidden class=anchor aria-hidden=true href=#clean-up>#</a></h3><p>把測試用的 deployment 幹掉</p><pre><code>kubectl delete deployment DEPLOYMENT_NAME
</code></pre><h1 id=小結>小結<a hidden class=anchor aria-hidden=true href=#小結>#</a></h1><ul><li>簡述 kafka 在 kubernetes 上運行的狀況，連線方法</li><li>Demo 一個小程式</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://chechia.net/tags/%E9%90%B5%E4%BA%BA%E8%B3%BD2019/>鐵人賽2019</a></li><li><a href=https://chechia.net/tags/kafka/>Kafka</a></li><li><a href=https://chechia.net/tags/kubernetes/>Kubernetes</a></li><li><a href=https://chechia.net/tags/ithome/>Ithome</a></li></ul><nav class=paginav><a class=prev href=https://chechia.net/posts/2019-09-25-kafka-operation-scripts/><span class=title>« Prev</span><br><span>Kafka Operation Scripts</span>
</a><a class=next href=https://chechia.net/posts/2019-09-23-kafka-introduction/><span class=title>Next »</span><br><span>Kafka-introduction</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on x" href="https://x.com/intent/tweet/?text=Kafka-basic-usage&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f&amp;hashtags=%e9%90%b5%e4%ba%ba%e8%b3%bd2019%2ckafka%2ckubernetes%2cithome"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f&amp;title=Kafka-basic-usage&amp;summary=Kafka-basic-usage&amp;source=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f&title=Kafka-basic-usage"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on whatsapp" href="https://api.whatsapp.com/send?text=Kafka-basic-usage%20-%20https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on telegram" href="https://telegram.me/share/url?text=Kafka-basic-usage&amp;url=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Kafka-basic-usage on ycombinator" href="https://news.ycombinator.com/submitlink?t=Kafka-basic-usage&u=https%3a%2f%2fchechia.net%2fposts%2f2019-09-24-kafka-basic-usage%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://chechia.net/>Che-Chia Chang</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>